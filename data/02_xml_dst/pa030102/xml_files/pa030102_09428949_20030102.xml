<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE patent-application-publication SYSTEM "pap-v16-2002-01-01.dtd" [
<!ENTITY US20030004714A1-20030102-D00000.TIF SYSTEM "US20030004714A1-20030102-D00000.TIF" NDATA TIF>
<!ENTITY US20030004714A1-20030102-D00001.TIF SYSTEM "US20030004714A1-20030102-D00001.TIF" NDATA TIF>
<!ENTITY US20030004714A1-20030102-D00002.TIF SYSTEM "US20030004714A1-20030102-D00002.TIF" NDATA TIF>
<!ENTITY US20030004714A1-20030102-D00003.TIF SYSTEM "US20030004714A1-20030102-D00003.TIF" NDATA TIF>
]>
<patent-application-publication>
<subdoc-bibliographic-information>
<document-id>
<doc-number>20030004714</doc-number>
<kind-code>A1</kind-code>
<document-date>20030102</document-date>
</document-id>
<publication-filing-type>new</publication-filing-type>
<domestic-filing-data>
<application-number>
<doc-number>09428949</doc-number>
</application-number>
<application-number-series-code>09</application-number-series-code>
<filing-date>19991028</filing-date>
<continued-prosecution-application>This is a publication of a continued prosecution application (CPA) filed under 37 CFR 1.53(d).</continued-prosecution-application>
</domestic-filing-data>
<technical-information>
<classification-ipc>
<classification-ipc-primary>
<ipc>G10L015/00</ipc>
</classification-ipc-primary>
<classification-ipc-edition>07</classification-ipc-edition>
</classification-ipc>
<classification-us>
<classification-us-primary>
<uspc>
<class>704</class>
<subclass>231000</subclass>
</uspc>
</classification-us-primary>
</classification-us>
<title-of-invention>SYSTEM AND METHOD FOR RESOLVING DECODING AMBIGUITY VIA DIALOG</title-of-invention>
</technical-information>
<inventors>
<first-named-inventor>
<name>
<given-name>DIMITRI</given-name>
<family-name>KANEVSKY</family-name>
</name>
<residence>
<residence-us>
<city>OSSINING</city>
<state>NY</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</first-named-inventor>
<inventor>
<name>
<given-name>TAUFIQUE</given-name>
<family-name>SAMDANI</family-name>
</name>
<residence>
<residence-us>
<city>TARRYTOWN</city>
<state>NY</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
<inventor>
<name>
<given-name>WLODEK</given-name>
<family-name>ZADROZNY</family-name>
</name>
<residence>
<residence-us>
<city>TARRYTOWN</city>
<state>NY</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
<inventor>
<name>
<given-name>ALEXANDER</given-name>
<family-name>ZLATSIN</family-name>
</name>
<residence>
<residence-us>
<city>YORKTOWN HEIGHTS</city>
<state>NY</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
</inventors>
<correspondence-address>
<name-1>LAW OFFICE OF CHARLES W. PETERSON, JR.</name-1>
<name-2></name-2>
<address>
<address-1>P.O. BOX 710627</address-1>
<city>OAK HILL</city>
<state>VA</state>
<postalcode>20171</postalcode>
<country>
<country-code>US</country-code>
</country>
</address>
</correspondence-address>
</subdoc-bibliographic-information>
<subdoc-abstract>
<paragraph id="A-0001" lvl="0">A method of language recognition wherein decoding ambiguities are identified and at least partially resolved intermediate to the language decoding procedures to reduce the subsequent number of final decoding alternatives. The user is questioned about identified decoding ambiguities as they are being decoded. There are two language decoding levels: fast match and detailed match. During the fast match decoding level a large potential candidate list is generated, very quickly. Then, during the more comprehensive (and slower) detailed match decoding level, the fast match candidate list is applied to the ambiguity to reduce the potential selections for final recognition. During the detailed match decoding level a unique candidate is selected for decoding. Decoding may be interactive and, as each ambiguity is encountered, recognition suspended to present questions to the user that will discriminate between potential response classes. Thus, recognition performance and accuracy is improved by interrupting recognition, intermediate to the decoding process, and allowing the user to select appropriate response classes to narrow the number of final decoding alternatives. </paragraph>
</subdoc-abstract>
<subdoc-description>
<summary-of-invention>
<section>
<heading lvl="1">BACKGROUND OF THE INVENTION </heading>
<paragraph id="P-0001" lvl="0"><number>&lsqb;0001&rsqb;</number> 1. Field of the Invention </paragraph>
<paragraph id="P-0002" lvl="0"><number>&lsqb;0002&rsqb;</number> The present invention is related to language recognition methods for understanding language input from a user, and more particularly, to a method and apparatus for improving language recognition performance and accuracy by resolving ambiguities in the language input using an intermediate to the recognition process. </paragraph>
<paragraph id="P-0003" lvl="0"><number>&lsqb;0003&rsqb;</number> 2. Background Description </paragraph>
<paragraph id="P-0004" lvl="0"><number>&lsqb;0004&rsqb;</number> Existing language recognition decoders such as automatic speech recognition (ASR) systems, automatic handwriting recognition (AHR) systems and machine translation (MT) systems must deal with large numbers of decoding alternatives in their particular decoding process. Examples of these decoding alternatives are candidate word lists, N-best lists, e.g., for ASR. Because the number of decoding alternatives may be so large, decoding errors occur very frequently. </paragraph>
<paragraph id="P-0005" lvl="0"><number>&lsqb;0005&rsqb;</number> There are several approaches to minimizing such errors. Typically, in these approaches, users are allowed to correct errors only after the decoder has produced an output. Unfortunately, these approaches still result in too many decoding errors and a cumbersome process, i.e., requiring users to correct all of the errors which, ideally, would be caught by the system. </paragraph>
<paragraph id="P-0006" lvl="0"><number>&lsqb;0006&rsqb;</number> Other approaches include systems such as using ASR in a voice response telephone system to make appointments or place orders. In such a voice response system, after the user speaks the system repeats its understanding and provides the user with an opportunity to verify whether the system has recognized the utterance correctly. This may require several iterations to reach the correct result. </paragraph>
<paragraph id="P-0007" lvl="0"><number>&lsqb;0007&rsqb;</number> So, a recognition system can misrecognize the phrase &ldquo;meet at seven&rdquo; having a temporal sense as being &ldquo;meet at Heaven&rdquo; which may have a positional sense, e.g., as the name of a restaurant. Unfortunately, using these prior art system requires the user to do more than just indicate that the recognition is incorrect, otherwise the recognition system still has not been informed of the correct response. In order to improve its recognition capability, the system must be informed of the correct response. Further, repeating the recognition decoding or querying other alternative responses increases user interaction time and inconvenience. </paragraph>
<paragraph id="P-0008" lvl="0"><number>&lsqb;0008&rsqb;</number> Thus, there is a need for language response systems with improved recognition accuracy. </paragraph>
</section>
<section>
<heading lvl="1">SUMMARY OF THE INVENTION </heading>
<paragraph id="P-0009" lvl="0"><number>&lsqb;0009&rsqb;</number> It is a purpose of the invention to provide a method and system for improving language decoding performance and accuracy; </paragraph>
<paragraph id="P-0010" lvl="0"><number>&lsqb;0010&rsqb;</number> It is another purpose of the invention to resolve language decoding ambiguities during voice recognition, thereby improving language decoding performance and accuracy. </paragraph>
<paragraph id="P-0011" lvl="0"><number>&lsqb;0011&rsqb;</number> The present invention is a method of language recognition wherein decoding ambiguities are identified and at least partially resolved intermediate to the language decoding procedures. The user is questioned about these identified decoding ambiguities as they are being decoded. These identified decoding ambiguities are resolved early to reduce the subsequent number of final decoding alternatives. This early ambiguity resolution significantly reduces both decoding time and the number of questions that the user may have to answer for correct system recognition. In the preferred embodiment speech recognition system there are two language decoding levels: fast match and detailed match. During the fast match decoding level a comparatively large potential candidate list is generated, very quickly. Then, during the more comprehensive (and slower) detailed match decoding level, the fast match candidate list is applied to the ambiguity to reduce the potential selections for final recognition. During the detailed match decoding level a unique candidate is selected for decoding. In one embodiment decoding is interactive and, as each ambiguity is encountered, recognition is suspended to present questions to the user that will discriminate between potential response classes. Thus, recognition performance and accuracy is improved by interrupting recognition, intermediate to the decoding process, and allowing the user to select appropriate response classes to narrow the number of final decoding alternatives.</paragraph>
</section>
</summary-of-invention>
<brief-description-of-drawings>
<section>
<heading lvl="1">BRIEF DESCRIPTION OF THE DRAWINGS </heading>
<paragraph id="P-0012" lvl="0"><number>&lsqb;0012&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 1</cross-reference> is a structural diagram for the preferred embodiment language recognition system with intermediate decoding ambiguity recognition; </paragraph>
<paragraph id="P-0013" lvl="0"><number>&lsqb;0013&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 2</cross-reference> is a flow diagram of the preferred embodiment language recognition method of resolving decoding ambiguities that occur in an inner module of an interactive conversational decoding system; </paragraph>
<paragraph id="P-0014" lvl="0"><number>&lsqb;0014&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 3</cross-reference> is a flow diagram showing how questions are generated.</paragraph>
</section>
</brief-description-of-drawings>
<detailed-description>
<section>
<heading lvl="1">DETAILED DESCRIPTION OF A PREFERRED EMBODIMENT OF THE INVENTION </heading>
<paragraph id="P-0015" lvl="0"><number>&lsqb;0015&rsqb;</number> Turning now to the drawings and more particularly <cross-reference target="DRAWINGS">FIG. 1</cross-reference> shows a structural diagram for the preferred embodiment language recognition system <highlight><bold>100</bold></highlight> with intermediate decoding ambiguity recognition. The decoding system may be, for example, an automatic speech recognition (ASR) system, an automatic handwriting recognition (AHR) system or, a machine translation (MT) system. </paragraph>
<paragraph id="P-0016" lvl="0"><number>&lsqb;0016&rsqb;</number> In the preferred embodiment speech recognition system <highlight><bold>100</bold></highlight> there are two decoding levels: fast match and detailed match. During the fast match decoding level a large approximate candidate list is generated very quickly. Then, during the more comprehensive (and slower) detailed match decoding level the fast match candidate list is applied for final recognition. During the detailed match decoding level a unique candidate is decoded and selected. </paragraph>
<paragraph id="P-0017" lvl="0"><number>&lsqb;0017&rsqb;</number> In this example a user <highlight><bold>102</bold></highlight> is shown speaking into a microphone <highlight><bold>104</bold></highlight> to provide language input to an automatic speech recognition system <highlight><bold>106</bold></highlight>. The speech recognition system includes a fast match speech processing module <highlight><bold>108</bold></highlight> for the fast match, which identifies ambiguous words or phrases, and a detailed match decode processing module <highlight><bold>110</bold></highlight> for the detailed match. The speech is passed directly to the detailed match processing module <highlight><bold>110</bold></highlight> until the fast match speech processing module <highlight><bold>108</bold></highlight> encounters an ambiguity. When an ambiguity is encountered, the ambiguity is classified and the processed speech is passed to the detailed match decoding module <highlight><bold>110</bold></highlight> for decoding. Then, the result is output <highlight><bold>112</bold></highlight>, e.g., on a video display. </paragraph>
<paragraph id="P-0018" lvl="0"><number>&lsqb;0018&rsqb;</number> However, at some point during input, the user <highlight><bold>102</bold></highlight> may include an ambiguous statement, which the fast match module <highlight><bold>108</bold></highlight> identifies, initially, and passes the partially decoded ambiguous input to resolve the ambiguity in module <highlight><bold>114</bold></highlight>. Ambiguities may be identified initially, for example, by similar sounding words for speech, or by similarly spelled words in handwritten text. Then, in module <highlight><bold>116</bold></highlight> potential characteristic choice classes are identified. Choice classes may include for example, time/space or noun/verb. Then, questions are generated in <highlight><bold>118</bold></highlight>. Answers to the questions <highlight><bold>118</bold></highlight> will serve to resolve the ambiguity and to classify the type of statement/phrase being considered. These questions <highlight><bold>118</bold></highlight> are passed back to the user <highlight><bold>102</bold></highlight>, e.g. verbally on speaker <highlight><bold>119</bold></highlight>, and the user&apos;s response serves to classify the ambiguity. The resulting partially decoded speech with the classified ambiguity is passed on to detailed match decode <highlight><bold>110</bold></highlight> for final decoding. </paragraph>
<paragraph id="P-0019" lvl="0"><number>&lsqb;0019&rsqb;</number> As each ambiguity is identified, the system passes appropriate questions to the user <highlight><bold>102</bold></highlight> to classify the ambiguities prior to passing the user input on to the detailed match module <highlight><bold>110</bold></highlight> for further decoding. Thus according to the present invention, these classified decoding ambiguities are resolved early in the recognition process to reduce the ultimate number of decoding alternatives presented to the user. This early ambiguity resolution significantly reduces overall decoding time and, further, reduces the overall number of questions passed to the user to ultimately guide system recognition to produce the desired result. </paragraph>
<paragraph id="P-0020" lvl="0"><number>&lsqb;0020&rsqb;</number> For example, simple key words such as prepositions may be used to initially identify ambiguities. Thus, if the word &ldquo;at&rdquo; is encountered by the dialog conversational system, there may be several potential decoding alternatives. The key word &ldquo;at&rdquo; may be appropriate when referring to time and space functions. So, for each occurrence of &ldquo;at&rdquo; the system may present the user with a question like: &ldquo;Are you talking about a place where to meet&quest;&ldquo;If the user answers &ldquo;YES,&rdquo; then only decoding alternatives related to &ldquo;space&rdquo; are considered in the final detailed match decoding module <highlight><bold>110</bold></highlight>. Otherwise, alternatives related to &ldquo;time&rdquo; are considered. In general any response that could be answered with more than one of who, what, when, where are potentially ambiguous. </paragraph>
<paragraph id="P-0021" lvl="0"><number>&lsqb;0021&rsqb;</number> In one embodiment of the invention decoding is interactive and, as each ambiguity is encountered, recognition is suspended to allow presenting questions to the user discriminate between potential selection classes. So, in the above example, discerning between &ldquo;meet at heaven&rdquo; and &ldquo;meet at seven,&rdquo; by posing an intermediate question to classify the phrase (e.g., &ldquo;A time&quest;&rdquo;) eliminates one decoding choice and results in decoding the correct phrase. Thus, recognition performance and accuracy is improved by selecting appropriate classes that narrow the field of potential final decoding alternatives, intermediate to the decoding process. </paragraph>
<paragraph id="P-0022" lvl="0"><number>&lsqb;0022&rsqb;</number> In a more elaborate example, several candidates may be available for space, e.g. meet at heaven, meet in hell, and for time e.g., meet at seven, meet at six, all produced during the fast match decoding of speech processing module <highlight><bold>108</bold></highlight>. The same classification question (A time&quest;) narrows the selection to two candidates: either the spatial candidates, meet at heaven and meet in hell; or, the temporal candidates, meet at seven and meet at six. After answering the questions to narrow the set of candidates, decoding continues normally in the detailed match decoding module <highlight><bold>110</bold></highlight> until a single candidate is selected from this narrowed set. </paragraph>
<paragraph id="P-0023" lvl="0"><number>&lsqb;0023&rsqb;</number> In a typical prior art language decoder, only the detailed match decode of decoding process <highlight><bold>110</bold></highlight> was applied to find a best choice. However, for the preferred embodiment method, decoding is interrupted as soon as an ambiguity is identified by the fast match speech processing module <highlight><bold>108</bold></highlight>. Then, the appropriate question is identified and presented to the user <highlight><bold>102</bold></highlight> to narrow the list of fast match choices, before continuing with the detailed match of the decoding process module <highlight><bold>110</bold></highlight> using the narrowed list. Decoding ambiguities may include things such as, whether a particular phrase describes a time or space relationship; whether the phrase describes a noun, verb or adjective; and/or, what value the phrase describes, e.g., time, length, weight, age, period, price, etc. </paragraph>
<paragraph id="P-0024" lvl="0"><number>&lsqb;0024&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 2</cross-reference> is a flow diagram of the preferred embodiment language recognition method <highlight><bold>120</bold></highlight> of resolving decoding ambiguities by an inner module (fast match decode <highlight><bold>108</bold></highlight>) of an interactive conversational decoding system. The inner module may include an acoustic processing module, a language model module, a semantic module, a syntactic module, a parser and/or a signal processing module. After identifying a decoding ambiguity a set of intermediate decoding alternatives are identified by the inner decoding module in step <highlight><bold>122</bold></highlight>. Intermediate decoding alternatives may include those that are widely used by existing decoders. Such intermediate decoding alternatives may include, for example only, a candidate word list, a candidate phrase list, a candidate sentence list, a vocabulary, a fast match list, a detailed match list, an N-best list, a candidate acoustic feature set, a dialects and/or language set, a handwriting feature set, a phoneme set, a letter set, a channel and/or environment condition (noise, music etc.) set. </paragraph>
<paragraph id="P-0025" lvl="0"><number>&lsqb;0025&rsqb;</number> Next, in step <highlight><bold>124</bold></highlight> final decoding alternatives classes are identified for the whole decoding system. The set of intermediate decoding alternatives determine the potential final decoding class alternatives. In one preferred embodiment the final decoding alternative classes belong to the same set as the intermediate decoding alternatives. In this embodiment only the choices/alternatives are reduced in the final set. So, the decoder using the &ldquo;list of candidate phrases&rdquo; as a criteria for selecting alternatives may select &ldquo;meet at heaven&rdquo; and &ldquo;meet at seven&rdquo; as possible alternatives at the intermediate stage. Then, the final decoding alternative will be narrowed to &ldquo;meet at seven.&rdquo; </paragraph>
<paragraph id="P-0026" lvl="0"><number>&lsqb;0026&rsqb;</number> Examples of final decoding alternatives that would not belong to the set of final decoding alternatives include acoustic data mapped to cepstra by the decoder. The decoder may have several different processes for mapping acoustic data to cepstra that depend on gender, age and nationality. Thus, the decoder may identify speaker characteristics, automatically, before mapping acoustic data into cepstra. In this example, if the decoder is unable to select between choices that are characteristic to the speaker, then the decoder may ask the user a gender related question This type of intermediate question is not directly related to the final decoding alternatives. </paragraph>
<paragraph id="P-0027" lvl="0"><number>&lsqb;0027&rsqb;</number> The potential final decoding alternative classes may be selected to include features such as, for example, time, space, duration, and semantic class. Further, the potential final decoding alternative classes may include more general classes such as a grammatical class directed to grammatical structure, sentence parsing, word type; a personal characteristics class directed to sex, age, profession or a personal profile; a class of topics including medical, legal, personal and business topics; a class of goals such as what to buy, where to go, where or how to rest where to vacation; a business model class such as ordering tickets, ordering goods, requesting information, making a purchase; and/or, a customer profile class including various customers buying habits and needs. The final decoding alternatives will belong to one or more classes that are created based on these features. In the above example, &ldquo;meet at-heaven&rdquo; would belong to a space class, while &ldquo;meet at seven&rdquo; would belong to a time class. By identifying the appropriate classes for a particular domain, and matching the particular phrase/choice/alternative a particular class prior to decoding, the system will ask the appropriate question to eliminate the ambiguity. </paragraph>
<paragraph id="P-0028" lvl="0"><number>&lsqb;0028&rsqb;</number> So, in step <highlight><bold>126</bold></highlight> appropriate questions are selected, preferably, each to halve the number of alternative class choices. The selected questions have been previously matched to each previously identified class and characterize the identified final decoding alternative classes. Next, in step <highlight><bold>128</bold></highlight>, the questions are presented to the user. In the above example, an appropriate question may be &ldquo;Are you talking about a place to meet&quest;&rdquo; If the answer to this question is &ldquo;YES&rdquo; then, the system selects the alternative &ldquo;meet at heaven;&rdquo; if the answer is &ldquo;NO&rdquo; then &ldquo;meet at seven&rdquo; is selected. Appropriate questions must just discriminate between classes and need not discriminate between particular words. </paragraph>
<paragraph id="P-0029" lvl="0"><number>&lsqb;0029&rsqb;</number> Having queried the user, the user&apos;s responses, which may be verbal, typed or mouse activity, are processed (How&quest;) in step <highlight><bold>130</bold></highlight> converting the response to something usable by the decoding system. Based on the user&apos;s response, in step <highlight><bold>132</bold></highlight> the set of intermediate decoding alternatives is narrowed, eliminating choices that are incongruous with the user&apos;s response. If it is determined in step <highlight><bold>134</bold></highlight> that all ambiguities have been resolved, then the full decoding cycle is resumed to produce the final decoding output using the narrowed set of intermediate decoding alternatives. </paragraph>
<paragraph id="P-0030" lvl="0"><number>&lsqb;0030&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 3</cross-reference> is a flow diagram showing question generation in step <highlight><bold>126</bold></highlight>. The set of final decoding class alternatives <highlight><bold>140</bold></highlight> are generated in step <highlight><bold>124</bold></highlight>. In step <highlight><bold>142</bold></highlight>, user related classes are selected from an established class list <highlight><bold>144</bold></highlight>. Questions associated with any particular class may ask whether user phrases are related to or, belong to a specific class or, are about the relationship between classes that are associated with the decoding alternatives. In step <highlight><bold>146</bold></highlight> the relationship between the class and the phrase is verified. </paragraph>
<paragraph id="P-0031" lvl="0"><number>&lsqb;0031&rsqb;</number> Once appropriate questions <highlight><bold>148</bold></highlight> have been identified and verified, the list of questions is optimized in step <highlight><bold>150</bold></highlight> such that the questions selected minimize the number of questions <highlight><bold>152</bold></highlight> eventually presented to the user and with sufficient selectivity. Further, question optimization <highlight><bold>150</bold></highlight> may be based on probability metrics <highlight><bold>154</bold></highlight> of final decoding alternative classes that provide a measure of probability of eliminating each question class in the course of question queries. Training data, stored as transcribed dialogs <highlight><bold>156</bold></highlight> between users and service providers may be labeled with classes to provide a basis for an estimate <highlight><bold>158</bold></highlight> that serves as the probability metric <highlight><bold>154</bold></highlight>. Training data stored as a textual corpus <highlight><bold>160</bold></highlight> previously labeled with classes may also provide a basis for the estimate <highlight><bold>158</bold></highlight>. Further, the estimate of probability metrics <highlight><bold>158</bold></highlight> may be denied from counting words or phrases <highlight><bold>162</bold></highlight> belonging to classes and sequences of classes and, estimating probabilities of the class sequences for given sequences of words from these counts. Also, probability model parameters estimates <highlight><bold>164</bold></highlight> may be used, such as Gaussian models, normal log models, Laplacian models, etc. </paragraph>
<paragraph id="P-0032" lvl="0"><number>&lsqb;0032&rsqb;</number> Further, classes may be selected based on a particular type of activity to which the language recognition is related such as a business activity, e.g., ordering tickets, ordering goods, requesting information, or a purchase. To further facilitate classification for a business using the preferred language recognition system, a customer profile may be maintained with customer related information such as the customer&apos;s buying habits, buying needs, and buying history. The customer&apos;s profession, e.g., doctor, lawyer, may also be considered. If known, the user&apos;s ultimate goals may be considered. Thus, it may be advantageous to know whether the user is determining, for example, what to buy, where to go, where to rest, how to rest, a vacation destination. </paragraph>
<paragraph id="P-0033" lvl="0"><number>&lsqb;0033&rsqb;</number> Thus, through intermediate ambiguity classification the preferred embodiment language recognition system improves the likelihood of expeditiously achieving the correct result without burdening the user with an unending string of questions. </paragraph>
<paragraph id="P-0034" lvl="0"><number>&lsqb;0034&rsqb;</number> While the invention has been described in terms of preferred embodiments, those skilled in the art will recognize that the invention can be practiced with modification within the spirit and scope of the appended claims. </paragraph>
</section>
</detailed-description>
</subdoc-description>
<subdoc-claims>
<heading lvl="1">We claim: </heading>
<claim id="CLM-00001">
<claim-text><highlight><bold>1</bold></highlight>. A language recognition system for recognizing language input from a user, said language input including ambiguities, said system being capable of recognizing and resolving certain ones of said ambiguities, said system comprising: 
<claim-text>a decoder system receiving user input and transcribing said user input into phrases; </claim-text>
<claim-text>an intermediate decoding module identifying a set of decoding alternatives corresponding to an identified ambiguity; </claim-text>
<claim-text>a classifier attaching classes to said set of decoding alternatives; </claim-text>
<claim-text>a questioner module constructing an optimal questions responsive to a given set of classes from said classifier, said optimal questions being constructed to reduce the number of classes; and </claim-text>
<claim-text>an assistant interactive system querying the user with questions related to said identified ambiguity and receiving corresponding user responses. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00002">
<claim-text><highlight><bold>2</bold></highlight>. A language recognition system as in <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, said decoder system comprising: 
<claim-text>a fast match decode module receiving said user input and generating a potential candidate list from said user input; and </claim-text>
<claim-text>a detailed match decode module receiving said potential candidate list from said fast match decode module when ambiguities are not found in said user input and receiving a classified input when said candidate list includes one or more ambiguities. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00003">
<claim-text><highlight><bold>3</bold></highlight>. A Speech recognition system as in <dependent-claim-reference depends_on="CLM-00002">claim 2</dependent-claim-reference>, wherein the decoder system is an automatic speech recognition system, said speech recognition system further comprising a microphone, user input being provided from said microphone in response to a user speaking into said microphone. </claim-text>
</claim>
<claim id="CLM-00004">
<claim-text><highlight><bold>4</bold></highlight>. A handwriting recognition system as in <dependent-claim-reference depends_on="CLM-00002">claim 2</dependent-claim-reference>, wherein the decoder system is an automatic speech recognition system. </claim-text>
</claim>
<claim id="CLM-00005">
<claim-text><highlight><bold>5</bold></highlight>. A language recognition method, said method comprising the steps of: 
<claim-text>a) receiving language input from a user and checking said language input for an ambiguity; </claim-text>
<claim-text>b) converting said language input to an output when an ambiguity is not found in said language input and returning to step (a); </claim-text>
<claim-text>c) identifying an intermediate decoding alternative set when an ambiguity is found; </claim-text>
<claim-text>d) identifying a final decoding class alternative responsive to said intermediate decoding alternative; </claim-text>
<claim-text>e) identifying a final decoding class responsive to said identified final decoding class alternative; </claim-text>
<claim-text>f) presenting questions to said user responsive to said identified final decoding class; and </claim-text>
<claim-text>g) resolving said ambiguity responsive to user responses to said questions and returning to step (a). </claim-text>
</claim-text>
</claim>
<claim id="CLM-00006">
<claim-text><highlight><bold>6</bold></highlight>. A language recognition method as in <dependent-claim-reference depends_on="CLM-00005">claim 5</dependent-claim-reference>, wherein the set of intermediate decoding alternatives identified in step (c) include a candidate word list, a candidate phrase list, a candidate sentence list, a vocabulary, a fast match list, a detailed match list, an N-best list, a candidate acoustic feature set, a dialects and/or language set, a handwriting feature set, a phoneme set, a letter set a channel, and an environment condition set. </claim-text>
</claim>
<claim id="CLM-00007">
<claim-text><highlight><bold>7</bold></highlight>. A language recognition method as in <dependent-claim-reference depends_on="CLM-00006">claim 6</dependent-claim-reference>, wherein the set of final decoding alternatives identified in step (d) include a candidate word list, a candidate phrase list, a candidate sentence list, a vocabulary, a fast match list, a detailed match list, an N-best list, a candidate acoustic feature set, a dialects and/or language set, a handwriting feature set, a phoneme set, a letter set, a channel and an environment condition set. </claim-text>
</claim>
<claim id="CLM-00008">
<claim-text><highlight><bold>8</bold></highlight>. A language recognition method as in <dependent-claim-reference depends_on="CLM-00005">claim 5</dependent-claim-reference>, wherein the questions presented in step (f) are selected to classify the identified ambiguity as being related to one of at least a pair classes selected from a plurality of feature classes. </claim-text>
</claim>
<claim id="CLM-00009">
<claim-text><highlight><bold>9</bold></highlight>. A language recognition method as in <dependent-claim-reference depends_on="CLM-00008">claim 8</dependent-claim-reference>, wherein the plurality of feature classes comprises: 
<claim-text>a temporal class; </claim-text>
<claim-text>a spatial class; </claim-text>
<claim-text>a durational class; </claim-text>
<claim-text>a semantic class; </claim-text>
<claim-text>a grammatical class; </claim-text>
<claim-text>a personal characteristic class; </claim-text>
<claim-text>a topical class; </claim-text>
<claim-text>a goal related class; </claim-text>
<claim-text>a business model class; and, </claim-text>
<claim-text>a customer class. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00010">
<claim-text><highlight><bold>10</bold></highlight>. A language recognition method as in <dependent-claim-reference depends_on="CLM-00009">claim 9</dependent-claim-reference>, wherein the grammatical class comprises sentence structure, sentence parsing and word classification. </claim-text>
</claim>
<claim id="CLM-00011">
<claim-text><highlight><bold>11</bold></highlight>. A language recognition method as in <dependent-claim-reference depends_on="CLM-00009">claim 9</dependent-claim-reference>, wherein the personal characteristic class comprises a plurality of user characteristics including a personal profile, user sex, user age and user profession. </claim-text>
</claim>
<claim id="CLM-00012">
<claim-text><highlight><bold>12</bold></highlight>. A language recognition method as in <dependent-claim-reference depends_on="CLM-00009">claim 9</dependent-claim-reference>, wherein the topical class comprises a medical information class, a legal information class, a business information class and a personal information class. </claim-text>
</claim>
<claim id="CLM-00013">
<claim-text><highlight><bold>13</bold></highlight>. A language recognition method as in <dependent-claim-reference depends_on="CLM-00009">claim 9</dependent-claim-reference>, wherein the business model class includes business related activities including placing ticket orders, placing orders for goods, requesting information and making purchases. </claim-text>
</claim>
<claim id="CLM-00014">
<claim-text><highlight><bold>14</bold></highlight>. A language recognition method as in <dependent-claim-reference depends_on="CLM-00008">claim 8</dependent-claim-reference>, wherein the step (f) of presenting questions further comprises selecting questions from a plurality of questions, selected said questions being an optimum set of questions, said optimum set of questions selected to minimize the number of questions presented to a user and to minimize the number of potential alternatives remaining after receiving a user response. </claim-text>
</claim>
<claim id="CLM-00015">
<claim-text><highlight><bold>15</bold></highlight>. A language recognition method as in <dependent-claim-reference depends_on="CLM-00011">claim 14</dependent-claim-reference>, wherein the set of optimum questions are selected from a plurality of questions by determining a probability metric, the probability metric providing a measure of the probability that a response will eliminate one or more of said final decoding classes. </claim-text>
</claim>
<claim id="CLM-00016">
<claim-text><highlight><bold>16</bold></highlight>. A language recognition method as in <dependent-claim-reference depends_on="CLM-00011">claim 15</dependent-claim-reference>, wherein the probability metric is derived from training data. </claim-text>
</claim>
<claim id="CLM-00017">
<claim-text><highlight><bold>17</bold></highlight>. A language recognition method as in <dependent-claim-reference depends_on="CLM-00011">claim 16</dependent-claim-reference>, wherein the training data is a textual corpus labeled with classes. </claim-text>
</claim>
<claim id="CLM-00018">
<claim-text><highlight><bold>18</bold></highlight>. A language recognition method as in <dependent-claim-reference depends_on="CLM-00011">claim 16</dependent-claim-reference>, wherein the training data is a plurality of transcribed user-server dialogs labeled with classes. </claim-text>
</claim>
<claim id="CLM-00019">
<claim-text><highlight><bold>19</bold></highlight>. A computer program product for language recognition, said computer program product comprising a computer usable medium having computer readable program code thereon, said computer readable program code comprising: 
<claim-text>computer readable program code means for checking language input from a user for an ambiguity; </claim-text>
<claim-text>computer readable program code means for converting said language input to an output when an ambiguity is not found in said language input; </claim-text>
<claim-text>computer readable program code means for identifying an intermediate decoding alternative set when an ambiguity is found; </claim-text>
<claim-text>computer readable program code means for identifying a final decoding class alternative from said intermediate decoding alternative; </claim-text>
<claim-text>computer readable program code means for identifying a final decoding class from said identified final decoding class alternative; </claim-text>
<claim-text>computer readable program code means for presenting questions about said identified final decoding class to said user; and </claim-text>
<claim-text>computer readable program code means for using user responses to said questions to resolve ambiguities. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00020">
<claim-text><highlight><bold>20</bold></highlight>. A computer program product as in <dependent-claim-reference depends_on="CLM-00011">claim 19</dependent-claim-reference>, wherein the computer readable program code means for presenting questions to said user comprises: 
<claim-text>computer readable program code means for classifying the identified ambiguity as being related to one of at least a pair classes of a plurality of feature classes. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00021">
<claim-text><highlight><bold>21</bold></highlight>. A computer program product as in <dependent-claim-reference depends_on="CLM-00022">claim 20</dependent-claim-reference>, wherein the computer readable program code means for presenting questions to said user further comprises: 
<claim-text>computer readable program code means for selecting an optimum set of questions from a plurality of questions, said optimum set of questions minimizing the number of questions presented to said user and minimizing the number of potential alternatives remaining after receiving response from said user. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00022">
<claim-text><highlight><bold>22</bold></highlight>. A computer program product as in <dependent-claim-reference depends_on="CLM-00022">claim 21</dependent-claim-reference>, wherein the computer readable program code means for presenting questions to said user further comprises: 
<claim-text>computer readable program code means for determining a probability metric from a plurality of questions, the probability metric being a measure of the probability that a response will eliminate one or more of said final decoding classes. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00023">
<claim-text><highlight><bold>23</bold></highlight>. A computer program product as in <dependent-claim-reference depends_on="CLM-00022">claim 22</dependent-claim-reference>, wherein the computer readable program code means for presenting questions to said user further comprises: 
<claim-text>computer readable program code means for deriving the probability metric from training data.</claim-text>
</claim-text>
</claim>
</subdoc-claims>
<subdoc-drawings id="DRAWINGS">
<heading lvl="0" align="CENTER">Drawings</heading>
<representative-figure>1</representative-figure>
<figure id="figure-D00000">
<image id="EMI-D00000" file="US20030004714A1-20030102-D00000.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00001">
<image id="EMI-D00001" file="US20030004714A1-20030102-D00001.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00002">
<image id="EMI-D00002" file="US20030004714A1-20030102-D00002.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00003">
<image id="EMI-D00003" file="US20030004714A1-20030102-D00003.TIF" imf="TIFF" ti="DR"/>
</figure>
</subdoc-drawings>
</patent-application-publication>
