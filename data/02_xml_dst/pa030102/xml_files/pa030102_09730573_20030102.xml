<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE patent-application-publication SYSTEM "pap-v16-2002-01-01.dtd" [
<!ENTITY US20030002715A1-20030102-D00000.TIF SYSTEM "US20030002715A1-20030102-D00000.TIF" NDATA TIF>
<!ENTITY US20030002715A1-20030102-D00001.TIF SYSTEM "US20030002715A1-20030102-D00001.TIF" NDATA TIF>
<!ENTITY US20030002715A1-20030102-D00002.TIF SYSTEM "US20030002715A1-20030102-D00002.TIF" NDATA TIF>
<!ENTITY US20030002715A1-20030102-D00003.TIF SYSTEM "US20030002715A1-20030102-D00003.TIF" NDATA TIF>
<!ENTITY US20030002715A1-20030102-D00004.TIF SYSTEM "US20030002715A1-20030102-D00004.TIF" NDATA TIF>
<!ENTITY US20030002715A1-20030102-D00005.TIF SYSTEM "US20030002715A1-20030102-D00005.TIF" NDATA TIF>
<!ENTITY US20030002715A1-20030102-D00006.TIF SYSTEM "US20030002715A1-20030102-D00006.TIF" NDATA TIF>
]>
<patent-application-publication>
<subdoc-bibliographic-information>
<document-id>
<doc-number>20030002715</doc-number>
<kind-code>A1</kind-code>
<document-date>20030102</document-date>
</document-id>
<publication-filing-type>new</publication-filing-type>
<domestic-filing-data>
<application-number>
<doc-number>09730573</doc-number>
</application-number>
<application-number-series-code>09</application-number-series-code>
<filing-date>20001207</filing-date>
</domestic-filing-data>
<foreign-priority-data>
<priority-application-number>
<doc-number>PQ4640</doc-number>
</priority-application-number>
<filing-date>19991214</filing-date>
<country-code>AU</country-code>
</foreign-priority-data>
<technical-information>
<classification-ipc>
<classification-ipc-primary>
<ipc>G06K009/00</ipc>
</classification-ipc-primary>
<classification-ipc-edition>07</classification-ipc-edition>
</classification-ipc>
<classification-us>
<classification-us-primary>
<uspc>
<class>382</class>
<subclass>118000</subclass>
</uspc>
</classification-us-primary>
<classification-us-secondary>
<uspc>
<class>382</class>
<subclass>224000</subclass>
</uspc>
</classification-us-secondary>
<classification-us-secondary>
<uspc>
<class>382</class>
<subclass>309000</subclass>
</uspc>
</classification-us-secondary>
</classification-us>
<title-of-invention>Visual language classification system</title-of-invention>
</technical-information>
<inventors>
<first-named-inventor>
<name>
<given-name>Julie</given-name>
<middle-name>Rae</middle-name>
<family-name>Kowald</family-name>
</name>
<residence>
<residence-non-us>
<city>Newport</city>
<country-code>AU</country-code>
</residence-non-us>
</residence>
<authority-applicant>INV</authority-applicant>
</first-named-inventor>
</inventors>
<correspondence-address>
<name-1>FITZPATRICK CELLA HARPER &amp; SCINTO</name-1>
<name-2></name-2>
<address>
<address-1>30 ROCKEFELLER PLAZA</address-1>
<city>NEW YORK</city>
<state>NY</state>
<postalcode>10112</postalcode>
<country>
<country-code>US</country-code>
</country>
</address>
</correspondence-address>
</subdoc-bibliographic-information>
<subdoc-abstract>
<paragraph id="A-0001" lvl="0">Disclosed is a method and system (<highlight><bold>500</bold></highlight>) for automated classification of a digital image (<highlight><bold>502</bold></highlight>). The method analyses the image for the presence of a human face. A determination is then made regarding the size of the located face compared to the size of the image(FIGS. 1A-1G) to classify the image based on the relative size of the face. Alternatively, the position of the face within the image can be used to determine the classification. With a classified image, particularly forming part of a sequence of classified images, editing (<highlight><bold>514</bold></highlight>) of the sequence may be performed dependent upon the classification to achieve a desired aesthetic effect. The editing may be performed with the aid of an editing template (<highlight><bold>706</bold></highlight>). </paragraph>
</subdoc-abstract>
<subdoc-description>
<summary-of-invention>
<section>
<heading lvl="1">TECHNICAL FIELD OF THE INVENTION </heading>
<paragraph id="P-0001" lvl="0"><number>&lsqb;0001&rsqb;</number> The present invention relates generally to the classification of image data and, in particular, to a form of automated classification that permits an editor to automatically generate emotive presentations of the image data. </paragraph>
</section>
<section>
<heading lvl="1">BACKGROUND </heading>
<paragraph id="P-0002" lvl="0"><number>&lsqb;0002&rsqb;</number> The editing of video of sequences of images (eg. films, video, slide shows), to achieve a desired reaction from an audience traditionally requires input from a human editor who employs techniques other than the mere sequencing of images over a time line. To achieve an understanding by the audience of the intended message or purpose of the production, the editor must draw upon human interpretation methods which are then applied to moving or still images that form the sequence. </paragraph>
<paragraph id="P-0003" lvl="0"><number>&lsqb;0003&rsqb;</number> Film makers use many techniques to obtain a desired meaning from images, such techniques including the identification and application of different shot types, both moving and still, the use of different camera angles, different lens types and also film effects. The process of obtaining meaning from the images that make up the final production commences with a story or message that is then translated into a storyboard that is used by the film crew and film director as a template. Once the film is captured, the editor is then given the resulting images and a shot list for sequencing. It is at an early stage of production, when the screen writer translates the written story or script to a storyboard, that written language becomes visual language. This occurs due to the method by which the audience is told the story and must interpret the message. The visual nature of a moving image generally only has dialogue relevant to the character&apos;s experience and, in most cases, is absent of explicit narrative relative to the story being told and the emotional state of the characters within the story. The screen writers must therefore generate this additional information using the visual language obtained from different shot types. </paragraph>
<paragraph id="P-0004" lvl="0"><number>&lsqb;0004&rsqb;</number> Examples of different shot types or images are seen in <cross-reference target="DRAWINGS">FIGS. 1A</cross-reference> to <highlight><bold>1</bold></highlight>G. <cross-reference target="DRAWINGS">FIG. 1A</cross-reference> is representative of an extreme long shot (ELS) which is useful for establishing the characters in their environment, and also orientating the audience as to the particular location. <cross-reference target="DRAWINGS">FIG. 1B</cross-reference> is representative of a long shot (LS) which is also useful for establishing the characters in their environment and orientating the audience as to the location. In some instances, an ELS is considered more dramatic than the LS. <cross-reference target="DRAWINGS">FIG. 1C</cross-reference> is representative of a medium long shot (MLS) in which the characters are closer to the viewer and indicates, in a transition from a long shot, subjects of importance to the story. Typically for human subjects, an MLS views those subjects from the knees upwards. <cross-reference target="DRAWINGS">FIG. 1D</cross-reference> is indicative of a medium shot (MS) in which human characters are generally shown from the waist upwards, and the shot assists the viewer interpreting the characters reactions to their environment and any particular dialogue taking place. <cross-reference target="DRAWINGS">FIG. 1E</cross-reference> is indicative of a medium closeup (MCU) in which human characters are generally shown from the chest upwards. The MCU is useful for dialogue and communication interpretation including the emotion of the speaking characters. <cross-reference target="DRAWINGS">FIG. 1F</cross-reference> is indicative of a closeup (CU) which for human characters frames the forehead and shoulders within the shot, and is useful for clear understanding of the emotions associated with any particular dialogue. The closeup is used to consciously place the audience in the position of the character being imaged to achieve a greater dramatic effect. <cross-reference target="DRAWINGS">FIG. 1G</cross-reference> is representative of an extreme closeup (ECU) formed by a very tight shot of a portion of the face and demonstrates beyond the dialogue the full dramatic effect of intended emotion. An ECU can be jarring or threatening to the audience in some cases and is often used in many thriller or horror movies. It will further be apparent from the sequence of images in <cross-reference target="DRAWINGS">FIGS. 1A</cross-reference> to <highlight><bold>1</bold></highlight>G that different shots clearly can display different meaning. For example, neither of <cross-reference target="DRAWINGS">FIGS. 1F and 1G</cross-reference> indicate that the subject is seen flying a kite, nor do <cross-reference target="DRAWINGS">FIGS. 1D</cross-reference> or <highlight><bold>1</bold></highlight>E place the kite flying subject on a farm indicated by the cow seen in <cross-reference target="DRAWINGS">FIGS. 1A</cross-reference> to <highlight><bold>1</bold></highlight>C. Further, it is not apparent from <cross-reference target="DRAWINGS">FIG. 1A</cross-reference> that the subject is smiling or indeed that the subject&apos;s eyes are open. </paragraph>
<paragraph id="P-0005" lvl="0"><number>&lsqb;0005&rsqb;</number> A photograph or moving image of a person incorporating a full body shot will be interpreted by the viewer as having a different meaning to a shot of exactly the same person, where the image consists of only a closeup of the face of the subject. A full-length body shot is typically interpreted by a viewer as informative and is useful to determine the sociological factors of the subject and the relationship of the subject to the particular environment. </paragraph>
<paragraph id="P-0006" lvl="0"><number>&lsqb;0006&rsqb;</number> An example of this is illustrated in <cross-reference target="DRAWINGS">FIGS. 2A</cross-reference> to <highlight><bold>2</bold></highlight>C which show the same subject matter presented with three different shot types. <cross-reference target="DRAWINGS">FIG. 2A</cross-reference> is a wide shot of the subject within the landscape and is informative as to the location, subject and activity taken close within the scene. <cross-reference target="DRAWINGS">FIG. 2B</cross-reference> is a mid-shot of the subject with some of the surrounding landscape, and changes the emphasis from the location and activity to the character of the subject. <cross-reference target="DRAWINGS">FIG. 2C</cross-reference> provides a closeup of the subject and draws the audience to focus upon the subject. </paragraph>
<paragraph id="P-0007" lvl="0"><number>&lsqb;0007&rsqb;</number> Panning is a technique used by screen writers to help the audience participate in the absorption of information within a scene. The technique is commonly used with open landscapes or when establishing shots are used in movie productions. A straight shot, obtained when the camera does not move, contrasts the effectiveness of a pan. With a straight shot, the viewer is forced to move their eyes around the scene, searching for information, as opposed to how the pan feeds information to the viewer thus not requiring the viewer to seek out a particular message. The movement of the camera within a pan directs the audience as to those elements within a scene that should be observed and, when used correctly, is intended to mimic the human method of information interpretation and absorption. <cross-reference target="DRAWINGS">FIG. 3A</cross-reference> is an example of a still shot including a number of image elements (eg. the sun, the house, the cow, the person and the kite) which the audience may scan for information. In film, a still shot is typically used as an establishing shot so as to orientate the audience with the location and the relationship to the story. The screen writer relies upon this type of shot to make sense of any following scenes. <cross-reference target="DRAWINGS">FIG. 3B</cross-reference> demonstrates an example of a panning technique combined with a zoom, spread amongst four consecutive frames. </paragraph>
<paragraph id="P-0008" lvl="0"><number>&lsqb;0008&rsqb;</number> Further, differing camera angles, as opposed to direct, straight shots, are often used to generate meaning from the subject, such meaning not otherwise being available due to dialogue alone. For example, newspaper and television journalists often use altered camera angles to solicit propaganda about preferred election candidates. For example, interviews recorded from a low angle present the subject as superior to the audience, whereas the presentation of the same subject may be altered if taken from a high angle to give an inferior interpretation. The same technique is commonly used in movie making to dramatically increase the effect of an antagonist and their victim. When the victim is shot from a high angle, they not only appear as weak and vulnerable, but the audience empathises with the character also experiences their fear. </paragraph>
<paragraph id="P-0009" lvl="0"><number>&lsqb;0009&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 4A</cross-reference> is indicative of an eye level shot which is a standard shot contrasting with angles used in other shots and seen in <cross-reference target="DRAWINGS">FIGS. 4B</cross-reference> to <highlight><bold>4</bold></highlight>E. <cross-reference target="DRAWINGS">FIG. 4B</cross-reference> shows a high angle shot and is used to place the subject in an inferior position. <cross-reference target="DRAWINGS">FIG. 4C</cross-reference> is indicative of a low angle shot where the camera angle is held low with the subject projecting them as superior. <cross-reference target="DRAWINGS">FIG. 4D</cross-reference> is indicative of an oblique angle shot where the camera is held off-centre influencing the audience to interpret the subject as out of the ordinary, or as unbalanced in character. <cross-reference target="DRAWINGS">FIG. 4E</cross-reference> is representative of a Dutch angle shot which is often used to generate a hurried, &ldquo;no time to waste&rdquo; or bizarre effect of the subject. The audience is conveyed a message that something has gone astray in either a positive or negative fashion. </paragraph>
<paragraph id="P-0010" lvl="0"><number>&lsqb;0010&rsqb;</number> There are many other types of images or shots in addition to those discussed above that can give insight to the particular story being presented. Tracking shots follow the subject allowing the audience the experience of being part of the action. Panning gives meaning and designates importance to subjects within a scene as well as providing a panoramic view of the scene. A &ldquo;swish&rdquo; pan is similar however is used more as a transition within a scene, quickly sweeping from one subject to another, thus generating a blurred effect. Tilt shots consist of moving the camera from one point up or down, thus mimicking the way in which humans evaluate a person or vertical object absorbing the information presented thereby. A hand-held shot portrays to the audience that the filming is taking place immediately, and if often used to best effect when associated with shots taken when the camera is supported (eg. using a tripod or boom). </paragraph>
<paragraph id="P-0011" lvl="0"><number>&lsqb;0011&rsqb;</number> To understand the impact visual language has on presenting images in a more meaningful way, it is appropriate to compare the results of contemporary motion pictures with earlier attempts of film making. Early examples of motion pictures consisted of full shots of the characters from the feet upwards reflecting the transition from stage acting. For example, the Charlie Chaplin era of film making and story telling contrasts sharply with later dramatic, emotion filled motion pictures. Pioneering director D. W. Griffiths notably first introduced the use of a pallet of shot types for the purpose of creating drama in film. This arose from a desire of the audience to explore the emotional experience of the characters of the film. </paragraph>
<paragraph id="P-0012" lvl="0"><number>&lsqb;0012&rsqb;</number> Film makers also use other techniques to tell their story, such techniques including the choice of lens and film effects. These are all used to encourage the audience to understand the intended message or purpose of the production. The audience does not need to understand how, or even be aware that, these techniques have been applied to the images. In fact, if applied properly with skill, the methods will not even be apparent to the audience. </paragraph>
<paragraph id="P-0013" lvl="0"><number>&lsqb;0013&rsqb;</number> The skill required by the successful film maker is typically only acquired through many years of tuition and practice as well as through the collaboration of many experts to achieve a successfully crafted message. Amateur film makers and home video makers in contrast often lack the skill and the opportunity to understand or employ such methods. However, amateur and home film makers, being well exposed to professional film productions have a desire for their own productions to be refined to some extent approaching that of professional productions, if not those of big-budget Hollywood extravaganzas. Whilst there currently exists many film schools that specialise in courses to educate potential film makers with such techniques, attendance at such courses is often prohibitive to the amateur film maker. Other techniques currently available that may assist the amateur film maker typically includes software products to aid in the sequencing of images and/or interactive education techniques for tutoring prospective film makers. However, current software approaches have not been widely adopted due to prohibitive costs and skill required for use being excessive for small (domestic) productions. </paragraph>
<paragraph id="P-0014" lvl="0"><number>&lsqb;0014&rsqb;</number> Time is also a major factor in respect to the current techniques of film editing to unskilled editor. Typically, the time taken to plan shots and their sequencing is substantial and is typically out of the realistic scope of an average home/amateur film maker. </paragraph>
<paragraph id="P-0015" lvl="0"><number>&lsqb;0015&rsqb;</number> It is therefore desirable to provide a means by which unskilled (amateur) movie makers can create visual productions that convey a desired emotive effect to an audience without a need for extensive planning or examination of shot types. </paragraph>
</section>
<section>
<heading lvl="1">SUMMARY OF THE INVENTION </heading>
<paragraph id="P-0016" lvl="0"><number>&lsqb;0016&rsqb;</number> This need is addressed through the automated classification of images and/or shots into various emotive categories thereby permitting editing to achieve a desired emotive effect. </paragraph>
<paragraph id="P-0017" lvl="0"><number>&lsqb;0017&rsqb;</number> According to a first aspect of the present disclosure, there is provided a method for automated classification of a digital image, said method comprising the steps of: </paragraph>
<paragraph id="P-0018" lvl="2"><number>&lsqb;0018&rsqb;</number> analysing said image for the presence of a human face; </paragraph>
<paragraph id="P-0019" lvl="2"><number>&lsqb;0019&rsqb;</number> determining a size of the located face with respect to a size of said image; and </paragraph>
<paragraph id="P-0020" lvl="2"><number>&lsqb;0020&rsqb;</number> classifying said image based on the relative size of said face with respect to said image. </paragraph>
<paragraph id="P-0021" lvl="0"><number>&lsqb;0021&rsqb;</number> According to a second aspect of the present disclosure, there is provided a method for automated classification of a digital image, said method comprising the steps of: </paragraph>
<paragraph id="P-0022" lvl="2"><number>&lsqb;0022&rsqb;</number> analysing said image for the presence of a human face; </paragraph>
<paragraph id="P-0023" lvl="2"><number>&lsqb;0023&rsqb;</number> determining a position of the located face with respect to a frame of said image; and </paragraph>
<paragraph id="P-0024" lvl="2"><number>&lsqb;0024&rsqb;</number> classifying said image based on the relative position of said face with respect to said image frame. </paragraph>
<paragraph id="P-0025" lvl="0"><number>&lsqb;0025&rsqb;</number> According to another aspect of the present disclosure, there is provided apparatus for implementing any one of the aforementioned methods. </paragraph>
<paragraph id="P-0026" lvl="0"><number>&lsqb;0026&rsqb;</number> According to another aspect of the present disclosure there is provided a computer program product including a computer readable medium having recorded thereon a computer program for implementing any one of the methods described above.</paragraph>
</section>
</summary-of-invention>
<brief-description-of-drawings>
<section>
<heading lvl="1">BRIEF DESCRIPTION OF THE DRAWINGS </heading>
<paragraph id="P-0027" lvl="0"><number>&lsqb;0027&rsqb;</number> One or more embodiments of the present invention will now be described with reference to the drawings, in which: </paragraph>
<paragraph id="P-0028" lvl="0"><number>&lsqb;0028&rsqb;</number> <cross-reference target="DRAWINGS">FIGS. 1A</cross-reference> to <highlight><bold>1</bold></highlight>G depict a number of shot ranges used by film makers; </paragraph>
<paragraph id="P-0029" lvl="0"><number>&lsqb;0029&rsqb;</number> <cross-reference target="DRAWINGS">FIGS. 2A</cross-reference> to <highlight><bold>2</bold></highlight>C depict three different shot types used by film makers; </paragraph>
<paragraph id="P-0030" lvl="0"><number>&lsqb;0030&rsqb;</number> <cross-reference target="DRAWINGS">FIGS. 3A and 3B</cross-reference> depict the effect of a pan in influencing the emotional state of the viewer; </paragraph>
<paragraph id="P-0031" lvl="0"><number>&lsqb;0031&rsqb;</number> <cross-reference target="DRAWINGS">FIGS. 4A</cross-reference> to <highlight><bold>4</bold></highlight>E depict various angled camera shots also used by film makers; </paragraph>
<paragraph id="P-0032" lvl="0"><number>&lsqb;0032&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 5</cross-reference> is a schematic block diagram representation of an image recording and production system; </paragraph>
<paragraph id="P-0033" lvl="0"><number>&lsqb;0033&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 6</cross-reference> is a schematic block diagram of a general purpose computer system upon which the disclosed arrangements can be practiced; and </paragraph>
<paragraph id="P-0034" lvl="0"><number>&lsqb;0034&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 7</cross-reference> is a flow chart depicting the use of templates for video editing.</paragraph>
</section>
</brief-description-of-drawings>
<detailed-description>
<section>
<heading lvl="1">DETAILED DESCRIPTION INCLUDING BEST MODE </heading>
<paragraph id="P-0035" lvl="0"><number>&lsqb;0035&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 5</cross-reference> shows a schematic representation of an image recording and production system <highlight><bold>500</bold></highlight> where a scene <highlight><bold>502</bold></highlight> is captured using an image recording device <highlight><bold>504</bold></highlight>, such as a digital video camera or digital still camera. When the scene <highlight><bold>502</bold></highlight> is captured by a still camera, typically a sequence of still images is recorded, in effect complementing the sequence of images that might be recorded by a video camera. Associated with the capture of the images is the generation of capture data <highlight><bold>506</bold></highlight> which is output from the camera <highlight><bold>504</bold></highlight> and typically comprises image data <highlight><bold>506</bold></highlight><highlight><italic>a</italic></highlight>, video data <highlight><bold>506</bold></highlight><highlight><italic>b</italic></highlight>, audio data <highlight><bold>506</bold></highlight><highlight><italic>c </italic></highlight>and &ldquo;camera&rdquo; metadata <highlight><bold>506</bold></highlight><highlight><italic>d</italic></highlight>. The camera metadata <highlight><bold>506</bold></highlight> represents metadata usually generated automatically by the camera or manually entered by the user of the camera. Such can include image or frame number, a real-time of capture possibly include a date, details regarding camera settings (aperture, exposure etc.) and ambient information such as light measurements, to name but a few </paragraph>
<paragraph id="P-0036" lvl="0"><number>&lsqb;0036&rsqb;</number> Where appropriate, the capture data <highlight><bold>504</bold></highlight> recorded by the camera <highlight><bold>504</bold></highlight> is transferred <highlight><bold>508</bold></highlight> to a mass storage arrangement <highlight><bold>510</bold></highlight>, typically associated with a computing system, whereupon the images are made available via an interconnection <highlight><bold>520</bold></highlight> to a visual language classification system <highlight><bold>522</bold></highlight>. The classification system <highlight><bold>508</bold></highlight> generates metadata which is configured for convenient editing by the film maker. The visual language classification system <highlight><bold>522</bold></highlight> outputs classification data <highlight><bold>524</bold></highlight>, configured as further metadata, which is associated with each image and which may be stored within a mass storage unit <highlight><bold>526</bold></highlight>. The classification data <highlight><bold>524</bold></highlight> in the store <highlight><bold>526</bold></highlight> may be output to an editing module <highlight><bold>514</bold></highlight> which, through accessing the image data via a connection <highlight><bold>512</bold></highlight> to the store <highlight><bold>510</bold></highlight>, provides for the formation of an edited sequence <highlight><bold>528</bold></highlight> which may be output to a presentation unit <highlight><bold>516</bold></highlight> for display via a display unit <highlight><bold>518</bold></highlight>, such as a television display, or storage in a mass storage device <highlight><bold>519</bold></highlight>. In some implementations, the stores <highlight><bold>510</bold></highlight>, <highlight><bold>526</bold></highlight> and <highlight><bold>519</bold></highlight> may be integrally formed. </paragraph>
<paragraph id="P-0037" lvl="0"><number>&lsqb;0037&rsqb;</number> The classification system <highlight><bold>522</bold></highlight> performs content analysis to analyse the images residing in the store <highlight><bold>510</bold></highlight>. The analysis performed within the classification system <highlight><bold>522</bold></highlight> is configured to provide information about the intention of the photographer at the time of capturing the image or image sequence. Such analysis may comprise the detection of human faces and preferably other visually distinct features including landscape features such as the sky, green grass, sandy or brown earth, or other particular shapes such as motor vehicles, buildings and the like, from the image data. Audio analysis where appropriate can be used to identify specific events within the sequence of images such a person talking, the passing of a motor car, or the crack of a ball hitting a bat in a sports game, such as baseball or cricket, for example. The classification system <highlight><bold>522</bold></highlight> provides metadata related to or indicative of the content identified within an image sequence, or at the particular image within the sequence. </paragraph>
<paragraph id="P-0038" lvl="0"><number>&lsqb;0038&rsqb;</number> One specific example of content analysis that may be applied by classification system <highlight><bold>522</bold></highlight> is that of face detection, that permits identification and tracking of particular human subjects in images or sequences thereof. An example of a face detection arrangement that may be used in the arrangement of <cross-reference target="DRAWINGS">FIG. 5</cross-reference> is that described in U.S. Pat. No. 5,642,431-A (Poggio et. al.). Another example is that disclosed in Australian Patent Publication No. AU-A-33982/99. Such face detection arrangements typically identify within an image frame a group or area of pixels which are skin coloured and thus may represent a face, thereby enabling that group or area, and thus the face, to be tagged by metadata and monitored. Such monitoring may include establishing a bounding box about the height and width of the detected face and thereafter tracking changes or movement in the box across a number of image frames. </paragraph>
<paragraph id="P-0039" lvl="0"><number>&lsqb;0039&rsqb;</number> In the sequence of images of <cross-reference target="DRAWINGS">FIGS. 1A</cross-reference> to <highlight><bold>1</bold></highlight>G, the fine content of <cross-reference target="DRAWINGS">FIGS. 1A and 1B</cross-reference> are generally too small to permit accurate face detection. As such, those frames may be classified as non-face images. However in each of <cross-reference target="DRAWINGS">FIGS. 1C</cross-reference> to <highlight><bold>1</bold></highlight>G, the face of the person flying the kite is quite discernible and a significant feature of each respective image. Thus, those images may be automatically classified as face images, such classification being identified as metadata <highlight><bold>524</bold></highlight> generated by content analysis performed by the classification system <highlight><bold>522</bold></highlight> and linked or otherwise associated with the metadata <highlight><bold>506</bold></highlight><highlight><italic>d </italic></highlight>provided with the images. </paragraph>
<paragraph id="P-0040" lvl="0"><number>&lsqb;0040&rsqb;</number> Further, and in a preferred implementation, the size of the detected face, as a proportion of the overall image size, is used to establish and record the type of shot. For example, simple rules may be established to identify the type of shot. A first rule can be that, where a face is detected, but the face is substantially smaller than the image in which the face is detected, that image may be classified as a far shot. A similar rule is where a face is detected which is sized substantially the same as the image. This may be classified as a close-up. An extreme close-up may be where the face occupies the entire image or where it is substantially the same size as the image but extends beyond the edges of the image. </paragraph>
<paragraph id="P-0041" lvl="0"><number>&lsqb;0041&rsqb;</number> In another example, in <cross-reference target="DRAWINGS">FIG. 1</cross-reference>C, which is a MLS, the face represents about 2% of the image. In <cross-reference target="DRAWINGS">FIG. 1</cross-reference>D, the face occupies about 4% of the image, this being a MS. For <cross-reference target="DRAWINGS">FIG. 1E, a</cross-reference> MCU delivers the face at a size of about 10% of the image. The CU shot of <cross-reference target="DRAWINGS">FIG. 1F</cross-reference> provides the face at about 60% of the image, and for a ECU, the face is in excess of about 80% of the image. A suitable set of rules may thus be established to define the type of shot relative to the subject, whether or not the subject is a face or some other identifiable image structure (eg. cow, house, motor vehicle, etc). Example rules are set out below:  
<table-cwu id="TABLE-US-00001">
<number>1</number>
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="1" colwidth="91PT" align="left"/>
<colspec colname="2" colwidth="126PT" align="left"/>
<thead>
<row>
<entry></entry>
</row>
<row><entry namest="1" nameend="2" align="center" rowsep="1"></entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry>Medium Long Shot (MLS)</entry>
<entry>subject &lt; 2.5% of the image;</entry>
</row>
<row>
<entry>Medium Shot (MS)</entry>
<entry>2.5% &lt; subject &lt; 10% of the image;</entry>
</row>
<row>
<entry>Medium Close Up (MCU)</entry>
<entry>10% &lt; subject &lt; 30% of the image;</entry>
</row>
<row>
<entry>Close Up (CU)</entry>
<entry>30% &lt; subject &lt; 80% of the image; and</entry>
</row>
<row>
<entry>Extreme Close Up (ECU)</entry>
<entry>subject &gt; 80% of the image.</entry>
</row>
<row><entry namest="1" nameend="2" align="center" rowsep="1"></entry>
</row>
</tbody>
</tgroup>
</table>
</table-cwu>
</paragraph>
<paragraph id="P-0042" lvl="0"><number>&lsqb;0042&rsqb;</number> Where desired, the film maker may vary the rules depending on the particular type of source footage available, or depending on a particular editing effect desired to be achieved. </paragraph>
<paragraph id="P-0043" lvl="0"><number>&lsqb;0043&rsqb;</number> Another example of content analysis for classification is camera tilt angle. This can be assessed by examining the relative position of a detected face in the image frame. For example, as seen in <cross-reference target="DRAWINGS">FIG. 4</cross-reference>A, where the face is detected centrally within the image frame, this may be classified as a eye-level shot. In <cross-reference target="DRAWINGS">FIG. 4</cross-reference>B, where the subject is positioned towards the bottom of the frame, such may be classified as a high angle shot. the positioning of the detected face may be correlated with a tiling of the image frame so as to provide the desired classification. Tiles within the frame may be pre-classified as eye-level, high shot, low shot, left side, and right side. The location of the detected face in certain tiles may then be used to determine an average tile location and thus classify the image according to the position of the average face tile. Such an approach may be readily applied to the images of <cross-reference target="DRAWINGS">FIGS. 4A</cross-reference> to <highlight><bold>4</bold></highlight>D. </paragraph>
<paragraph id="P-0044" lvl="0"><number>&lsqb;0044&rsqb;</number> The Dutch shot of <cross-reference target="DRAWINGS">FIG. 4E</cross-reference> may be determined by detecting edges within the image. Such edges may be detected using any one of a large number of known edge detection arrangements. Edges in images often indicate the horizon, or some other horizontal edge, or vertical edges such as those formed by building walls. An edge that is detected as being substantially non-vertical and non-horizontal may thus indicate a Dutch shot. Classification may be performed by comparing an angle of inclination of the detected edge with the image frame. Where the angle is about 0 degrees or about 90 degrees, such may be indicative of an horizon or vertical wall respectively. Such may be a traditional shot. However, where the angle of inclination is substantially between these values, a Dutch shot may be indicated. Preferred angles of inclination for such detection may be between 30 and 60 degrees, but may be determined by the user where desired. </paragraph>
<paragraph id="P-0045" lvl="0"><number>&lsqb;0045&rsqb;</number> In an alternative implementation, the visual language classification system can permit the user to supplement the classification with other terms relating to the emotive message conveyed by the scene. Such manually entered metadata may include terms such as &ldquo;happy&rdquo;, &ldquo;smiling&rdquo;, &ldquo;leisure&rdquo;, and &ldquo;fun&rdquo; in the example of <cross-reference target="DRAWINGS">FIGS. 1C</cross-reference> to <highlight><bold>1</bold></highlight>G. More complicated descriptions may also be entered, such as &ldquo;kite flying&rdquo;. This manually enter metadata that can supplement the automatically generated metadata and be stored with the automatically generated metadata. </paragraph>
<paragraph id="P-0046" lvl="0"><number>&lsqb;0046&rsqb;</number> As a result of such processing, the store <highlight><bold>526</bold></highlight> is formed to include metadata representative of the content of source images to be used to form the final production. The metadata not only includes timing and sequencing (eg. scene number etc.) information, but also information indicative of the content of the images and shot types which can be used as prompts in the editing process to follow. </paragraph>
<paragraph id="P-0047" lvl="0"><number>&lsqb;0047&rsqb;</number> With the database <highlight><bold>526</bold></highlight> formed, the user may then commence editing the selected images. This is done by invoking an editing system <highlight><bold>514</bold></highlight> which extracts the appropriate images or sequence of images from the store <highlight><bold>510</bold></highlight>. Using the information contained within the metadata store <highlight><bold>526</bold></highlight>, the user may conveniently edit particular images. The database information may be used to define fade-in and fade-out points, images where a change in zoom is desired, points of interest within individual images which can represent focal centres for zooming operations either or both as source or target, amongst many others. </paragraph>
<paragraph id="P-0048" lvl="0"><number>&lsqb;0048&rsqb;</number> Editing performed by the editing system <highlight><bold>514</bold></highlight> may operate using the classifications <highlight><bold>524</bold></highlight> in a variety of ways. For example, the user may wish to commence an image sequence with a long shot, and hence may enter into the system <highlight><bold>514</bold></highlight> a request for all long shots to be listed. The system <highlight><bold>514</bold></highlight> then interrogates the store <highlight><bold>526</bold></highlight> to form a pick-list of images that have been previously classified as a long shot. The user may then select a long shot from the list to commence the edited sequence. The classification thus substantially reduces the user&apos;s editing time by providing a ready source of searchable information regarding each image or shot sequence. Another example is where the user wishes to show the emotion &ldquo;fear&rdquo; in the faces of the subjects. Since faces are typically not detected in any significant detail for anything under a medium shot, a search of the store <highlight><bold>526</bold></highlight> may be made for all medium shots, close-ups and extreme close-ups. A corresponding pick list results from which the user can conveniently review a generally smaller number of images than the total number available to determine those that show &ldquo;fear&rdquo;. User entered metadata such as &ldquo;fear&rdquo; may then supplement the automatically generated classification for those images that display such an emotion. </paragraph>
<paragraph id="P-0049" lvl="0"><number>&lsqb;0049&rsqb;</number> The automated content analysis of images as discussed above permits the rapid processing of sequences of images to facilitate the formation of an enhanced edited result. For example, where a video source is provided having 25 frames per second, a 5 second shot requires the editing of 125 frames. To perform manual face detection and focal point establishment on each frame is time consuming and prone to inconsistent results due to human inconsistency. Through automation by content analysis, the positions of the face since each frame may be located according to consistently applied rules. All that is then necessary is form the user to select the start and end points and the corresponding edit functions (eg. zoom values from. 0% at the start, and 60% at the end). </paragraph>
<paragraph id="P-0050" lvl="0"><number>&lsqb;0050&rsqb;</number> Metadata analysis of the source material may include the following: </paragraph>
<paragraph id="P-0051" lvl="2"><number>&lsqb;0051&rsqb;</number> (i) time code and date data; </paragraph>
<paragraph id="P-0052" lvl="2"><number>&lsqb;0052&rsqb;</number> (ii) GPS data; </paragraph>
<paragraph id="P-0053" lvl="2"><number>&lsqb;0053&rsqb;</number> (iii) image quality analysis (sharpness, colour, content quality, etc.); </paragraph>
<paragraph id="P-0054" lvl="2"><number>&lsqb;0054&rsqb;</number> (iv) original shot type detection; </paragraph>
<paragraph id="P-0055" lvl="2"><number>&lsqb;0055&rsqb;</number> (v) object detection and custom object detection (determined by the author); </paragraph>
<paragraph id="P-0056" lvl="2"><number>&lsqb;0056&rsqb;</number> (vi) movement detection; </paragraph>
<paragraph id="P-0057" lvl="2"><number>&lsqb;0057&rsqb;</number> (vii) face detection; </paragraph>
<paragraph id="P-0058" lvl="2"><number>&lsqb;0058&rsqb;</number> (viii) audio detection; </paragraph>
<paragraph id="P-0059" lvl="2"><number>&lsqb;0059&rsqb;</number> (ix) collision detection; </paragraph>
<paragraph id="P-0060" lvl="2"><number>&lsqb;0060&rsqb;</number> (x) tile (interframe structure) analysis; and </paragraph>
<paragraph id="P-0061" lvl="2"><number>&lsqb;0061&rsqb;</number> (xi) user entered metadata. </paragraph>
<paragraph id="P-0062" lvl="0"><number>&lsqb;0062&rsqb;</number> The method described above with reference to <cross-reference target="DRAWINGS">FIG. 5</cross-reference> is preferably practiced using a conventional general-purpose computer system <highlight><bold>600</bold></highlight>, such as that shown in <cross-reference target="DRAWINGS">FIG. 6</cross-reference> wherein the processes of <cross-reference target="DRAWINGS">FIG. 5</cross-reference> may be implemented as software, such as an application program executing within the computer system <highlight><bold>600</bold></highlight>. The software may be divided into two separate parts; one part for carrying out the classification and editing methods, and another part to manage the user interface between the latter and the user. The software may be stored in a computer readable medium, including the storage devices described below, for example. The software is loaded into the computer from the computer readable medium, and then executed by the computer. A computer readable medium having such software or computer program recorded on it is a computer program product. The use of the computer program product in the computer preferably effects an advantageous apparatus for classification and consequential editing of images or sequences of images. </paragraph>
<paragraph id="P-0063" lvl="0"><number>&lsqb;0063&rsqb;</number> The computer system <highlight><bold>600</bold></highlight> comprises a computer module <highlight><bold>601</bold></highlight>, input devices such as a keyboard <highlight><bold>602</bold></highlight> and mouse <highlight><bold>603</bold></highlight>, output devices including a printer <highlight><bold>615</bold></highlight> and a visual display device <highlight><bold>614</bold></highlight> and loud speaker <highlight><bold>617</bold></highlight>. A Modulator-Demodulator (Modem) transceiver device <highlight><bold>616</bold></highlight> is used by the computer module <highlight><bold>601</bold></highlight> for communicating to and from a communications network <highlight><bold>620</bold></highlight>, for example connectable via a telephone line <highlight><bold>621</bold></highlight> or other functional medium. The modem <highlight><bold>616</bold></highlight> can be used to obtain access to the Internet, and other network systems, such as a Local Area Network (LAN) or a Wide Area Network (WAN). </paragraph>
<paragraph id="P-0064" lvl="0"><number>&lsqb;0064&rsqb;</number> The computer module <highlight><bold>601</bold></highlight> typically includes at least one processor unit <highlight><bold>605</bold></highlight>, a memory unit <highlight><bold>606</bold></highlight>, for example formed from semiconductor random access memory (RAM) and read only memory (ROM), input/output (I/O) interfaces including a audio/video interface <highlight><bold>607</bold></highlight>, and an I/O interface <highlight><bold>613</bold></highlight> for the keyboard <highlight><bold>602</bold></highlight> and mouse <highlight><bold>603</bold></highlight> and optionally a joystick (not illustrated), and an interface <highlight><bold>608</bold></highlight> for the modem <highlight><bold>616</bold></highlight>. A storage device <highlight><bold>609</bold></highlight> is provided and typically includes a hard disk drive <highlight><bold>610</bold></highlight> and a floppy disk drive <highlight><bold>611</bold></highlight>. A magnetic tape drive (not illustrated) may also be used. A CD-ROM drive <highlight><bold>612</bold></highlight> is typically provided as a non-volatile source of data. The components <highlight><bold>605</bold></highlight> to <highlight><bold>613</bold></highlight> of the computer module <highlight><bold>601</bold></highlight>, typically communicate via an interconnected bus <highlight><bold>604</bold></highlight> and in a manner which results in a conventional mode of operation of the computer system <highlight><bold>600</bold></highlight> known to those in the relevant art. Examples of computers on which the described arrangements can be practised include IBM-PC&apos;s and compatibles, Sun Sparcstations or alike computer systems evolved therefrom. </paragraph>
<paragraph id="P-0065" lvl="0"><number>&lsqb;0065&rsqb;</number> Typically, the application program is resident on the hard disk drive <highlight><bold>610</bold></highlight> and read and controlled in its execution by the processor <highlight><bold>605</bold></highlight>. Intermediate storage of the program and any data fetched from the network <highlight><bold>620</bold></highlight> may be accomplished using the semiconductor memory <highlight><bold>606</bold></highlight>, possibly in concert with the hard disk drive <highlight><bold>610</bold></highlight>. In some instances, the application program may be supplied to the user encoded on a CD-ROM or floppy disk and read via the corresponding drive <highlight><bold>612</bold></highlight> or <highlight><bold>611</bold></highlight>, or alternatively may be read by the user from the network <highlight><bold>620</bold></highlight> via the modem device <highlight><bold>616</bold></highlight>. Still further, the software can also be loaded into the computer system <highlight><bold>600</bold></highlight> from other computer readable medium including magnetic tape, a ROM or integrated circuit, a magneto-optical disk, a radio or infra-red transmission channel between the computer module <highlight><bold>601</bold></highlight> and another device, a computer readable card such as a PCMCIA card, and the Internet and Intranets including e-mail transmissions and information recorded on Websites and the like. The foregoing is merely exemplary of relevant computer readable media. Other computer readable media may also be used. </paragraph>
<paragraph id="P-0066" lvl="0"><number>&lsqb;0066&rsqb;</number> The method described with reference to <cross-reference target="DRAWINGS">FIG. 6</cross-reference> may alternatively or additionally be implemented in dedicated hardware such as one or more integrated circuits performing the functions or sub functions of the system. Such dedicated hardware may include graphic processors, digital signal processors, or one or more microprocessors and associated memories. For example, specific visual effects such as zoom and image interpolation may be performed in specific hardware devices configured for such functions. Other processing modules, for example, used for face detection or audio processing, may be performed in dedicated DSP apparatus. </paragraph>
<paragraph id="P-0067" lvl="0"><number>&lsqb;0067&rsqb;</number> The description above with respect to <cross-reference target="DRAWINGS">FIG. 5</cross-reference> indicates how the editing system <highlight><bold>514</bold></highlight> may be used to create an output presentation based upon classifications derived from the image content. A further approach to editing may be achieved using a template-based approach <highlight><bold>700</bold></highlight> depicted in the flow chart of <cross-reference target="DRAWINGS">FIG. 7</cross-reference>, which for example may be implemented within the editing system <highlight><bold>514</bold></highlight>. The method <highlight><bold>700</bold></highlight> commences at step <highlight><bold>702</bold></highlight> where a desired clip, being a portion of footage between a single start-stop transition, is selected for processing. A number of clips may be processed in sequence to create a production. This is followed by step <highlight><bold>704</bold></highlight> where a desired template is selected for application to the clip. A template in this regard is a set of editing rules that may be applied to various shot and clip types to achieve a desired visual effect. Alternatively, a template need only be applied to a portion of a clip, or in some instances one or still images or video extracts for which processing is desired. Typically a number of templates <highlight><bold>706</bold></highlight> are available for selection <highlight><bold>708</bold></highlight>. Each template <highlight><bold>706</bold></highlight> may be established as a Boolean set of rules each with a number of default settings. An example template is depicted in Table 1 below and which defines particular visual effects that are top be applied to particular shot types.  
<table-cwu id="TABLE-US-00002">
<number>2</number>
<table frame="none" colsep="0" rowsep="0" pgwide="1">
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="1" colwidth="63PT" align="center"/>
<colspec colname="2" colwidth="203PT" align="center"/>
<thead>
<row>
<entry namest="1" nameend="2" align="center">TABLE 1</entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry></entry>
</row>
<row><entry namest="1" nameend="2" align="center" rowsep="1"></entry>
</row>
<row>
<entry>Template &num;2</entry>
<entry>Effect</entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="6">
<colspec colname="1" colwidth="63PT" align="center"/>
<colspec colname="2" colwidth="84PT" align="center"/>
<colspec colname="3" colwidth="21PT" align="center"/>
<colspec colname="4" colwidth="21PT" align="center"/>
<colspec colname="5" colwidth="21PT" align="center"/>
<colspec colname="6" colwidth="56PT" align="center"/>
<tbody valign="top">
<row>
<entry>Shot</entry>
<entry>Speed of replay</entry>
<entry></entry>
<entry>Zoom</entry>
<entry>Color</entry>
<entry></entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="12">
<colspec colname="1" colwidth="35PT" align="center"/>
<colspec colname="2" colwidth="28PT" align="center"/>
<colspec colname="3" colwidth="21PT" align="center"/>
<colspec colname="4" colwidth="21PT" align="center"/>
<colspec colname="5" colwidth="14PT" align="center"/>
<colspec colname="6" colwidth="14PT" align="center"/>
<colspec colname="7" colwidth="14PT" align="center"/>
<colspec colname="8" colwidth="21PT" align="center"/>
<colspec colname="9" colwidth="21PT" align="center"/>
<colspec colname="10" colwidth="21PT" align="center"/>
<colspec colname="11" colwidth="28PT" align="center"/>
<colspec colname="12" colwidth="28PT" align="center"/>
<tbody valign="top">
<row>
<entry>type</entry>
<entry>Select</entry>
<entry>x&frac14;</entry>
<entry>x&frac12;</entry>
<entry>x1</entry>
<entry>x2</entry>
<entry>x4</entry>
<entry>B&amp;W</entry>
<entry>time</entry>
<entry>filter</entry>
<entry>Sound</entry>
<entry>etc.</entry>
</row>
<row><entry namest="1" nameend="12" align="center" rowsep="1"></entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="12">
<colspec colname="1" colwidth="35PT" align="center"/>
<colspec colname="2" colwidth="28PT" align="center"/>
<colspec colname="3" colwidth="21PT" align="center"/>
<colspec colname="4" colwidth="21PT" align="center"/>
<colspec colname="5" colwidth="14PT" align="center"/>
<colspec colname="6" colwidth="14PT" align="center"/>
<colspec colname="7" colwidth="14PT" align="center"/>
<colspec colname="8" colwidth="21PT" align="center"/>
<colspec colname="9" colwidth="21PT" align="char" char="."/>
<colspec colname="10" colwidth="21PT" align="center"/>
<colspec colname="11" colwidth="28PT" align="center"/>
<colspec colname="12" colwidth="28PT" align="center"/>
<tbody valign="top">
<row>
<entry>ECU</entry>
<entry>1</entry>
<entry>1</entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry>1</entry>
<entry>0</entry>
<entry>1</entry>
<entry>0</entry>
<entry></entry>
</row>
<row>
<entry>CU</entry>
<entry>1</entry>
<entry>1</entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry>1</entry>
<entry>0</entry>
<entry>1</entry>
<entry>0</entry>
</row>
<row>
<entry>MCU</entry>
<entry>1</entry>
<entry></entry>
<entry></entry>
<entry>1</entry>
<entry></entry>
<entry></entry>
<entry>1</entry>
<entry>&plus;2</entry>
<entry>1</entry>
<entry>0</entry>
</row>
<row>
<entry>MS</entry>
<entry>0</entry>
</row>
<row>
<entry>MLS</entry>
<entry>0</entry>
</row>
<row>
<entry>LS</entry>
<entry>0</entry>
</row>
<row>
<entry>Other &num;1</entry>
<entry>1</entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry>1</entry>
<entry>1</entry>
<entry>0</entry>
<entry>1</entry>
<entry>1</entry>
</row>
<row>
<entry>Other &num;2</entry>
<entry>0</entry>
</row>
<row><entry namest="1" nameend="12" align="center" rowsep="1"></entry>
</row>
</tbody>
</tgroup>
</table>
</table-cwu>
</paragraph>
<paragraph id="P-0068" lvl="0"><number>&lsqb;0068&rsqb;</number> In the template of Table 1, the various shot types are listed based upon face detection criteria described above. Two &ldquo;other&rdquo; shot types are shown, these for example being where no face is detected or some other detectable event may be determined. Such for example may be frames containing a white coloured motor racing car of particular interest to the user, as compared to other coloured racing cars that may have been captured. Such a racing car may be detected by the classification system <highlight><bold>522</bold></highlight> being arranged to detect both a substantial region of the colour white and also substantial movement of that colour thereby permitting such frames to be classified as &ldquo;Other&num;1&rdquo;. The movement may be actual movement of the racing car across the frame over a series of adjacent frames, or relative movement where the racing car appears substantially stationary within the series of adjacent frames, whilst substantial movement of the background occurs. Such a classification may be formed independent of the ECU, CU, MCU etc. approach described above. As seen from Table 1, each of ECU, CU, MCU and Other&num;1 shot types are selected for inclusion in the edited presentation. </paragraph>
<paragraph id="P-0069" lvl="0"><number>&lsqb;0069&rsqb;</number> The template (ie. template &num;2) selected <highlight><bold>710</bold></highlight> may altered according to a user determination made in step <highlight><bold>712</bold></highlight>. Where alteration is desired, step <highlight><bold>714</bold></highlight> follows which permits the user to modify the Boolean values within the template table. As seen above, those shot types not selected (ie. MS, MLS, LS and Other&num;2) are disabled from the table, as indicated by the shading thereof. Those selected shot types may then have their corresponding effects modified by the user. As shown a number of different speeds of replay are provide, the selection of one for any shot type disabling the others for the same shot type. As seen each of the ECU and CU are selected to replay at quarter speed, whereas the MCU replays at natural speed. The racing car captured by the Other&num;1 shot type is selected for replay at four times speed to fulfil the user&apos;s desire to accentuate the differences between facial and motor car shots. Each of the selected shots has a monochrome (B&amp;W) setting selected, thereby removing colour variation, although a colour filter effect has been enabled. Such an effect may provide a constant orange/brown tinge to the entire frame and in this example would result in the images been reproduced with an aged-sepia effect. Sound is seen disabled on the facial shots but enabled on the racing car shots. </paragraph>
<paragraph id="P-0070" lvl="0"><number>&lsqb;0070&rsqb;</number> A zoom feature is also provided to permit translations between adjacent shot types. As seen in the example of Table 1, MCU shots are subject to a zoom of &ldquo;&plus;2&rdquo;, this notation representing a zoom-in to the next shot type (ie. CU) with the zoom occurring over a period of 2 seconds. Typically, during the zoom, the image is automatically cropped to retain a size within that of the display. Zoom-outs are also possible and are indicated by a minus symbol (&minus;). Durations may be specified in seconds, frames, or as being instantaneous (eg. &plus;&plus;), the later directly creating a new frame for inclusion in the edited production. The transitions for zoom in Table 1 are specified as occurring between adjacent shot types. Alternatively the degree of zoom and the zoom duration may be separately specified for each shot type (eg: MCU : 150%: 25 frames; CU: 200%: 10 frames; ECU : 30% : 50 frames). In this fashion, the edited production may show for a particular shot type a zoom to another shot type over a predetermined period thereby enhancing the emotional effect of the production. For example, a zoom from an MCU to an ECU may form part of a &ldquo;dramatic&rdquo; template, being one where ECU&apos;s are used to focus the viewer&apos;s attention on the central character. A &ldquo;tribute&rdquo; template may include a zoom from a MCU to a CU. </paragraph>
<paragraph id="P-0071" lvl="0"><number>&lsqb;0071&rsqb;</number> Other types of image editing effects may be applied within a template as desired. </paragraph>
<paragraph id="P-0072" lvl="0"><number>&lsqb;0072&rsqb;</number> Once modified, the template is stored and control returns to step <highlight><bold>704</bold></highlight> where the user may select the template just modified. Once a template has been selected, step <highlight><bold>716</bold></highlight> follows where the sequence of clips is derived form the camera metadata retained in the store <highlight><bold>718</bold></highlight>. Once the correct sequence is formed, the sequence is edited in step <highlight><bold>720</bold></highlight> by applying the selected template to the sequence. This step involves sourcing firstly the classification metadata from the store <highlight><bold>718</bold></highlight> to determine the shots types and then sourcing the video data to which the various effected selected for that shot may be applied. This results ion the output presentation of step <highlight><bold>722</bold></highlight> which may be sent for storage or directly reproduced to a display arrangement. </paragraph>
<paragraph id="P-0073" lvl="0"><number>&lsqb;0073&rsqb;</number> It will be appreciated that a variety of templates may be created, each having the capacity to impose on the source image data a particular emotive editing style in response to the classification of shot types contained therein. Further, individual clips or scenes may be edited using different templates thereby altering the presentation style based upon the subject matter. Accordingly, a family visit to the motor races may include scenes depicting a picnic lunch using substantially natural footage but limited to MS&apos;s and MLS&apos;s, action scenes edited in the manner described above with respect to Table 1, and super-action scenes where substantial slow motion is used to accentuate a crash during the race. The crash may be classified by the user supplementing the metadata of that portion of footage with a tag indicating importance. Also, whilst the template of Table 1 relies predominantly on shot distance, other classifications such as tilt angle as discussed above may alternatively or additionally be included. </paragraph>
</section>
<section>
<heading lvl="1">Industrial Applicability </heading>
<paragraph id="P-0074" lvl="0"><number>&lsqb;0074&rsqb;</number> The arrangements described are applicable to the image editing and reproduction industries and find particular application with amateur movie makers who are trained in the intricacies of shot and subject identification, and consequential editing based thereupon. </paragraph>
<paragraph id="P-0075" lvl="0"><number>&lsqb;0075&rsqb;</number> The foregoing describes only some embodiments of the present invention, and modifications and/or changes can be made thereto without departing from the scope and spirit of the present invention, the described embodiments being illustrative and not restrictive. </paragraph>
</section>
</detailed-description>
</subdoc-description>
<subdoc-claims>
<claim id="CLM-00001">
<claim-text><highlight><bold>1</bold></highlight>. A method for automated classification of a digital image, said method comprising the steps of: 
<claim-text>analysing said image for the presence of a human face; </claim-text>
<claim-text>determining a size of the located face with respect to a size of said image; and </claim-text>
<claim-text>classifying said image based on the relative size of said face with respect to said image. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00002">
<claim-text><highlight><bold>2</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference> wherein said image is classified using a term which provides information about an intention of a photographer whom captured said image. </claim-text>
</claim>
<claim id="CLM-00003">
<claim-text><highlight><bold>3</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference> or <highlight><bold>2</bold></highlight> wherein said image is classified as a far-shot if the size of said located face is substantially less than the size of said image. </claim-text>
</claim>
<claim id="CLM-00004">
<claim-text><highlight><bold>4</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference> or <highlight><bold>2</bold></highlight> wherein said image is classified as a close-up where the size of said located face substantially corresponds with the size of said image </claim-text>
</claim>
<claim id="CLM-00005">
<claim-text><highlight><bold>5</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference> or <highlight><bold>2</bold></highlight> wherein said image is classified as an extreme close-up where only a part of said located face appears within said image. </claim-text>
</claim>
<claim id="CLM-00006">
<claim-text><highlight><bold>6</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference> or <highlight><bold>2</bold></highlight> wherein said classifying comprises associating a size of said located face with a set of predetermined thresholds for a size of a human face image. </claim-text>
</claim>
<claim id="CLM-00007">
<claim-text><highlight><bold>7</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference> or <highlight><bold>2</bold></highlight> wherein said image is classified as a far shot if said image contains a face and the size of said located face is below a first predetermined threshold compared to the size of said image. </claim-text>
</claim>
<claim id="CLM-00008">
<claim-text><highlight><bold>8</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00007">claim 7</dependent-claim-reference> wherein said image is classified as an extreme close up if the size of said located face is above a second predetermined threshold compared to the size of said image. </claim-text>
</claim>
<claim id="CLM-00009">
<claim-text><highlight><bold>9</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00008">claim 8</dependent-claim-reference> wherein said image is classified as a close-up if the size of said located face is below said second predetermined threshold and above a third predetermined threshold compared to the size of said image. </claim-text>
</claim>
<claim id="CLM-00010">
<claim-text><highlight><bold>10</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00009">claim 9</dependent-claim-reference> wherein said image is classified is a medium shot if the size of said located face is greater than said first predetermined threshold and less than said third predetermined threshold. </claim-text>
</claim>
<claim id="CLM-00011">
<claim-text><highlight><bold>11</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference> wherein said analysing comprises interpreting information provided with said image. </claim-text>
</claim>
<claim id="CLM-00012">
<claim-text><highlight><bold>12</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00011">claim 11</dependent-claim-reference> wherein said image comprises a frame of a digital video sequence of images. </claim-text>
</claim>
<claim id="CLM-00013">
<claim-text><highlight><bold>13</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00011">claim 12</dependent-claim-reference> wherein said information is associated with other frames of said sequence. </claim-text>
</claim>
<claim id="CLM-00014">
<claim-text><highlight><bold>14</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference> wherein said analysing comprises detecting one or more regions of said image at which skin coloured pixels are located in order to locate said face. </claim-text>
</claim>
<claim id="CLM-00015">
<claim-text><highlight><bold>15</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference> wherein said determining approximates the size of said located face by a height and width of a bounding rectangle that encloses said face. </claim-text>
</claim>
<claim id="CLM-00016">
<claim-text><highlight><bold>16</bold></highlight>. A method for automated classification of a digital image, said method comprising the steps of: 
<claim-text>analysing said image for the presence of a human face; </claim-text>
<claim-text>determining a position of the located face with respect to a frame of said image; and </claim-text>
<claim-text>classifying said image based on the relative position of said face with respect to said image frame. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00017">
<claim-text><highlight><bold>17</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00011">claim 16</dependent-claim-reference> wherein said image is classified using a term which provides information about an intention of a photographer whom captured said image. </claim-text>
</claim>
<claim id="CLM-00018">
<claim-text><highlight><bold>18</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00011">claim 16</dependent-claim-reference> or <highlight><bold>17</bold></highlight> wherein said image is classified as a high-shot if the position of said located face is substantially toward a bottom of said image frame. </claim-text>
</claim>
<claim id="CLM-00019">
<claim-text><highlight><bold>19</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00011">claim 16</dependent-claim-reference> or <highlight><bold>17</bold></highlight> wherein said image is classified as a eye-level shot where the position of said located face substantially corresponds with a centre of said image frame. </claim-text>
</claim>
<claim id="CLM-00020">
<claim-text><highlight><bold>20</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00011">claim 16</dependent-claim-reference> or <highlight><bold>17</bold></highlight> wherein said image is classified as a low shot where the position of said located face is substantially toward a top of said image frame. </claim-text>
</claim>
<claim id="CLM-00021">
<claim-text><highlight><bold>21</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00011">claim 16</dependent-claim-reference> or <highlight><bold>17</bold></highlight> wherein said image is classified as a left shot where the position of said located face is substantially toward a right hand side of said image frame. </claim-text>
</claim>
<claim id="CLM-00022">
<claim-text><highlight><bold>22</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00011">claim 16</dependent-claim-reference> or <highlight><bold>17</bold></highlight> wherein said image is classified as a right shot where the position of said located face is substantially toward a left hand side of said image frame. </claim-text>
</claim>
<claim id="CLM-00023">
<claim-text><highlight><bold>23</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00011">claim 16</dependent-claim-reference> or <highlight><bold>17</bold></highlight> wherein said image is classified as a low shot where the position of said located face is substantially toward a top of said image frame. </claim-text>
</claim>
<claim id="CLM-00024">
<claim-text><highlight><bold>24</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00011">claim 16</dependent-claim-reference> wherein said analysing comprises interpreting information provided with said image. </claim-text>
</claim>
<claim id="CLM-00025">
<claim-text><highlight><bold>25</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00011">claim 16</dependent-claim-reference> wherein said image comprises a frame of a digital video sequence of images. </claim-text>
</claim>
<claim id="CLM-00026">
<claim-text><highlight><bold>26</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00022">claim 25</dependent-claim-reference> wherein said information is associated with other frames of said sequence. </claim-text>
</claim>
<claim id="CLM-00027">
<claim-text><highlight><bold>27</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference> further comprising the steps of: 
<claim-text>detecting an edge within said image; </claim-text>
<claim-text>determining an angle of inclination between said edge and an axis of said image frame; </claim-text>
<claim-text>classifying said image as a Dutch shot where said angle of inclination is between predetermined angles of inclination. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00028">
<claim-text><highlight><bold>28</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00022">claim 27</dependent-claim-reference> wherein said predetermined angles of inclination comprise 30 and 60 degrees. </claim-text>
</claim>
<claim id="CLM-00029">
<claim-text><highlight><bold>29</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00011">claim 16</dependent-claim-reference> further comprising: 
<claim-text>analysing said image for the presence of a predetermined non-human component; </claim-text>
<claim-text>assessing said predetermined component with respect to at least one further criteria; and </claim-text>
<claim-text>where said criteria is met, classifying said image based upon the presence of said predetermined component. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00030">
<claim-text><highlight><bold>30</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00022">claim 29</dependent-claim-reference> wherein said predetermined component comprises a colour of a distinct region of said image. </claim-text>
</claim>
<claim id="CLM-00031">
<claim-text><highlight><bold>31</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00022">claim 29</dependent-claim-reference> wherein said criteria comprises at least a relative motion of said predetermined component within said image. </claim-text>
</claim>
<claim id="CLM-00032">
<claim-text><highlight><bold>32</bold></highlight>. A method of processing an input sequence of images, said method comprising the steps of: 
<claim-text>classifying each said image of said sequence using a method according to <dependent-claim-reference depends_on="CLM-00011">claim 1;</dependent-claim-reference> and </claim-text>
<claim-text>editing said sequence using said classification to form an output sequence of images. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00033">
<claim-text><highlight><bold>33</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00033">claim 31</dependent-claim-reference> wherein said editing comprises applying an edit function to each said image of said input sequence, those ones of said images not satisfying said edit function being omitted from said output sequence. </claim-text>
</claim>
<claim id="CLM-00034">
<claim-text><highlight><bold>34</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00033">claim 34</dependent-claim-reference> wherein said editing comprises establishing an editing template for said sequence, each said edit function forming a component of said template and corresponding to one of said image classifications. </claim-text>
</claim>
<claim id="CLM-00035">
<claim-text><highlight><bold>35</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00033">claim 33</dependent-claim-reference> wherein said edit function comprises at least one effect for application to the image, said effect being selected from the group consisting of visual effects and audible effects. </claim-text>
</claim>
<claim id="CLM-00036">
<claim-text><highlight><bold>36</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00033">claim 35</dependent-claim-reference> wherein said visual effects are selected from the group consisting of reproduction speed variation, zooming, blurring, and colour variation. </claim-text>
</claim>
<claim id="CLM-00037">
<claim-text><highlight><bold>37</bold></highlight>. Apparatus for automated classification of a digital image, said comprising: 
<claim-text>means for analysing said image for the presence of a human face; </claim-text>
<claim-text>means for determining a size of the located face with respect to a size of said image; and </claim-text>
<claim-text>means for classifying said image based on the relative size of said face with respect to said image. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00038">
<claim-text><highlight><bold>38</bold></highlight>. Apparatus according to <dependent-claim-reference depends_on="CLM-00033">claim 37</dependent-claim-reference> wherein: 
<claim-text>(i) said image is classified as a far-shot if the size of said located face is substantially less than the size of said image; </claim-text>
<claim-text>(ii) said image is classified as a close-up where the size of said located face substantially corresponds with the size of said image; and </claim-text>
<claim-text>(iii) said image is classified as an extreme close-up where only a part of said located face appears within said image. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00039">
<claim-text><highlight><bold>39</bold></highlight>. Apparatus according to <dependent-claim-reference depends_on="CLM-00033">claim 37</dependent-claim-reference> wherein said means for classifying associates a size of said located face with a set of predetermined thresholds for a size of a human face image. </claim-text>
</claim>
<claim id="CLM-00040">
<claim-text><highlight><bold>40</bold></highlight>. Apparatus according to <dependent-claim-reference depends_on="CLM-00033">claim 39</dependent-claim-reference> wherein: 
<claim-text>(i) said image is classified as a far shot if said image contains a face and the size of said located face is below a first predetermined threshold compared to the size of said image; </claim-text>
<claim-text>(ii) said image is classified as an extreme close up if the size of said located face is above a second predetermined threshold compared to the size of said image; </claim-text>
<claim-text>(iii) said image is classified as a close-up if the size of said located face is below said second predetermined threshold and above a third predetermined threshold compared to the size of said image; and </claim-text>
<claim-text>(iv) said image is classified is a medium shot if the size of said located face is greater than said first predetermined threshold and less than said third predetermined threshold. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00041">
<claim-text><highlight><bold>41</bold></highlight>. Apparatus according to <dependent-claim-reference depends_on="CLM-00033">claim 37</dependent-claim-reference> wherein said analysing comprises interpreting information provided with said image. </claim-text>
</claim>
<claim id="CLM-00042">
<claim-text><highlight><bold>42</bold></highlight>. Apparatus according to <dependent-claim-reference depends_on="CLM-00044">claim 41</dependent-claim-reference> wherein said image comprises a frame of a digital video sequence of images. </claim-text>
</claim>
<claim id="CLM-00043">
<claim-text><highlight><bold>43</bold></highlight>. Apparatus according to <dependent-claim-reference depends_on="CLM-00044">claim 41</dependent-claim-reference> wherein said means for analysing detects one or more regions of said image at which skin coloured pixels are located in order to locate said face. </claim-text>
</claim>
<claim id="CLM-00044">
<claim-text><highlight><bold>44</bold></highlight>. Apparatus according to <dependent-claim-reference depends_on="CLM-00044">claim 43</dependent-claim-reference> wherein said means for determining approximates the size of said located face by a height and width of a bounding rectangle that encloses said face. </claim-text>
</claim>
<claim id="CLM-00045">
<claim-text><highlight><bold>45</bold></highlight>. Apparatus for automated classification of a digital image, said apparatus comprising: 
<claim-text>means for analysing said image for the presence of a human face; </claim-text>
<claim-text>means for determining a position of the located face with respect to a frame of said image; and </claim-text>
<claim-text>means for classifying said image based on the relative position of said face with respect to said image frame. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00046">
<claim-text><highlight><bold>46</bold></highlight>. Apparatus according to <dependent-claim-reference depends_on="CLM-00044">claim 45</dependent-claim-reference> wherein: 
<claim-text>(i) said image is classified as a high-shot if the position of said located face is substantially toward a bottom of said image frame; </claim-text>
<claim-text>(ii) said image is classified as a eye-level shot where the position of said located face substantially corresponds with a centre of said image frame; </claim-text>
<claim-text>(iii) said image is classified as a low shot where the position of said located face is substantially toward a top of said image frame; </claim-text>
<claim-text>(iv) said image is classified as a left shot where the position of said located face is substantially toward a right hand side of said image frame; </claim-text>
<claim-text>(v) said image is classified as a right shot where the position of said located face is substantially toward a left hand side of said image frame; </claim-text>
<claim-text>(vi) said image is classified as a low shot where the position of said located face is substantially toward a top of said image frame. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00047">
<claim-text><highlight><bold>47</bold></highlight>. Apparatus according to <dependent-claim-reference depends_on="CLM-00044">claim 46</dependent-claim-reference> wherein said analysing comprises interpreting information provided with said image. </claim-text>
</claim>
<claim id="CLM-00048">
<claim-text><highlight><bold>48</bold></highlight>. Apparatus according to <dependent-claim-reference depends_on="CLM-00044">claim 46</dependent-claim-reference> wherein said image comprises a frame of a digital video sequence of images. </claim-text>
</claim>
<claim id="CLM-00049">
<claim-text><highlight><bold>49</bold></highlight>. Apparatus according to <dependent-claim-reference depends_on="CLM-00044">claim 48</dependent-claim-reference> wherein said information is associated with other frames of said sequence. </claim-text>
</claim>
<claim id="CLM-00050">
<claim-text><highlight><bold>50</bold></highlight>. Apparatus according to <dependent-claim-reference depends_on="CLM-00033">claim 37</dependent-claim-reference> further comprising: 
<claim-text>means for detecting an edge within said image; </claim-text>
<claim-text>means for determining an angle of inclination between said edge and an axis of said image frame; </claim-text>
<claim-text>means for classifying said image as a Dutch shot where said angle of inclination is between predetermined angles of inclination. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00051">
<claim-text><highlight><bold>51</bold></highlight>. Apparatus according to <dependent-claim-reference depends_on="CLM-00033">claim 37</dependent-claim-reference> further comprising: 
<claim-text>means for analysing said image for the presence of a predetermined non-human component; </claim-text>
<claim-text>means for assessing said predetermined component with respect to at least one further criteria; and </claim-text>
<claim-text>where said criteria is met, classifying said image based upon the presence of said predetermined component. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00052">
<claim-text><highlight><bold>52</bold></highlight>. Apparatus according to <dependent-claim-reference depends_on="CLM-00055">claim 51</dependent-claim-reference> wherein said predetermined component comprises a colour of a distinct region of said image. </claim-text>
</claim>
<claim id="CLM-00053">
<claim-text><highlight><bold>53</bold></highlight>. Apparatus according to <dependent-claim-reference depends_on="CLM-00055">claim 51</dependent-claim-reference> wherein said criteria comprises at least a relative motion of said predetermined component within said image. </claim-text>
</claim>
<claim id="CLM-00054">
<claim-text><highlight><bold>54</bold></highlight>. Apparatus for processing an image sequence, said apparatus comprising: 
<claim-text>classification apparatus according to <dependent-claim-reference depends_on="CLM-00033">claim 37</dependent-claim-reference> for determining a classification for each image of said sequence; and </claim-text>
<claim-text>means for editing said sequence using said classification to form an output sequence of images. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00055">
<claim-text><highlight><bold>55</bold></highlight>. Apparatus according to <dependent-claim-reference depends_on="CLM-00055">claim 54</dependent-claim-reference> wherein said means for editing comprises applying an edit function to each said image of said input sequence, those ones of said images not satisfying said edit function being omitted from said output sequence. </claim-text>
</claim>
<claim id="CLM-00056">
<claim-text><highlight><bold>56</bold></highlight>. Apparatus according to <dependent-claim-reference depends_on="CLM-00055">claim 55</dependent-claim-reference> wherein said editing comprises establishing an editing template for said sequence, each said edit function forming a component of said template and corresponding to one of said image classifications. </claim-text>
</claim>
<claim id="CLM-00057">
<claim-text><highlight><bold>57</bold></highlight>. Apparatus according to <dependent-claim-reference depends_on="CLM-00055">claim 56</dependent-claim-reference> wherein said edit function comprises at least one effect for application to the image, said effect being selected from the group consisting of visual effects and audible effects. </claim-text>
</claim>
<claim id="CLM-00058">
<claim-text><highlight><bold>58</bold></highlight>. Apparatus according to <dependent-claim-reference depends_on="CLM-00055">claim 57</dependent-claim-reference> wherein said visual effects are selected from the group consisting of reproduction speed variation, zooming, blurring, and colour variation. </claim-text>
</claim>
<claim id="CLM-00059">
<claim-text><highlight><bold>59</bold></highlight>. A computer readable medium incorporating a computer program product operable upon computer apparatus for automated classification of a digital image, said computer program product comprising: 
<claim-text>code for analysing said image for the presence of a human face; </claim-text>
<claim-text>code for determining a size of the located face with respect to a size of said image; and </claim-text>
<claim-text>code for classifying said image based on the relative size of said face with respect to said image. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00060">
<claim-text><highlight><bold>60</bold></highlight>. A computer readable medium according to <dependent-claim-reference depends_on="CLM-00055">claim 59</dependent-claim-reference> wherein: 
<claim-text>(i) said image is classified as a far-shot if the size of said located face is substantially less than the size of said image; </claim-text>
<claim-text>(ii) said image is classified as a close-up where the size of said located face substantially corresponds with the size of said image; and </claim-text>
<claim-text>(iii) said image is classified as an extreme close-up where only a part of said located face appears within said image. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00061">
<claim-text><highlight><bold>61</bold></highlight>. A computer readable medium according to <dependent-claim-reference depends_on="CLM-00066">claim 60</dependent-claim-reference> wherein said classifying comprises associating a size of said located face with a set of predetermined thresholds for a size of a human face image. </claim-text>
</claim>
<claim id="CLM-00062">
<claim-text><highlight><bold>62</bold></highlight>. A computer readable medium according to <dependent-claim-reference depends_on="CLM-00066">claim 61</dependent-claim-reference> wherein: 
<claim-text>(i) said image is classified as a far shot if said image contains a face and the size of said located face is below a first predetermined threshold compared to the size of said image; </claim-text>
<claim-text>(ii) said image is classified as an extreme close up if the size of said located face is above a second predetermined threshold compared to the size of said image; </claim-text>
<claim-text>(iii) said image is classified as a close-up if the size of said located face is below said second predetermined threshold and above a third predetermined threshold compared to the size of said image; and </claim-text>
<claim-text>(iv) said image is classified is a medium shot if the size of said located face is greater than said first predetermined threshold and less than said third predetermined threshold. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00063">
<claim-text><highlight><bold>63</bold></highlight>. A computer readable medium according to <dependent-claim-reference depends_on="CLM-00055">claim 59</dependent-claim-reference> wherein said analysing comprises interpreting information provided with said image. </claim-text>
</claim>
<claim id="CLM-00064">
<claim-text><highlight><bold>64</bold></highlight>. A computer readable medium according to <dependent-claim-reference depends_on="CLM-00066">claim 63</dependent-claim-reference> wherein said image comprises a frame of a digital video sequence of images. </claim-text>
</claim>
<claim id="CLM-00065">
<claim-text><highlight><bold>65</bold></highlight>. A computer readable medium according to <dependent-claim-reference depends_on="CLM-00066">claim 64</dependent-claim-reference> wherein said information is associated with other frames of said sequence. </claim-text>
</claim>
<claim id="CLM-00066">
<claim-text><highlight><bold>66</bold></highlight>. A computer readable medium according to <dependent-claim-reference depends_on="CLM-00055">claim 59</dependent-claim-reference> wherein said analysing comprises detecting one or more regions of said image at which skin coloured pixels are located in order to locate said face. </claim-text>
</claim>
<claim id="CLM-00067">
<claim-text><highlight><bold>67</bold></highlight>. A computer readable medium according to <dependent-claim-reference depends_on="CLM-00055">claim 59</dependent-claim-reference> wherein said determining approximates the size of said located face by a height and width of a bounding rectangle that encloses said face. </claim-text>
</claim>
<claim id="CLM-00068">
<claim-text><highlight><bold>68</bold></highlight>. A computer readable medium according to <dependent-claim-reference depends_on="CLM-00055">claim 59</dependent-claim-reference> further comprising: 
<claim-text>code for analysing said image for the presence of a human face; </claim-text>
<claim-text>code for determining a position of the located face with respect to a frame of said image; and </claim-text>
<claim-text>code for classifying said image based on the relative position of said face with respect to said image frame. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00069">
<claim-text><highlight><bold>69</bold></highlight>. A computer readable medium according to <dependent-claim-reference depends_on="CLM-00066">claim 68</dependent-claim-reference> wherein: 
<claim-text>(i) said image is classified as a high-shot if the position of said located face is substantially toward a bottom of said image frame; </claim-text>
<claim-text>(ii) said image is classified as a eye-level shot where the position of said located face substantially corresponds with a centre of said image frame; </claim-text>
<claim-text>(iii) said image is classified as a low shot where the position of said located face is substantially toward a top of said image frame; </claim-text>
<claim-text>(iv) said image is classified as a left shot where the position of said located face is substantially toward a right hand side of said image frame; </claim-text>
<claim-text>(v) said image is classified as a right shot where the position of said located face is substantially toward a left hand side of said image frame; </claim-text>
<claim-text>(vi) said image is classified as a low shot where the position of said located face is substantially toward a top of said image frame. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00070">
<claim-text><highlight><bold>70</bold></highlight>. A computer readable medium according to <dependent-claim-reference depends_on="CLM-00066">claim 69</dependent-claim-reference> wherein said analysing comprises interpreting information provided with said image. </claim-text>
</claim>
<claim id="CLM-00071">
<claim-text><highlight><bold>71</bold></highlight>. A computer readable medium according to <dependent-claim-reference depends_on="CLM-00066">claim 69</dependent-claim-reference> wherein said image comprises a frame of a digital video sequence of images. </claim-text>
</claim>
<claim id="CLM-00072">
<claim-text><highlight><bold>72</bold></highlight>. A computer readable medium according to <dependent-claim-reference depends_on="CLM-00077">claim 71</dependent-claim-reference> wherein said information is associated with other frames of said sequence. </claim-text>
</claim>
<claim id="CLM-00073">
<claim-text><highlight><bold>73</bold></highlight>. A computer readable medium according to <dependent-claim-reference depends_on="CLM-00077">claim 72</dependent-claim-reference> further comprising: 
<claim-text>code for detecting an edge within said image; </claim-text>
<claim-text>code for determining an angle of inclination between said edge and an axis of said image frame; </claim-text>
<claim-text>code for classifying said image as a Dutch shot where said angle of inclination is between predetermined angles of inclination. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00074">
<claim-text><highlight><bold>74</bold></highlight>. A computer readable medium according to <dependent-claim-reference depends_on="CLM-00077">claim 73</dependent-claim-reference> wherein said predetermined angles of inclination comprise 30 and 60 degrees. </claim-text>
</claim>
<claim id="CLM-00075">
<claim-text><highlight><bold>75</bold></highlight>. A computer readable medium according to <dependent-claim-reference depends_on="CLM-00077">claim 74</dependent-claim-reference> further comprising: 
<claim-text>code for analysing said image for the presence of a predetermined non-human component; </claim-text>
<claim-text>code for assessing said predetermined component with respect to at least one further criteria; and </claim-text>
<claim-text>where said criteria is met, classifying said image based upon the presence of said predetermined component. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00076">
<claim-text><highlight><bold>76</bold></highlight>. A computer readable medium according to <dependent-claim-reference depends_on="CLM-00077">claim 75</dependent-claim-reference> wherein said predetermined component comprises a colour of a distinct region of said image. </claim-text>
</claim>
<claim id="CLM-00077">
<claim-text><highlight><bold>77</bold></highlight>. A computer readable medium according to <dependent-claim-reference depends_on="CLM-00077">claim 76</dependent-claim-reference> wherein said criteria comprises at least a relative motion of said predetermined component within said image. </claim-text>
</claim>
<claim id="CLM-00078">
<claim-text><highlight><bold>78</bold></highlight>. A computer readable medium incorporating a computer program product for processing an input sequence of images, comprising: 
<claim-text>code for classifying each said image of said sequence using the computer program product of claim <highlight><bold>77</bold></highlight>; and </claim-text>
<claim-text>code for editing said sequence using said classification to form an output sequence of images. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00079">
<claim-text><highlight><bold>79</bold></highlight>. A computer readable medium according to <dependent-claim-reference depends_on="CLM-00077">claim 78</dependent-claim-reference> wherein said editing comprises applying an edit function to each said image of said input sequence, those ones of said images not satisfying said edit function being omitted from said output sequence. </claim-text>
</claim>
<claim id="CLM-00080">
<claim-text><highlight><bold>80</bold></highlight>. A computer readable medium according to <dependent-claim-reference depends_on="CLM-00077">claim 79</dependent-claim-reference> wherein said editing comprises establishing an editing template for said sequence, each said edit function forming a component of said template and corresponding to one of said image classifications. </claim-text>
</claim>
<claim id="CLM-00081">
<claim-text><highlight><bold>81</bold></highlight>. A computer readable medium according to claim <highlight><bold>80</bold></highlight> wherein said edit function comprises at least one effect for application to the image, said effect being selected from the group consisting of visual effects and audible effects. </claim-text>
</claim>
<claim id="CLM-00082">
<claim-text><highlight><bold>82</bold></highlight>. A computer readable medium according to claim <highlight><bold>81</bold></highlight> wherein said visual effects are selected from the group consisting of reproduction speed variation, zooming, blurring, and colour variation. </claim-text>
</claim>
<claim id="CLM-00083">
<claim-text><highlight><bold>83</bold></highlight>. An edited sequence of images formed through implementation of a series of images according to any one of the preceding claims.</claim-text>
</claim>
</subdoc-claims>
<subdoc-drawings id="DRAWINGS">
<heading lvl="0" align="CENTER">Drawings</heading>
<representative-figure>5</representative-figure>
<figure id="figure-D00000">
<image id="EMI-D00000" file="US20030002715A1-20030102-D00000.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00001">
<image id="EMI-D00001" file="US20030002715A1-20030102-D00001.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00002">
<image id="EMI-D00002" file="US20030002715A1-20030102-D00002.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00003">
<image id="EMI-D00003" file="US20030002715A1-20030102-D00003.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00004">
<image id="EMI-D00004" file="US20030002715A1-20030102-D00004.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00005">
<image id="EMI-D00005" file="US20030002715A1-20030102-D00005.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00006">
<image id="EMI-D00006" file="US20030002715A1-20030102-D00006.TIF" imf="TIFF" ti="DR"/>
</figure>
</subdoc-drawings>
</patent-application-publication>
