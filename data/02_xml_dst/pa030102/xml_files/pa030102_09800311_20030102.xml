<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE patent-application-publication SYSTEM "pap-v16-2002-01-01.dtd" [
<!ENTITY US20030004914A1-20030102-D00000.TIF SYSTEM "US20030004914A1-20030102-D00000.TIF" NDATA TIF>
<!ENTITY US20030004914A1-20030102-D00001.TIF SYSTEM "US20030004914A1-20030102-D00001.TIF" NDATA TIF>
<!ENTITY US20030004914A1-20030102-D00002.TIF SYSTEM "US20030004914A1-20030102-D00002.TIF" NDATA TIF>
<!ENTITY US20030004914A1-20030102-D00003.TIF SYSTEM "US20030004914A1-20030102-D00003.TIF" NDATA TIF>
<!ENTITY US20030004914A1-20030102-D00004.TIF SYSTEM "US20030004914A1-20030102-D00004.TIF" NDATA TIF>
<!ENTITY US20030004914A1-20030102-D00005.TIF SYSTEM "US20030004914A1-20030102-D00005.TIF" NDATA TIF>
<!ENTITY US20030004914A1-20030102-D00006.TIF SYSTEM "US20030004914A1-20030102-D00006.TIF" NDATA TIF>
<!ENTITY US20030004914A1-20030102-D00007.TIF SYSTEM "US20030004914A1-20030102-D00007.TIF" NDATA TIF>
<!ENTITY US20030004914A1-20030102-D00008.TIF SYSTEM "US20030004914A1-20030102-D00008.TIF" NDATA TIF>
<!ENTITY US20030004914A1-20030102-D00009.TIF SYSTEM "US20030004914A1-20030102-D00009.TIF" NDATA TIF>
<!ENTITY US20030004914A1-20030102-D00010.TIF SYSTEM "US20030004914A1-20030102-D00010.TIF" NDATA TIF>
<!ENTITY US20030004914A1-20030102-D00011.TIF SYSTEM "US20030004914A1-20030102-D00011.TIF" NDATA TIF>
<!ENTITY US20030004914A1-20030102-D00012.TIF SYSTEM "US20030004914A1-20030102-D00012.TIF" NDATA TIF>
<!ENTITY US20030004914A1-20030102-D00013.TIF SYSTEM "US20030004914A1-20030102-D00013.TIF" NDATA TIF>
<!ENTITY US20030004914A1-20030102-D00014.TIF SYSTEM "US20030004914A1-20030102-D00014.TIF" NDATA TIF>
<!ENTITY US20030004914A1-20030102-D00015.TIF SYSTEM "US20030004914A1-20030102-D00015.TIF" NDATA TIF>
<!ENTITY US20030004914A1-20030102-D00016.TIF SYSTEM "US20030004914A1-20030102-D00016.TIF" NDATA TIF>
<!ENTITY US20030004914A1-20030102-D00017.TIF SYSTEM "US20030004914A1-20030102-D00017.TIF" NDATA TIF>
<!ENTITY US20030004914A1-20030102-D00018.TIF SYSTEM "US20030004914A1-20030102-D00018.TIF" NDATA TIF>
<!ENTITY US20030004914A1-20030102-D00019.TIF SYSTEM "US20030004914A1-20030102-D00019.TIF" NDATA TIF>
<!ENTITY US20030004914A1-20030102-D00020.TIF SYSTEM "US20030004914A1-20030102-D00020.TIF" NDATA TIF>
<!ENTITY US20030004914A1-20030102-D00021.TIF SYSTEM "US20030004914A1-20030102-D00021.TIF" NDATA TIF>
<!ENTITY US20030004914A1-20030102-D00022.TIF SYSTEM "US20030004914A1-20030102-D00022.TIF" NDATA TIF>
<!ENTITY US20030004914A1-20030102-D00023.TIF SYSTEM "US20030004914A1-20030102-D00023.TIF" NDATA TIF>
<!ENTITY US20030004914A1-20030102-D00024.TIF SYSTEM "US20030004914A1-20030102-D00024.TIF" NDATA TIF>
<!ENTITY US20030004914A1-20030102-D00025.TIF SYSTEM "US20030004914A1-20030102-D00025.TIF" NDATA TIF>
<!ENTITY US20030004914A1-20030102-D00026.TIF SYSTEM "US20030004914A1-20030102-D00026.TIF" NDATA TIF>
<!ENTITY US20030004914A1-20030102-D00027.TIF SYSTEM "US20030004914A1-20030102-D00027.TIF" NDATA TIF>
<!ENTITY US20030004914A1-20030102-D00028.TIF SYSTEM "US20030004914A1-20030102-D00028.TIF" NDATA TIF>
<!ENTITY US20030004914A1-20030102-D00029.TIF SYSTEM "US20030004914A1-20030102-D00029.TIF" NDATA TIF>
<!ENTITY US20030004914A1-20030102-D00030.TIF SYSTEM "US20030004914A1-20030102-D00030.TIF" NDATA TIF>
<!ENTITY US20030004914A1-20030102-D00031.TIF SYSTEM "US20030004914A1-20030102-D00031.TIF" NDATA TIF>
<!ENTITY US20030004914A1-20030102-D00032.TIF SYSTEM "US20030004914A1-20030102-D00032.TIF" NDATA TIF>
]>
<patent-application-publication>
<subdoc-bibliographic-information>
<document-id>
<doc-number>20030004914</doc-number>
<kind-code>A1</kind-code>
<document-date>20030102</document-date>
</document-id>
<publication-filing-type>new</publication-filing-type>
<domestic-filing-data>
<application-number>
<doc-number>09800311</doc-number>
</application-number>
<application-number-series-code>09</application-number-series-code>
<filing-date>20010302</filing-date>
</domestic-filing-data>
<technical-information>
<classification-ipc>
<classification-ipc-primary>
<ipc>G06F007/00</ipc>
</classification-ipc-primary>
<classification-ipc-edition>07</classification-ipc-edition>
</classification-ipc>
<classification-us>
<classification-us-primary>
<uspc>
<class>707</class>
<subclass>001000</subclass>
</uspc>
</classification-us-primary>
</classification-us>
<title-of-invention>System, method and apparatus for conducting a phrase search</title-of-invention>
</technical-information>
<inventors>
<first-named-inventor>
<name>
<given-name>Michael</given-name>
<middle-name>W.</middle-name>
<family-name>McGreevy</family-name>
</name>
<residence>
<residence-us>
<city>Sunnyvale</city>
<state>CA</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</first-named-inventor>
</inventors>
<correspondence-address>
<name-1>NASA AMES RESEARCH CENTER</name-1>
<name-2>ATTN: PATENT COUNSEL</name-2>
<address>
<address-1>MAIL STOP 202A-3</address-1>
<city>MOFFETT FIELD</city>
<state>CA</state>
<postalcode>94035-1000</postalcode>
<country>
<country-code>US</country-code>
</country>
</address>
</correspondence-address>
</subdoc-bibliographic-information>
<subdoc-abstract>
<paragraph id="A-0001" lvl="0">A phrase search is a method of searching a database for subsets of the database that are relevant to an input query. First, a number of relational models of subsets of a database are provided. A query is then input. The query can include one or more sequences of terms. Next, a relational model of the query is created. The relational model of the query is then compared to each one of the relational models of subsets of the database. The identifiers of the relevant subsets are then output. </paragraph>
</subdoc-abstract>
<subdoc-description>
<summary-of-invention>
<section>
<heading lvl="1">FIELD OF THE INVENTION </heading>
<paragraph id="P-0001" lvl="0"><number>&lsqb;0001&rsqb;</number> The present invention relates to relational analysis and representation, database information retrieval and search engine technology and, more specifically, a system and method of analyzing data in context. </paragraph>
</section>
<section>
<heading lvl="1">BACKGROUND OF THE INVENTION </heading>
<paragraph id="P-0002" lvl="0"><number>&lsqb;0002&rsqb;</number> The vast amount of text and other types of information available in electronic form have contributed substantially to an &ldquo;information glut.&rdquo; In response, researchers are creating a variety of methods to address the need to efficiently access electronically stored information. Current methods are typically based on finding and exploiting patterns in collections of text. Variations among the methods and the factions are primarily due to varying allegiances to linguistics, quantitative analysis, representations of domain expertise, and the practical demands of the applications. Typical applications involve finding items of interest from large collections of text, having appropriate items routed to the correct people, and condensing the contents of many documents into a summary form. </paragraph>
<paragraph id="P-0003" lvl="0"><number>&lsqb;0003&rsqb;</number> One known application includes various forms of, and attempts to improve upon, keyword search type technologies. These improvements include statistical analysis and analysis based upon grammar or parts of speech. Statistical analysis generally relies upon the concept that common or often-repeated terms are of greater importance than less common or rarely used terms. Parts of speech attach importance to different terms based upon whether the term is a noun, verb, pronoun, adverb, adjective, article, etc. Typically a noun would have more importance than an article therefore nouns would be processed where articles would be ignored. </paragraph>
<paragraph id="P-0004" lvl="0"><number>&lsqb;0004&rsqb;</number> Other known methods of processing electronic information include various methods of retrieving text documents. One example is the work of Hawking, D. A. and Thistlewaite, P. B.: Proximity Operators&mdash;So Near And Yet So Far. In D. K. Harman, (ed.) Proc. Fourth Text Retrieval Conf. (TREC), pp 131-144, NIST Special Publication 500-236, 1996. Hawking, D. A. and Thistlewaite, P. B.: Relevance Weighting Using Distance Between Term Occurrences. Technical Report TR-CS-96-08, Department of Computer Science, Australian National University, June 1996 (Hawking and Thistlewaite (1995, 1996)) on the PADRE system. </paragraph>
<paragraph id="P-0005" lvl="0"><number>&lsqb;0005&rsqb;</number> The PADRE system applies complex proximity metrics to determine the relevance of documents. PADRE measures the spans of text that contain clusters of any number of target words. Thus, PADRE is based on complex, multi-way (&ldquo;N-ary&rdquo;) relations. PADRE&apos;s spans and clusters have complex, non-intuitive, and somewhat arbitrary definitions. Each use of PADRE to rank documents requires a user to manually select and specify a small group of words that might be closely clustered in the text. PADRE relevance criteria are based on the assumption that the greatest relevance is achieved when all of the target words are closest to each other. PADRE relevance criteria are generated manually, by the user&apos;s own &ldquo;human free association.&rdquo; PADRE, therefore, is imprecise and often generates inaccurate search/comparison results. </paragraph>
<paragraph id="P-0006" lvl="0"><number>&lsqb;0006&rsqb;</number> Other prior art methods include various methodologies of data mining. See for example: Fayyad, U.; Piatetsky-Shapiro, G.; and Smyth, P: The KDD Process for Extracting Useful Knowledge from Volumes of Data. Comm. ACM, vol. 39, no. 11, 1996, pp. 27-34 (Fayyad, et al., 1996). Search engines Zorn, P.; Emanoil, M.; Marshall, L; and Panek, M.: Advanced Web Searching: Tricks of the Trade. ONLINE, vol. 20, no. 3, 1996, pp. 14-28, (Zorn, et al., 1996). Discourse analysis Kitani, T.; Eriguchi, Y.; and Hara, M.: Pattern Matching and Discourse Processing in Information Extraction from Japanese Text. JAIR, vol. 2, 1994, pp. 89-100, (Kitani, et al., 1994). Information extraction Cowie, J. and Lehnert, W.: Information Extraction. Comm. ACM, vol. 39, no. 1, 1996, pp. 81-91, (Cowie, et al., 1996). Information filtering Foltz, P. W. and Dumais, S. T.: Personalized Information Delivery&mdash;An Analysis of Information Filtering Methods. Comm. ACM, vol. 35, no. 12, 1992, pp. 51-60, (Foltz, et al., 1992). Information retrieval Salton, G.: Developments in Automatic Text Retrieval, Science, vol. 253, 1991, pp. 974-980, (Salton Developments . . . 1991) and digital libraries Fox, E. A.; Akscyn, R. M.; Furuta, R. K.; and Leggett, J. J.: Digital Libraries&mdash;Introduction. Comm. ACM., vol. 38, no. 4, pp. 22-28, 1995 (Fox, et al. 1995). Cutting across these approaches are concerns about how to subdivide words and collections of words into useful pieces, how to categorize the pieces, how to detect and utilize various relations among the pieces, and how transform the many pieces into a smaller number of representative pieces. </paragraph>
<paragraph id="P-0007" lvl="0"><number>&lsqb;0007&rsqb;</number> Most keyword search methods use term indexing such as used by Salton, G.: A blueprint for automatic indexing. ACM SIGIR Forum, vol. 16, no. 2, 1981. Reprinted in ACM SIGIR Forum, vol. 31, no. 1, 1997, pp. 23-36. (Salton, A blueprint . . . 1981), where a word list represents each document and internal query. As a consequence, given a keyword as a user query, these methods use merely the presence of the keyword in documents as the main criterion of relevance. Some methods such as Jing, Y. and Croft, W. B.: An Association Thesaurus for Information Retrieval. Technical Report 94-17, University of Massachusetts, 1994 (Jing and Croft, 1994); Gauch, S., and Wang, J.: Corpus analysis for TREC 5 query expansion. Proc. TREC 5, NIST SP 500-238, 1996, pp. 537-547 (Gauch &amp; Wang, 1996); Xu, J., and Croft, W.: Query expansion using local and global document analysis. Proc. ACM SIGIR, 1996, pp. 4-11. (Xu and Croft, 1996); McDonald, J., Ogden, W., and Foltz, P.: Interactive information retrieval using term relationship networks. Proc. TREC 6, NIST SP 500-240, 1997, pp. 379-383 (McDonald, Ogden, and Foltz, 1997), utilize term associations to identify or display additional query keywords that are associated with the user-supplied keywords. This results in, &ldquo;query drift&rdquo;. Query drift occurs when the additional query keywords retrieve documents that are poorly related or unrelated to the original keywords. Further, term index methods are ineffective in ranking documents on the basis of keywords in context. </paragraph>
<paragraph id="P-0008" lvl="0"><number>&lsqb;0008&rsqb;</number> In the proximity indexing method of Hawking and Thistlewaite (1996, 1996), a query consists of a user-identified collection of words. These query words are compared with the words in the documents of the database. The search method seeks documents containing length-limited sequences of words that contain subsets of the query words. Documents containing greater numbers of query words in shorter sequences of words are considered to have greater relevance. Further, as with other conventional term indexing schemes, the method of Hawking et al. allows a single query term to be used to identify documents containing the term, but cannot rank the identified documents containing the single query term according to the relevance of the documents to the contexts of the single query term within each document. </paragraph>
<paragraph id="P-0009" lvl="0"><number>&lsqb;0009&rsqb;</number> Most phrase search and retrieval methods that currently exist, such as Fagan, J. L.: Experiments in automatic phrase indexing for document retrieval: A comparison of syntactic and non-syntactic methods. Ph.D. thesis TR87-868, Department of Computer Science, Cornell University, 1987 (Fagan (1987)); Croft, W. B., Turtle, H. R., and Lewis, D. D.: The use of phrases and structure queries in information retrieval. Proc. ACM SIGIR, 1991, pp. 32-45 (Croft, Turtle, and Lewis (1991)); Gey, F. C., and Chen, A.: Phrase discovery for English and cross-language retrieval at TREC 6. Proc. TREC 6, NIST SP 500-240, 1997, pp. 637-644 (Gey and Chen (1997); Gutwin, C., Paynter, G., Witten, I. H., Nevill-Manning, C., and Frank E.: Improving browsing in digital libraries with keyphrase indexes. TR 98-1, Computer Science Department, University of Saskatchewan, 1998 (Gutwin, Paynter, Witten, Nevill-Manning, and Frank (1998)); Jones, S., and Stavely, M.: Phrasier: A system for interactive document retrieval using keyphrases. Proc. ACM SIGIR, 1999, pp. 160-167 (Jones and Staveley (1999)), and Jing and Croft (1994) all treat query phrases as single terms, and typically rely on lists of key phrases that have been generated at some previous time, to represent each document. This approach allows little flexibility in matching query phrases with similar phrases in the text, and this approach requires that all possible phrases be identified in advance, typically using statistical or &ldquo;natural language processing&rdquo; (NLP) methods. </paragraph>
<paragraph id="P-0010" lvl="0"><number>&lsqb;0010&rsqb;</number> NLP phrase search methods are subject to problems such as mistagging, as described by Fagan (1987). Statistical phrase search methods, such as in Turpin, A., and Moffat, A.: Statistical phrases for vector-space information retrieval. Proc. ACM SIGIR, 1999, pp. 309-310 (Turpin and Moffat (1999)), depend on phrase frequency, and therefore are ineffective in searching for most phrases because most phrases occur infrequently. Croft, Turtle, and Lewis (1991) also dismisses the concept of implicitly representing phrases as term associations. Further, the pair-wise association metric of Croft, Turtle, and Lewis (1991) does not include or suggest a measurement of degree or direction of word proximity. Instead, the association method of Croft, Turtle, and Lewis (1991) uses entire documents as the contextual scope, and considers any two words that occur in the same document as being related to the same extent that any other pair of words in the document are related. </paragraph>
<paragraph id="P-0011" lvl="0"><number>&lsqb;0011&rsqb;</number> There are several methods of displaying phrases contained in collections of text as a way to assist a user in domain analysis or query formulation and refinement. Known methods such as Godby, C. J.: Two techniques for the identification of phrases in full text. Annual Review of OCLC Research. Online Computer Library Center, Dublin, Ohio, 1994 (Godby (1994)); Normore, L., Bendig, M., and Godby, C. J.: WordView: Understanding words in context. Proc. Intell. User Interf., 1999, pp. 194 (Normore, Bendig, and Godby (1999)); Zamir, E., and Etzioni, E.: Grouper: A dynamic clustering interface to web search results. Proc. 8<highlight><superscript>th </superscript></highlight>International World Wide Web Conference (WWW8), 1999 (Zamir and Etzioni, (1999)); Gutwin, Paynter, Witten, Nevill-Manning, and Frank (1998); and Jones and Staveley (1999), maintain explicit and incomplete lists of phrases. Some phrase generation methods such as Church, K., Gale, W., Hanks, P., and Hindle, D.: Using statistics in lexical analysis. In U. Zemik (ed.), Lexical Acquisition: Using On-Line Resources To Build A Lexicon. Lawrence Earlbaum, Hillsdale, N.J., 1991 (Church, Gale, Hanks, and Hindle (1991)); Gey and Chen (1997); and Godby (1994), use contextual association to identify important word pairs, but do not identify longer phrases, or do not use the same associative method to identify phrases having more than two words. Some known methods such as Gelbart, D., and Smith, J. C.: Beyond boolean search: FLEXICON, a legal text-based intelligent system. Proc. ACM Artificial Intelligence &amp; Law, 1991, pp. 225-234 (Gelbart and Smith (1991)); Gutwin, Paynter, Witten, Nevill-Manning, and Frank (1998); and Jones and Staveley (1999) rely on manual identification of phrases at a critical point in the process. </paragraph>
<paragraph id="P-0012" lvl="0"><number>&lsqb;0012&rsqb;</number> The &ldquo;natural language processing&rdquo; (NLP) methods such as Godby (1994); Jing and Croft (1994); Gutwin, Paynter, Witten, Nevill-Manning, and Frank (1998); Jones and Staveley (1999); and de Lima, E. F., and Pedersen, J. O.: Phrase recognition and expansion for short, precision-biased queries based on a query log. Proc. ACM SIGIR, 1999, pp. 145-152 (de Lima and Pedersen (1999)), classify words by part of speech using grammatical taggers and apply a grammar-based set of allowable patterns. These methods typically remove all punctuation and stopwords as a preliminary step, and most then discover only simple or compound nouns leaving all other phrases unrecognizable. </paragraph>
<paragraph id="P-0013" lvl="0"><number>&lsqb;0013&rsqb;</number> Keyphind and Phrasier methods of Gutwin, Paynter, Witten, Nevill-Manning, and Frank (1998) and Jones and Staveley (1999), identify some of the phrases in sets of documents that are relevant to initial user queries, and require users to select among the identified phrases to refine subsequent searches. Keyphind and Phrasier then rely on Natural Language Processing (NLP) methods of grammatical tagging and require pre-existing lists of identifiable phrases. In addition, Keyphind and Phrasier apply very restrictive limits on usable phrases, which significantly reduces the number and types of phrases that can be identified in documents. Keyphind and Phrasier&apos;s methods restrict the amount of phrase information available for determinations of document relevance. </paragraph>
</section>
<section>
<heading lvl="1">SUMMARY OF THE INVENTION </heading>
<paragraph id="P-0014" lvl="0"><number>&lsqb;0014&rsqb;</number> In accordance with one aspect of the present invention, a phrase search is a method of searching a database for subsets of the database that are relevant to an input query. First, a number of relational models of subsets of a database are provided. A query is then input. The query can include one or more sequences of terms. Next, a relational model of the query is created. The relational model of the query is then compared to each one of the relational models of subsets of the database. The identifiers of the relevant subsets are then output. </paragraph>
</section>
</summary-of-invention>
<brief-description-of-drawings>
<section>
<heading lvl="1">BRIEF DESCRIPTION OF THE DRAWINGS </heading>
<paragraph id="P-0015" lvl="0"><number>&lsqb;0015&rsqb;</number> The present invention is illustrated by way of example and not limitation in the figures of the accompanying drawings in which like references indicate similar elements. </paragraph>
<paragraph id="P-0016" lvl="0"><number>&lsqb;0016&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 1</cross-reference> illustrates one embodiment of a process <highlight><bold>100</bold></highlight> of producing a relational model of a database; </paragraph>
<paragraph id="P-0017" lvl="0"><number>&lsqb;0017&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 2</cross-reference> illustrates one embodiment of a process <highlight><bold>200</bold></highlight> to combine a number of relational models of databases to produce one relational model; </paragraph>
<paragraph id="P-0018" lvl="0"><number>&lsqb;0018&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 3</cross-reference> illustrates one embodiment of a process <highlight><bold>300</bold></highlight> to determine a non-directional contextual metric (NDCM) for each one of the term pairs within a context window; </paragraph>
<paragraph id="P-0019" lvl="0"><number>&lsqb;0019&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 4</cross-reference> illustrates one embodiment of a process <highlight><bold>400</bold></highlight> to determine a left contextual metric (LCM) for each one of the term pairs within a context window; </paragraph>
<paragraph id="P-0020" lvl="0"><number>&lsqb;0020&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 5</cross-reference> illustrates one embodiment of a process <highlight><bold>500</bold></highlight> to determine a right contextual metric (RCM) for each one of the term pairs within a context window; </paragraph>
<paragraph id="P-0021" lvl="0"><number>&lsqb;0021&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 6</cross-reference> illustrates one embodiment of a process <highlight><bold>600</bold></highlight> to determine a directional contextual metric (DCM) for each one of the term pairs within a context window; </paragraph>
<paragraph id="P-0022" lvl="0"><number>&lsqb;0022&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 6A</cross-reference> shows one embodiment of a relational model represented in a network model diagram; </paragraph>
<paragraph id="P-0023" lvl="0"><number>&lsqb;0023&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 7</cross-reference> illustrates one embodiment of an overview of a keyterm search process; </paragraph>
<paragraph id="P-0024" lvl="0"><number>&lsqb;0024&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 8</cross-reference> illustrates one embodiment of expanding the query; </paragraph>
<paragraph id="P-0025" lvl="0"><number>&lsqb;0025&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 9</cross-reference> illustrates one process of reducing the number of matching relations to a number of unique relations; </paragraph>
<paragraph id="P-0026" lvl="0"><number>&lsqb;0026&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 10</cross-reference> illustrates one embodiment of a process of comparing a relational model of the query to each one of the relational models of subsets; </paragraph>
<paragraph id="P-0027" lvl="0"><number>&lsqb;0027&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 11</cross-reference> illustrates an overview of one embodiment of the phrase search process; </paragraph>
<paragraph id="P-0028" lvl="0"><number>&lsqb;0028&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 12</cross-reference> shows one process where the query includes a number of query fields; </paragraph>
<paragraph id="P-0029" lvl="0"><number>&lsqb;0029&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 13</cross-reference> illustrates a method of combining the query field models; </paragraph>
<paragraph id="P-0030" lvl="0"><number>&lsqb;0030&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 14</cross-reference> illustrates one embodiment of comparing a query model to each one of the relational models of subsets; </paragraph>
<paragraph id="P-0031" lvl="0"><number>&lsqb;0031&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 15</cross-reference> illustrates one embodiment of a process of re-weighting a query model; </paragraph>
<paragraph id="P-0032" lvl="0"><number>&lsqb;0032&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 16</cross-reference> shows one embodiment of generating phrases from a database of text; </paragraph>
<paragraph id="P-0033" lvl="0"><number>&lsqb;0033&rsqb;</number> <cross-reference target="DRAWINGS">FIGS. 17 and 17</cross-reference>A illustrate a process of determining the phrases, which are contextually related to the query, from the model of the database such as in block <highlight><bold>1608</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 16</cross-reference>; </paragraph>
<paragraph id="P-0034" lvl="0"><number>&lsqb;0034&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 18</cross-reference> illustrates one method of updating the conditional list of phrases; </paragraph>
<paragraph id="P-0035" lvl="0"><number>&lsqb;0035&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 19</cross-reference> shows one embodiment of phrase discovery; </paragraph>
<paragraph id="P-0036" lvl="0"><number>&lsqb;0036&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 20</cross-reference> shows an overview of one embodiment of the phrase extraction process; </paragraph>
<paragraph id="P-0037" lvl="0"><number>&lsqb;0037&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 20A</cross-reference> illustrates one embodiment of the phrase starting positions process; </paragraph>
<paragraph id="P-0038" lvl="0"><number>&lsqb;0038&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 20B</cross-reference> illustrates one embodiment of saving single term phrases; </paragraph>
<paragraph id="P-0039" lvl="0"><number>&lsqb;0039&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 20C</cross-reference> shows one embodiment of saving a phrase by combining the current phrase into the phrase list; </paragraph>
<paragraph id="P-0040" lvl="0"><number>&lsqb;0040&rsqb;</number> <cross-reference target="DRAWINGS">FIGS. 20D and 20E</cross-reference> illustrate two embodiments of extracting selected multiterm phrases at each starting position; </paragraph>
<paragraph id="P-0041" lvl="0"><number>&lsqb;0041&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 21</cross-reference> illustrates one embodiment of culling the extracted phrases; </paragraph>
<paragraph id="P-0042" lvl="0"><number>&lsqb;0042&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 22</cross-reference> illustrates one embodiment of gathering related phrases; </paragraph>
<paragraph id="P-0043" lvl="0"><number>&lsqb;0043&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 22A</cross-reference> illustrates one embodiment of ranking the phrases output from the extracting and culling processes; </paragraph>
<paragraph id="P-0044" lvl="0"><number>&lsqb;0044&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 22B</cross-reference> illustrates one embodiment of ranking the selected phrases; </paragraph>
<paragraph id="P-0045" lvl="0"><number>&lsqb;0045&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 22C</cross-reference> illustrates one embodiment of a process of emphasizing the locally relevant relations and de-emphasizing the globally relevant relations; </paragraph>
<paragraph id="P-0046" lvl="0"><number>&lsqb;0046&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 22D</cross-reference> illustrates one embodiment of emphasizing the locally relevant phrases and de-emphasizing the globally relevant phrases; and </paragraph>
<paragraph id="P-0047" lvl="0"><number>&lsqb;0047&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 23</cross-reference> shows a high-level block diagram of a computer system. </paragraph>
</section>
</brief-description-of-drawings>
<detailed-description>
<section>
<heading lvl="1">DETAILED DESCRIPTION </heading>
<paragraph id="P-0048" lvl="0"><number>&lsqb;0048&rsqb;</number> As will be described in more detail below, various methods of searching and extracting information from a database are described. The first described method is a method of contextually analyzing and modeling a database. The second described method is a method a searching a model of a database for subsets of the database that are relevant to a keyterm. The third described method is a method a searching a model of a database for subsets of the database that are relevant to a phrase. The fourth method described is a method of generating a list of phrases from a model of a database. The fifth described method is a method of discovering phrases in a database. Additional, alternative embodiments are also described. </paragraph>
</section>
<section>
<heading lvl="1">Modeling a Database </heading>
<paragraph id="P-0049" lvl="0"><number>&lsqb;0049&rsqb;</number> A method and apparatus for contextually analyzing and modeling a database is disclosed. The database and/or a model of the database can also be searched, compared and portions extracted therefrom. For one embodiment, contextual analysis converts bodies of data, such as a database or a subset of a database, into a number of contextual associations or relations. The value of each contextual relation can be expressed as a metric value. Further, metric values can also include a directional metric value or indication. </paragraph>
<paragraph id="P-0050" lvl="0"><number>&lsqb;0050&rsqb;</number> For one embodiment, the contextual associations of a term provide contextual meaning of the term. For example, the term &ldquo;fatigue&rdquo; can refer to human physical tiredness such as &ldquo;Fatigue impaired the person&apos;s judgment.&rdquo; Or &ldquo;fatigue&rdquo; can refer to breakdown of the structure of a material such as &ldquo;Metal fatigue caused the aluminum coupling to break.&rdquo; A first aggregation of associations between term pairs such as: &ldquo;fatigue&rdquo; and &ldquo;person&rdquo;, &ldquo;fatigue&rdquo; and &ldquo;impaired&rdquo;, and &ldquo;fatigue&rdquo; and &ldquo;judgment&rdquo; can be clearly differentiated from a second aggregation of associations such as &ldquo;metal&rdquo; and &ldquo;fatigue&rdquo;, &ldquo;fatigue&rdquo; and &ldquo;aluminum&rdquo;, &ldquo;fatigue&rdquo; and &ldquo;coupling&rdquo;, and &ldquo;fatigue&rdquo; and &ldquo;break&rdquo;. Thus, when searching a database of subsets for subsets containing the notion of &ldquo;fatigue&rdquo; in the sense of human physical tiredness, subsets having greater similarity to the first aggregation of associations are more likely to include the appropriate sense of &ldquo;fatigue&rdquo;, so these subsets would be retrieved. Further, the contextual associations found in the retrieved subsets can both refine and extend the contextual meaning of the term &ldquo;fatigue&rdquo;. </paragraph>
<paragraph id="P-0051" lvl="0"><number>&lsqb;0051&rsqb;</number> The database to be modeled can include text and the examples presented below use text to more clearly illustrate the invention. Other types of data could also be equivalently used in alternative embodiments. Some examples of the types of data contemplated include but are not limited to: text (e.g. narratives, reports, literature, punctuation, messages, electronic mail, internet text, and web site information); linguistic patterns; grammatical tags; alphabetic, numeric, and alphanumeric data and strings; sound, music, voice, audio data, audio encoding, and vocal encoding; biological and medical information, data, representations, sequences, and patterns; genetic sequences, representations, and analogs; protein sequences, presentations, and analogs; computer software, hardware, firmware, input, internal information, output, and their representations and analogs; and patterned or sequential symbols, data, items, objects, events, causes, time spans, actions, attributes, entities, relations, and representations. </paragraph>
<paragraph id="P-0052" lvl="0"><number>&lsqb;0052&rsqb;</number> Modeling a database can also include representing the database as a collection or list of contextual relations, wherein each relation is an association of two terms, so that each relation includes a term pair. A model can represent any body or database of terms, wherein a term is a specific segment of the data from the database. Using a text database, a term could be a word or a portion of a word such as a syllable. A term in a DNA database for example, could be a particular DNA sequence or segment or a portion thereof. A term in a music database could be one or more notes, rests, chords, key changes, measures, or passages. Examples of databases that could be modeled include a body of terms, such as a collection of one or more narrative documents, or only a single term, or a single phrase. A collection of multiple phrases could also be modeled. In addition, combinations and subdivisions of the above examples could also be modeled as described in more detail below. </paragraph>
<paragraph id="P-0053" lvl="0"><number>&lsqb;0053&rsqb;</number> Relevance ranking a collection of models is a method of quantifying the degree of similarity of a first model (i.e., a criterion model) and each one of the models in the collection, and assigning a rank ordering to the models in the collection according to their degree of similarity to the first model. The same rank ordering can also be assigned, for example, to the collection of identifiers of the models in the collection, or a collection of subsets of a database represented by the models of the collection. The features of the criterion model are compared to the features of each one of the collection of other models. As will be described in more detail below, the features can include the relations and the contextual measurements, i.e. the relational metric values of the relations in the models. The collection of other models is then ranked in order of similarity to the criterion model. As an example: the criterion model is a model of a query. The criterion model is then compared to a number of models of narratives. Then each one of the corresponding narratives is ranked according to the corresponding level of similarity of that narrative&apos;s corresponding model to the criterion model. As another alternative, the criteria model can represent any level of text and combination of text, or data from the database, or combination of segments of sets of databases. </paragraph>
</section>
<section>
<heading lvl="1">Relations and Relational Metrics </heading>
<paragraph id="P-0054" lvl="0"><number>&lsqb;0054&rsqb;</number> A relation includes a pair of terms also referred to as a term pair, and a number of types of relational metrics. The term pair includes a first term and a second term. Each one of the types of relational metrics represents a type of contextual association between the two terms. A relation can be represented in the form of: term1, term2, metric1, metric2, . . . metricN. One example of a relation is: crew, fatigue, 6, 4, . . . 8. </paragraph>
<paragraph id="P-0055" lvl="0"><number>&lsqb;0055&rsqb;</number> A relation can represent different levels of context in the body of text within which the term pair occurs. At one level, the relation can describe the context of one instance or occurrence of the term pair within a database. In another level, a summation relation can represent a summation of all instances of the term pair within a database or within a set of specified subsets of the database. A model of a database is a collection of such summation relations that represent all occurrences of all term pairs that occur within the database being modeled. </paragraph>
<paragraph id="P-0056" lvl="0"><number>&lsqb;0056&rsqb;</number> For one embodiment, a term from a database is selected and the contextual relationship between the selected term and every other term in the database can be determined. For example, given a database of 100 terms, the first term is selected and then paired with each of the other 99 terms in the database. For each of the 99 term pairs the metrics are calculated. This results in 99 relations. Then the second term is selected and paired with each of the other 99 terms and so forth. The process continues until each one of the 100 terms in the database has been selected, paired with each one of the other 99 terms and the corresponding metric values calculated. As the database grows larger, the number of relations created in this embodiment also grows exponentially larger. As the number of terms separating the selected term from the paired term increases, the relationship between the terms becomes less and less significant. In one alternative, if a term is one of a group of terms to be excluded, then no relations containing the term are determined. </paragraph>
<paragraph id="P-0057" lvl="0"><number>&lsqb;0057&rsqb;</number> The contextual analysis can be conducted within a sliding window referred to as a context window. The context window selects and analyzes one context window-sized portion of the database at a time and then the context window is incremented, term-by-term, through the database to analyze all of the term pairs in the database. For example, in a 100-term database, using a 10-term context window, the context window is initially applied to the first 10 terms, terms 1-10. The relations between each one of the terms and the other 9 terms in the context window are determined. Then, the context window is shifted one term to encompass terms 2-11 of the database and the relations between each one of the terms and the other 9 terms in the context window are determined. The process continues until the entire database has been analyzed. A smaller context window captures the more local associations among terms. A larger context window captures more global associations among terms. The context window can be centered on a selected term. In one alternative, redundant relations can be eliminated by including only a single relation between a term in one position within the database and another term in another position in the database. </paragraph>
<paragraph id="P-0058" lvl="0"><number>&lsqb;0058&rsqb;</number> In one embodiment of contextual analysis, a term in the sequence of terms in a database or subset of a database is selected. Relations are determined between the selected term and each of the other terms in a left context window associated with the selected term, and relations are also determined between the selected term and each of the terms in a right context window associated with the selected term. In one alternative, the left context window can contain L terms and the right context window can contain R terms. In another alternative, each context window can contain C terms, that is, L&equals;R&equals;C. A left context window of size C can include the selected term, up to C&minus;1 of the terms that precede the selected term, and no terms that follow the selected term. A right context window of size C can include the selected term, and up to C&minus;1 of the terms that follow the selected term, and no terms that precede the selected term. A context window of size C can include fewer than C terms if the selected term is at or near the beginning or end of the sequence of terms. For example, if the selected term is the 6<highlight><superscript>th </superscript></highlight>term in a sequence, then only 5 terms precede the selected term, and if the left context window is of size C&equals;10, only 6 terms, the selected term and the 5 terms that precede the selected term, appear in the left context window. In a similar example, if the selected term is the 95<highlight><superscript>th </superscript></highlight>term in a sequence of 100 terms, then only 5 terms follow the selected term, and if the right context window is of size C&equals;10, only 6 terms, the selected term and the 5 terms that follow the selected term, appear in the right context window. After relations are determined for a selected term, a subsequent term can be selected from the terms that have not yet been selected from the sequence of terms, and relations can be determined for the new selected term as described above. The process can continue until all terms in the sequence of terms have been selected, and all relations have been determined for the selected terms. Alternatively, the process can continue until all of the terms in the sequence of terms that are also in a collection of terms of interest have been selected, and all relations have been determined for the selected terms. In one alternative, redundant relations can be eliminated by including only a single relation between a term in one position within the database and a term in another position within the database. </paragraph>
<paragraph id="P-0059" lvl="0"><number>&lsqb;0059&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 1</cross-reference> illustrates one embodiment of a process <highlight><bold>100</bold></highlight> of producing a relational model of a database. A database to be modeled is provided in process block <highlight><bold>102</bold></highlight>. A context window is selected in block <highlight><bold>104</bold></highlight>. Alternatively, the size of the context window can be varied. The size of the context window can be manually selected. The context window can automatically adjust to an average size of a portion of the database being modeled. For example, the portion could be a sentence, a phrase, a paragraph or any other subset of the database. The size of the context window can vary as a function of the data being scanned. </paragraph>
<paragraph id="P-0060" lvl="0"><number>&lsqb;0060&rsqb;</number> A first term from the database is selected in block <highlight><bold>106</bold></highlight>. Several relations are determined in block <highlight><bold>108</bold></highlight>. Each relation includes a number of types of contextual metrics between the selected term and each one of the terms included in the context window. Various processes to determine various types of contextual metrics are described more fully below. Next, a subsequent term is selected in blocks <highlight><bold>110</bold></highlight>, <highlight><bold>112</bold></highlight> and the relations that include the new selected term are determined. </paragraph>
<paragraph id="P-0061" lvl="0"><number>&lsqb;0061&rsqb;</number> When the relations including the last term from the database have been determined, there are no subsequent terms so the collected relations are summarized. A first relation having a selected term pair is selected in block <highlight><bold>114</bold></highlight>. All other instances of the relations having the selected term pair are then summarized into a summation relation in block <highlight><bold>116</bold></highlight>. The summation relation includes the term pair and a number of types relational summation metrics (RSMs). Each one of the types of RSMs includes a summation of the corresponding types of metrics of each instance of the term pair. The RSM can be a sum of the corresponding types of metrics of each instance of the term pair. Alternatively, the RSM can be a normalized sum of the corresponding types of metrics of each instance of the term pair. For another alternative, the RSM can be a scaled sum of the corresponding types of metrics of each instance of the term pair. The RSM can also be equal to the metric value of one type of contextual metric for the one instance of the term pair that has the highest magnitude of the selected type of contextual metric, of all instances of the term pair. Other methods of producing a summation metric of the corresponding types of metrics of each instance of the term pair as known to one skilled in the art are also contemplated as various additional embodiments. </paragraph>
<paragraph id="P-0062" lvl="0"><number>&lsqb;0062&rsqb;</number> The summation relation is then included in a relational model of the database in block <highlight><bold>118</bold></highlight>. The process of summarizing relations continues in blocks <highlight><bold>120</bold></highlight>, <highlight><bold>122</bold></highlight>, until a last relation is summarized and then the relational model of the database is output at block <highlight><bold>124</bold></highlight>. The relational model of the database can be output in the form of a list of relations, or a sorted list of relations or, one of the types of RSMs can be selected and the relations sorted in the order of the selected RSM. Alternatively, the summation relations can be accumulated, as each instance of a relation is determined. </paragraph>
<paragraph id="P-0063" lvl="0"><number>&lsqb;0063&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 2</cross-reference> illustrates one embodiment of a process <highlight><bold>200</bold></highlight> to combine a number of relational models of databases to produce one relational model. <cross-reference target="DRAWINGS">FIG. 2</cross-reference> illustrates combining a first relational model of a first database and a second relational model of a second database in block <highlight><bold>202</bold></highlight> but additional models can be easily combined through a similar process or through iterative use of the process <highlight><bold>200</bold></highlight>. A first summation relation from the first relational model is selected in block <highlight><bold>204</bold></highlight>. A combined summation relation including the term pair from the selected summation relation is then determined by reviewing each of the relations in the second relational model that include the term pair from the selected relation in block <highlight><bold>206</bold></highlight>. The combined summation relation is determined as described above in <cross-reference target="DRAWINGS">FIG. 1</cross-reference>. The combined summation relation is then included in the combined relational model. The process continues through each one of the summation relations in the first model in blocks <highlight><bold>210</bold></highlight>, <highlight><bold>212</bold></highlight>. Then, each one of the summation relations in the second relational model that contain term pairs that are not included the first relational model are then included in the combined relational model in blocks <highlight><bold>214</bold></highlight>, <highlight><bold>216</bold></highlight>. The combined relational model is then output at block <highlight><bold>218</bold></highlight>. </paragraph>
<paragraph id="P-0064" lvl="0"><number>&lsqb;0064&rsqb;</number> Various types of relational metrics are contemplated. Some examples of the types of relational metrics are described in more detail below. The examples described are merely illustrative of the types of relational metrics contemplated and should not be read as exhaustive or limited to the examples described. One of the types of relational metrics is a standard relational metric, also referred to as a non-directional contextual metric (NDCM). Another type of relational metric is a left contextual metric (LCM). Another type of relational metric is a right contextual metric (RCM). Yet another type of relational metric is a directional contextual metric (DCM). Still another type of relational metric is a scaled frequency metric (SFM). Each of the above-described metrics is more fully described below. Additional types of relational metrics are also contemplated and one skilled in the art could conceive of several additional contextual metrics that could be also used as described below. </paragraph>
<paragraph id="P-0065" lvl="0"><number>&lsqb;0065&rsqb;</number> A relation with a term pair and multiple types of contextual metrics can be presented in any form. One form of expressing such a relation is the term pair followed by a list of the contextual metric values. Examples include: term1, term2, NDCM, or term1, term2, NDCM, LCM, RCM, or term1, term2, NDCM, DCM, SFM, or term1, term2, NDCM, LCM, RCM . . . &ldquo;Nth&rdquo; contextual metric. </paragraph>
</section>
<section>
<heading lvl="1">Calculating Metric Values </heading>
<paragraph id="P-0066" lvl="0"><number>&lsqb;0066&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 3</cross-reference> illustrates one embodiment of a process <highlight><bold>300</bold></highlight> to determine a non-directional contextual metric (NDCM) for each one of the term pairs within a context window. First, a starting term T<highlight><bold>1</bold></highlight> is selected and identified in block <highlight><bold>302</bold></highlight>. A first term in the context window is identified as T<highlight><bold>2</bold></highlight> in block <highlight><bold>304</bold></highlight>. An NDCM is then determined in block <highlight><bold>306</bold></highlight>. The NDCM&equals;C&minus;1&minus;N, where C is equal to a number of terms in the context window, and N is equal to a number of terms occurring between a first term and a second term of the term pair. The relation containing the term pair T<highlight><bold>1</bold></highlight>, T<highlight><bold>2</bold></highlight> and the NDCM is then output in block <highlight><bold>308</bold></highlight>. The process <highlight><bold>300</bold></highlight> continues to determine NDCMs for each of the remaining term pairs whose first terms occur within the context window and that start with T<highlight><bold>1</bold></highlight>, in blocks <highlight><bold>310</bold></highlight>, <highlight><bold>312</bold></highlight>. For example, the non-directional contextual metric of a term pair (A, B) is measured with respect to the number N of terms that occur between the terms A and B. If terms A and B are immediately adjacent, no terms are between A and B and therefore N&equals;0 and the NDCM is equal to C&minus;1&minus;0. </paragraph>
<paragraph id="P-0067" lvl="0"><number>&lsqb;0067&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 4</cross-reference> illustrates one embodiment of a process <highlight><bold>400</bold></highlight> to determine a left contextual metric (LCM) for each one of the term pairs within a context window. First a starting term T<highlight><bold>1</bold></highlight> is selected and identified in block <highlight><bold>402</bold></highlight>. A first term in the context window is identified as T<highlight><bold>2</bold></highlight> in block <highlight><bold>404</bold></highlight>. A LCM is then determined in block <highlight><bold>406</bold></highlight>. The LCM value associated with a particular occurrence of a term pair (T<highlight><bold>1</bold></highlight>, T<highlight><bold>2</bold></highlight>) in a subset is LCM(T<highlight><bold>1</bold></highlight>, T<highlight><bold>2</bold></highlight>). If T<highlight><bold>2</bold></highlight> follows T<highlight><bold>1</bold></highlight> in a subset, then LCM(T<highlight><bold>1</bold></highlight>, T<highlight><bold>2</bold></highlight>) is equal to 0. If T<highlight><bold>2</bold></highlight> precedes T<highlight><bold>1</bold></highlight> in the subset, then LCM(T<highlight><bold>1</bold></highlight>, T<highlight><bold>2</bold></highlight>) is equal to C&minus;1&minus;N, where C is equal to a number of terms in the context window, and N is equal to a number of terms occurring between T<highlight><bold>1</bold></highlight> and T<highlight><bold>2</bold></highlight>. The relation containing the term pair T<highlight><bold>1</bold></highlight>, T<highlight><bold>2</bold></highlight> and the LCM is then output in block <highlight><bold>408</bold></highlight>. The process <highlight><bold>400</bold></highlight> continues to determine LCMs for each of the remaining term pairs in the context window that start with T<highlight><bold>1</bold></highlight> in blocks <highlight><bold>410</bold></highlight>, <highlight><bold>412</bold></highlight>. If, for example, the terms T<highlight><bold>1</bold></highlight> and T<highlight><bold>2</bold></highlight> occur in the order of T<highlight><bold>2</bold></highlight> followed by T<highlight><bold>1</bold></highlight> and T<highlight><bold>2</bold></highlight> occurs 3 terms to the left of T<highlight><bold>1</bold></highlight>, and a context window is 8, then the LCM(T<highlight><bold>1</bold></highlight>, T<highlight><bold>2</bold></highlight>) would be C&minus;1&minus;N&equals;8&minus;1&minus;2&equals;5. For another example, if terms T<highlight><bold>1</bold></highlight> and T<highlight><bold>2</bold></highlight> occur in the order of T<highlight><bold>1</bold></highlight> and then T<highlight><bold>2</bold></highlight> and a context window is 8, then T<highlight><bold>2</bold></highlight> occurs to the right of T<highlight><bold>1</bold></highlight>, then the LCM(T<highlight><bold>1</bold></highlight>, T<highlight><bold>2</bold></highlight>) is equal to zero since LCM(T<highlight><bold>1</bold></highlight>, T<highlight><bold>2</bold></highlight>) is zero for all occurrences of T<highlight><bold>2</bold></highlight> that follow this occurrence of T<highlight><bold>1</bold></highlight> within the context window. </paragraph>
<paragraph id="P-0068" lvl="0"><number>&lsqb;0068&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 5</cross-reference> illustrates one embodiment of a process <highlight><bold>500</bold></highlight> to determine a right contextual metric (RCM) for each one of the term pairs within a context window. First a starting term T<highlight><bold>1</bold></highlight> is selected and identified in block <highlight><bold>502</bold></highlight>. A first term in the context window is identified as T<highlight><bold>2</bold></highlight> in block <highlight><bold>504</bold></highlight>. An RCM is then determined in block <highlight><bold>506</bold></highlight>. The RCM value associated with a particular occurrence of a term pair (T<highlight><bold>1</bold></highlight>, T<highlight><bold>2</bold></highlight>) in a subset is RCM(T<highlight><bold>1</bold></highlight>, T<highlight><bold>2</bold></highlight>). If T<highlight><bold>2</bold></highlight> precedes T<highlight><bold>1</bold></highlight> in the subset, then RCM(T<highlight><bold>1</bold></highlight>, T<highlight><bold>2</bold></highlight>)&equals;0. If T<highlight><bold>2</bold></highlight> follows T<highlight><bold>1</bold></highlight> in the subset, then RCM(T<highlight><bold>1</bold></highlight>, T<highlight><bold>2</bold></highlight>) is equal to C&minus;1&minus;N, where C is equal to a number of terms in the context window, and N is equal to a number of terms occurring between T<highlight><bold>1</bold></highlight> and T<highlight><bold>2</bold></highlight>. The relation containing the term pair T<highlight><bold>1</bold></highlight>, T<highlight><bold>2</bold></highlight> and the RCM is then output in block <highlight><bold>508</bold></highlight>. The process <highlight><bold>500</bold></highlight> continues to determine RCMs for each of the remaining term pairs in the context window that start with T<highlight><bold>1</bold></highlight> in blocks <highlight><bold>510</bold></highlight>, <highlight><bold>512</bold></highlight>. If, for example the terms T<highlight><bold>1</bold></highlight> and T<highlight><bold>2</bold></highlight> occur in the order of T<highlight><bold>1</bold></highlight> and then T<highlight><bold>2</bold></highlight>, and T<highlight><bold>2</bold></highlight> occurs 3 terms to the right of T<highlight><bold>1</bold></highlight>, and a context window is 8, then the RCM(T<highlight><bold>1</bold></highlight>, T<highlight><bold>2</bold></highlight>) would be C&minus;1&minus;N&equals;8&minus;1&minus;2&equals;5. For another example, if the terms T<highlight><bold>1</bold></highlight> and T<highlight><bold>2</bold></highlight> occur in the order of T<highlight><bold>2</bold></highlight> and then T<highlight><bold>1</bold></highlight> and a context window is 8, then the RCM(T<highlight><bold>1</bold></highlight>, T<highlight><bold>2</bold></highlight>) is equal to 0, because the RCM(T<highlight><bold>1</bold></highlight>, T<highlight><bold>2</bold></highlight>) is zero for all occurrences of T<highlight><bold>2</bold></highlight> that precede this occurrence of T<highlight><bold>1</bold></highlight> in the context window. </paragraph>
<paragraph id="P-0069" lvl="0"><number>&lsqb;0069&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 6</cross-reference> illustrates one embodiment of a process <highlight><bold>600</bold></highlight> to determine a directional contextual metric (DCM) for each one of the term pairs within a context window. First a starting term T<highlight><bold>1</bold></highlight> is selected and identified in block <highlight><bold>602</bold></highlight>. A first term in the context window is identified as T<highlight><bold>2</bold></highlight> in block <highlight><bold>604</bold></highlight>. A DCM is then determined in block <highlight><bold>606</bold></highlight>. The DCM(T<highlight><bold>1</bold></highlight>, T<highlight><bold>2</bold></highlight>) is equal to RCM(T<highlight><bold>1</bold></highlight>, T<highlight><bold>2</bold></highlight>)&minus;LCM(T<highlight><bold>1</bold></highlight>, T<highlight><bold>2</bold></highlight>) and is applied to relations whose terms are ordered to ensure that RCM is greater than or equal to LCM. Alternatively, DCMs of less than zero can be accommodated. The relation containing the term pair T<highlight><bold>1</bold></highlight>, T<highlight><bold>2</bold></highlight> and the DCM is then output in block <highlight><bold>608</bold></highlight>. The process <highlight><bold>600</bold></highlight> continues to determine DCMs for each of the remaining term pairs in the context window that start with T<highlight><bold>1</bold></highlight> in blocks <highlight><bold>610</bold></highlight>, <highlight><bold>612</bold></highlight>. </paragraph>
<paragraph id="P-0070" lvl="0"><number>&lsqb;0070&rsqb;</number> The scaled frequency metric (SFM) is equal to (C&minus;1&minus;N)*&lcub;(2F<highlight><subscript>M</subscript></highlight>&minus;F<highlight><subscript>1</subscript></highlight>&minus;F<highlight><subscript>2</subscript></highlight>)/2F<highlight><subscript>M</subscript></highlight>&rcub;. C is equal to the number of terms in the context window. N is equal to the number of terms occurring between a first term and a second term of the term pair. F<highlight><subscript>M </subscript></highlight>is equal to a frequency of occurrences of a most frequent term in the database. F<highlight><subscript>1 </subscript></highlight>is equal to a frequency of occurrences of a first term of the term pair in the database. F<highlight><subscript>2 </subscript></highlight>is equal to a frequency of occurrences of a second term of the term pair in the database. </paragraph>
<paragraph id="P-0071" lvl="0"><number>&lsqb;0071&rsqb;</number> In the following example sentence, which contains one instance of the term ENGLISH followed by one instance of the term PHRASEOLOGY, the term PHRASEOLOGY is in the right context of the term ENGLISH, and the term ENGLISH is in the left context of the term PHRASEOLOGY. </paragraph>
<paragraph id="P-0072" lvl="0"><number>&lsqb;0072&rsqb;</number> BETTER ENGLISH SPEAKING FOREIGN CTLRS AND USE OF STD PHRASEOLOGY IS NEEDED. </paragraph>
<paragraph id="P-0073" lvl="0"><number>&lsqb;0073&rsqb;</number> Using a context window (C) equal to 10 terms, treating the sentence as the entire database, and observing that there are N&equals;7 terms between ENGLISH and PHRASEOLOGY, the corresponding metrics have the following values: </paragraph>
<paragraph id="P-0074" lvl="0"><number>&lsqb;0074&rsqb;</number> The NDCM(ENGLISH, PHRASEOLOGY), or the measure of the extent that ENGLISH and PHRASEOLOGY are in the same context, is equal to: </paragraph>
<paragraph lvl="0"><in-line-formula><highlight><italic>C&minus;</italic></highlight>1<highlight><italic>&minus;N&equals;</italic></highlight>10&minus;1&minus;7&equals;2 &emsp;&emsp;Equation 1 </in-line-formula></paragraph>
<paragraph id="P-0075" lvl="0"><number>&lsqb;0075&rsqb;</number> The NDCM(ENGLISH, PHRASEOLOGY) is the same as NDCM(PHRASEOLOGY,ENGLISH) since direction does not matter for calculating the NDCM. </paragraph>
<paragraph id="P-0076" lvl="0"><number>&lsqb;0076&rsqb;</number> The RCM(ENGLISH, PHRASEOLOGY), or the measure of the contextual association of ENGLISH followed by PHRASEOLOGY, is equal to: </paragraph>
<paragraph lvl="0"><in-line-formula><highlight><italic>C&minus;</italic></highlight>1<highlight><italic>&minus;N&equals;</italic></highlight>10&minus;1&minus;7&equals;2 &emsp;&emsp;Equation 1.1 </in-line-formula></paragraph>
<paragraph id="P-0077" lvl="0"><number>&lsqb;0077&rsqb;</number> The LCM(ENGLISH, PHRASEOLOGY), or the measure of the contextual association of ENGLISH preceded by PHRASEOLOGY, is equal to 0 since there are no incidences of PHRASEOLOGY which precede an incidence of ENGLISH. </paragraph>
<paragraph id="P-0078" lvl="0"><number>&lsqb;0078&rsqb;</number> The RCM(PHRASEOLOGY, ENGLISH) or the measure of the contextual association of PHRASEOLOGY followed by ENGLISH, is equal to 0 since there are no incidences of ENGLISH which follow an incidence of PHRASEOLOGY. </paragraph>
<paragraph id="P-0079" lvl="0"><number>&lsqb;0079&rsqb;</number> The LCM(PHRASEOLOGY, ENGLISH), the measure of the contextual association of PHRASEOLOGY preceded by ENGLISH, is equal to: </paragraph>
<paragraph lvl="0"><in-line-formula><highlight><italic>C&minus;</italic></highlight>1<highlight><italic>&minus;N&equals;</italic></highlight>10&minus;1&minus;7&equals;2 &emsp;&emsp;Equation 1.2 </in-line-formula></paragraph>
<paragraph id="P-0080" lvl="0"><number>&lsqb;0080&rsqb;</number> The above example describes how to determine the types of contextual metrics for one instance of one term pair in a database of terms. Typically, a single term pair occurs multiple times throughout a database. One embodiment of a summation relation includes a summation of the corresponding types of contextual metrics for each one of several occurrences of a term pair throughout the database. </paragraph>
<paragraph id="P-0081" lvl="0"><number>&lsqb;0081&rsqb;</number> The following is an example of combining multiple relations for the same term pair across all of the shared contexts in a database to determine a single summation relation that represents that term pair in that database. Table 1.1 illustrates three schematic lines of text representing excerpts from a database being modeled, where the items &ldquo;t&rdquo; are terms that are not terms of interest and do not include term A or term B, and the contextual relationship between terms A and B is the relation of interest. No other instances of terms A and B occur within the database.  
<table-cwu id="TABLE-US-00001">
<number>1</number>
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="11">
<colspec colname="1" colwidth="35PT" align="center"/>
<colspec colname="2" colwidth="14PT" align="center"/>
<colspec colname="3" colwidth="21PT" align="center"/>
<colspec colname="4" colwidth="7PT" align="center"/>
<colspec colname="5" colwidth="28PT" align="center"/>
<colspec colname="6" colwidth="14PT" align="center"/>
<colspec colname="7" colwidth="28PT" align="center"/>
<colspec colname="8" colwidth="14PT" align="center"/>
<colspec colname="9" colwidth="21PT" align="center"/>
<colspec colname="10" colwidth="7PT" align="center"/>
<colspec colname="11" colwidth="28PT" align="center"/>
<thead>
<row>
<entry namest="1" nameend="11" align="center">TABLE 1.1</entry>
</row>
<row>
<entry></entry>
</row>
<row><entry namest="1" nameend="11" align="center" rowsep="1"></entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry>1.</entry>
<entry>. . .</entry>
<entry>t</entry>
<entry>t</entry>
<entry>t</entry>
<entry>A</entry>
<entry>B</entry>
<entry>t</entry>
<entry>t</entry>
<entry>t</entry>
<entry>. . .</entry>
</row>
<row>
<entry>2.</entry>
<entry>. . .</entry>
<entry>t</entry>
<entry>t</entry>
<entry>A</entry>
<entry>t</entry>
<entry>B</entry>
<entry>A</entry>
<entry>t</entry>
<entry>t</entry>
<entry>. . .</entry>
</row>
<row>
<entry>3.</entry>
<entry>. . .</entry>
<entry>t</entry>
<entry>t</entry>
<entry>t</entry>
<entry>B</entry>
<entry>B</entry>
<entry>A</entry>
<entry>t</entry>
<entry>t</entry>
<entry>. . .</entry>
</row>
<row><entry namest="1" nameend="11" align="center" rowsep="1"></entry>
</row>
</tbody>
</tgroup>
</table>
</table-cwu>
</paragraph>
<paragraph id="P-0082" lvl="0"><number>&lsqb;0082&rsqb;</number> Table 1.2 illustrates the relations of each instance of the paired terms A and B, using a context window of C&equals;3 terms. The line numbering indicates the line number containing the relation. For example, &ldquo;2.1&rdquo; is the first relation from line 2 above, and &ldquo;2.2&rdquo; is the second relation from that line. Each relation can take either of the two forms, as shown. The forms are equivalent.  
<table-cwu id="TABLE-US-00002">
<number>2</number>
<table frame="none" colsep="0" rowsep="0" pgwide="1">
<tgroup align="left" colsep="0" rowsep="0" cols="12">
<colspec colname="OFFSET" colwidth="21PT" align="left"/>
<colspec colname="1" colwidth="28PT" align="center"/>
<colspec colname="2" colwidth="28PT" align="center"/>
<colspec colname="3" colwidth="28PT" align="center"/>
<colspec colname="4" colwidth="21PT" align="center"/>
<colspec colname="5" colwidth="21PT" align="center"/>
<colspec colname="6" colwidth="28PT" align="center"/>
<colspec colname="7" colwidth="28PT" align="center"/>
<colspec colname="8" colwidth="35PT" align="center"/>
<colspec colname="9" colwidth="28PT" align="center"/>
<colspec colname="10" colwidth="21PT" align="center"/>
<colspec colname="11" colwidth="21PT" align="center"/>
<thead>
<row>
<entry></entry>
<entry namest="OFFSET" nameend="11" align="center">TABLE 1.2</entry>
</row>
<row>
<entry></entry>
<entry></entry>
</row>
<row>
<entry></entry>
<entry namest="OFFSET" nameend="11" align="center" rowsep="1"></entry>
</row>
<row>
<entry></entry>
<entry>term_1</entry>
<entry>term_2</entry>
<entry>NDCM</entry>
<entry>LCM</entry>
<entry>RCM</entry>
<entry></entry>
<entry>term_1</entry>
<entry>Term_2</entry>
<entry>NDCM</entry>
<entry>LCM</entry>
<entry>RCM</entry>
</row>
<row>
<entry></entry>
<entry namest="OFFSET" nameend="11" align="center" rowsep="1"></entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry></entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="12">
<colspec colname="1" colwidth="21PT" align="center"/>
<colspec colname="2" colwidth="28PT" align="center"/>
<colspec colname="3" colwidth="28PT" align="center"/>
<colspec colname="4" colwidth="28PT" align="center"/>
<colspec colname="5" colwidth="21PT" align="center"/>
<colspec colname="6" colwidth="21PT" align="center"/>
<colspec colname="7" colwidth="28PT" align="center"/>
<colspec colname="8" colwidth="28PT" align="center"/>
<colspec colname="9" colwidth="35PT" align="center"/>
<colspec colname="10" colwidth="28PT" align="center"/>
<colspec colname="11" colwidth="21PT" align="center"/>
<colspec colname="12" colwidth="21PT" align="center"/>
<tbody valign="top">
<row>
<entry>1.0.</entry>
<entry>A</entry>
<entry>B</entry>
<entry>2</entry>
<entry>0</entry>
<entry>2</entry>
<entry>same as</entry>
<entry>B</entry>
<entry>A</entry>
<entry>2</entry>
<entry>2</entry>
<entry>0</entry>
</row>
<row>
<entry>2.1.</entry>
<entry>A</entry>
<entry>B</entry>
<entry>1</entry>
<entry>0</entry>
<entry>1</entry>
<entry>same as</entry>
<entry>B</entry>
<entry>A</entry>
<entry>1</entry>
<entry>1</entry>
<entry>0</entry>
</row>
<row>
<entry>2.2.</entry>
<entry>A</entry>
<entry>B</entry>
<entry>2</entry>
<entry>2</entry>
<entry>0</entry>
<entry>same as</entry>
<entry>B</entry>
<entry>A</entry>
<entry>2</entry>
<entry>0</entry>
<entry>2</entry>
</row>
<row>
<entry>3.1.</entry>
<entry>A</entry>
<entry>B</entry>
<entry>1</entry>
<entry>1</entry>
<entry>0</entry>
<entry>same as</entry>
<entry>B</entry>
<entry>A</entry>
<entry>1</entry>
<entry>0</entry>
<entry>1</entry>
</row>
<row>
<entry>3.2.</entry>
<entry>A</entry>
<entry>B</entry>
<entry>2</entry>
<entry>2</entry>
<entry>0</entry>
<entry>same as</entry>
<entry>B</entry>
<entry>A</entry>
<entry>2</entry>
<entry>0</entry>
<entry>2</entry>
</row>
<row>
<entry>RSM</entry>
<entry></entry>
<entry></entry>
<entry>8</entry>
<entry>5</entry>
<entry>3</entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry>8</entry>
<entry>3</entry>
<entry>5</entry>
</row>
<row><entry namest="1" nameend="12" align="center" rowsep="1"></entry>
</row>
</tbody>
</tgroup>
</table>
</table-cwu>
</paragraph>
<paragraph id="P-0083" lvl="0"><number>&lsqb;0083&rsqb;</number> If lines 1-3 were the only lines in the database containing terms A and B, the above relations would be summed to produce a summation relation (RS) having relational summation metrics (RSMs) representing the overall contextual association of terms A and B in the database. The summation relation can be expressed in either one of two equivalent forms shown in Table 1.3:  
<table-cwu id="TABLE-US-00003">
<number>3</number>
<table frame="none" colsep="0" rowsep="0" pgwide="1">
<tgroup align="left" colsep="0" rowsep="0" cols="12">
<colspec colname="OFFSET" colwidth="14PT" align="left"/>
<colspec colname="1" colwidth="28PT" align="center"/>
<colspec colname="2" colwidth="28PT" align="center"/>
<colspec colname="3" colwidth="28PT" align="center"/>
<colspec colname="4" colwidth="21PT" align="center"/>
<colspec colname="5" colwidth="21PT" align="center"/>
<colspec colname="6" colwidth="28PT" align="center"/>
<colspec colname="7" colwidth="28PT" align="center"/>
<colspec colname="8" colwidth="28PT" align="center"/>
<colspec colname="9" colwidth="28PT" align="center"/>
<colspec colname="10" colwidth="21PT" align="center"/>
<colspec colname="11" colwidth="21PT" align="center"/>
<thead>
<row>
<entry></entry>
<entry namest="OFFSET" nameend="11" align="center">TABLE 1.3</entry>
</row>
<row>
<entry></entry>
<entry></entry>
</row>
<row>
<entry></entry>
<entry namest="OFFSET" nameend="11" align="center" rowsep="1"></entry>
</row>
<row>
<entry></entry>
<entry>term_1</entry>
<entry>term_2</entry>
<entry>NDCM</entry>
<entry>LCM</entry>
<entry>RCM</entry>
<entry></entry>
<entry>term_1</entry>
<entry>term_2</entry>
<entry>NDCM</entry>
<entry>LCM</entry>
<entry>RCM</entry>
</row>
<row>
<entry></entry>
<entry namest="OFFSET" nameend="11" align="center" rowsep="1"></entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry></entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="12">
<colspec colname="1" colwidth="14PT" align="center"/>
<colspec colname="2" colwidth="28PT" align="center"/>
<colspec colname="3" colwidth="28PT" align="center"/>
<colspec colname="4" colwidth="28PT" align="center"/>
<colspec colname="5" colwidth="21PT" align="center"/>
<colspec colname="6" colwidth="21PT" align="center"/>
<colspec colname="7" colwidth="28PT" align="center"/>
<colspec colname="8" colwidth="28PT" align="center"/>
<colspec colname="9" colwidth="28PT" align="center"/>
<colspec colname="10" colwidth="28PT" align="center"/>
<colspec colname="11" colwidth="21PT" align="center"/>
<colspec colname="12" colwidth="21PT" align="center"/>
<tbody valign="top">
<row>
<entry>RS</entry>
<entry>A</entry>
<entry>B</entry>
<entry>8</entry>
<entry>5</entry>
<entry>3</entry>
<entry>same as</entry>
<entry>B</entry>
<entry>A</entry>
<entry>8</entry>
<entry>3</entry>
<entry>5</entry>
</row>
<row><entry namest="1" nameend="12" align="center" rowsep="1"></entry>
</row>
</tbody>
</tgroup>
</table>
</table-cwu>
</paragraph>
<paragraph id="P-0084" lvl="0"><number>&lsqb;0084&rsqb;</number> Often the term pairs occur in varying orders. The first term in a term pair A, B is A in one occurrence, and B in another occurrence. Several of the relational metrics such as RCM and LCM, have a direction component, i.e. that the direction or order of the term pair is significant to the metric value as described above. Therefore, to create an accurate summation relation of A, B of all occurrences of the term pair A, B in the database, a direction or order of each occurrence of the term pair A, B must be adjusted to the same direction. </paragraph>
<paragraph id="P-0085" lvl="0"><number>&lsqb;0085&rsqb;</number> The order of term pairs in the relations of models is most preferably shown in the same order as the typical reading order in the database. That is: </paragraph>
<paragraph id="P-0086" lvl="0"><number>&lsqb;0086&rsqb;</number> If RCM(A, B)&gt;LCM(A, B), then the summation relation is preferably expressed as: A, B, NDCM(A, B), LCM(A, B), RCM(A, B). </paragraph>
<paragraph id="P-0087" lvl="0"><number>&lsqb;0087&rsqb;</number> Conversely: </paragraph>
<paragraph id="P-0088" lvl="0"><number>&lsqb;0088&rsqb;</number> If RCM(B, A)&gt;LCM(B, A) then the summation relation is preferably expressed as B, A, NDCM(B,A), LCM(B,A), RCM(B,A). </paragraph>
<paragraph id="P-0089" lvl="0"><number>&lsqb;0089&rsqb;</number> In this instance (Table 1.3) the RCM(B, A) is greater than the LCM(B, A) and therefore B followed by A is in the typical reading order (i.e. left to right). Therefore, Table 1.4 shows the form of the expressing relationship between terms A and B that would be used in the model representing the summation relation (RS) of the term pair (A, B) within the database:  
<table-cwu id="TABLE-US-00004">
<number>4</number>
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="5">
<colspec colname="1" colwidth="56PT" align="center"/>
<colspec colname="2" colwidth="28PT" align="center"/>
<colspec colname="3" colwidth="56PT" align="center"/>
<colspec colname="4" colwidth="21PT" align="center"/>
<colspec colname="5" colwidth="56PT" align="center"/>
<thead>
<row>
<entry namest="1" nameend="5" align="center">TABLE 1.4</entry>
</row>
<row>
<entry></entry>
</row>
<row><entry namest="1" nameend="5" align="center" rowsep="1"></entry>
</row>
<row>
<entry>term<highlight><subscript>&mdash;</subscript></highlight></entry>
<entry>term<highlight><subscript>&mdash;</subscript></highlight></entry>
<entry>NDCM</entry>
<entry>LCM</entry>
<entry>RCM</entry>
</row>
<row><entry namest="1" nameend="5" align="center" rowsep="1"></entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry>1</entry>
<entry>2</entry>
<entry></entry>
<entry></entry>
<entry></entry>
</row>
<row>
<entry>B</entry>
<entry>A</entry>
<entry>8</entry>
<entry>3</entry>
<entry>5</entry>
</row>
<row><entry namest="1" nameend="5" align="center" rowsep="1"></entry>
</row>
</tbody>
</tgroup>
</table>
</table-cwu>
</paragraph>
<paragraph id="P-0090" lvl="0"><number>&lsqb;0090&rsqb;</number> The above summation relation could also be interpreted as saying that when terms A and B are contextually associated, term A tends to follow term B and to a lesser extent A precedes B, with the degree of contextual association indicated by the metrics. This relationship can be observed in text lines 1-3 of Table 1.2. A model of a database consists of a collection of such relations for all term pairs of interest which exist within the database. </paragraph>
<paragraph id="P-0091" lvl="0"><number>&lsqb;0091&rsqb;</number> For one embodiment of a relation expressed in terms of A followed by B, the relation is preferably written in the form: A, B, NDCM(A,B), LCM(A,B), RCM(A,B). If for some reason the above relation must be expressed in terms of B followed by A, then the relation can be rewritten in the form of: B, A, NDCM(B,A), LCM(B,A), RCM(B,A), where NDCM(B, A)&equals;NDCM(A, B), LCM(B, A)&equals;RCM (A, B), and RCM(B, A)&equals;LCM(A, B). Of course, if additional types of metrics were included in the relation and those additional types of metrics included a directional component, then those additional types of metrics would also have to be recalculated when the written expression of the relation is reversed. </paragraph>
<paragraph id="P-0092" lvl="0"><number>&lsqb;0092&rsqb;</number> The context window used to calculate the above-described metric values can have any one of a number of sizes. A context window can have a pre-selected number of terms. Typically, a context window is equal to a level of context desired by the user. Examples include: an average sentence length, or an average paragraph length, or an average phrase length, or a similar relationship to the text or the database. For an alternative embodiment, the context window can be entirely independent from the any relation to the database being analyzed such as a pre-selected number chosen by a user or a default process setting. Alternatively, the context window can vary as a function of the position of the context window within the text, or the contents of the context window. </paragraph>
<paragraph id="P-0093" lvl="0"><number>&lsqb;0093&rsqb;</number> A model of a database or subset includes summation relations and each summation relation includes several types of the relational summation metrics (RSMs) for each term pair. A model of a database or subset can be represented in a variety of forms including, but not limited to, a list of relations, a matrix of relations, and a network of relations. An example of a list representation of relations is shown in Table 1.5. An example of a matrix representation of the relations of Table 1.5 is shown in Table 1.6. An example of a network representation of the relations in Tables 1.5 and 1.6 is shown in <cross-reference target="DRAWINGS">FIG. 6A</cross-reference>.  
<table-cwu id="TABLE-US-00005">
<number>5</number>
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="4">
<colspec colname="OFFSET" colwidth="21PT" align="left"/>
<colspec colname="1" colwidth="77PT" align="left"/>
<colspec colname="2" colwidth="49PT" align="left"/>
<colspec colname="3" colwidth="70PT" align="center"/>
<thead>
<row>
<entry></entry>
<entry namest="OFFSET" nameend="3" align="center">TABLE 1.5</entry>
</row>
<row>
<entry></entry>
<entry></entry>
</row>
<row>
<entry></entry>
<entry namest="OFFSET" nameend="3" align="center" rowsep="1"></entry>
</row>
<row>
<entry></entry>
<entry>term_1</entry>
<entry>term_2</entry>
<entry>NDCM</entry>
</row>
<row>
<entry></entry>
<entry namest="OFFSET" nameend="3" align="center" rowsep="1"></entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry></entry>
<entry>Flight</entry>
<entry>800</entry>
<entry>1725&ensp;</entry>
</row>
<row>
<entry></entry>
<entry>TWA</entry>
<entry>Flight</entry>
<entry>1486&ensp;</entry>
</row>
<row>
<entry></entry>
<entry>TWA</entry>
<entry>800</entry>
<entry>1461&ensp;</entry>
</row>
<row>
<entry></entry>
<entry>fuel</entry>
<entry>tanks</entry>
<entry>849</entry>
</row>
<row>
<entry></entry>
<entry>Aviation</entry>
<entry>Federal</entry>
<entry>693</entry>
</row>
<row>
<entry></entry>
<entry>Federal</entry>
<entry>Administration</entry>
<entry>668</entry>
</row>
<row>
<entry></entry>
<entry>Aviation</entry>
<entry>Administration</entry>
<entry>662</entry>
</row>
<row>
<entry></entry>
<entry>National</entry>
<entry>Transportation</entry>
<entry>602</entry>
</row>
<row>
<entry></entry>
<entry>Safety</entry>
<entry>Transportation</entry>
<entry>600</entry>
</row>
<row>
<entry></entry>
<entry>National</entry>
<entry>Safety</entry>
<entry>589</entry>
</row>
<row>
<entry></entry>
<entry>Safety</entry>
<entry>Board</entry>
<entry>580</entry>
</row>
<row>
<entry></entry>
<entry>TWA</entry>
<entry>Explosion</entry>
<entry>554</entry>
</row>
<row>
<entry></entry>
<entry>Transportation</entry>
<entry>Board</entry>
<entry>532</entry>
</row>
<row>
<entry></entry>
<entry>National</entry>
<entry>Board</entry>
<entry>522</entry>
</row>
<row>
<entry></entry>
<entry>800</entry>
<entry>Explosion</entry>
<entry>415</entry>
</row>
<row>
<entry></entry>
<entry>Flight</entry>
<entry>Explosion</entry>
<entry>408</entry>
</row>
<row>
<entry></entry>
<entry>Fuel</entry>
<entry>Explosion</entry>
<entry>333</entry>
</row>
<row>
<entry></entry>
<entry>Recommendations</entry>
<entry>Urgent</entry>
<entry>252</entry>
</row>
<row>
<entry></entry>
<entry>Tanks</entry>
<entry>Heat</entry>
<entry>197</entry>
</row>
<row>
<entry></entry>
<entry>Fuel</entry>
<entry>Heat</entry>
<entry>190</entry>
</row>
<row>
<entry></entry>
<entry>Aviation</entry>
<entry>Safety</entry>
<entry>187</entry>
</row>
<row>
<entry></entry>
<entry>Fuel</entry>
<entry>Federal</entry>
<entry>171</entry>
</row>
<row>
<entry></entry>
<entry namest="OFFSET" nameend="3" align="center" rowsep="1"></entry>
</row>
</tbody>
</tgroup>
</table>
</table-cwu>
</paragraph>
<paragraph id="P-0094" lvl="0"><number>&lsqb;0094&rsqb;</number>  
<table-cwu id="TABLE-US-00006">
<number>6</number>
<table frame="none" colsep="0" rowsep="0" pgwide="1">
<tgroup align="left" colsep="0" rowsep="0" cols="17">
<colspec colname="OFFSET" colwidth="63PT" align="left"/>
<colspec colname="1" colwidth="14PT" align="center"/>
<colspec colname="2" colwidth="21PT" align="center"/>
<colspec colname="3" colwidth="21PT" align="center"/>
<colspec colname="4" colwidth="14PT" align="center"/>
<colspec colname="5" colwidth="21PT" align="center"/>
<colspec colname="6" colwidth="21PT" align="center"/>
<colspec colname="7" colwidth="21PT" align="center"/>
<colspec colname="8" colwidth="14PT" align="center"/>
<colspec colname="9" colwidth="21PT" align="center"/>
<colspec colname="10" colwidth="14PT" align="center"/>
<colspec colname="11" colwidth="21PT" align="center"/>
<colspec colname="12" colwidth="21PT" align="center"/>
<colspec colname="13" colwidth="21PT" align="center"/>
<colspec colname="14" colwidth="21PT" align="center"/>
<colspec colname="15" colwidth="21PT" align="center"/>
<colspec colname="16" colwidth="14PT" align="center"/>
<thead>
<row>
<entry></entry>
<entry namest="OFFSET" nameend="16" align="center">TABLE 1.6</entry>
</row>
<row>
<entry></entry>
<entry></entry>
</row>
<row>
<entry></entry>
<entry namest="OFFSET" nameend="16" align="center" rowsep="1"></entry>
</row>
<row>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry>R</entry>
</row>
<row>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry>A</entry>
<entry></entry>
<entry>T</entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry>E</entry>
</row>
<row>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry>D</entry>
<entry></entry>
<entry>R</entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry>C</entry>
</row>
<row>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry>M</entry>
<entry></entry>
<entry>A</entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry>O</entry>
</row>
<row>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry>I</entry>
<entry></entry>
<entry>N</entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry>M</entry>
</row>
<row>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry>N</entry>
<entry></entry>
<entry>S</entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry>M</entry>
</row>
<row>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry>I</entry>
<entry></entry>
<entry>P</entry>
<entry></entry>
<entry></entry>
<entry>E</entry>
<entry></entry>
<entry>E</entry>
</row>
<row>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry>A</entry>
<entry>S</entry>
<entry>N</entry>
<entry>O</entry>
<entry></entry>
<entry></entry>
<entry>X</entry>
<entry></entry>
<entry>N</entry>
</row>
<row>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry>F</entry>
<entry>V</entry>
<entry>T</entry>
<entry>A</entry>
<entry>R</entry>
<entry></entry>
<entry></entry>
<entry>P</entry>
<entry></entry>
<entry>D</entry>
</row>
<row>
<entry></entry>
<entry></entry>
<entry>F</entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry>E</entry>
<entry>I</entry>
<entry>R</entry>
<entry>T</entry>
<entry>T</entry>
<entry>S</entry>
<entry></entry>
<entry>L</entry>
<entry>U</entry>
<entry>A</entry>
</row>
<row>
<entry></entry>
<entry></entry>
<entry>L</entry>
<entry></entry>
<entry></entry>
<entry>T</entry>
<entry></entry>
<entry>D</entry>
<entry>A</entry>
<entry>A</entry>
<entry>I</entry>
<entry>A</entry>
<entry>A</entry>
<entry>B</entry>
<entry>O</entry>
<entry>R</entry>
<entry>T</entry>
</row>
<row>
<entry></entry>
<entry></entry>
<entry>I</entry>
<entry></entry>
<entry>F</entry>
<entry>A</entry>
<entry>H</entry>
<entry>E</entry>
<entry>T</entry>
<entry>T</entry>
<entry>O</entry>
<entry>T</entry>
<entry>F</entry>
<entry>O</entry>
<entry>S</entry>
<entry>G</entry>
<entry>I</entry>
</row>
<row>
<entry></entry>
<entry>T</entry>
<entry>G</entry>
<entry>8</entry>
<entry>U</entry>
<entry>N</entry>
<entry>E</entry>
<entry>R</entry>
<entry>I</entry>
<entry>I</entry>
<entry>N</entry>
<entry>I</entry>
<entry>E</entry>
<entry>A</entry>
<entry>I</entry>
<entry>E</entry>
<entry>O</entry>
</row>
<row>
<entry></entry>
<entry>W</entry>
<entry>H</entry>
<entry>0</entry>
<entry>E</entry>
<entry>K</entry>
<entry>A</entry>
<entry>A</entry>
<entry>O</entry>
<entry>O</entry>
<entry>A</entry>
<entry>O</entry>
<entry>T</entry>
<entry>R</entry>
<entry>O</entry>
<entry>N</entry>
<entry>N</entry>
</row>
<row>
<entry></entry>
<entry>A</entry>
<entry>T</entry>
<entry>0</entry>
<entry>L</entry>
<entry>S</entry>
<entry>T</entry>
<entry>L</entry>
<entry>N</entry>
<entry>N</entry>
<entry>L</entry>
<entry>N</entry>
<entry>Y</entry>
<entry>D</entry>
<entry>N</entry>
<entry>T</entry>
<entry>S</entry>
</row>
<row>
<entry></entry>
<entry namest="OFFSET" nameend="16" align="center" rowsep="1"></entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry></entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="17">
<colspec colname="1" colwidth="63PT" align="left"/>
<colspec colname="2" colwidth="14PT" align="center"/>
<colspec colname="3" colwidth="21PT" align="center"/>
<colspec colname="4" colwidth="21PT" align="center"/>
<colspec colname="5" colwidth="14PT" align="center"/>
<colspec colname="6" colwidth="21PT" align="center"/>
<colspec colname="7" colwidth="21PT" align="center"/>
<colspec colname="8" colwidth="21PT" align="center"/>
<colspec colname="9" colwidth="14PT" align="center"/>
<colspec colname="10" colwidth="21PT" align="center"/>
<colspec colname="11" colwidth="14PT" align="center"/>
<colspec colname="12" colwidth="21PT" align="center"/>
<colspec colname="13" colwidth="21PT" align="center"/>
<colspec colname="14" colwidth="21PT" align="center"/>
<colspec colname="15" colwidth="21PT" align="center"/>
<colspec colname="16" colwidth="21PT" align="center"/>
<colspec colname="17" colwidth="14PT" align="center"/>
<tbody valign="top">
<row>
<entry>TWA</entry>
<entry></entry>
<entry>1486</entry>
<entry>1461</entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry>554</entry>
<entry></entry>
<entry></entry>
</row>
<row>
<entry>Flight</entry>
<entry></entry>
<entry></entry>
<entry>1725</entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry>408</entry>
</row>
<row>
<entry>800</entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry>415</entry>
</row>
<row>
<entry>Fuel</entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry>489</entry>
<entry>190</entry>
<entry>171</entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry>333</entry>
</row>
<row>
<entry>Tanks</entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry>197</entry>
</row>
<row>
<entry>Heat</entry>
</row>
<row>
<entry>Federal</entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry>668</entry>
</row>
<row>
<entry>Aviation</entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry>693</entry>
<entry></entry>
<entry>662</entry>
<entry></entry>
<entry></entry>
<entry>187</entry>
</row>
<row>
<entry>Administration</entry>
</row>
<row>
<entry>National</entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry>602</entry>
<entry>589</entry>
<entry>522</entry>
</row>
<row>
<entry>Transportation</entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry>532</entry>
</row>
<row>
<entry>Safety</entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry>600</entry>
<entry></entry>
<entry>580</entry>
</row>
<row>
<entry>Board</entry>
</row>
<row>
<entry>Explosion</entry>
</row>
<row>
<entry>Urgent</entry>
</row>
<row>
<entry>Recommendations</entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry></entry>
<entry>252</entry>
</row>
<row><entry namest="1" nameend="17" align="center" rowsep="1"></entry>
</row>
</tbody>
</tgroup>
</table>
</table-cwu>
</paragraph>
<paragraph id="P-0095" lvl="0"><number>&lsqb;0095&rsqb;</number> At the extreme, the contextual relations of all term pairs in a database could be determined, but this is not necessary because a database or subset can be effectively modeled by retaining only those relations having stronger contextual relations as indicated by larger values of the relational metrics. Thus, the potentially large number of relations can be reduced to a smaller and more manageable number of relations. Appropriate methods of reducing the number of relations in a model are preferably those that result in the more representative relations being retained and the less representative relations being eliminated. </paragraph>
<paragraph id="P-0096" lvl="0"><number>&lsqb;0096&rsqb;</number> A threshold value can be used to reduce the number of relations in a relational model eliminating those relations having a metric value below a certain threshold value. Alternatively, a specific type of metric or summation metric value can be selected as the metric to compare to the threshold value. Another method to reduce the number of relations in a relational model is by selecting a pre-selected number of the relations having the highest metric values. First, one of the types of metric values or summation metric values is selected. Then the pre-selected number of relations having a greatest value of the selected type of metric value is selected from the relations in the relational model. </paragraph>
</section>
<section>
<heading lvl="1">Keyterm Search </heading>
<paragraph id="P-0097" lvl="0"><number>&lsqb;0097&rsqb;</number> Keyterm search is a method of retrieving from a database a number of subsets of the database that are most relevant to a criterion model derived from one or more keyterms. The retrieved subsets can also be ranked according to their corresponding relevance to the criterion model. One embodiment of a keyterm search is a method of searching a database. First, several relational models are provided. Each one of the relational models includes one relational model of at least one subset of the database. Next, a query is input. A criterion model is then created. The criterion model is a relational model that is based on the query. The criterion model is then compared to each one of the relational models of subsets. The identifiers of the subsets relevant to the query are then output. </paragraph>
<paragraph id="P-0098" lvl="0"><number>&lsqb;0098&rsqb;</number> FIGS. <highlight><bold>7</bold></highlight>-<highlight><bold>10</bold></highlight> show various embodiments of applying keyterm searching to several relational models of subsets of a database. <cross-reference target="DRAWINGS">FIG. 7</cross-reference> illustrates one embodiment of an overview of a keyterm search process <highlight><bold>700</bold></highlight>. First, a number of relational models of subsets of a database are provided in block <highlight><bold>702</bold></highlight>. The subsets can be any level of subset of the database from at least two terms to the entire database. Each one of the relational models includes one relational model of at least one subset of the database. A query is input in block <highlight><bold>704</bold></highlight> for comparing to the relational models of subsets of the database. The query can include one term or multiple terms. Next, the query is expanded and modeled to create a criterion model in block <highlight><bold>708</bold></highlight>, as will be more fully described below. The criterion model is then compared to each one of the relational models of subsets of the database in block <highlight><bold>710</bold></highlight> that is also described in more detail below. The identifiers of the relevant subsets are then output in block <highlight><bold>712</bold></highlight>. </paragraph>
<paragraph id="P-0099" lvl="0"><number>&lsqb;0099&rsqb;</number> As an alternative form of input to the keyterm search process, the input query can consist of a query model. A query model can provide detailed control of the relevance criteria embodied in an input query. As a further alternative, the input query can consist of a selected portion of a previously output query model. One alternative method of selecting a portion of an output query model includes selecting a number of relations whose term pairs contain any of a selected group of terms. Another alternative method of selecting a portion of an output query model includes selecting a number of relations having selected metrics greater than a selected threshold value. As another alternative, the input query model can be a model of a subset of a database. As another alternative, the input query model can be a model of a subset of a database having relational metrics that have been multiplied by one or more of a collection of scale factors. As a further alternative, the input query model can be created by manually creating term pairs and corresponding metric values. When a query model is used as an input query, the process of expanding the query and creating a relational model of the query shown in block <highlight><bold>708</bold></highlight> includes passing the input query model to the comparing process shown in block <highlight><bold>710</bold></highlight>. </paragraph>
<paragraph id="P-0100" lvl="0"><number>&lsqb;0100&rsqb;</number> Many alternative forms of outputs of the keyterm search process are useful. Outputting the identifiers of the relevant subsets <highlight><bold>712</bold></highlight> can also include outputting the types of relevance metrics corresponding to each one of the subsets. It is also useful to select one of the types of relevance metrics, to sort the identifiers of subsets in order of magnitude of the selected type of relevance metric, and then to output the identifiers of subsets in order of magnitude of the selected type of relevance metric. For another alternative, the selected type of relevance metric can include a combination of types of relevance metrics. The selected type of relevance metric can also include a weighted sum of types of relevance metrics or a weighted product of the types of relevance metrics. </paragraph>
<paragraph id="P-0101" lvl="0"><number>&lsqb;0101&rsqb;</number> Outputting the identifiers of the relevant subsets in block <highlight><bold>712</bold></highlight> can also include normalizing each one of the corresponding intersection metrics of all intersection relations. Outputting the identifiers of the relevant subsets in block <highlight><bold>712</bold></highlight> can also include outputting the relational model of the query, i.e. the criterion model. Outputting the criterion model is useful to assist a user in directing and focusing additional keyterm searches. Outputting the identifiers of the relevant subsets can also include displaying a pre-selected number of subsets in order of magnitude of a selected type of relevance metric. </paragraph>
<paragraph id="P-0102" lvl="0"><number>&lsqb;0102&rsqb;</number> Another useful alternative output is displaying or highlighting the term pairs or term pair relations that indicate the relevance of a particular subset. For example, one or a selected number of the shared term pairs in each one of the subsets are highlighted, if the terms within each one of the shared term pairs occur within the context window. To reduce the number of displayed shared term pairs, only those shared term pairs that have the greatest magnitude of a selected type of relevance metric are displayed or highlighted. Still another useful output is displaying the shared term pairs that occur in the corresponding subsets. For example, outputting the identifiers of the relevant subsets in block <highlight><bold>712</bold></highlight> can also include displaying one or a selected number of shared term pairs that occur in each one of the subsets, wherein the terms within each one of the shared term pairs occur within a context window. </paragraph>
<paragraph id="P-0103" lvl="0"><number>&lsqb;0103&rsqb;</number> Displaying metric values associated with the displayed shared term pairs is also useful. For example, the output display can also include, for each one of the shared term pairs, displaying an NDCM<highlight><subscript>Q1</subscript></highlight>, and NDCM<highlight><subscript>S1 </subscript></highlight>and a product equal to &lsqb;ln NDCM<highlight><subscript>Q1</subscript></highlight>&rsqb;*&lsqb;ln NDCM<highlight><subscript>S1</subscript></highlight>&rsqb;. The NDCM<highlight><subscript>Q1 </subscript></highlight>is equal to a non-directional contextual metric of the shared term pair in the query, and the NDCM<highlight><subscript>S1 </subscript></highlight>is equal to a non-directional contextual metric of the shared term pair in the subset. The NDCM<highlight><subscript>Q1 </subscript></highlight>and the NDCM<highlight><subscript>S1 </subscript></highlight>must each be greater than 1. </paragraph>
<paragraph id="P-0104" lvl="0"><number>&lsqb;0104&rsqb;</number> As described above, the input query can include a single term or multiple terms. The query can also be transformed when first input. Transforming the query is useful for standardizing the language of a query to the terms used in the database, to which the query derived criterion model will be compared. For example, if an input query was &ldquo;aircraft, pilot&rdquo; and the database used only the corresponding abbreviations &ldquo;ACFT, PLT&rdquo;, then applying a criterion model based on the input query &ldquo;aircraft, pilot&rdquo; would not be very useful. Therefore a transformed query, which transformed &ldquo;aircraft, pilot&rdquo; to &ldquo;ACFT, PLT&rdquo;, would yield useful results in a keyterm search. </paragraph>
<paragraph id="P-0105" lvl="0"><number>&lsqb;0105&rsqb;</number> Transforming the query includes replacing a portion of the first query with an alternate portion. One embodiment of replacing a portion of the query with an alternate portion is a method of finding an alternate portion that is cross-referenced in a look-up table such as a hash table. A hash table includes a number of hash chains and each one of the hash chains corresponds to a first section of the portion of the query and includes several terms or phrases beginning with that first section of the query. The hash chain includes several alternative portions. Each of the alternative portions corresponds to one of the first portions of the query. The subsets of the database can also be transformed, as described above, with respect to the query. </paragraph>
<paragraph id="P-0106" lvl="0"><number>&lsqb;0106&rsqb;</number> Often a query is very short and concise, such as a single term. Another useful alternative is to expand the query to include terms related to the input query term or terms. Many approaches have attempted to expand the query through various methods that typically result in query drift, i.e. where the query begins to include very broad concepts and several unrelated meanings. A query expanded in such a manner is not very useful as the resulting searches produce subsets that are not directly related to the input query. The method of expanding the query described below, substantially maintains the focus and directness of the query while still expanding the query to obtain results including very closely related concepts. </paragraph>
<paragraph id="P-0107" lvl="0"><number>&lsqb;0107&rsqb;</number> Expanding the query is also referred to as creating a gleaning model of the query. <cross-reference target="DRAWINGS">FIG. 8</cross-reference> illustrates one embodiment of expanding the query <highlight><bold>800</bold></highlight> and includes a process of first comparing the query to each one of the models of the subsets of the database in block <highlight><bold>802</bold></highlight>. The matching relations are extracted from the models of the subsets of the database. Each one of the matching relations has a term pair, including a term that matches at least one term in the query, and a related term, in block <highlight><bold>804</bold></highlight>. The matching relation also includes a number of relational summation metrics. </paragraph>
<paragraph id="P-0108" lvl="0"><number>&lsqb;0108&rsqb;</number> In one embodiment, a matching term is identical to a query term. For example, the term &ldquo;fatigue&rdquo; matches the query term &ldquo;fatigue&rdquo;. Alternatively, a term that contains a query term can also match that query term. For example, the terms &ldquo;fatigued&rdquo; and &ldquo;fatigues&rdquo; are matching terms to the query term &ldquo;fatigue&rdquo;. In another alternative, a term that is either identical to a query term, or a term that contains a query term, matches that query term. For example, three terms that match the query term &ldquo;fatigue&rdquo; are &ldquo;fatigue&rdquo;, &ldquo;fatigues&rdquo;, and &ldquo;fatigued&rdquo;. As a further example, four terms that match the query term &ldquo;fatigu&rdquo; are &ldquo;fatigue&rdquo;, &ldquo;fatigues&rdquo;, &ldquo;fatigued&rdquo;, and &ldquo;fatiguing&rdquo;. The matching relations found when expanding the query can also be reduced to only the unique relations, by eliminating any repeating relations from the matching relations. </paragraph>
<paragraph id="P-0109" lvl="0"><number>&lsqb;0109&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 9</cross-reference> illustrates one process <highlight><bold>900</bold></highlight> of reducing the number of matching relations to a number of unique relations. The process <highlight><bold>900</bold></highlight> includes first, selecting one of the matching relations in block <highlight><bold>902</bold></highlight>. The next step is determining if a term pair from the selected matching relation is included in one of the unique relations in block <highlight><bold>906</bold></highlight>. If the selected term pair is not included in one of the unique relations, then the selected matching relation is included in the unique relations in block <highlight><bold>910</bold></highlight>. If the selected term pair is included in one of the unique relations in block <highlight><bold>906</bold></highlight>, then the order of the term pair in the matching relation must be compared to the order of the term pair in the unique relation in block <highlight><bold>912</bold></highlight>. If the order is not the same in both the selected matching relation and the unique relation, then the order of the term pair in the selected matching relation is reversed in block <highlight><bold>914</bold></highlight> and the corresponding metrics containing directional elements are recalculated in block <highlight><bold>916</bold></highlight>, as described above. For example, the values of the LCM and the RCM of the selected matching relation must be exchanged when the stated order of the term pair is reversed. Once the order of the term pair in the selected matching relation and the order of the term pair in the unique relation are the same, then the types of relational summation metrics (RSMs) for the unique relation are replaced with a summation of the corresponding types of RSMs of the selected matching relation and the previous corresponding types of RSMs of the unique relation in block <highlight><bold>918</bold></highlight>. In short, the RSMs are accumulated in the unique relation having the same term pair. The process <highlight><bold>900</bold></highlight> then repeats for any subsequent matching relations in blocks <highlight><bold>920</bold></highlight>, <highlight><bold>922</bold></highlight>. </paragraph>
<paragraph id="P-0110" lvl="0"><number>&lsqb;0110&rsqb;</number> Another approach to reducing the number of matching relations can also include eliminating each one of the matching relations having a corresponding type of RSM less than a threshold value. Still another approach to reducing the number of matching relations can also include extracting matching relations from a pre-selected quantity of relational models. Each one of the matching relations that has a corresponding type of RSM less than a threshold value is then eliminated. Further, selecting a pre-selected number of matching relations that have the greatest value of the corresponding type of RSM can also reduce the number of matching relations. </paragraph>
<paragraph id="P-0111" lvl="0"><number>&lsqb;0111&rsqb;</number> Another aspect of expanding the query can also include determining a typical direction for each one of the matching relations. The typical direction is the most common direction or order of the term pair in the text represented by the relation. If the RCM is greater than the LCM, then the typical direction is the first term followed by the second term. If the LCM is greater than the RCM, then the typical direction is the second term followed by the first term. In one alternative of determining a typical direction, if the LCM is larger than the RCM, then the order of the term pair in the matching relation is reversed, and the value of the RCM is exchanged with the value of the LCM. </paragraph>
<paragraph id="P-0112" lvl="0"><number>&lsqb;0112&rsqb;</number> Expanding the query can also include sorting the unique relations in order of prominence. Prominence is equal to a magnitude of a selected metric. </paragraph>
<paragraph id="P-0113" lvl="0"><number>&lsqb;0113&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 10</cross-reference> illustrates one embodiment of a process <highlight><bold>1000</bold></highlight> of comparing a relational model of the query to each one of the relational models of subsets. The process <highlight><bold>1000</bold></highlight> includes determining the relevance metrics for each one of the relational models of the subsets. This is initiated by determining an intersection model of the relational model of the query and the model of the first subset. Determining an intersection model can include determining a number of intersectional relations in block <highlight><bold>1004</bold></highlight>. Each one of the intersectional relations has a shared term pair and the shared term pair is present in at least one relation in each of the query model and the first subset relational model. Each intersectional relation also has a number of intersection metrics (IM). Each IM is equal to a function of RSM<highlight><subscript>Q1 </subscript></highlight>and RSM<highlight><subscript>S1</subscript></highlight>. RSM<highlight><subscript>Q1 </subscript></highlight>is a type of relational summation metric in the relational model of the query and RSM<highlight><subscript>S1 </subscript></highlight>is a corresponding type of relational summation metric in the relational model of the first one of the relational models of the subsets. Next, a relevance metric for each one of the types of relational summation metrics is determined. Each one of the relevance metrics includes a function of the corresponding type of relational summation metrics of each one of the intersection relations in block <highlight><bold>1006</bold></highlight>. The process repeats in blocks <highlight><bold>1008</bold></highlight> and <highlight><bold>1010</bold></highlight> for any additional models of subsets. </paragraph>
<paragraph id="P-0114" lvl="0"><number>&lsqb;0114&rsqb;</number> The function of RSM<highlight><subscript>Q1 </subscript></highlight>and RSM<highlight><subscript>S1 </subscript></highlight>could alternatively be equal to &lsqb;ln RSM<highlight><subscript>Q1</subscript></highlight>&rsqb;*&lsqb;ln RSM<highlight><subscript>S1</subscript></highlight>&rsqb;, if RSM<highlight><subscript>Q1 </subscript></highlight>and RSM<highlight><subscript>S1 </subscript></highlight>are each greater than or equal to 1. For another alternative embodiment function of RSM<highlight><subscript>Q1 </subscript></highlight>and RSM<highlight><subscript>S1 </subscript></highlight>could equal &lsqb;RSM<highlight><subscript>Q1</subscript></highlight>&rsqb;*&lsqb;RSM<highlight><subscript>S1</subscript></highlight>&rsqb;. </paragraph>
<paragraph id="P-0115" lvl="0"><number>&lsqb;0115&rsqb;</number> Determining an intersection model can also include applying a scaling factor to the summation of the corresponding IMs. One scaling factor is a subset emphasis factor (SEF)&equals;S<highlight><subscript>s</subscript></highlight>/R, wherein S<highlight><subscript>s </subscript></highlight>is equal to a sum of a selected type of relational metrics from the subset for all shared relations and R is equal to a sum of the selected type of relational metric in the subset. Another scaling factor is a query emphasis factor (QEF)&equals;S<highlight><subscript>q</subscript></highlight>/Q. S<highlight><subscript>q </subscript></highlight>is equal to a sum of a selected type of relational metrics from the query for all shared relations. Q is equal to a sum of the selected type of relational metric in the relevance model of the query. Another scaling factor is a length emphasis factor (LEF)&equals;L<highlight><subscript>s</subscript></highlight>/T where, L<highlight><subscript>s </subscript></highlight>is equal to a number of terms in the subset and T is equal to a number greater than a number of terms in a largest subset of the database. Still another scaling factor is an alternate length emphasis factor (LEF<highlight><subscript>alt</subscript></highlight>)&equals;L<highlight><subscript>cap</subscript></highlight>/T where, L<highlight><subscript>cap </subscript></highlight>is equal to the lesser of either a number of terms in the subset or an average number of terms in each one of the subsets, and T is equal to a number greater than a number of terms in a largest subset of the database. </paragraph>
<paragraph id="P-0116" lvl="0"><number>&lsqb;0116&rsqb;</number> For another alternative output, a representation of the model of the query or a model of a subset can be output. Such representations can include table-formatted text, or a network diagram, or a graphical representation of the model. </paragraph>
<paragraph id="P-0117" lvl="0"><number>&lsqb;0117&rsqb;</number> For another alternative embodiment of keyterm search, multiple queries can be applied to the keyterm search processes described above. A first query is processed as described above. Next, a second query is input, and then a relational model of the second query is created. Then the relational model of the second query is compared to each one of the relational models of the subsets. A second set of identifiers of the subsets relevant to the second query is then output. Finally, the second set of relevance metrics for the second query is combined with the relevance metrics for the first query to create a combined output. An alternative embodiment can also include determining a third set of identifiers of the subsets consisting of identifiers of the subsets present in both the first and second sets of subsets. A selected combined relevance metric for each one of the identifiers of the subsets that is present in both the first set of identifiers of the subsets and the second set of identifiers of the subsets is greater than zero. Combining the sets of identifiers can also include calculating a product of a first type of first relevance metric and a first type of a second relevance metric. </paragraph>
<paragraph id="P-0118" lvl="0"><number>&lsqb;0118&rsqb;</number> Another alternative also includes determining a third set of identifiers of the subsets consisting of identifiers of the subsets present in either the first or second set of subsets. A selected combined relevance metric for each one of the identifiers of the subsets that is present in either the first set of identifiers of the subsets or the second set of identifiers of the subsets, or both, is greater than zero. In one embodiment, combining the sets of identifiers also includes calculating a summation of a first type of first relevance metric and a first type of a second relevance metric. </paragraph>
<paragraph id="P-0119" lvl="0"><number>&lsqb;0119&rsqb;</number> This application is intended to cover any adaptations or variations of the present invention. For example, those of ordinary skill within the art will appreciate that the keyterm search process can be executed in varying orders instead of being executed in the order as described above. </paragraph>
<paragraph id="P-0120" lvl="0"><number>&lsqb;0120&rsqb;</number> Using keyterm search is easy. All that is required is to provide the keyterm or keyterms of interest. Then the subsets of a database, such as the narratives of the Aviation Safety Reporting System (ASRS) database, are sorted according to their relevance to the query, the most relevant narratives are displayed with the relevant sections highlighted. Examples of keyterm search applied to the ASRS database are shown below to illustrate several important details. </paragraph>
<paragraph id="P-0121" lvl="0"><number>&lsqb;0121&rsqb;</number> Using a query term &ldquo;engage&rdquo; to find narratives relevant to &ldquo;engage&rdquo;, the keyterm &ldquo;engage&rdquo; is input to the keyterm search and the most relevant narratives, with their relevant sections highlighted, are displayed. Additional outputs can include a complete list of relevant narratives, and the criterion model used to search the ASRS database. The following is an example of a relevant narrative: </paragraph>
<paragraph id="P-0122" lvl="1"><number>&lsqb;0122&rsqb;</number> ON FEBRUARY/XX/95 AT ABOUT XA00 PM SAN JUAN TIME WE DEPARTED RWY 8 ENRTE TO MIAMI. WE INTERCEPTED THE JAAWS 9 DEP, AND SHORTLY AFTER PASSING THROUGH 10000 FT WE WERE CLRED DIRECT (RNAV ) TO JUNUR, WHICH IS A POINT IN THE CLAMI 1 ARR INTO MIAMI. I THEN ENGAGED THE AUTOPLT AND TURNED THE ACFT IN THE DIRECTION OF THE WAYPOINT (JUNUR) WE WERE CLRED TO. AT THIS POINT I AM NOT SURE IF I ENGAGED THE AUX NAV PORTION OF THE AUTOPLT. THE REASON I SAY THIS IS BECAUSE APPROX 1 HR LATER WE DISCOVERED THAT THE AUX NAV PORTION OF THE AUTOPLT WAS NOT ENGAGED AND WE HAD DRIFTED ABOUT 45 NM OFF COURSE. IT IS UNKNOWN WHETHER THE AUX NAV WAS NEVER ENGAGED OR IF THE KNOB WAS SOMEHOW KNOCKED OFF DURING THE FLT. I DO REMEMBER PASSING ALMOST DIRECTLY OVER GTK VOR WHICH IS ALONG THE NORMAL RTE THE ACFT WOULD TAKE IF THE OMEGA WERE ENGAGED. 2 SCENARIOS ARE POSSIBLE. THE OMEGA WAS NEVER ENGAGED, AND DUE TO LIGHT HIGH ALT WINDS, THE ACFT AFTER INITIALLY BEING POINTED IN THE CORRECT DIRECTION, ONLY BEGAN TO DRIFT DRAMATICALLY AFTER PASSING GTK VOR. OR, THE AUX NAV KNOB WAS ACCIDENTLY DISENGAGED AND WAS NOT NOTICED. THERE IS NO AURAL OR OTHER TYPE WARNING WHEN THE OMEGA BECOMES DISENGAGED. THERE IS A GREEEN &lsquo;AUX NAV&rsquo; LIGHT THAT IS ILLUMINATED WHEN ENGAGED, BUT THE LIGHT IS NOT VERY OBVIOUS TO THE CREW. SOME TYPE OF OBVIOUS WARNING (HAD IT BEEN AVAILABLE ) WOULD HAVE ALERTED THE CREW IN THE EVENT OF AN INADVERTENT DISCONNECT. ONE THING WE FOUND UNUSUAL DURING OUR FLT WAS THAT ATC NEVER SAID A WORD TO US DURING OUR SMALL DETOUR. (300563) </paragraph>
<paragraph id="P-0123" lvl="0"><number>&lsqb;0123&rsqb;</number> The default pattern-matching behavior of keyterm search is a &ldquo;contained match&rdquo;. This means that any term that contains the string of characters &ldquo;engage&rdquo; is considered to be a match. So, narratives containing the following terms are retrieved:  
<table-cwu id="TABLE-US-00007">
<number>7</number>
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="5">
<colspec colname="1" colwidth="42PT" align="left"/>
<colspec colname="2" colwidth="42PT" align="left"/>
<colspec colname="3" colwidth="49PT" align="left"/>
<colspec colname="4" colwidth="42PT" align="left"/>
<colspec colname="5" colwidth="42PT" align="left"/>
<thead>
<row>
<entry></entry>
</row>
<row><entry namest="1" nameend="5" align="center" rowsep="1"></entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry>engage</entry>
<entry>engaged</entry>
<entry>disengage</entry>
<entry>disengaged</entry>
<entry>reengage</entry>
</row>
<row>
<entry>reengaged</entry>
<entry>engagement</entry>
<entry>disengagement</entry>
</row>
<row><entry namest="1" nameend="5" align="center" rowsep="1"></entry>
</row>
</tbody>
</tgroup>
</table>
</table-cwu>
</paragraph>
<paragraph id="P-0124" lvl="0"><number>&lsqb;0124&rsqb;</number> In the example narrative, the term &ldquo;engaged&rdquo; appears 7 times, &ldquo;disengaged&rdquo; appears twice, and &ldquo;engage&rdquo; does not appear. This shows the value of allowing the &ldquo;contained match&rdquo; as the default. A user need not know the various forms of the term that appear in the narratives, but can find the narratives that are clearly relevant to the input keyterm &ldquo;engage.&rdquo;</paragraph>
<paragraph id="P-0125" lvl="0"><number>&lsqb;0125&rsqb;</number> Not only are the various forms of the term &ldquo;engage&rdquo; highlighted in the example narrative, but other terms are also highlighted. These other terms are often found in the context of &ldquo;engage&rdquo; in the ASRS database. Highlighting can be limited to a pre-selected number of the most prominent contextual associations of the keyterm in the database. The default number is 1000. Of course the keyterm search could limit highlighting to just the keyterm(s), or to contextual associations that have some fraction of the prominence of the most prominent association in the database or the particular narrative. </paragraph>
<paragraph id="P-0126" lvl="0"><number>&lsqb;0126&rsqb;</number> The display of the most relevant narratives can suffice, but a deeper understanding of which contextual associations contribute to the relevance of each narrative can also be presented. By referring to a data table that is displayed after each narrative, it is possible to identify the terms in the narrative that are most often found in the context of the query term(s). Table 2.1 shows a top portion of a data table for the example narrative:  
<table-cwu id="TABLE-US-00008">
<number>8</number>
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="5">
<colspec colname="1" colwidth="63PT" align="left"/>
<colspec colname="2" colwidth="56PT" align="left"/>
<colspec colname="3" colwidth="49PT" align="center"/>
<colspec colname="4" colwidth="14PT" align="center"/>
<colspec colname="5" colwidth="35PT" align="center"/>
<thead>
<row>
<entry namest="1" nameend="5" align="center">TABLE 2.1</entry>
</row>
<row>
<entry></entry>
</row>
<row><entry namest="1" nameend="5" align="center" rowsep="1"></entry>
</row>
<row>
<entry>W1</entry>
<entry>W2</entry>
<entry>A</entry>
<entry>B</entry>
<entry>C</entry>
</row>
<row><entry namest="1" nameend="5" align="center" rowsep="1"></entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry>ENGAGED</entry>
<entry>AUTOPLT</entry>
<entry>17905&emsp;</entry>
<entry>70</entry>
<entry>41.6048</entry>
</row>
<row>
<entry>NOT</entry>
<entry>ENGAGED</entry>
<entry>2484&ensp;</entry>
<entry>72</entry>
<entry>33.4334</entry>
</row>
<row>
<entry>NAV</entry>
<entry>ENGAGED</entry>
<entry>898</entry>
<entry>94</entry>
<entry>30.8952</entry>
</row>
<row>
<entry>ENGAGED</entry>
<entry>ALT</entry>
<entry>6015&ensp;</entry>
<entry>27</entry>
<entry>28.6804</entry>
</row>
<row>
<entry>ENGAGED</entry>
<entry>LIGHT</entry>
<entry>508</entry>
<entry>74</entry>
<entry>26.8164</entry>
</row>
<row>
<entry>OMEGA</entry>
<entry>ENGAGED</entry>
<entry>386</entry>
<entry>87</entry>
<entry>26.5982</entry>
</row>
<row>
<entry>DISENGAGED</entry>
<entry>NOT</entry>
<entry>896</entry>
<entry>39</entry>
<entry>24.9047</entry>
</row>
<row>
<entry>ENGAGED</entry>
<entry>BUT</entry>
<entry>984</entry>
<entry>24</entry>
<entry>21.902&ensp;</entry>
</row>
<row>
<entry>NEVER</entry>
<entry>ENGAGED</entry>
<entry>159</entry>
<entry>73</entry>
<entry>21.7479</entry>
</row>
<row>
<entry>AUX</entry>
<entry>ENGAGED</entry>
<entry>117</entry>
<entry>94</entry>
<entry>21.636&ensp;</entry>
</row>
<row>
<entry>CLRED</entry>
<entry>ENGAGED</entry>
<entry>364</entry>
<entry>26</entry>
<entry>19.2135</entry>
</row>
<row>
<entry>ENGAGED</entry>
<entry>COURSE</entry>
<entry>239</entry>
<entry>32</entry>
<entry>18.98&emsp;</entry>
</row>
<row>
<entry>OMEGA</entry>
<entry>DISENGAGED</entry>
<entry>202</entry>
<entry>34</entry>
<entry>18.7189</entry>
</row>
<row>
<entry>WARNING</entry>
<entry>DISENGAGED</entry>
<entry>202</entry>
<entry>34</entry>
<entry>18.7189</entry>
</row>
<row><entry namest="1" nameend="5" align="center" rowsep="1"></entry>
</row>
</tbody>
</tgroup>
</table>
</table-cwu>
</paragraph>
<paragraph id="P-0127" lvl="0"><number>&lsqb;0127&rsqb;</number> Each line in Table 2.1 represents a contextual association between two terms (i.e., the terms in columns W1 and W2). Column A is a measure of the strength of the contextual association of the term pair in the whole ASRS database. Column B is a measure of the strength of the same contextual association in this narrative. Column C is a combination of these two metrics and represents a measure of the contextual association of the paired terms. In this table, C is the product of the natural logarithms of A and B. The value of C is large when the values of both A and B are large. The relations are sorted on column C. </paragraph>
<paragraph id="P-0128" lvl="0"><number>&lsqb;0128&rsqb;</number> Term pairs toward the top of the list have stronger contextual associations. The top relation, for example, is between ENGAGED and AUTOPLT (i.e., autopilot). This relation is at the top of the list because AUTOPLT is very often found in the context of ENGAGED in the ASRS database (as indicated by 17905 in column A) and that relationship is also relatively prominent in this narrative (as indicated by 70 in column B). The term ENGAGED is in column W1, and the term AUTOPLT is in W2 because ENGAGED tends to precede AUTOPLT in the narratives of the ASRS database. In general, each pair of terms appears in the more typical order. </paragraph>
<paragraph id="P-0129" lvl="0"><number>&lsqb;0129&rsqb;</number> The contextual relationship between ENGAGED and AUTOPLT can be seen in the following excerpts from the example narrative: </paragraph>
<paragraph id="P-0130" lvl="2"><number>&lsqb;0130&rsqb;</number> I THEN ENGAGED THE AUTOPLT </paragraph>
<paragraph id="P-0131" lvl="2"><number>&lsqb;0131&rsqb;</number> IF I ENGAGED THE AUX NAV PORTION OF THE AUTOPLT </paragraph>
<paragraph id="P-0132" lvl="2"><number>&lsqb;0132&rsqb;</number> THE AUX NAV PORTION OF THE AUTOPLT WAS NOT ENGAGED </paragraph>
<paragraph id="P-0133" lvl="0"><number>&lsqb;0133&rsqb;</number> An additional advantage of the contained match rule is that a term such as &ldquo;engage&rdquo; can be used as a query. This would match several forms of &ldquo;engage&rdquo;, including not only those listed earlier, but also &ldquo;engaging&rdquo; and &ldquo;disengaging&rdquo;. Alternatively, an exact match can also be required so that only narratives containing the term &ldquo;engage&rdquo; would be retrieved. </paragraph>
<paragraph id="P-0134" lvl="0"><number>&lsqb;0134&rsqb;</number> A search for narratives relevant to &ldquo;rest&rdquo; requires the use of the &ldquo;exact match&rdquo; option. That is because the default &ldquo;contained match&rdquo; option that worked so well in the previous example becomes a liability when the query is contained in too many terms. &ldquo;Rest&rdquo; is such a query, as indicated by the following long list of terms from the ASRS database that contain &ldquo;rest&rdquo;:  
<table-cwu id="TABLE-US-00009">
<number>9</number>
<table frame="none" colsep="0" rowsep="0" pgwide="1">
<tgroup align="left" colsep="0" rowsep="0" cols="4">
<colspec colname="1" colwidth="77PT" align="left"/>
<colspec colname="2" colwidth="70PT" align="left"/>
<colspec colname="3" colwidth="70PT" align="left"/>
<colspec colname="4" colwidth="56PT" align="left"/>
<thead>
<row>
<entry></entry>
</row>
<row><entry namest="1" nameend="4" align="center" rowsep="1"></entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry>RESTR</entry>
<entry>REST</entry>
<entry>RESTRICTION</entry>
<entry>RESTRICTIONS</entry>
</row>
<row>
<entry>NEAREST</entry>
<entry>RESTART</entry>
<entry>RESTRS</entry>
<entry>INTEREST</entry>
</row>
<row>
<entry>RESTARTED</entry>
<entry>RESTORED</entry>
<entry>INTERESTED</entry>
<entry>INTERESTING</entry>
</row>
<row>
<entry>RESTATED</entry>
<entry>ARRESTED</entry>
<entry>RESTED</entry>
<entry>ARREST</entry>
</row>
<row>
<entry>RESTORE</entry>
<entry>UNRESTRICTED</entry>
<entry>RESTRICT</entry>
<entry>FOREST</entry>
</row>
<row>
<entry>RESTRICTING</entry>
<entry>RESTRICTIVE</entry>
<entry>UNRESTR</entry>
<entry>RESTING</entry>
</row>
<row>
<entry>RESTAURANT</entry>
<entry>ARRESTING</entry>
<entry>RESTROOM</entry>
<entry>RESTRICTED</entry>
</row>
<row>
<entry>RESTS</entry>
<entry>CRESTVIEW</entry>
<entry>RESTARTING</entry>
<entry>CREST</entry>
</row>
<row>
<entry>INTERESTS</entry>
<entry>RESTATE</entry>
<entry>RESTRICTS</entry>
<entry>PRESTART</entry>
</row>
<row>
<entry>INTERESTINGLY</entry>
<entry>RESTORING</entry>
<entry>RESTRAINT</entry>
<entry>RESTRAINED</entry>
</row>
<row>
<entry>RESTRAINTS</entry>
<entry>BREST</entry>
<entry>OVERESTIMATED</entry>
<entry>RESTATING</entry>
</row>
<row>
<entry>RESTORATION</entry>
<entry>RESTRAINING</entry>
<entry>ARMREST</entry>
<entry>RESTLESS</entry>
</row>
<row>
<entry>UNDERESTIMATED</entry>
</row>
<row><entry namest="1" nameend="4" align="center" rowsep="1"></entry>
</row>
</tbody>
</tgroup>
</table>
</table-cwu>
</paragraph>
<paragraph id="P-0135" lvl="0"><number>&lsqb;0135&rsqb;</number> To find narratives relevant to &ldquo;rest&rdquo;, input the keyterm &ldquo;rest&rdquo; to keyterm search and select the &ldquo;exact match&rdquo; option. The most relevant narratives are displayed, with their corresponding relevant sections highlighted. The following is one of the most relevant narratives: </paragraph>
<paragraph id="P-0136" lvl="1"><number>&lsqb;0136&rsqb;</number> CREW REST REGS: UNFORTUNATELY, EVERY ONCE IN A WHILE FOR A VARIETY OF REASONS, THIS REG (DESIGNED TO ENSURE PROPERLY RESTED PLTS) GETS FORGOTTEN&excl; TRY AND FIGURE THIS ONE. 2 DAY PAIRING SCHEDULE FOR 10 PLUS 09, THE FIRST DAY SHOW TIME IS LATE EVENING AND FLT TIME IS SCHEDULED FOR 3 PLUS 44. DUE TO MECHANICAL PROBLEM WE PUSHED: 20 LATE, WX IN THE AREA DELAYED OUR TKOF. WITH AN UNSCHEDULED FUEL STOP WE LANDED AND PARKED AT THE DEST GATE 1 PLUS 51 LATE. ORIGINALLY WE WERE SCHEDULED FOR 10 PLUS 16 LAYOVER. OUR COMPANY&apos;S STD RESPONSE WHEN CALLED TO CHK CREW REST IS 8 PLUS 44 BLOCK TO BLOCK (XX AND 8 PLUS 44&equals;A PUSH TIME OF XXY) SINCE OUR PUSH TIME WAS SCHEDULED FOR XXY THERE WAS NOT A CONFLICT IN OUR THINKING. AT EARLY SCHEDULING AWOKE THE CAPT, INFORMING HIM THAT THE FO AND SO &lsquo;REQUIRED 9 PLUS 45&rsquo; BLOCK TO BLOCK CREW REST. WE ALL SHOWED AS PLANNED THE PREVIOUS EVENING FOR SCHEDULED VAN. THE CAPT INFORMED FO AND I ABOUT CALL FROM SCHEDULES, IT JUST DID NOT MAKE SENSE. WE FLEW 4 PLUS 13 THE NIGHT BEFORE AND WERE SCHEDULED TO FLY 6 PLUS 25 THIS DAY. WHAT WERE WE TO DO&quest; GO BACK TO OUR ROOMS AND SLEEP FOR ANOTHER 45 MINS&quest; WE SHOWED ON THE ACFT (8 PLUS 51 FROM BLOCK IN) ACFT WAS BOARDED NORMALLY AND WE SAT WITH THE PARKING BRAKE SET SO AS NOT TO TRIP ACARS UNTIL SCHEDULING GOT THEIR IMPOSED 9 PLUS 45 BLOCK TO BLOCK, HOWEVER, I SEE THAT 1) THEY INTERRUPTED CAPT CREW REST. 2) THEIR REST INTERPRETATION WAS SOMEHOW FLAWED (ALTHOUGH APPRECIATED WHEN WE GET &lsquo;MORE&rsquo; REST). 3) &lsquo;MORE&rsquo; REST I DO NOT NEED SPENT SITTING 54 MINS WITH PARKING BRAKE SET&mdash;WAITING TO BE LEGAL. MY AIRLINE USES FAR MIN REST AS NORMAL PRACTICE AND ROUTINELY VIOLATES CREW REST FOR PERHAPS MISINTERPRETED REST REGS REQUIRED. I FEEL 1) FAA MUST MAKE BOTH FLT TIME AND DUTY TIME HENCE REST TIMES EASIER TO UNDERSTAND (THROW OUT INTERPRETATIONS)&excl; 2) HOLD CREW SCHEDULERS ACCOUNTABLE FOR VIOLATIONS OF CREW REST, A GOOD SCHEDULE PRACTICE WOULD HAVE BEEN TO INFORM US ON ARR THE PREVIOUS NIGHT OF REST REQUIRED. (183457) </paragraph>
<paragraph id="P-0137" lvl="0"><number>&lsqb;0137&rsqb;</number> The terms CREW, REQUIRED, BLOCK, NOT, DUTY, CAPT (i.e., captain), FAR (i.e., Federal Aviation Regulations), REGS (i.e., regulations), LEGAL, FAA (i.e., Federal Aviation Administration), NIGHT, FEEL, SCHEDULED, and others are highlighted in the narrative because they are often found in the context of REST in the narratives of the ASRS database. </paragraph>
<paragraph id="P-0138" lvl="0"><number>&lsqb;0138&rsqb;</number> The needs of many users will be satisfied by the display of the most relevant narratives, but others might wish to better understand the relevance of each narrative. The data table that is displayed after each narrative includes the relative association of REST with the terms found most often in the context of REST. The following Table 2.2 is a top portion of a data table for the example narrative:  
<table-cwu id="TABLE-US-00010">
<number>10</number>
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="6">
<colspec colname="OFFSET" colwidth="14PT" align="left"/>
<colspec colname="1" colwidth="42PT" align="left"/>
<colspec colname="2" colwidth="49PT" align="left"/>
<colspec colname="3" colwidth="35PT" align="center"/>
<colspec colname="4" colwidth="21PT" align="center"/>
<colspec colname="5" colwidth="56PT" align="center"/>
<thead>
<row>
<entry></entry>
<entry namest="OFFSET" nameend="5" align="center">TABLE 2.2</entry>
</row>
<row>
<entry></entry>
<entry></entry>
</row>
<row>
<entry></entry>
<entry namest="OFFSET" nameend="5" align="center" rowsep="1"></entry>
</row>
<row>
<entry></entry>
<entry>term1</entry>
<entry>term2</entry>
<entry>A</entry>
<entry>B</entry>
<entry>C</entry>
</row>
<row>
<entry></entry>
<entry namest="OFFSET" nameend="5" align="center" rowsep="1"></entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry></entry>
<entry>CREW</entry>
<entry>REST</entry>
<entry>9241</entry>
<entry>264&ensp;</entry>
<entry>50.9163</entry>
</row>
<row>
<entry></entry>
<entry>REST</entry>
<entry>REQUIRED</entry>
<entry>2281</entry>
<entry>115&ensp;</entry>
<entry>36.6896</entry>
</row>
<row>
<entry></entry>
<entry>BLOCK</entry>
<entry>REST</entry>
<entry>1181</entry>
<entry>124&ensp;</entry>
<entry>34.0992</entry>
</row>
<row>
<entry></entry>
<entry>REST</entry>
<entry>NOT</entry>
<entry>4639</entry>
<entry>44</entry>
<entry>31.9471</entry>
</row>
<row>
<entry></entry>
<entry>DUTY</entry>
<entry>REST</entry>
<entry>4595</entry>
<entry>43</entry>
<entry>31.7172</entry>
</row>
<row>
<entry></entry>
<entry>CAPT</entry>
<entry>REST</entry>
<entry>1302</entry>
<entry>66</entry>
<entry>30.0468</entry>
</row>
<row>
<entry></entry>
<entry>FAR</entry>
<entry>REST</entry>
<entry>1534</entry>
<entry>56</entry>
<entry>29.5285</entry>
</row>
<row>
<entry></entry>
<entry>REST</entry>
<entry>REGS</entry>
<entry>&ensp;643</entry>
<entry>93</entry>
<entry>29.3084</entry>
</row>
<row>
<entry></entry>
<entry>LEGAL</entry>
<entry>REST</entry>
<entry>1606</entry>
<entry>47</entry>
<entry>28.4199</entry>
</row>
<row>
<entry></entry>
<entry>REST</entry>
<entry>FAA</entry>
<entry>1207</entry>
<entry>54</entry>
<entry>28.3054</entry>
</row>
<row>
<entry></entry>
<entry>NIGHT</entry>
<entry>REST</entry>
<entry>2375</entry>
<entry>34</entry>
<entry>27.4095</entry>
</row>
<row>
<entry></entry>
<entry>REST</entry>
<entry>FEEL</entry>
<entry>&ensp;462</entry>
<entry>60</entry>
<entry>25.1211</entry>
</row>
<row>
<entry></entry>
<entry>REST</entry>
<entry>SCHEDULED</entry>
<entry>2372</entry>
<entry>24</entry>
<entry>24.6982</entry>
</row>
<row>
<entry></entry>
<entry>REST</entry>
<entry>NEED</entry>
<entry>&ensp;693</entry>
<entry>42</entry>
<entry>24.4482</entry>
</row>
<row>
<entry></entry>
<entry>REST</entry>
<entry>SCHEDULE</entry>
<entry>&ensp;852</entry>
<entry>35</entry>
<entry>23.99&emsp;</entry>
</row>
<row>
<entry></entry>
<entry namest="OFFSET" nameend="5" align="center" rowsep="1"></entry>
</row>
</tbody>
</tgroup>
</table>
</table-cwu>
</paragraph>
<paragraph id="P-0139" lvl="0"><number>&lsqb;0139&rsqb;</number> The format of Table 2.2 was described in the previous example. In this case Table 2.2 indicates, for example, that CREW is often found in the context of REST in both the database and in this narrative, and CREW typically precedes REST in the database. Further, since the value in column C is greater than that for any of the other term pairs, the contextual association of CREW and REST is stronger than that of any of the other term pairs. The other contextual associations can be interpreted in a similar fashion. </paragraph>
<paragraph id="P-0140" lvl="0"><number>&lsqb;0140&rsqb;</number> To find narratives relevant to &ldquo;emergency&rdquo;, the keyterm &ldquo;emergency&rdquo; is input to keyterm search and the most relevant narratives are retrieved and displayed, with the corresponding relevant sections highlighted. The following is an example narrative: </paragraph>
<paragraph id="P-0141" lvl="1"><number>&lsqb;0141&rsqb;</number> A FEW MINS AFTER REACHING FL350 CABIN RAPIDLY DEPRESSURIZED. COCKPIT CREW VERIFIED RAPID DECOMPRESSION, BEGAN EMER DSCNT, DECLARED AN EMER CONDITION WITH ARTCC AND SIMULTANEOUSLY REQUESTED A DIRECT VECTOR TO THE NEAREST SUITABLE ARPT WHICH WAS DETERMINED BY CAPT TO BE STL 110 MI AWAY. ALL EMER CHECKLISTS AND NORMAL CHECKLISTS COMPLETED AND AN UNEVENTFUL APCH AND LNDG WAS MADE. NO INJURIES. I HAVE UNFORTUNATELY DONE 2 EMER DSCNTS IN THE LAST 18 MONTHS DUE TO THE SAME COMPUTER FAILURE OF THE PRESSURIZATION SYS. THE ODDS AGAINST THAT ARE STAGGERING. I BELIEVE THIS ACFT&apos;S AUTO CABIN CTLRS SHOULD BE LOOKED AT CAREFULLY. ALSO, EMER PROC TRAINING AT MY COMPANY FOR EMER DSCNTS NEEDS TO BE REVIEWED AND MODIFIED AS WELL AS THOUGHT GIVEN TO MANY FACTORS NEVER DISCUSSED DURING TRAINING. (110788) </paragraph>
<paragraph id="P-0142" lvl="0"><number>&lsqb;0142&rsqb;</number> The term &ldquo;emergency&rdquo; does not appear in the narrative because the ASRS abbreviates the term &ldquo;emergency&rdquo; as &ldquo;emer&rdquo;. Keyterm search automatically maps or transforms the input keyterm to the ASRS abbreviations, as long as those transformations or mappings are contained in the mapping file used by keyterm search. The mapping file can also be updated or disabled. The highlighted terms include the keyterm (as abbreviated by the ASRS) and those terms that are often found in the context of the query in the narratives of the ASRS database. </paragraph>
<paragraph id="P-0143" lvl="0"><number>&lsqb;0143&rsqb;</number> A search for narratives relevant to &ldquo;language&rdquo;, &ldquo;English&rdquo;, or &ldquo;phraseology&rdquo; in a database can be initiated by inputting the keyterms &ldquo;language&rdquo;, &ldquo;English&rdquo;, and &ldquo;phraseology&rdquo; to keyterm search. Keyterm search then retrieves and ranks the narratives of the database according to their relevance to the typical or selected contexts of these terms in the database. The following is an example of one of the most relevant narratives retrieved and displayed by keyterm search of the ASRS database: </paragraph>
<paragraph id="P-0144" lvl="1"><number>&lsqb;0144&rsqb;</number> TKOF CLRNC WAS MISUNDERSTOOD BY CREW. TWR CTLR&apos;S ENGLISH WAS NOT VERY CLR AND HE USED INCORRECT PHRASEOLOGY WHICH CAUSED AN APPARENT ALT &lsquo;BUST.&rsquo; ATC CLRNC WAS TO 9000 FT, WHICH IS NORMAL FOR THEM. WE WERE USING RWY 21. TKOF CLRNC WAS &lsquo;CLRED FOR TKOF, RWY HDG 210 DEGS, CONTACT DEP.&rsquo; DEP SAID WE WERE CLRED TO 2100 FT (AS WE WERE PASSING 3000 FT). EVIDENTLY THE &lsquo;21&rsquo; AFTER &lsquo;RWY HDG&rsquo; WAS MEANT AS AN AMENDED ALT CLRNC. IF PROPER PHRASEOLOGY HAD BEEN USED, I AM SURE WE WOULD HAVE EITHER UNDERSTOOD OR ASKED FOR A CLARIFICATION. PROPER PHRASEOLOGY IS EVEN MORE IMPORTANT WHEN SPEAKING TO PEOPLE WHOSE PRIMARY LANGUAGE IS NOT ENGLISH. PLTS SHOULD UNDERSTAND THIS BECAUSE OF TRYING TO GIVE POS RPTS, ETC, TO SO MANY DIFFERENT PEOPLE. (236336) </paragraph>
<paragraph id="P-0145" lvl="0"><number>&lsqb;0145&rsqb;</number> The following are some relevant sentences from other highly relevant narratives: </paragraph>
<paragraph id="P-0146" lvl="1"><number>&lsqb;0146&rsqb;</number> EXTREMELY DIFFICULT TO COPY CLRNC BECAUSE OF POOR ENGLISH OF CTLR AND NO SPANISH BY PLTS. (306637) </paragraph>
<paragraph id="P-0147" lvl="1"><number>&lsqb;0147&rsqb;</number> I THINK AN IMMEDIATE REVIEW OF RELATED FIX NAMES FOR SIMILAR SOUNDING NAMES AS PRONOUNCED BY THE LCL SPEAKER&apos;S LANGUAGE IS ESSENTIAL. (242971) </paragraph>
<paragraph id="P-0148" lvl="1"><number>&lsqb;0148&rsqb;</number> THE COM BTWN THE FRENCH CTLRS AND ENGLISH SPEAKING PLTS HAS BEEN POOR FOR SOME TIME, AND IS GETTING WORSE. (301205) </paragraph>
<paragraph id="P-0149" lvl="1"><number>&lsqb;0149&rsqb;</number> FLYING A LOT OF TIME IN CENTRAL AND S AMERICA, I EXPERIENCE THAT ATC CTLRS DON&apos;T HAVE FLUENT TALKING AND UNDERSTANDING OF THE ENGLISH LANGUAGE, AS THE WAY HAS TO BE CONSIDERING THAT ENGLISH IS THE UNIVERSAL AND INTL LANGUAGE IN AVIATION. (302310) </paragraph>
<paragraph id="P-0150" lvl="1"><number>&lsqb;0150&rsqb;</number> THE RPTR SAID THAT HE OFTEN HEARS IMPROPER PHRASEOLOGY DURING HIS FOREIGN OPS. (352400) </paragraph>
<paragraph id="P-0151" lvl="1"><number>&lsqb;0151&rsqb;</number> MAIQUETIA ATC IS MOST ASSUREDLY BELOW THE ICAO STD FOR ENGLISH SPEAKING CTLRS. (318067) </paragraph>
<paragraph id="P-0152" lvl="1"><number>&lsqb;0152&rsqb;</number> ALTHOUGH ENGLISH IS THE OFFICIAL LANGUAGE OF TRINIDAD, LCL DIALECT MAKES IT DIFFICULT TO UNDERSTAND CTLRS. (294060) </paragraph>
<paragraph id="P-0153" lvl="1"><number>&lsqb;0153&rsqb;</number> BETTER ENGLISH SPEAKING FOREIGN CTLRS AND USE OF STD PHRASEOLOGY IS NEEDED. (268223) </paragraph>
<paragraph id="P-0154" lvl="1"><number>&lsqb;0154&rsqb;</number> SITUATIONAL AWARENESS IS NONEXISTENT WHEN CTLRS SPEAK TO EVERYONE ELSE IN A FOREIGN LANGUAGE AND TO YOU IN BROKEN ENGLISH&excl; (344832) </paragraph>
<paragraph id="P-0155" lvl="1"><number>&lsqb;0155&rsqb;</number> TWR PHRASEOLOGY WAS NON STD AND HIS COMMAND OF ENGLISH WAS LIMITED, BUT WE WERE CLRED TO LAND. (332620) </paragraph>
<paragraph id="P-0156" lvl="0"><number>&lsqb;0156&rsqb;</number> Given the keyterms used in this search, the top-ranked narratives typically describe incidents involving miscommunication between air traffic controllers and flight crews due to language barriers, including poor use of the English language and the use of non-standard phraseology. For each search keyterm, here are some of the typical contexts, as indicated by the query models and reflected in the excerpts above: </paragraph>
<paragraph id="P-0157" lvl="0"><number>&lsqb;0157&rsqb;</number> &ldquo;Language&rdquo; is often found in the context of barriers, English and Spanish, clearances, air traffic controllers, ATC, problems, differences, and difficulties. </paragraph>
<paragraph id="P-0158" lvl="0"><number>&lsqb;0158&rsqb;</number> &ldquo;English&rdquo; is often found in the context of speaking and understanding; these attributes of English: poor, broken, or limited; Spanish and French; air traffic controllers; and pilots. </paragraph>
<paragraph id="P-0159" lvl="0"><number>&lsqb;0159&rsqb;</number> &ldquo;Phraseology&rdquo; is often found in the context of standard or proper usage, ATC, air traffic controllers, towers, clearances, and runways. </paragraph>
<paragraph id="P-0160" lvl="0"><number>&lsqb;0160&rsqb;</number> While the top narratives retrieved in this search all involve &ldquo;ATC language barrier factors&rdquo; it should be noted that there was no requirement that the narratives should involve ATC. Since the typical contexts of language barrier factors do, in fact, involve ATC, the top narratives also involved ATC. As a consequence, however, as one goes farther down the list of relevant narratives, at some point reports will be found that involve language barrier factors but not ATC. </paragraph>
<paragraph id="P-0161" lvl="0"><number>&lsqb;0161&rsqb;</number> Keyterm search will take any number of keyterms as queries, as in the above examples, but each term is treated individually. A search on the keyterms &ldquo;frequency congestion&rdquo; will return narratives that contain either one or both of these keyterms and their corresponding contexts. There is no guarantee, however, that both of the keyterms will appear in the top-ranked narratives because the search treats each query term as an independent item. </paragraph>
<paragraph id="P-0162" lvl="0"><number>&lsqb;0162&rsqb;</number> To address this kind of situation, keyterm search can also include a logical intersection of multiple searches. The query for each search can be specified by one or more keyterms. In this example, the &ldquo;frequency&rdquo; search uses the query &ldquo;freq freqs&rdquo; and requires an exact match. This query avoids matches on terms such as &ldquo;frequently&rdquo;. The &ldquo;congestion&rdquo; search uses the query &ldquo;congestion congested&rdquo; and requires an exact match. This query avoids matches on &ldquo;uncongested&rdquo;. Keyterm search then retrieves and relevance-ranks narratives that contain both &ldquo;frequency&rdquo; in context and &ldquo;congestion&rdquo; in context. </paragraph>
<paragraph id="P-0163" lvl="0"><number>&lsqb;0163&rsqb;</number> The following are excerpts from some of the most relevant narratives: </paragraph>
<paragraph id="P-0164" lvl="1"><number>&lsqb;0164&rsqb;</number> SEVERAL ATTEMPTS WERE MADE TO CONTACT TWR, BUT DUE TO EXTREME CONGESTION ON THIS FREQ NO LNDG CLRNC WAS OBTAINED . . . FREQ 124.15 WAS SO CONGESTED THAT NO ACFT COULD XMIT ON THIS REQ . . . CORRECTIVE ACTIONS: . . . NOTAM FREQ 124.75 AS AN ALTERNATE FREQ ON ATIS &lsqb;.&rsqb; DECREASE CONGESTION OF TWR FREQ. (151711) </paragraph>
<paragraph id="P-0165" lvl="1"><number>&lsqb;0165&rsqb;</number> I FINALLY SWITCHED BACK TO THE ORIGINAL CTLR FREQ BUT, DUE TO CONGESTED FREQ, I SWITCHED TO THE TWR FREQ TO GET THROUGH, WHICH I FINALLY DID . . . MAYBE ON SUBSEQUENT FLTS, IF THIS PROB SHOULD COME ABOUT, IT MIGHT BE A GOOD IDEA TO ALWAYS LEAVE ONE OF THE RADIOS SET TO THE LAST FREQ TO GO BACK TO WHEN THE FREQ GETS BUSY OR WHEN NOBODY SEEMS TO BE WORKING THAT FREQ. (237353) </paragraph>
<paragraph id="P-0166" lvl="1"><number>&lsqb;0166&rsqb;</number> AFTER CLRING RWY 33L, WE WERE UNABLE TO CONTACT GND CTL DUE TO FREQ CONGESTION . . . TAXIING INBND WITHOUT FIRST RECEIVING A CLRNC IS NOT AT ALL UNUSUAL AT FREQ CONGESTED ARPTS. IN SIMILAR SITS AT BWI AND ELSEWHERE, IF THE FREQ IS BLOCKED AND A CUSTOMARY TAXI RTE IS KNOWN AND CLR OF TFC, NEARLY AL&lsqb;L&rsqb; CAPTS I HAVE OBSERVED WOULD PROCEED SLOWLY, AS WE DID. WE PROGRESSED FARTHER THAN MOST ONLY BECAUSE THE FREQ WAS CONGESTED LONGER, IN PART BECAUSE THE CTLR WOULD NOT UNKEY HIS MIC WHILE MAKING MULTIPLE XMISSIONS. (173324) </paragraph>
<paragraph id="P-0167" lvl="1"><number>&lsqb;0167&rsqb;</number> BECAUSE OF EXTREME FREQ CONGESTION, ABBREVIATED TAXI INSTRUCTIONS ARE GIVEN AT ORD . . . THE FREQ CONGESTION AND CTLR WORKLOAD AT ORD MAKE IT HARD TO VERIFY INSTRUCTIONS THAT ARE UNCLR. WE ATTEMPTED CONTACT A FEW TIMES BEFORE BEING TOLD TO TURN NEAR THE BARRICADES, BUT WERE THEN GIVEN AN IMMEDIATE FREQ CHANGE WHICH PREVENTED PROMPT FEEDBACK FROM THE CTLR WHO GAVE US THE INSTRUCTIONS. TO THEIR CREDIT, THEY DID SPOT THE ERROR QUICKLY AND CALLED ON TWR FREQ WITH NEW INSTRUCTIONS. (WE MAY NOT HAVE HEARD SOME CALLS DUE TO RECEPTION PROBS.) THE CONGESTION AT ORD WOULD BE TOUGH TO FIX, BUT BETTER ARPT SIGNS SHOWING TAXI RTES THROUGH THE CONSTRUCTION AREAS WILL DEFINITELY CUT DOWN ON FUTURE PROBS. (252779) </paragraph>
<paragraph id="P-0168" lvl="0"><number>&lsqb;0168&rsqb;</number> These and other relevant narratives indicate that the topics &ldquo;frequency&rdquo; and &ldquo;congestion&rdquo; are often found in the same contexts, but that the exact phrase &ldquo;frequency congestion&rdquo; is not always present. Instead, many forms are found, such as: </paragraph>
<paragraph id="P-0169" lvl="2"><number>&lsqb;0169&rsqb;</number> CONGESTION ON THIS FREQ </paragraph>
<paragraph id="P-0170" lvl="2"><number>&lsqb;0170&rsqb;</number> FREQ 124.15 WAS SO CONGESTED </paragraph>
<paragraph id="P-0171" lvl="2"><number>&lsqb;0171&rsqb;</number> CONGESTION OF TWR FREQ </paragraph>
<paragraph id="P-0172" lvl="2"><number>&lsqb;0172&rsqb;</number> CONGESTED FREQ </paragraph>
<paragraph id="P-0173" lvl="2"><number>&lsqb;0173&rsqb;</number> FREQ CONGESTION </paragraph>
<paragraph id="P-0174" lvl="2"><number>&lsqb;0174&rsqb;</number> FREQ CONGESTED </paragraph>
<paragraph id="P-0175" lvl="2"><number>&lsqb;0175&rsqb;</number> FREQ WAS CONGESTED </paragraph>
<paragraph id="P-0176" lvl="0"><number>&lsqb;0176&rsqb;</number> A phrase search would also be useful for finding narratives relevant to &ldquo;frequency congestion&rdquo;. The preceding phrases suggest that an effective search would use a variety of phrase forms as queries, including: </paragraph>
<paragraph id="P-0177" lvl="2"><number>&lsqb;0177&rsqb;</number> FREQ CONGESTION </paragraph>
<paragraph id="P-0178" lvl="2"><number>&lsqb;0178&rsqb;</number> FREQ CONGESTED </paragraph>
<paragraph id="P-0179" lvl="2"><number>&lsqb;0179&rsqb;</number> CONGESTION FREQ </paragraph>
<paragraph id="P-0180" lvl="2"><number>&lsqb;0180&rsqb;</number> CONGESTED FREQ </paragraph>
<paragraph id="P-0181" lvl="0"><number>&lsqb;0181&rsqb;</number> Additional phrases include the plural form, &ldquo;freqs&rdquo;. </paragraph>
<paragraph id="P-0182" lvl="2"><number>&lsqb;0182&rsqb;</number> FREQS CONGESTION </paragraph>
<paragraph id="P-0183" lvl="2"><number>&lsqb;0183&rsqb;</number> FREQS CONGESTED </paragraph>
<paragraph id="P-0184" lvl="2"><number>&lsqb;0184&rsqb;</number> CONGESTION FREQS </paragraph>
<paragraph id="P-0185" lvl="2"><number>&lsqb;0185&rsqb;</number> CONGESTED </paragraph>
<paragraph id="P-0186" lvl="0"><number>&lsqb;0186&rsqb;</number> Most keyword search methods use term indexing such as used by Salton, 1981, where a word list represents each document and internal query. As a consequence, given a keyword as a user query, these methods use the presence of the keyword in documents as the main criterion of relevance. In contrast, keyterm search described herein uses indexing by term association, where a list of contextually associated term pairs represents each document and internal query. Given a keyterm as a user query, keyterm search uses not only the presence of the keyterm in the database being searched but also the contexts of the keyterm as the criteria of relevance. This allows retrieved documents to be sorted on their relevance to the keyterm in context. </paragraph>
<paragraph id="P-0187" lvl="0"><number>&lsqb;0187&rsqb;</number> Some methods such as Jing and Croft (1994), Gauch and Wang (1996), Xu and Croft (1996), and McDonald, Ogden, and Foltz (1997), utilize term associations to identify or display additional query keywords that are associated with the user-input keywords. These methods do not use term association to represent documents and queries, however, and instead rely on term indexing. As a consequence, &ldquo;query drift&rdquo; occurs when the additional query keywords retrieve documents that are poorly related or unrelated to the original keywords. Further, term index methods are ineffective in ranking documents on the basis of keyterms in context. </paragraph>
<paragraph id="P-0188" lvl="0"><number>&lsqb;0188&rsqb;</number> Unlike the keyterm search method described herein, the proximity indexing method of Hawking and Thistlewaite (1996, 1996) does not create a model of the query or models of the documents of the database. In the Hawking and Thistlewaite (1996, 1996) method, a query consists of a user-identified collection of words. These query words are compared with the words in the documents of the database. This search method of Hawking and Thistlewaite (1996, 1996) seeks documents containing length-limited sequences of words that contain subsets of the query words. Documents containing greater numbers of query words in shorter sequences of words are considered to have greater relevance. This is substantially different from the method of keyterm search described herein. </paragraph>
<paragraph id="P-0189" lvl="0"><number>&lsqb;0189&rsqb;</number> Further, as with conventional term indexing schemes, the method of Hawking and Thistlewaite (1996, 1996) allows a single query term to be used to identify documents containing the term, but unlike the keyterm search method described herein, the Hawking and Thistlewaite (1996, 1996) method cannot rank the identified documents containing the term according to the relevance of the documents to the contexts of the single query term within each document. </paragraph>
</section>
<section>
<heading lvl="1">Phrase Search </heading>
<paragraph id="P-0190" lvl="0"><number>&lsqb;0190&rsqb;</number> Although phrase search is similar in many aspects to keyterm search described above, there are two major differences between them. First, the form and interpretation of the query in phrase search are different from the form and interpretation of the query in keyterm search. Second, the method of assembly of the query model in phrase search is different from the method of assembly of the query model in keyterm search. </paragraph>
<paragraph id="P-0191" lvl="0"><number>&lsqb;0191&rsqb;</number> A phrase search query includes one or more query fields, and each query field can contain a sequence of terms. When applied to text, each phrase search query field can include a sequence of words such as two or more words, a phrase, a sentence, a paragraph, a document, or a collection of documents. In the following description, the word &ldquo;phrase&rdquo; is intended to be representative of any sequence of terms. Phrase search utilizes relationships among the terms in each phrase in forming the query model. In contrast, keyterm search includes no concept of query fields, and a keyterm query includes one or more terms that are treated as separate terms. Like keyterm search, phrase search can be applied to any type of sequential information. </paragraph>
<paragraph id="P-0192" lvl="0"><number>&lsqb;0192&rsqb;</number> A phrase search query model is assembled differently from a keyterm search query model. The keyterm query model is based on a gleaning process that expands the query by collecting matching relations and then reducing those relations to a unique set of relations. In phrase search, each query field in a phrase search query is modeled using the process of self-modeling a database as described above, and then the models of the phrase search query fields are combined as will be described in detail below to form a single phrase search query model. </paragraph>
<paragraph id="P-0193" lvl="0"><number>&lsqb;0193&rsqb;</number> FIGS. <highlight><bold>11</bold></highlight>-<highlight><bold>15</bold></highlight> illustrate various embodiments of phrase search. <cross-reference target="DRAWINGS">FIG. 11</cross-reference> illustrates an overview of one embodiment of the phrase search process <highlight><bold>1100</bold></highlight>. First, a number of relational models of subsets of a database are provided in block <highlight><bold>1102</bold></highlight>. Each one of the relational models includes one relational model of one subset of the database. A query is input in block <highlight><bold>1104</bold></highlight> to be compared to the relational models of subsets of the database. For one embodiment, the query includes one phrase. For another embodiment, the query includes multiple phrases. Next, a relational model of the query is created in block <highlight><bold>1106</bold></highlight>. The relational model of the query is then compared to each one of the relational models of subsets of the database in block <highlight><bold>1108</bold></highlight> that is described in more detail below. The identifiers of the relevant subsets are then output in block <highlight><bold>1110</bold></highlight>. For an alternative embodiment, the query can also be transformed as described above in keyterm search. </paragraph>
<paragraph id="P-0194" lvl="0"><number>&lsqb;0194&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 12</cross-reference> shows one process <highlight><bold>1200</bold></highlight> where the query includes a number of query fields. A relational model of the contents of each one of the query fields is created in block <highlight><bold>1202</bold></highlight>. Next, in block <highlight><bold>1204</bold></highlight>, the models of query fields are combined. <cross-reference target="DRAWINGS">FIG. 13</cross-reference> illustrates one embodiment of a method <highlight><bold>1204</bold></highlight> of combining the query field models. A first relation from a first one of the query field models is selected in block <highlight><bold>1302</bold></highlight>. A query model is initialized as being empty in block <highlight><bold>1304</bold></highlight>. Then the term pair from the selected query model is compared to the relations in the query model in block <highlight><bold>1306</bold></highlight>. If the term pair is not already in a relation in the query model, then the selected relation is included in the query model in block <highlight><bold>1310</bold></highlight>. If the term pair is already included in one of the relations of the query model, then the order of the term pair in the selected relation and the order of the term pair in the query model are compared in block <highlight><bold>1312</bold></highlight>. If the order is not the same, then the order of the term pair in the selected relation is reversed in block <highlight><bold>1314</bold></highlight> and the directional metrics recalculated in block <highlight><bold>1316</bold></highlight>, i.e. the value of LCM and the value of RCM of the selected relation are exchanged. Once the order of the term pair in the selected relation and the order of the term pair in the query model are the same, then each of the corresponding types of relational metrics of the relation in the query model and the selected relation is combined in a summation of each type and the summation results replace the previous values of the corresponding types of metrics in the relation in the query model in block <highlight><bold>1318</bold></highlight>. This process continues through the remainder of the relations in the selected query field model in blocks <highlight><bold>1320</bold></highlight>, <highlight><bold>1322</bold></highlight>. Once all relations of the first query field model have been processed then a subsequent query field model is selected in block <highlight><bold>1324</bold></highlight> and a first relation from the subsequent query field model is selected in block <highlight><bold>1326</bold></highlight> and this query field model is processed in blocks <highlight><bold>1306</bold></highlight>-<highlight><bold>1322</bold></highlight>. Once all of the query field models have been processed, then the resulting query model is output in block <highlight><bold>1328</bold></highlight>. </paragraph>
<paragraph id="P-0195" lvl="0"><number>&lsqb;0195&rsqb;</number> Inputting the query can also include assigning a weight to at least one of the query fields. Each one of the RSMs corresponding to the selected query field is scaled by a factor determined by the assigned weight. This allows each query field to be given an importance value relative to the other query fields. </paragraph>
<paragraph id="P-0196" lvl="0"><number>&lsqb;0196&rsqb;</number> Stopterms play an important role in phrase search because some queries will contain one or more stopterms. Stopterms can include any terms, but in one alternative, stopterms include words such as &ldquo;a&rdquo;, &ldquo;an&rdquo;, &ldquo;the&rdquo;, &ldquo;of&rdquo;, &ldquo;to&rdquo;, and &ldquo;on&rdquo;. In phrase search, the user can add terms to, or remove terms from, the list of stopterms. </paragraph>
<paragraph id="P-0197" lvl="0"><number>&lsqb;0197&rsqb;</number> In one alternative of phrase search, a search finds subsets that contain a particular phrase that includes particular stopterms, such as &ldquo;on approach to the runway&rdquo;. In another alternative of phrase search, stopterms are ignored and a search finds subsets containing phrases whose non-stopterms match the query phrase or phrases. For example, in the query &ldquo;We were on approach to the runway at LAX&rdquo; the words &ldquo;we&rdquo;, &ldquo;were&rdquo;, &ldquo;on&rdquo;, &ldquo;to&rdquo;, &ldquo;the&rdquo;, and &ldquo;at&rdquo; could, if the user so indicated, be considered to be stopterms, and the query would match subsets containing sequences such as &ldquo;He was on approach to runway 25L, a mile from LAX&rdquo;. In another embodiment, a query &ldquo;on approach to the runway&rdquo; matches all occurrences in subsets of &ldquo;on approach to the runway&rdquo; as well as similar phrases in subsets such as &ldquo;on approach to runway 25R&rdquo;. Preferably the exact matches are listed first in the output. </paragraph>
<paragraph id="P-0198" lvl="0"><number>&lsqb;0198&rsqb;</number> In phrase search, a query model can be modified as a function of the stopterms in the query. Recall that each query model contains relations, and each relation contains a term pair and associated relational summation metrics (RSMs). When a query model is created based on a query such as &ldquo;on approach to the runway&rdquo;, that query model can include query model term pairs such as &ldquo;on, approach&rdquo;, &ldquo;on, to&rdquo;, &ldquo;approach, runway&rdquo;, as well as others. One alternative is to eliminate all relations containing stopterms. As another alternative, stopterms can be retained and treated just like any other term. In yet another alternative, relations containing one or more stopterms can be differentiated from others. For example, in order to adjust the weight of each relation to favor topical term pairs such as &ldquo;approach, runway&rdquo; over terms pairs containing one stopterm such as &ldquo;the, runway&rdquo;, and term pairs containing two stopterms such as &ldquo;on, to&rdquo;, it is possible to modify the metrics of each relation as a function of the stopterms contained in the term pairs. </paragraph>
<paragraph id="P-0199" lvl="0"><number>&lsqb;0199&rsqb;</number> If neither a first term in the query model term pair nor a second term in the query model term pair is one of the stopterms then the RSMs are increased. For another embodiment, if both a first term in the query model term pair and a second term in the query model term pair are included in the set of stopterms then the RSMs are decreased. Alternatively, if either but not both a first term in the query model term pair or a second term in the query model term pair is one of the sets of stopterms then the RSMs are unchanged. </paragraph>
<paragraph id="P-0200" lvl="0"><number>&lsqb;0200&rsqb;</number> A set of emphasis terms can also be provided. Emphasis terms are terms that are used to provide added emphasis to the items that contain the emphasis terms. The set of emphasis terms can include any terms. Typically the set of emphasis terms includes terms of greater importance in a particular search. For one embodiment, if both a first term in the query term pair and a second term in the query term pair are included in the set of emphasis terms then the RSMs are increased. For another embodiment, if either but not both a first term in the query term pair or a second term in the query term pair is one of the set of emphasis terms then the RSMs are unchanged. </paragraph>
<paragraph id="P-0201" lvl="0"><number>&lsqb;0201&rsqb;</number> For still another alternative if neither a first term in the query model term pair nor a second term in the query model term pair is one of the emphasis terms then the RSMs are decreased. </paragraph>
<paragraph id="P-0202" lvl="0"><number>&lsqb;0202&rsqb;</number> Another alternative embodiment includes a list of stop relations. A stop relation is a relation that does not necessarily include stopterms but is treated similarly to a stopterm in that stop relations may be excluded, or given more or less relevance weighting, etc., as described above for stopterms. Each one of the stop relations includes a first term and a second term and a number of types of relational metrics. For one embodiment, any stop relations in the relational model of the query are eliminated from the query. Eliminating a stop relation blocks the collection of the related concepts described by the stop relation. For example, returning to the fatigue example described above, a stop relation might include the term pair &ldquo;fatigue&rdquo; and &ldquo;metal&rdquo;. Eliminating the &ldquo;fatigue, metal&rdquo; stop relation from the model of the query results in removing that contextual association from consideration as a relevant feature. </paragraph>
<paragraph id="P-0203" lvl="0"><number>&lsqb;0203&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 14</cross-reference> illustrates one embodiment <highlight><bold>1108</bold></highlight> of comparing a query model to each one of the relational models of subsets. The process <highlight><bold>1400</bold></highlight> includes determining the relevance metrics for each one of the relational models of the subsets. This is initiated by determining an intersection model of the relational model of the query and the model of the first subset. Determining an intersection model can include determining the intersectional relations in block <highlight><bold>1404</bold></highlight>. Each one of the intersectional relations has a shared term pair. The shared term pair is present in at least one relation in each of the query model and the first subset relational model. Each intersectional relation also has a number of intersection metrics (IMs). Each IM is equal to a function of RSM<highlight><subscript>Q1 </subscript></highlight>and RSM<highlight><subscript>S1</subscript></highlight>. RSM<highlight><subscript>Q1 </subscript></highlight>is a type of relational summation metric in the relational model of the query, and RSM<highlight><subscript>S1 </subscript></highlight>is a corresponding type of relational summation metric in the relational model of the first one of the relational models of the subsets. Next, a relevance metric for each one of the types of relational summation metrics is determined. Each one of the relevance metrics includes a function of the corresponding type of relational summation metrics of each one of the intersection relations in block <highlight><bold>1406</bold></highlight>. The process is repeated in blocks <highlight><bold>1408</bold></highlight> and <highlight><bold>1410</bold></highlight> for any additional models of subsets. Alternatively, the function of RSM<highlight><subscript>Q1 </subscript></highlight>and RSM<highlight><subscript>S1 </subscript></highlight>is equal to &lsqb;RSM<highlight><subscript>Q1</subscript></highlight>&rsqb;*&lsqb;RSM<highlight><subscript>S1</subscript></highlight>&rsqb;. The function of the corresponding IMs of all intersection relations can also include a summation of all of the RSM<highlight><subscript>Q1 </subscript></highlight>of each one of the first query relations that are included in the intersection relations. </paragraph>
<paragraph id="P-0204" lvl="0"><number>&lsqb;0204&rsqb;</number> Determining an intersection model can also include applying a scaling factor to the function of the corresponding intersection metrics. Various embodiments of applying the scaling factor are described above in the keyterm search and are similarly applicable to phrase search. </paragraph>
<paragraph id="P-0205" lvl="0"><number>&lsqb;0205&rsqb;</number> Calculating a set of first relevance metrics for a first one of the relational models of the subsets can also include assigning a zero relevance to a particular subset if all term pairs of the relational model of the first query are not included in the relational model of the particular subset. </paragraph>
<paragraph id="P-0206" lvl="0"><number>&lsqb;0206&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 15</cross-reference> illustrates one embodiment of a process of re-weighting a query model <highlight><bold>1500</bold></highlight>. First, the query model is selected in block <highlight><bold>1502</bold></highlight>. Then a global model is selected in block <highlight><bold>1504</bold></highlight>. The global model is a model of a large fraction of a database, an entire database, or a number of databases. The modeled database or databases can include a number of subsets that are similar to, or identical to, the subsets to which the query model will be compared. Alternatively, the global model can include a number of relations in common with the selected query model. Next, a first relation in the selected model of the query is selected in block <highlight><bold>1506</bold></highlight>. Next, a relation is included in a re-weighted query model in block <highlight><bold>1508</bold></highlight>. The relation in the re-weighted query model includes the same term pairs as the selected relation. Each one of the corresponding types of metrics of the relation in the re-weighted query model are equal to the result of dividing the corresponding type of metric in the selected relation by the corresponding type of metric in the relation from the global model. The process continues in blocks <highlight><bold>1510</bold></highlight> and <highlight><bold>1512</bold></highlight> until all relations in the query model are re-weighted. Then the re-weighted query model is output in block <highlight><bold>1514</bold></highlight>. </paragraph>
<paragraph id="P-0207" lvl="0"><number>&lsqb;0207&rsqb;</number> The resulting metrics in the re-weighted query models can each be multiplied by the frequencies, within a selected collection of subsets, of each term of the term pair of the relation. Alternatively, the resulting metrics are each multiplied by the frequencies, within a selected collection of query fields, of each term of the term pair of the relation. For another alternative, the resulting metrics are multiplied by the frequency of one of the terms of the term pair. </paragraph>
<paragraph id="P-0208" lvl="0"><number>&lsqb;0208&rsqb;</number> The primary effect of re-weighting the query model is to reduce the influence of relations that are prominent in large numbers of subsets relative to those that are less prominent in those subsets. This effect is combined with the already present range of influence of relations in the query model, as indicated by the range of magnitudes of the corresponding metrics of the relations, which is a function of the degree of contextual association of those relations in the query. Re-weighting ensures that common and generic relations are reduced in influence in the re-weighted query model relative to less common and less generic relations. For example, the relation between &ldquo;approach&rdquo; and &ldquo;runway&rdquo; is very common among subsets of the ASRS database, while the relation between &ldquo;terrain&rdquo; and &ldquo;FMS&rdquo; (flight management system) is much less common. As a consequence, in a re-weighted query model, the relation between &ldquo;approach&rdquo; and &ldquo;runway&rdquo; would be reduced in influence relative to the relation between &ldquo;terrain&rdquo; and &ldquo;FMS&rdquo;. The additional and optional effect of multiplying by the frequencies of the terms is to favor those relations whose individual terms are more prominent in a particular selected collection of subsets, or within a particular selected collection of query fields. This disfavors relations with terms that are less prominent in the collection, even if the relations are relatively rare among large numbers of subsets. </paragraph>
<paragraph id="P-0209" lvl="0"><number>&lsqb;0209&rsqb;</number> Many alternative forms of output of the phrase search process are useful, and the alternative forms are similar to those described above in keyword search. A difference in the phrase search output is the determination of metric values associated with the displayed shared term pairs. The output display for phrase search can also include, for each one of the plurality of shared term pairs, 1) displaying a feedback metric of the query (FBM<highlight><subscript>Q1</subscript></highlight>) equal to a combination of an LCM<highlight><subscript>Q1 </subscript></highlight>and an RCM<highlight><subscript>Q1</subscript></highlight>, and 2) displaying a feedback metric of the subset FBM<highlight><subscript>S1 </subscript></highlight>equal to a combination of an LCM<highlight><subscript>S1 </subscript></highlight>and an RCM<highlight><subscript>S1</subscript></highlight>, and 3) displaying a product equal to &lsqb;FBM<highlight><subscript>Q1</subscript></highlight>&rsqb;*&lsqb;FBM<highlight><subscript>S1</subscript></highlight>&rsqb;. LCM<highlight><subscript>Q1 </subscript></highlight>is equal to a left contextual metric of the shared term pair in the query. RCM<highlight><subscript>Q1 </subscript></highlight>is equal to a right contextual metric of the shared term pair in the query. LCM<highlight><subscript>S1 </subscript></highlight>is equal to a left contextual metric of the shared term pair in the subset. RCM<highlight><subscript>S1 </subscript></highlight>is equal to a right contextual metric of the shared term pair in the subset. </paragraph>
<paragraph id="P-0210" lvl="0"><number>&lsqb;0210&rsqb;</number> For another alternative embodiment of phrase search, multiple queries can be applied to the phrase search processes described above, with each phrase search query including multiple query fields. The processes of performing multiple queries in phrase search are similar to the processes of performing multiple queries in keyterm search, as described above in keyterm search. </paragraph>
<paragraph id="P-0211" lvl="0"><number>&lsqb;0211&rsqb;</number> This application is intended to cover any adaptations or variations of the present invention. For example, those of ordinary skill within the art will appreciate that the phrase search process can be executed in varying orders instead of being executed in the order as described above. </paragraph>
<paragraph id="P-0212" lvl="0"><number>&lsqb;0212&rsqb;</number> The use of phrase search is illustrated below by various searches of the Aviation Safety Reporting System (ASRS) database of incident report narratives. As described below, phrase search easily finds incident narratives in the ASRS database that contain phrases of interest. As examples, and to illustrate some important considerations, several phrase searches are presented here, including: &ldquo;conflict alert&rdquo;, &ldquo;frequency congestion&rdquo;, &ldquo;cockpit resource management&rdquo;, &ldquo;similar sounding callsign(s)&rdquo;, and &ldquo;fit crew fatigue&rdquo;. These examples are representative of phrase searches that would be useful to the ASRS. </paragraph>
<paragraph id="P-0213" lvl="0"><number>&lsqb;0213&rsqb;</number> The simplest phrase search uses a single phrase as the query. This can be helpful when looking for a thing, concept, or action that is expressed using multiple terms, such as &ldquo;conflict alert.&rdquo; A &ldquo;conflict alert&rdquo; is &ldquo;A function of certain air traffic control automated systems designed to alert radar controllers to existing or pending situations recognized by the program parameters that require his immediate attention/action.&rdquo; (DOT: Air Traffic Control, Air Traffic Service, U.S. Dept. of Transportation, 7110.65C, 1982.) </paragraph>
<paragraph id="P-0214" lvl="0"><number>&lsqb;0214&rsqb;</number> A search for the narratives that contain the phrase &ldquo;conflict alert&rdquo; is simple. The user merely enters the phrase. Phrase search retrieves and displays the most relevant narratives, with instances of the phrase highlighted. An additional output includes the highlighted narratives, a complete list of relevant narratives, and the criterion model used to search the phrase database. The following is one of the most relevant narratives found by phrase search: </paragraph>
<paragraph id="P-0215" lvl="1"><number>&lsqb;0215&rsqb;</number> THIS ASRS RPT IS ADDRESSED TO THE ARTS IIA CONFLICT ALERT FEATURE USED IN MANY TRACONS IN THE COUNTRY. THIS FEATURE IS DESIGNED TO BE AN AID TO CTLRS IN PREDICTING IMPENDING CONFLICTIONS OF AIR TFC. THE ACTUAL OP OF THE CONFLICT ALERT IS THAT IT DOES NOT ACTIVATE, IN THE MAJORITY OF CASES, UNTIL THE ACFT ARE IN VERY CLOSE PROX OR HAVE ALREADY PASSED EACH OTHER. THE LATEST VERSION (A2.07) BECAME OPERATIONAL LAST MONTH AND THE PROB STILL EXISTS. THE SOFTWARE PROGRAM MUST BE IMMENSE AND I&apos;M SURE THAT IT MUST BE A MONUMENTAL TASK TO DEBUG, HOWEVER, IT MUST BE DONE TO MAKE THE CONFLICT ALERT FEATURE A USABLE TOOL FOR CTLRS. A UCR RPT HAS BEEN SUBMITTED TO THE FAA. THE CONFLICT ALERT IS SUPPOSED TO PROJECT ACFT COURSES AND RATES OF CLB AND ALARM WHEN AN IMMINENT CONFLICT IS DETECTED. MY PAST EXPERIENCES WITH ARTS III AND ARTS IIIA PROVED THIS TO BE THE CASE. UNFORTUNATELY THE ARTS IIA SYS HAS NEVER FUNCTIONED AS WELL FROM THE ONSET TO THE PRESENT DAY. ARTS IIA VERSION A2.07 IS CURRENTLY IN USE AND THE CONFLICT ALERT HAS, IN MY ESTIMATION, LIMITED USE TO THE CTLR AS AN AID IN PREDICTING CONFLICTS. IT FUNCTIONS MORE AS AN IMMINENT COLLISION ALERT OR AN &lsquo;AFTER THE FACT ALERT&rsquo; (YOU JUST HAD A DEAL). THE AURAL/VISUAL ALARM DOES NOT ACTIVATE UNTIL THE ACFT ARE IN VERY CLOSE PROX AND IMMEDIATE ACTION IS REQUIRED TO PREVENT A COLLISION, OR THE ACFT HAVE ALREADY PASSED EACH OTHER AND NOTHING CAN BE DONE (EXCEPT TURN YOURSELF IN)&excl; &excl; THE MAJORITY OF DATA CONCERNING CONFLICT ALERT ALARMS WAS RECEIVED ON ACFT UTILIZING VISUAL SEPARATION METHODS (WHEN THE SEPARATION IS VASTLY REDUCED). THE CONFLICT ALERT FEATURE COULD BE A VALUABLE SEPARATION TOOL FOR THE CTLR IF IT WERE TO OPERATE AS DESIRED. THIS SHORTCOMING MUST HAVE SURFACED IN THE TESTING OF ARTS IIA BEFORE GOING OPERATIONAL. I ASSUME &lsquo;DEBUGGING&rsquo; A PROGRAM OF THIS SIZE MUST BE A MONUMENTAL TASK AND THIS IS WHY I HAVE WAITED THIS LONG TO INITIATE THE PAPERWORK. VERSION A2.07 WAS JUST RELEASED IN AUGUST AND THERE WAS NO CHANGE IN THE OP OF THE CONFLICT ALERT FEATURE. (251367) </paragraph>
<paragraph id="P-0216" lvl="0"><number>&lsqb;0216&rsqb;</number> Since the phrase &ldquo;conflict alert&rdquo; is found in exactly the form of the query, and since there are many occurrences of the phrase, this narrative is considered to be highly relevant. </paragraph>
<paragraph id="P-0217" lvl="0"><number>&lsqb;0217&rsqb;</number> A search for the narratives that contain the phrase &ldquo;frequency congestion&rdquo; is also simple. Inputting the phrase &ldquo;frequency congestion&rdquo; initiates the phrase search. In the keyterm search described above on &ldquo;frequency&rdquo; and &ldquo;congestion&rdquo;, however, multiple forms of the phrase &ldquo;frequency congestion&rdquo; were found in the ASRS database and others are possible. The forms include: </paragraph>
<paragraph id="P-0218" lvl="2"><number>&lsqb;0218&rsqb;</number> FREQ CONGESTION </paragraph>
<paragraph id="P-0219" lvl="2"><number>&lsqb;0219&rsqb;</number> FREQ CONGESTED </paragraph>
<paragraph id="P-0220" lvl="2"><number>&lsqb;0220&rsqb;</number> CONGESTION FREQ </paragraph>
<paragraph id="P-0221" lvl="2"><number>&lsqb;0221&rsqb;</number> CONGESTED FREQ </paragraph>
<paragraph id="P-0222" lvl="2"><number>&lsqb;0222&rsqb;</number> FREQS CONGESTION </paragraph>
<paragraph id="P-0223" lvl="2"><number>&lsqb;0223&rsqb;</number> FREQS CONGESTED </paragraph>
<paragraph id="P-0224" lvl="2"><number>&lsqb;0224&rsqb;</number> CONGESTION FREQS </paragraph>
<paragraph id="P-0225" lvl="2"><number>&lsqb;0225&rsqb;</number> CONGESTED FREQS </paragraph>
<paragraph id="P-0226" lvl="0"><number>&lsqb;0226&rsqb;</number> If the user provides these phrases as the query, phrase search finds the narratives that contain one or more of them, then displays the most relevant narratives, with instances of the phrase highlighted. The following is one of the highly relevant narratives retrieved by phrase search: </paragraph>
<paragraph id="P-0227" lvl="1"><number>&lsqb;0227&rsqb;</number> WE WERE CLRED A CIVET 1 ARR TO LAX. THE ARR ENDS AT ARNES AT 10000 FT WITH THE NOTE &lsquo;EXPECT ILS APCH.&rsquo; WE WERE SWITCHED TO APCH CTL AROUND ARNES. THERE WAS AN ACFT COMING BACK TO LAND AFTER TKOF AND THUS THE FREQ WAS CONGESTED. WE WERE BLOCKED ON SEVERAL ATTEMPTS TO CONTACT APCH CTL AND WERE UNABLE TO CHK IN. WE CONTINUED OUR DSCNT MEETING THE ALT CONSTRAINTS FOR ILS RWY 25L. SOMEWHERE AFTER &lsquo;FUELR,&rsquo; APCH CTL CALLED US AND TOLD US TO LEVELOFF AT 7000 FT AND THAT WE WERE ONLY CLRED TO 10000 FT. THE QUESTION IS, &lsquo;IF YOU ARE UNABLE TO CONTACT APCH CTL, ARE YOU IN A LOST COM SIT&quest;&rsquo; IF YOU LEVELOFF AT ARNES, YOU VERY QUICKLY FIND YOURSELF TOO HIGH TO LAND. DO YOU FLY ALL THE WAY TO THE ARPT AT 10000 FT OR DO YOU FLY THE ILS APCH&quest; IS FREQ CONGESTION A LEGITIMATE LOST COM SIT&quest; CALLBACK CONVERSATION WITH RPTR REVEALED THE FOLLOWING INFO: RPTR SENT 2 CAPT RPTS TO HIS COMPANY QUESTIONING THE PROC, BUT AS YET, NO ANSWER. HE WAS NOT SURE WHAT WAS HIS CLRNC LIMIT BECAUSE THE CIVET 1 ARR ENDS AT ARNES WITH A NOTE TO &lsquo;EXPECT ILS APCH.&rsquo; THE RPTR THOUGHT THAT PERHAPS WHEN UNABLE TO OBTAIN APCH CLRNC PRIOR TO ARNES AND IF IT WAS A CLRNC LIMIT, THEN HE SHOULD ENTER HOLDING AS DEPICTED ON THE CHART. TO CLARIFY, THE SOCAL APCH CTLR SUPVR WAS CONTACTED AND HE SAID THAT THE ACFT WAS CLRED TO THE ARPT AS PART OF THE ORIGINAL CLRNC AND THAT THE ARR IS NOT A CLRNC LIMIT. ALSO, THAT THE ACFT MUST MAINTAIN THE LAST ASSIGNED ALT AND, IF APCH CTLR MESSES UP AND DOESN&apos;T GIVE THE APCH CLRNC, THEN THE ACFT IS EXPECTED TO MAINTAIN ALT AND CONTINUE INBOUND ON THE LOC COURSE. THE SUPVR SAID THAT THE ACFT DEFINITELY SHOULD NOT ENTER HOLDING, BUT CONTINUE INBOUND AT THE LAST ASSIGNED ALT. (306082) </paragraph>
<paragraph id="P-0228" lvl="0"><number>&lsqb;0228&rsqb;</number> The above narrative is relevant because it contains two of the query phrases. One is in exact form (&ldquo;FREQ CONGESTION&rdquo;) and one is nearly in exact form (&ldquo;FREQ WAS CONGESTED&rdquo;). </paragraph>
<paragraph id="P-0229" lvl="0"><number>&lsqb;0229&rsqb;</number> A search for the narratives that contain the phrase &ldquo;cockpit resource management&rdquo; is simple, but it raises two issues. First, the ASRS uses many abbreviations, and the term &ldquo;management&rdquo; is one of the terms abbreviated. To save the user from having to know the abbreviations, phrase search maps terms to ASRS abbreviations as described above. The second issue raised by a search for narratives containing the phrase &ldquo;cockpit resource management&rdquo; is the fact that the phrase has more than 2 terms. As a consequence, the phrase search can retrieve narratives containing only part of the phrase. The default, however, is to require that the whole phrase be present in each retrieved narrative. </paragraph>
<paragraph id="P-0230" lvl="0"><number>&lsqb;0230&rsqb;</number> Inputting the phrase: &ldquo;cockpit resource management&rdquo; initiates the phrase search. Phrase search maps the vocabulary of the phrase to the vocabulary of the ASRS narratives. In this case, the result is &ldquo;cockpit resource mgmnt&rdquo;, and this phrase is used as the actual query phrase. Phrase search then retrieves the narratives containing the phrase &ldquo;cockpit resource mgmnt&rdquo;, and the most relevant narratives are displayed with all instances of the phrase highlighted. The following is an example: </paragraph>
<paragraph id="P-0231" lvl="1"><number>&lsqb;0231&rsqb;</number> COPLT&apos;S BRASH ATTITUDE HAD BEEN A SORE SPOT WITH ME ALL MONTH AND REPEATED DISCUSSION WITH HIM HAD FAILED TO ACHIEVE ANY RESULTS. ALTHOUGH I NOTICED EARLY ON THAT HIS PLTING SKILLS DIDN&apos;T JUSTIFY HIS CONFIDENCE LEVEL AND I HAD RECOGNIZED THE NEED TO CONTINUALLY MONITOR HIS PERF, I HAD TO TAKE MY EYES OFF OF HIM FOR ABOUT 2 MINS (2 MINS&excl;&excl;). IN THAT PERIOD OF TIME HE DEVIATED OFF OUR RTING BY ABOUT 8 MI PROMPTING AN INQUIRY FROM ZAU. THE FO&apos;S ATTITUDE WAS &lsquo;OK, I MADE A MISTAKE&mdash;SO WHAT&quest;&rsquo; I BELIEVE (DUE TO INTERACTING WITH THIS INDIVIDUAL ON PREVIOUS TRIPS) THAT HE FELT HIS ROLE IN THE COCKPIT WAS ONE OF DECISION MAKER. ALTHOUGH I EXPLAINED TO HIM THAT WE WERE A TEAM, AND EACH MEMBER OF THE TEAM WAS ESSENTIAL TO OUR SAFETY, IT IS IN THE CAPT&apos;S JOB DESCRIPTION AS BEING THE FINAL AUTHORITY AS TO THE OP OF THE FLT. WITH THE ADVENT OF COCKPIT RESOURCE MGMNT I&apos;VE NOTICED A TENDENCY WITH SOME FO&apos;S TO IGNORE THE FACT THAT THERE IS A HIERARCHY WITHIN THE COCKPIT, TO THE POINT OF CONSIDERING THEMSELVES AUTONOMOUS (AS IN THIS EXTREME CASE). WHILE THE INTENT OF COCKPIT RESOURCE MGMNT IS OK, I MUST SAY THAT THE CREW&apos;S RELATIONSHIP WITH THE CAPT IS ONE OF ORDINATE-SUBORDINATE, AND COCKPIT RESOURCE MGMNT TENDS TO OVERLOOK OR MINIMIZE THIS CONCEPT. IF MY ASSESSMENT IS CORRECT, COCKPIT RESOURCE MGMNT SHOULD BE MODIFIED TO REFLECT THE REALITIES OF LINE OPS. (222230) </paragraph>
<paragraph id="P-0232" lvl="0"><number>&lsqb;0232&rsqb;</number> The narratives considered to be the most relevant are the ones that have the best and the most matches to the query phrase. Phrase search can optionally provide narratives that contain only a fragment of the phrase, such as &ldquo;resource management&rdquo;. In that case, narratives containing only fragments of the phrase would be added at the bottom of the list of relevant narratives. The following are some example excerpts from narratives containing only fragments of the phrase &ldquo;cockpit resource management&rdquo;: </paragraph>
<paragraph id="P-0233" lvl="1"><number>&lsqb;0233&rsqb;</number> THIS AIRLINE HAS EXERTED A LOT OF ENERGY TO PROMOTE CREW RESOURCE MGMNT, BUT ALL OF MY EFFORT TO PROVIDE USEFUL INPUT FAILED. ALL DURING THIS INCIDENT I WAS WELL AWARE OF PREVIOUS ACCIDENTS IN WHICH NO ONE CHALLENGED THE CAPT AS HE MADE IMPROPER DECISIONS. I WANTED TO MAKE SURE THAT THIS WOULD NOT HAPPEN DUE TO MY INACTION. I DISCOVERED MY LIMITATIONS IN THE FACE OF A CAPT WHO MADE IMPROPER DECISIONS. (279099) </paragraph>
<paragraph id="P-0234" lvl="1"><number>&lsqb;0234&rsqb;</number> FO IS LOW TIME AND &lsqb;CAPT&rsqb; ADMITS HE EXERCISED POOR COCKPIT MGMNT. SHOULD HAVE INSISTED THAT FO HELP WITH TAXI VIGILANCE. (202096) </paragraph>
<paragraph id="P-0235" lvl="1"><number>&lsqb;0235&rsqb;</number> . . . NEW HIRES OFTEN BITE THEIR TONGUES RATHER THAN CONFRONT CAPTS ABOUT COCKPIT CREW MGMNT PROBS, BECAUSE OF THE POSSIBILITY OF A NEGATIVE EVALUATION BEING SENT TO THE COMPANY, WHICH COULD EFFECT YOUR BEING KEPT ON THE JOB BEYOND PROBATION. MY RELUCTANCE TO WORK THIS OUT CAUSED ME TO PUT UP WITH A COCKPIT ENVIRONMENT THAT WAS LESS THAN SATISFACTORY. (143981) </paragraph>
<paragraph id="P-0236" lvl="1"><number>&lsqb;0236&rsqb;</number> LACK OF TRAINING COVERING COCKPIT MGMNT RESOURCES. (206734) </paragraph>
<paragraph id="P-0237" lvl="1"><number>&lsqb;0237&rsqb;</number> COCKPIT RESOURCES MGMNT HAS HELPED IN THE ACFT; MAYBE MORE PERSONAL CONTACT BTWN ATC AND PLTS WOULD DO THE SAME. (141625) </paragraph>
<paragraph id="P-0238" lvl="0"><number>&lsqb;0238&rsqb;</number> The benefit of matching phrase fragments is that a greater number of relevant reports can be found, even when the author of the narrative didn&apos;t get some standard phrase exactly right. Some of these reports can be highly relevant to the topics of interest. </paragraph>
<paragraph id="P-0239" lvl="0"><number>&lsqb;0239&rsqb;</number> A search for the narratives that contain the phrase &ldquo;similar sounding callsign&rdquo; raises three issues. The first issue is that the ASRS uses various forms of some terms and phrases. Sometimes &ldquo;call sign&rdquo; is used, while other times &ldquo;callsign&rdquo; is used. Similarly, &ldquo;descent&rdquo; is sometimes abbreviated as &ldquo;dscnt&rdquo; while other times it is &ldquo;dsnt&rdquo;. And there are other such examples. To achieve consistency, phrase search standardizes usage in the database and also in the query. This is accomplished using the same mapping technique that is applied to handle ASRS abbreviations. That is, the various forms of some terms are mapped to standard forms. Since &ldquo;call sign&rdquo; is more common, that is the form used consistently by phrase search. Thus, &ldquo;callsign&rdquo; is mapped to &ldquo;call sign&rdquo;. Similarly, &ldquo;callsigns&rdquo; is mapped to &ldquo;call signs&rdquo;. </paragraph>
<paragraph id="P-0240" lvl="0"><number>&lsqb;0240&rsqb;</number> The second issue involves singular and plural forms of phrases. Specifically, if a singular form is specified in the input, the plural form is often of interest as well, and vice versa. In this case, narratives containing the phrase &ldquo;similar sounding call sign&rdquo; (singular), &ldquo;similar sounding call signs&rdquo; (plural), or both might be of interest. Phrase search can require the user to input all forms of a phrase that are to be used as a query. </paragraph>
<paragraph id="P-0241" lvl="0"><number>&lsqb;0241&rsqb;</number> The third issue raised by this search involves phrase search&apos;s ranking of narratives when searching for long and/or multiple phrases. In the case of &ldquo;similar sounding call sign(s)&rdquo;, some narratives will contain both singular and plural forms of the phrase. Some narratives will contain only one of the forms. Some narratives will contain only fragments, such as &ldquo;similar call sign&rdquo;, or &ldquo;call signs&rdquo;. Phrase search&apos;s rank ordering of narratives containing these various forms is done in the order just described, as will be shown. This is a useful order, as it is in accordance with an intuitive sense of what constitutes a good match to the query phrases. The following are excerpts from some of the most relevant narratives: </paragraph>
<paragraph id="P-0242" lvl="1"><number>&lsqb;0242&rsqb;</number> BECAUSE WE HAD BEEN ON TWR FREQ FOR SO LONG, WE HAD NO AWARENESS OF THE OTHER ACFT WITH A SIMILAR CALL SIGN . . . THE FOLLOWING ARE CONTRIBUTING FACTORS. SIMILAR SOUNDING CALL SIGNS . . . DURING SIMULTANEOUS INTERSECTING RY DEPS, EXTREME CARE SHOULD BE TAKEN WITH ACFT HAVING LIKE CALL SIGN . . . THEY HAD MISUNDERSTOOD TKOF CLRNC FOR AN ACFT WITH A SIMILAR SOUNDING CALL SIGN, ON ANOTHER RWY. (198106) </paragraph>
<paragraph id="P-0243" lvl="1"><number>&lsqb;0243&rsqb;</number> WHILE INBOUND TO DTW METRO ARPT FROM KALAMAZOO, Mich., ON COMPANY XX50 THERE WERE 2 OTHER COMPANY FLTS: COMPANY XX53 AND COMPANY X50 WITH SIMILAR SOUNDING CALL SIGNS AS OURS . . . APPARENTLY WE WERE FOLLOWING A CLRNC FOR AN ACFT OF A SIMILAR SOUNDING CALL SIGN. I DID READ BACK THE ORIGINAL CLRNC WITH OUR OWN CALL SIGN, HOWEVER. THERE WAS MUCH CONFUSION WITH SIMILAR CALL SIGNS. (192640) </paragraph>
<paragraph id="P-0244" lvl="1"><number>&lsqb;0244&rsqb;</number> I VERIFIED THE ALT AND FREQ AS BEING CORRECT BUT DID NOT CATCH THE CALL SIGN . . . ALTHOUGH I DID NOT CLARIFY THE CORRECT CALL SIGN . . . I CANNOT IMAGINE WHY ANY PLT WOULD CLB WITHOUT QUESTION WHEN HE HAD JUST BEEN ISSUED 2 CONVERGING TARGETS AT ALTS ABOVE HIM . . . WE WERE INFORMED BY OUR UNION SAFETY CHAIRMAN THAT WE HAD ACCEPTED THE 13000 FT CLB AND FREQ CHANGE FOR ANOTHER FLT, ACR X, WITH A SIMILAR SOUNDING CALL SIGN . . . CORRECTIVE ACTION: REDUCE, IF NOT ELIMINATE, SIMILAR SOUNDING CALL SIGNS. (255236) </paragraph>
<paragraph id="P-0245" lvl="1"><number>&lsqb;0245&rsqb;</number> HE THEN STATED HE HAD ANOTHER COMPANY WITH A SIMILAR SOUNDING CALL SIGN ON THE FREQ . . . THIS SAME CTLR WAS ALSO WORKING 2 OTHER PAIRS OF OUR COMPANY FLTS WITH SIMILAR CALL SIGNS . . . MULTIPLE FLTS WITH SIMILAR SOUNDING SIGNS IN TODAY&apos;S CONGESTED ATC ENVIRONMENT IS DANGEROUS, AND OUR COMPANY HAS A BAD PRACTICE OF DOING THIS. I BELIEVE THEY DO IT FOR MARKETING REASONS, BUT RUNNING BANKS OF FLTS INTO A HUB AT PEAK HRS WITH SIMILAR SOUNDING CALL SIGNS IS NOT A GOOD PRACTICE, AND SHOULD BE STOPPED, THUS HELPING TO AVOID SOMEONE FROM MISUNDERSTANDING AND TAKING SOME OTHER FLT&apos;S CLRNC. THIS HAS THE POTENTIAL TO CREATE A VERY SERIOUS SIT. THIS CALL SIGN USAGE BY OUR COMPANY HAS RAISED THE IRE OF MANY PLTS, BUT OUR COMMENTS AND COMPLAINTS HAVE FALLEN ON DEAF EARS AT THE COMPANY. (236716) </paragraph>
<paragraph id="P-0246" lvl="1"><number>&lsqb;0246&rsqb;</number> THIS WAS A SIMILAR ENOUGH SOUNDING CALL SIGN THAT I BELIEVE SOME EFFORT SHOULD BE MADE TO DISTINGUISH BTWN THEM . . . FLT &num;S SHOULD BE READ READ DIGIT BY DIGIT AND WARNINGS SHOULD BE ISSUED FOR SIMILAR SOUNDING CALL SIGNS. (173196) </paragraph>
<paragraph id="P-0247" lvl="1"><number>&lsqb;0247&rsqb;</number> PROBS THAT NEED TO BE IDENTED: TOO MANY SIMILAR SOUNDING CALL SIGNS BY SAME COMPANY IN SAME VICINITY AT THE SAME TIME . . . NO ONE HAD SAID THERE WAS AN ACFT ON FREQ WITH A SIMILAR CALL SIGN AND WE HAD HEARD NO CALLS TO COMPANY ACR. WHEN THE FIRST CALL WAS MADE, THE FO WAS DISTR BY A FLT ATTENDANT IN THE COCKPIT ASKING ABOUT THE TEMP OF THE CABIN AND HE DID NOT HEAR THE CALL SIGN READ BY CTR. SUPPLEMENTAL INFO FROM ACN 224896: OUR CALL SIGN SAME COMPANY ACR SIMILAR TO ACR X . . . (224992) </paragraph>
<paragraph id="P-0248" lvl="0"><number>&lsqb;0248&rsqb;</number> The narratives considered the most relevant to multiple query phrases are the ones that best match, in whole or in part, the query phrases. The following observations illustrate the quality of the phrase matches relative to the rank ordering of the narratives. The narratives ranked 1-4 contain both of the query phrases: &ldquo;similar sounding call sign&rdquo; and &ldquo;similar sounding call signs&rdquo;. Phrase fragments are also found in these narratives, including one or more of: &ldquo;similar call sign(s)&rdquo;, &ldquo;similar sounding sign(s)&rdquo;, or &ldquo;call sign(s)&rdquo;. Narratives ranked 5-86 contain one or the other of the query phrases: &ldquo;similar sounding call sign&rdquo;or &ldquo;similar sounding call signs&rdquo;. Narratives in this group usually also contain one or more of the phrase fragments: &ldquo;similar call sign(s)&rdquo; or &ldquo;call sign(s)&rdquo;. Less common additions include: &ldquo;similar enough sounding call sign&rdquo;, &ldquo;similar to the call signs&rdquo;, &ldquo;similar acft call signs&rdquo;, &ldquo;similar-sounding but incorrect ident&rdquo;, and &ldquo;like sounding call signs&rdquo;. </paragraph>
<paragraph id="P-0249" lvl="0"><number>&lsqb;0249&rsqb;</number> Narratives ranked 87-91 contain one of the following: &ldquo;similar sounding call sign&rdquo;, &ldquo;similar sounding call signs&rdquo;, one of those phrases but with inclusions, or a collection of phrase fragments that, taken together, conveys the notion of &ldquo;similar sounding call sign(s)&rdquo;. For example, the 87th narrative contains only &ldquo;similar sounding acft call signs&rdquo;, and the 88th contains only &ldquo;similar sounding fit numbers&rdquo;, &ldquo;wrong call sign&rdquo;, and &ldquo;similar call signs&rdquo;. Narratives 92-181 do not contain the whole phrase. Most of them (83) contain the fragment &ldquo;similar call sign(s)&rdquo;, usually with some other fragments such as &ldquo;call sign(s)&rdquo; or &ldquo;similar sign(s)&rdquo;. The other seven narratives include fragments containing &ldquo;sounding&rdquo; but not &ldquo;similar&rdquo;, e.g., &ldquo;close sounding or transposable call signs&rdquo;. Narratives 182-200 contain only the fragments &ldquo;similar call sign(s)&rdquo; or &ldquo;call sign(s)&rdquo;. Narrative 182 is the highest-ranking narrative that contains only the fragment &ldquo;call sign(s)&rdquo;. Most of the many narratives beyond the 200th in rank contain only &ldquo;call sign(s)&rdquo;. </paragraph>
<paragraph id="P-0250" lvl="0"><number>&lsqb;0250&rsqb;</number> In summary, the rank ordering of the narratives provided by phrase search for long, multiple query phrases is appropriate. The highest ranked narratives (1-86) contain one or more instances of the query phrases &ldquo;similar sounding call sign&rdquo; and &ldquo;similar sounding call signs&rdquo; , while a transition group (87-91) at least conveys the notion of the query. The next large group (92-181) mostly contains &ldquo;similar call sign(s)&rdquo;, which is more general than &ldquo;similar sounding call sign(s)&rdquo;, but represents the next best match to the query. These are followed by a large group of narratives (increasingly common beginning with 182) that contain only &ldquo;call sign(s)&rdquo;, which is more general than &ldquo;similar call sign(s)&rdquo;, but represents the next best match to the query. The following Table 2.3 lists the accession numbers of the 91 ASRS incident reports that are most relevant to the phrase &ldquo;similar sounding callsign(s)&rdquo;:  
<table-cwu id="TABLE-US-00011">
<number>11</number>
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="5">
<colspec colname="OFFSET" colwidth="14PT" align="left"/>
<colspec colname="1" colwidth="49PT" align="left"/>
<colspec colname="2" colwidth="49PT" align="left"/>
<colspec colname="3" colwidth="49PT" align="left"/>
<colspec colname="4" colwidth="56PT" align="left"/>
<thead>
<row>
<entry></entry>
<entry namest="OFFSET" nameend="4" align="center">TABLE 2.3</entry>
</row>
<row>
<entry></entry>
<entry></entry>
</row>
<row>
<entry></entry>
<entry namest="OFFSET" nameend="4" align="center" rowsep="1"></entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry></entry>
<entry>&ensp;1. 236716</entry>
<entry>25. 165761</entry>
<entry>49. 342497</entry>
<entry>73. 178788</entry>
</row>
<row>
<entry></entry>
<entry>&ensp;2. 192640</entry>
<entry>26. 93653</entry>
<entry>50. 94979</entry>
<entry>74. 82543</entry>
</row>
<row>
<entry></entry>
<entry>&ensp;3. 198106</entry>
<entry>27. 202997</entry>
<entry>51. 339600</entry>
<entry>75. 325390</entry>
</row>
<row>
<entry></entry>
<entry>&ensp;4. 255236</entry>
<entry>28. 150627</entry>
<entry>52. 90769</entry>
<entry>76. 249352</entry>
</row>
<row>
<entry></entry>
<entry>&ensp;5. 173196</entry>
<entry>29. 374529</entry>
<entry>53. 152083</entry>
<entry>77. 328055</entry>
</row>
<row>
<entry></entry>
<entry>&ensp;6. 144720</entry>
<entry>30. 347810</entry>
<entry>54. 142766</entry>
<entry>78. 248464</entry>
</row>
<row>
<entry></entry>
<entry>&ensp;7. 273139</entry>
<entry>31. 351689</entry>
<entry>55. 217142</entry>
<entry>79. 135501</entry>
</row>
<row>
<entry></entry>
<entry>&ensp;8. 269000</entry>
<entry>32. 343860</entry>
<entry>56. 230971</entry>
<entry>80. 330230</entry>
</row>
<row>
<entry></entry>
<entry>&ensp;9. 95030</entry>
<entry>33. 142569</entry>
<entry>57. 160848</entry>
<entry>81. 192059</entry>
</row>
<row>
<entry></entry>
<entry>10. 310278</entry>
<entry>34. 144569</entry>
<entry>58. 308996</entry>
<entry>82. 160883</entry>
</row>
<row>
<entry></entry>
<entry>11. 224992</entry>
<entry>35. 89654</entry>
<entry>59. 307837</entry>
<entry>83. 262477</entry>
</row>
<row>
<entry></entry>
<entry>12. 249451</entry>
<entry>36. 139469</entry>
<entry>60. 306664</entry>
<entry>84. 105298</entry>
</row>
<row>
<entry></entry>
<entry>13. 370586</entry>
<entry>37. 136784</entry>
<entry>61. 282179</entry>
<entry>85. 133520</entry>
</row>
<row>
<entry></entry>
<entry>14. 143173</entry>
<entry>38. 334890</entry>
<entry>62. 112496</entry>
<entry>86. 266870</entry>
</row>
<row>
<entry></entry>
<entry>15. 366360</entry>
<entry>39. 332500</entry>
<entry>63. 276472</entry>
<entry>87. 108119</entry>
</row>
<row>
<entry></entry>
<entry>16. 139993</entry>
<entry>40. 210935</entry>
<entry>64. 109765</entry>
<entry>88. 85247</entry>
</row>
<row>
<entry></entry>
<entry>17. 104418</entry>
<entry>41. 146441</entry>
<entry>65. 273212</entry>
<entry>89. 92664</entry>
</row>
<row>
<entry></entry>
<entry>18. 333433</entry>
<entry>42. 206733</entry>
<entry>66. 286220</entry>
<entry>90. 217637</entry>
</row>
<row>
<entry></entry>
<entry>19. 246229</entry>
<entry>43. 86887</entry>
<entry>67. 173641</entry>
<entry>91. 266124</entry>
</row>
<row>
<entry></entry>
<entry>20. 361796</entry>
<entry>44. 158878</entry>
<entry>68. 298130</entry>
</row>
<row>
<entry></entry>
<entry>21. 364467</entry>
<entry>45. 246471</entry>
<entry>69. 299673</entry>
</row>
<row>
<entry></entry>
<entry>22. 259010</entry>
<entry>46. 201843</entry>
<entry>70. 120463</entry>
</row>
<row>
<entry></entry>
<entry>23. 337485</entry>
<entry>47. 343091</entry>
<entry>71. 304066</entry>
</row>
<row>
<entry></entry>
<entry>24. 268344</entry>
<entry>48. 342960</entry>
<entry>72. 304370</entry>
</row>
<row>
<entry></entry>
<entry namest="OFFSET" nameend="4" align="center" rowsep="1"></entry>
</row>
</tbody>
</tgroup>
</table>
</table-cwu>
</paragraph>
<paragraph id="P-0251" lvl="0"><number>&lsqb;0251&rsqb;</number> The results of searching for the phrase &ldquo;flight crew fatigue&rdquo; are less than satisfactory due to the small number of matched narratives. Only 8 of 67821 ASRS reports contain the phrase &ldquo;fit crew fatigue&rdquo;. This small number does not, however, reflect the true prevalence of narratives involving flight crew fatigue. As an alternative, the search can be limited to the phrase &ldquo;crew fatigue&rdquo;. A larger number of narratives contain &ldquo;crew fatigue&rdquo;. Among 67821 ASRS reports, a total of 102 narratives contain &ldquo;crew fatigue&rdquo;, and an additional 9 contain phrases such as &ldquo;crew&apos;s fatigue&rdquo;, &ldquo;crew member fatigue&rdquo;, or &ldquo;crew mental fatigue&rdquo;. This does not, however, reflect the true number of narratives on the subject. </paragraph>
<paragraph id="P-0252" lvl="0"><number>&lsqb;0252&rsqb;</number> Rather than doing a phrase search in this case, a keyterm search on &ldquo;fatigue&rdquo; would be more effective. Even better would be a search on &ldquo;fatigu&rdquo;, which would match &ldquo;fatigue&rdquo;, &ldquo;fatigued&rdquo;, and &ldquo;fatiguing&rdquo;. To increase the probability that the retrieved narratives involve flight crew fatigue, the search can be limited to the subset of the reports that were submitted by flight crews. In a keyterm search on &ldquo;fatigu&rdquo; among 36361 reports submitted by the flight crews of large aircraft there were 743 relevant narratives. A search among 67821 ASRS reports of all kinds found 1364 narratives relevant to &ldquo;fatigue&rdquo;, &ldquo;fatigued&rdquo;, or &ldquo;fatiguing&rdquo;. </paragraph>
<paragraph id="P-0253" lvl="0"><number>&lsqb;0253&rsqb;</number> Narratives that contain the topic of fatigue do not necessarily contain the terms &ldquo;fatigue&rdquo;, &ldquo;fatigued&rdquo;, or &ldquo;fatiguing&rdquo;. Phrase discovery, described below, more fully addresses this issue. Phrase discovery finds a large number of fatigue-related phrases such as &ldquo;duty time&rdquo;, &ldquo;crew rest&rdquo;, etc. The process of finding these phrases also finds ASRS reports that contain the topic of fatigue even if no forms of the term &ldquo;fatigue&rdquo; are present in the narratives. </paragraph>
<paragraph id="P-0254" lvl="0"><number>&lsqb;0254&rsqb;</number> Phrase search can also be used to search for a particular sentence that occurs only once in the database. Since phrase search represents phrases implicitly among the contextual relations of the documents, rather than explicitly as a pre-computed list, it is possible to find any phrase, or other sequence of terms, even if it occurs only once. In addition, even though contextual relations in the phrase database are limited in one embodiment to spans of 4 terms, indirect chains of relations allow longer phrases to be found. As an example, the following sentence can be used as a query: </paragraph>
<paragraph id="P-0255" lvl="1"><number>&lsqb;0255&rsqb;</number> THE ENTIRE CREW WAS DISTR, AND WE BOTH FAILED TO MONITOR THE PERF OF THE ACFT. </paragraph>
<paragraph id="P-0256" lvl="0"><number>&lsqb;0256&rsqb;</number> As an alternative, the following unabbreviated form of the sentence can be used as the query: </paragraph>
<paragraph id="P-0257" lvl="0"><number>&lsqb;0257&rsqb;</number> The entire crew was distracted, and we both failed to monitor the performance of the aircraft. </paragraph>
<paragraph id="P-0258" lvl="0"><number>&lsqb;0258&rsqb;</number> Given either query, phrase search identifies the relevant narrative and displays it with the relevant sections highlighted. Shown below is an excerpt. The query sentence is highlighted, as are additional fragments of the sentence. </paragraph>
<paragraph id="P-0259" lvl="1"><number>&lsqb;0259&rsqb;</number> I BELIEVE THAT THE COMPLEXITY OF FMS PROGRAMMING IS NOT ADDRESSED IN INITIAL TRAINING AT SCHOOL BECAUSE EACH ACFT HAS DIFFERENT EQUIP. HOWEVER, THIS LEAVES THE FLT CREW TO &lsquo;LEARN AS THEY FLY.&rsquo; THIS EFFECTIVELY TOOK MY FO OUT OF THE LOOP IN THAT IF HE WAS PROGRAMMING THE FMS, I COULD HAVE CONCENTRATED MORE ON MONITORING THE ACFT. I SHOULD HAVE LET THE FO FLY THE ACFT WITH THE AUTOPLT RATHER THAN ME DO ALL THE TASKS. THE ENTIRE CREW WAS DISTR, AND WE BOTH FAILED TO MONITOR THE PERF OF THE ACFT. I SHOULD HAVE JUST PUT MY HSI IN THE VOR MODE RATHER THAN DISPLAY FMS COURSE INFO. THIS WOULD HAVE ALLOWED US TO FOCUS MORE ON THE ACFT. (368360) </paragraph>
<paragraph id="P-0260" lvl="0"><number>&lsqb;0260&rsqb;</number> By doing the search using the option to include narratives containing only some of the fragments of the sentence, some near-matches can also found. These are ranked as less relevant than the one containing the whole sentence. Here are excerpts from narratives containing only fragments of the sentence: </paragraph>
<paragraph id="P-0261" lvl="1"><number>&lsqb;0261&rsqb;</number> I WAS DISTR BY THE CAPT&apos;S CONVERSATION AND WE BOTH FAILED TO MONITOR THE ACFT&apos;S DSCNT. (265142) </paragraph>
<paragraph id="P-0262" lvl="1"><number>&lsqb;0262&rsqb;</number> WHILE WE CONTINUED TO WONDER WHY THE DSCNT DID NOT OCCUR AS PROGRAMMED, IT WAS OBVIOUS THAT WE HAD BOTH FAILED TO MONITOR THE DSCNT, AS WE SHOULD HAVE. (253696) </paragraph>
<paragraph id="P-0263" lvl="1"><number>&lsqb;0263&rsqb;</number> WE WERE CLRD FOR THE OXI 2 ARR, FWA TRANSITION TO ORD, FO FLYING THE ACFT . . . ALTHOUGH WE HAD TUNED THE OXI 095 DEG RADIAL FOR THE TURN AT SPANN INTXN, WE FAILED TO TURN BECAUSE OF OUR DISTR . . . THE FO AND I DO NOT BELIEVE THAT WE MISSED A RADIO CALL, EVEN THOUGH WE WERE DISTR AND WERE OFF COURSE . . . I BELIEVE THAT MY FAILURE TO MONITOR THE FO&apos;S NAV WHILE I INVESTIGATED POSSIBLE ACFT ABNORMALITIES WAS THE MOST IMPORTANT CONSIDERATION IN THIS OCCURRENCE. (201659) </paragraph>
<paragraph id="P-0264" lvl="0"><number>&lsqb;0264&rsqb;</number> This example shows the ability of phrase search to find long or rare phrases, while also finding similar text if desired. </paragraph>
<paragraph id="P-0265" lvl="0"><number>&lsqb;0265&rsqb;</number> Most phrase search and retrieval methods that currently exist, such as Fagan (1987), Croft, Turtle, and Lewis (1991), Gey and Chen (1997), Jing and Croft (1994), Gutwin, Paynter, Witten, Nevill-Manning, and Frank (1998), and Jones and Staveley (1999), treat query phrases as single terms, and typically rely on lists of key phrases for each document. This approach allows little flexibility in matching query phrases with similar phrases in the text, and it requires that all possible phrases be identified in advance, typically using statistical or &ldquo;natural language processing&rdquo; (NLP) methods. In contrast, the phrase search method described herein represents phrases implicitly among contextual associations representing each document. This allows both exact matching of phrases and the option of flexible matching of phrases. In addition, the phrase search method eliminates the need for explicit and inevitably incomplete lists of phrases. </paragraph>
<paragraph id="P-0266" lvl="0"><number>&lsqb;0266&rsqb;</number> Since phrase search does not depend on phrase frequency, such as in Turpin and Moffat (1999), phrase search is not hampered by the infrequency of most phrases, which reduces the effectiveness of statistical phrase search methods. Since phrase search does not use NLP methods, it is not subject to problems such as mistagging as described by Fagan (1987). </paragraph>
<paragraph id="P-0267" lvl="0"><number>&lsqb;0267&rsqb;</number> Croft, Turtle, and Lewis (1991) dismiss the notion of implicitly representing phrases as term associations, but the association metric they tested is not as definitive as that described herein. Unlike phrase search, pair-wise associations of Croft, Turtle, and Lewis (1991) do not include or suggest a measurement of degree of proximity. Further, while phrase search restricts the scope of acceptable contexts to a few words and enforces term order, the association method of Croft, Turtle, and Lewis (1991) uses entire documents as the contextual scope, and uses no directional information. </paragraph>
<paragraph id="P-0268" lvl="0"><number>&lsqb;0268&rsqb;</number> Finally, unlike typical Internet search tools, phrase search can easily use large numbers of phrases as query phrases. </paragraph>
</section>
<section>
<heading lvl="1">Phrase Generation </heading>
<paragraph id="P-0269" lvl="0"><number>&lsqb;0269&rsqb;</number> The use of any phrase search tool requires the user to know or guess what phrases are likely to be in the database being searched. Phrase generation as described herein, and phrase discovery (described below) are two processes that can show the phrases that are likely to be useful queries. In addition, phrase generation and phrase discovery can also help the user to explore and understand the particular nuances of topics in the database. </paragraph>
<paragraph id="P-0270" lvl="0"><number>&lsqb;0270&rsqb;</number> Phrase generation differs from phrase discovery. Phrase generation assembles phrases from term pairs that are often found in a particular order and close together in the narratives of a database. That is, the phrases are assembled from phrase models. Many of the generated phrases are present in the narratives. Phrases are listed in order of their estimated frequency in the whole database. Phrase generation is a useful way of building phrases that are typically present, without actually storing and retrieving the phrases themselves. In contrast, phrase discovery scans narratives for all possible phrases and distills them down to those which are contextually relevant. </paragraph>
<paragraph id="P-0271" lvl="0"><number>&lsqb;0271&rsqb;</number> Phrase generation is used to show typical phrases that contain words or phrases of interest. The default is to produce the 10 most typical phrases, but a different number of phrases can also be specified. The output phrases can be used as query phrases for input to a phrase search described above or simply as a list of phrases representing the database. </paragraph>
<paragraph id="P-0272" lvl="0"><number>&lsqb;0272&rsqb;</number> Phrase generation is a method of generating sequences of terms (herein called phrases) that are likely to be present within a database consisting of a collection of one or more longer sequences of terms, such as text. <cross-reference target="DRAWINGS">FIG. 16</cross-reference> shows one embodiment of generating phrases from a database of text <highlight><bold>1600</bold></highlight>. First, a database is provided in block <highlight><bold>1602</bold></highlight>. A relational model of that database is created in block <highlight><bold>1604</bold></highlight>. The relational model of the database can include or, alternatively, exclude stopterms. Then, a query is input in block <highlight><bold>1606</bold></highlight>. The query includes a term or a phrase or multiple terms or multiple phrases or a combination thereof. Inputting the query can also include transforming the query as described above in keyterm search. Next, in block <highlight><bold>1608</bold></highlight>, a number of phrases are determined from a combination of terms including terms from both the query and from the relations in the relational model of the database that are contextually related to the query. The phrases are sorted in block <highlight><bold>1610</bold></highlight> and output in block <highlight><bold>1612</bold></highlight>. In one alternative, the output phrases can exclude stopterms. In another alternative, the output phrases can include any number of stopterms. In yet another alternative, the output phrases can be limited to phrases having no more than a pre-selected number of stopterms. </paragraph>
<paragraph id="P-0273" lvl="0"><number>&lsqb;0273&rsqb;</number> The process of determining the phrases in block <highlight><bold>1608</bold></highlight>, wherein terms in relations in the database model are contextually related to the query, can also be an iterative process. The iterative process initially uses the input phrases (where an input phrase can include one or more terms) as the starting phrases. A first copy of each starting phrase is extended by adding an appended term before the first copy of the starting phrase, if, for each term in the starting phrase, there is a corresponding non-zero-weighted directional contextual relation in the database model that includes both the appended term and the term in the starting phrase. In addition, a second copy of each starting phrase is extended by adding the appended term following the second copy of the starting phrase if, for each term in the starting phrase, there is a corresponding non-zero-weighted directional contextual relation in the database model that includes both the term in the starting phrase and the appended term. </paragraph>
<paragraph id="P-0274" lvl="0"><number>&lsqb;0274&rsqb;</number> A weight of each extended phrase is based on the metric values of the relations within the extended phrase. In one alternative, the weight of a phrase is equal to the least of the corresponding non-zero-weighted directional contextual metrics between the terms in the starting phrase and the appended term. Each extended phrase and the corresponding weight of the extended phrase are collected for later output. In a subsequent iteration, copies of the extended phrases are used as the starting phrases for further extension as described above. In one alternative, the process continues until all possible phrases, given the query and the relations in the model of the database, have been determined. In another alternative, the process continues until all possible phrases of a pre-selected maximum phrase length have been determined. The determined phrases are then output. In one alternative, a pre-selected number of the determined phrases are output. In another alternative, determined phrases having weights of at least a pre-selected magnitude are output. </paragraph>
<paragraph id="P-0275" lvl="0"><number>&lsqb;0275&rsqb;</number> Each output phrase can represent a concise summary of multiple similar phrases by representing the essence of the multiple similar phrases, as shown in the following example. Given an input of &ldquo;runway&rdquo; to the phrase generation process, and allowing one stopterm (e.g. to, the, our, their, other, on, an) in the output phrases, one of the output phrases is &ldquo;approach to runway&rdquo;. The phrase &ldquo;approach to runway&rdquo; represents multiple similar phrases such as: &ldquo;approach to runway&rdquo;, &ldquo;an approach to the runway&rdquo;, &ldquo;on approach to our runway&rdquo;, and &ldquo;their approach to the other runway&rdquo;. When the output phrase &ldquo;approach to runway&rdquo; is provided as a query phrase to a phrase search process, as described above in FIGS. <highlight><bold>11</bold></highlight>-<highlight><bold>15</bold></highlight>, the flexible phrase matching capability of phrase search enables the retrieved subsets of the database to include such phrases as &ldquo;approach to runway&rdquo;, &ldquo;an approach to the runway&rdquo;, &ldquo;on approach to our runway&rdquo;, and &ldquo;their approach to the other runway&rdquo;. Thus the output phrase &ldquo;approach to runway&rdquo; represents a concise summary of the multiple similar phrases. </paragraph>
<paragraph id="P-0276" lvl="0"><number>&lsqb;0276&rsqb;</number> Creating a relational model of a database in block <highlight><bold>1604</bold></highlight> can include providing a subset of relations in the database model. In one alternative, the entire set of relations in the database model can be provided. In another alternative, relations in the database having a function of the corresponding relational metric values greater than or equal to a threshold value can be provided. In another alternative, the function of the relational metric values is the smaller of the left contextual metric (LCM) value and the right contextual metric (RCM) value. In another alternative, the function of relational metric values is equal to the non-directional contextual metric (NDCM) value. In another alternative, the threshold value can be automatically adjusted so that a pre-selected number of phrases are output. </paragraph>
<paragraph id="P-0277" lvl="0"><number>&lsqb;0277&rsqb;</number> The entire process of phrase generation <highlight><bold>1600</bold></highlight> can also be an iterative process wherein a number of the phrases that are output in one iteration can be the input to a subsequent iteration. </paragraph>
<paragraph id="P-0278" lvl="0"><number>&lsqb;0278&rsqb;</number> <cross-reference target="DRAWINGS">FIGS. 17 and 17</cross-reference>A illustrate a process <highlight><bold>1608</bold></highlight> of determining the phrases, which are contextually related to the query, from the model of the database such as in block <highlight><bold>1608</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 16</cross-reference>. First, a threshold weight is assigned or set in block <highlight><bold>1702</bold></highlight>. For alternative embodiments, the weight can be selected manually or default to a function of the query. A phrase list (PL) including a list of base phrases is established by copying the input query into the PL at block in block <highlight><bold>1704</bold></highlight>. Each phrase or keyterm in the input query is copied as a base phrase in the PL. A first relation from the model of the database (DB<highlight><subscript>m</subscript></highlight>) is selected in block <highlight><bold>1706</bold></highlight>. The first term from the selected relation is identified as a contained term and the second term from the selected relation is identified as an appended term in block <highlight><bold>1708</bold></highlight>. Then the PL is analyzed to determine if any base phrases in the PL include the contained term in block <highlight><bold>1710</bold></highlight>. If no base phrases in the PL include the contained term then the process <highlight><bold>1700</bold></highlight> skips to block <highlight><bold>1740</bold></highlight> which will be described below. If the base phrases in the PL include the contained term, then the first one of the base phrases that includes the contained term in block <highlight><bold>1712</bold></highlight> is selected. The first base phrase and the appended term are concatenated into two candidate phrases in block <highlight><bold>1714</bold></highlight>. One candidate phrase is the appended term followed by the base phrase, the second candidate phrase is the base phrase followed by the appended term. The conditional list of phrases (CLP) is then updated in block <highlight><bold>1716</bold></highlight>. One embodiment of updating the CLP is described in more detail below regarding <cross-reference target="DRAWINGS">FIG. 18</cross-reference>. </paragraph>
<paragraph id="P-0279" lvl="0"><number>&lsqb;0279&rsqb;</number> Next, the first of the two candidate phrases is selected in block <highlight><bold>1718</bold></highlight>. For one embodiment, if the selected candidate phrase includes more than a pre-selected number of stopterms in block <highlight><bold>1720</bold></highlight>, then the selected phrase is deleted in block <highlight><bold>1726</bold></highlight> and the second candidate phrase is selected in block <highlight><bold>1728</bold></highlight>. If the selected candidate phrase does not include more than a pre-selected number of stopterms, then the number of links is evaluated in block <highlight><bold>1722</bold></highlight>. A link is equal to a relation between a contained term and an appended term in the candidate phrase. If the number of links found so far is not equal to the number of terms in the base phrase in block <highlight><bold>1722</bold></highlight>, then the second candidate phrase is selected in block <highlight><bold>1728</bold></highlight>. </paragraph>
<paragraph id="P-0280" lvl="0"><number>&lsqb;0280&rsqb;</number> If the number of links found so far is equal to the number of terms in the base phrase, then the link weights are evaluated in block <highlight><bold>1724</bold></highlight>. A link weight is equal to a directional metric of the selected relation. The directional metric corresponds to the order of occurrence of the contained term and the appended term in the selected candidate phrase. If all of the link weights between the terms of the selected base phrase and the appended term are not greater than zero, then the selected candidate phrase is deleted in block <highlight><bold>1726</bold></highlight>, and the second candidate phrase is selected in block <highlight><bold>1728</bold></highlight>. If all of the link weights between the terms of the selected base phrase and the appended term are greater than zero, then the selected candidate phrase is included in an interim phrase list (IPL) and then the second candidate phrase is selected in blocks <highlight><bold>1728</bold></highlight>, <highlight><bold>1732</bold></highlight> and the process described in blocks <highlight><bold>1720</bold></highlight>-<highlight><bold>1730</bold></highlight> is applied to the second candidate phrase. If the second candidate phrase has been previously processed, then a subsequent one of the base phrases that includes the contained term is selected in blocks <highlight><bold>1734</bold></highlight>, <highlight><bold>1736</bold></highlight> and the process in blocks <highlight><bold>1714</bold></highlight>-<highlight><bold>1736</bold></highlight> is applied to the newly selected base phrase. If there are no subsequent phrases in the base phrases including the contained term, then the process continues in block <highlight><bold>1740</bold></highlight>. </paragraph>
<paragraph id="P-0281" lvl="0"><number>&lsqb;0281&rsqb;</number> If the second term in the selected relation has not been processed as a contained term in block <highlight><bold>1740</bold></highlight>, then the second term from the selected relation is identified as a contained term and the first term from the selected relation is identified as an appended term in block <highlight><bold>1742</bold></highlight> and the process repeats at block <highlight><bold>1710</bold></highlight>. If the second term in the selected relation has been processed as a contained term in block <highlight><bold>1740</bold></highlight>, then if a subsequent relation is remaining in the DB<highlight><subscript>m</subscript></highlight>, the subsequent relation is selected and the process repeats at block <highlight><bold>1708</bold></highlight>. If no subsequent relations are remaining in the DB<highlight><subscript>m</subscript></highlight>, then for one embodiment, the phrases in the IPL are filtered. For one embodiment, shown in block <highlight><bold>1748</bold></highlight>, the phrases having a weight less than the threshold weight are eliminated from the IPL. For another embodiment, the weight of a phrase in the IPL is determined by the lowest single link weight in the phrase. Next, duplicate phrases are eliminated from the IPL in block <highlight><bold>1750</bold></highlight>. The number of phrases in the IPL could also be reduced by eliminating phrases that include more than a pre-selected number of stopterms. </paragraph>
<paragraph id="P-0282" lvl="0"><number>&lsqb;0282&rsqb;</number> Next, if the number of phrases remaining in the IPL is greater than zero in block <highlight><bold>1754</bold></highlight>, then the phrases in the IPL are added to the phrases in the interim buffer (IB) in block <highlight><bold>1756</bold></highlight>. Next the interim phrase list (IPL) replaces the phrase list (PL) and the process repeats from block <highlight><bold>1706</bold></highlight>. If the number of phrases remaining in the IPL is not greater than zero in block <highlight><bold>1754</bold></highlight>, then if the number of phrases in the IB is greater than or equal to a pre-selected number in block <highlight><bold>1760</bold></highlight>, then the phrases in the IB are sorted in block <highlight><bold>1764</bold></highlight> and output in block <highlight><bold>1766</bold></highlight>. If the number of phrases in the IB is not greater than or equal to a pre-selected number in block <highlight><bold>1760</bold></highlight>, then threshold weight is lowered and the process repeats at block <highlight><bold>1704</bold></highlight>. </paragraph>
<paragraph id="P-0283" lvl="0"><number>&lsqb;0283&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 18</cross-reference> illustrates one method <highlight><bold>1800</bold></highlight> of updating the conditional list of phrases (CLP) such as in block <highlight><bold>1716</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 17</cross-reference>. The first one of the two new candidate phrases is selected in block <highlight><bold>1802</bold></highlight>. If the selected candidate phrase is not in the CLP in block <highlight><bold>1804</bold></highlight>, then the candidate phrases is included in the CLP and the corresponding count of known relations is set to 1. Then, if the weight of the base phrase is not greater than a corresponding directional metric of the selected relation in block <highlight><bold>1808</bold></highlight>, then the corresponding weight of the candidate phrase in the CLP is set to equal the weight of the base phrase in block <highlight><bold>1810</bold></highlight> and proceed to in block <highlight><bold>1818</bold></highlight> below. If the weight of the base phrase is greater than a corresponding directional metric of the selected relation in block <highlight><bold>1808</bold></highlight>, then the corresponding weight of the candidate phrase in the CLP is set to equal the weight of the corresponding directional metric of the selected relation in block <highlight><bold>1816</bold></highlight> and proceed to block <highlight><bold>1818</bold></highlight> below. </paragraph>
<paragraph id="P-0284" lvl="0"><number>&lsqb;0284&rsqb;</number> If the selected candidate phrase is in the CLP in block <highlight><bold>1804</bold></highlight>, then the corresponding count of known relations is incremented in block <highlight><bold>1812</bold></highlight>. If the weight of the selected candidate phrase is greater than a corresponding directional metric of the selected relation in block <highlight><bold>1814</bold></highlight>, then the corresponding weight of the candidate phrase in the CLP is set to equal the weight of the corresponding directional metric of the selected relation in block <highlight><bold>1816</bold></highlight>. If the weight of the selected candidate phrase is not greater than a corresponding directional metric of the selected relation in block <highlight><bold>1814</bold></highlight>, then proceed to in block <highlight><bold>1818</bold></highlight>. In block <highlight><bold>1818</bold></highlight>, if the second of the two candidate phrases has not been processed, then the second of the two candidate phrases is selected and the process repeats at block <highlight><bold>1804</bold></highlight>. In block <highlight><bold>1818</bold></highlight>, if the second of the two candidate phrases has been processed the sub-process ends and the updated CLP is output. </paragraph>
<paragraph id="P-0285" lvl="0"><number>&lsqb;0285&rsqb;</number> This application is intended to cover any adaptations or variations of the present invention. For example, those of ordinary skill within the art will appreciate that the phrase generation process can be executed in varying orders instead of being executed in the order as described above. </paragraph>
<paragraph id="P-0286" lvl="0"><number>&lsqb;0286&rsqb;</number> Phrase generation is used to show typical phrases that contain terms or phrases of interest. The default is to produce the 10 most typical phrases, but a different number can also be specified. The output phrases can be used as query phrases for input to phrase search. </paragraph>
<paragraph id="P-0287" lvl="0"><number>&lsqb;0287&rsqb;</number> As an example, phrases containing the term &ldquo;rain&rdquo; can be generated. Given the term &ldquo;rain&rdquo;, and using the option to specify the number of generated phrases (30 in this case), phrase generation produces the following list:  
<table-cwu id="TABLE-US-00012">
<number>12</number>
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="1" colwidth="98PT" align="left"/>
<colspec colname="2" colwidth="119PT" align="left"/>
<thead>
<row>
<entry></entry>
</row>
<row><entry namest="1" nameend="2" align="center" rowsep="1"></entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry>LIGHT RAIN</entry>
<entry>MODERATE RAIN TURB</entry>
</row>
<row>
<entry>HVY RAIN</entry>
<entry>LIGHT RAIN TURB</entry>
</row>
<row>
<entry>RAIN SHOWERS</entry>
<entry>ENCOUNTERED RAIN TURB</entry>
</row>
<row>
<entry>FREEZING RAIN</entry>
<entry>LIGHT MODERATE RAIN TURB</entry>
</row>
<row>
<entry>MODERATE RAIN</entry>
<entry>ENCOUNTERED MODERATE RAIN</entry>
</row>
<row>
<entry></entry>
<entry>TURB</entry>
</row>
<row>
<entry>LIGHT MODERATE RAIN</entry>
<entry>ENCOUNTERED LIGHT RAIN TURB</entry>
</row>
<row>
<entry>HEAVY RAIN</entry>
<entry>ENCOUNTERED LIGHT</entry>
</row>
<row>
<entry></entry>
<entry>MODERATE RAIN TURB</entry>
</row>
<row>
<entry>RAIN SHOWER</entry>
<entry>VISIBILITY RAIN</entry>
</row>
<row>
<entry>RAIN FOG</entry>
<entry>VISIBILITY RAIN FOG</entry>
</row>
<row>
<entry>MODERATE HVY RAIN</entry>
<entry>VISIBILITY LIGHT RAIN</entry>
</row>
<row>
<entry>ENCOUNTERED RAIN</entry>
<entry>TURB RAIN</entry>
</row>
<row>
<entry>ENCOUNTERED MODERATE</entry>
<entry>TURB ENCOUNTERED RAIN</entry>
</row>
<row>
<entry>RAIN</entry>
</row>
<row>
<entry>ENCOUNTERED LIGHT RAIN</entry>
<entry>MODERATE TURB RAIN</entry>
</row>
<row>
<entry>ENCOUNTERED LIGHT</entry>
<entry>LIGHT TURB RAIN</entry>
</row>
<row>
<entry>MODERATE RAIN</entry>
</row>
<row>
<entry>RAIN TURB</entry>
<entry>ENCOUNTERED TURB RAIN</entry>
</row>
<row><entry namest="1" nameend="2" align="center" rowsep="1"></entry>
</row>
</tbody>
</tgroup>
</table>
</table-cwu>
</paragraph>
<paragraph id="P-0288" lvl="0"><number>&lsqb;0288&rsqb;</number> The phrases toward the beginning of the list are the ones that appear more often in the narratives of the ASRS database. So, for example, &ldquo;light rain&rdquo; is more common than &ldquo;moderate rain&rdquo;. Similarly, &ldquo;hvy rain&rdquo; is more common than &ldquo;heavy rain&rdquo;. Some of the listed phrases, such as &ldquo;light rain&rdquo;, typically appear in narratives exactly as shown. Other listed phrases, such as &ldquo;light moderate rain&rdquo;, typically appear in narratives with other terms intermixed. For example, the most common appearance of &ldquo;light moderate rain&rdquo; is &ldquo;light to moderate rain&rdquo;. </paragraph>
<paragraph id="P-0289" lvl="0"><number>&lsqb;0289&rsqb;</number> Phrase generation can also eliminate phrases containing terms that are not of interest at the moment. Eliminating terms not of interest is accomplished by identifying such terms as additions to a default stopterm list. For example, the user could add the terms LIGHT, MODERATE, ENCOUNTERED, TURB (i.e., turbulence), and CONDITIONS to eliminate the many variations on these themes. When re-running phrase generation with the expanded stopterm list, a revised list of phrases is generated. </paragraph>
<paragraph id="P-0290" lvl="0"><number>&lsqb;0290&rsqb;</number> Phrase generation can also allow a number of stopterms within each phrase. To avoid generating an excessive number of similar phrases, however, the default is to display only those phrases that contain no stopterms. Otherwise, given the query term &ldquo;rain&rdquo;, many phrases like the following would be output: </paragraph>
<paragraph id="P-0291" lvl="2"><number>&lsqb;0291&rsqb;</number> THE LIGHT RAIN </paragraph>
<paragraph id="P-0292" lvl="2"><number>&lsqb;0292&rsqb;</number> A LIGHT RAIN </paragraph>
<paragraph id="P-0293" lvl="2"><number>&lsqb;0293&rsqb;</number> SOME LIGHT RAIN </paragraph>
<paragraph id="P-0294" lvl="2"><number>&lsqb;0294&rsqb;</number> WAS LIGHT RAIN </paragraph>
<paragraph id="P-0295" lvl="2"><number>&lsqb;0295&rsqb;</number> ANY LIGHT RAIN </paragraph>
<paragraph id="P-0296" lvl="2"><number>&lsqb;0296&rsqb;</number> THE HVY RAIN </paragraph>
<paragraph id="P-0297" lvl="2"><number>&lsqb;0297&rsqb;</number> A HVY RAIN </paragraph>
<paragraph id="P-0298" lvl="2"><number>&lsqb;0298&rsqb;</number> SOME HVY RAIN </paragraph>
<paragraph id="P-0299" lvl="0"><number>&lsqb;0299&rsqb;</number> Phrase generation can also find phrases that contain other phrases. For example, given the query &ldquo;freezing rain&rdquo;, the following and other phrases would be generated:  
<table-cwu id="TABLE-US-00013">
<number>13</number>
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="1" colwidth="105PT" align="left"/>
<colspec colname="2" colwidth="112PT" align="left"/>
<thead>
<row>
<entry></entry>
</row>
<row><entry namest="1" nameend="2" align="center" rowsep="1"></entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry>FREEZING RAIN</entry>
<entry>MODERATE LIGHT FREEZING</entry>
</row>
<row>
<entry></entry>
<entry>RAIN</entry>
</row>
<row>
<entry>LIGHT FREEZING RAIN</entry>
<entry>MODERATE LIGHT FREEZING</entry>
</row>
<row>
<entry></entry>
<entry>RAIN CONDITIONS</entry>
</row>
<row>
<entry>FREEZING RAIN CONDITIONS</entry>
<entry>LIGHT MODERATE FREEZING</entry>
</row>
<row>
<entry></entry>
<entry>RAIN CONDITIONS</entry>
</row>
<row>
<entry>LIGHT FREEZING RAIN</entry>
<entry>FREEZING RAIN DRIZZLE</entry>
</row>
<row>
<entry>CONDITIONS</entry>
</row>
<row>
<entry>MODERATE FREEZING RAIN</entry>
<entry>LIGHT FREEZING RAIN DRIZZLE</entry>
</row>
<row>
<entry>MODERATE FREEZING RAIN</entry>
</row>
<row>
<entry>CONDITIONS</entry>
</row>
<row>
<entry>LIGHT MODERATE FREEZING</entry>
</row>
<row>
<entry>RAIN</entry>
</row>
<row><entry namest="1" nameend="2" align="center" rowsep="1"></entry>
</row>
</tbody>
</tgroup>
</table>
</table-cwu>
</paragraph>
<paragraph id="P-0300" lvl="0"><number>&lsqb;0300&rsqb;</number> When using phrase generation, user query terms are mapped (if necessary) to ASRS abbreviations and usage as described above. For example, &ldquo;runway&rdquo; is mapped to &ldquo;rwy&rdquo;. </paragraph>
<paragraph id="P-0301" lvl="0"><number>&lsqb;0301&rsqb;</number> Any phrase can be used as input to phrase search, including those produced by phrase generation. For example, for a search for the phrase &ldquo;light moderate rain&rdquo;, the following are excerpts from some of the most relevant narratives: </paragraph>
<paragraph id="P-0302" lvl="1"><number>&lsqb;0302&rsqb;</number> CONTRIBUTING FACTORS&mdash;LIGHT TO MODERATE RAIN WAS FALLING IN THE JFK AREA WITH STANDING WATER ON RAMP SURFACES&mdash;THIS COUPLED WITH LIGHTING ON THE CONCOURSE CAUSED A GLARE ON THE RAMP MAKING VIEW OF THE LEAD&mdash;IN LINE DIFFICULT. (86853) </paragraph>
<paragraph id="P-0303" lvl="1"><number>&lsqb;0303&rsqb;</number> THERE WERE LARGE AREAS OF LIGHT TO MODERATE RAIN SHOWERS AROUND THE LAX AREA . . . THE GPWS SOUNDED . . . I SUSPECT THIS WAS CAUSED BY THE EFFECT OF THE RAIN SHOWER ON THE GPWS. (233843) </paragraph>
<paragraph id="P-0304" lvl="1"><number>&lsqb;0304&rsqb;</number> JUST PRIOR TO FLYING INTO THE HAIL, ATC ASKED WHAT MY CONDITIONS WERE AND I RPTED LIGHT TO MODERATE RAIN. (373915) </paragraph>
<paragraph id="P-0305" lvl="0"><number>&lsqb;0305&rsqb;</number> The exact phrase &ldquo;light moderate rain&rdquo; never appears, but the phrase &ldquo;light to moderate rain&rdquo; is common. This shows the value of the flexible phrase matching available with phrase search. Of course, the phrase &ldquo;light to moderate rain&rdquo; could itself be used as a query phrase. </paragraph>
<paragraph id="P-0306" lvl="0"><number>&lsqb;0306&rsqb;</number> It is often helpful to use multiple phrases from the list produced by phrase generation as input to phrase search. For example, if the user were unsure of what phrases typically contain the term &ldquo;rest&rdquo; as it relates to fatigue, phrase generation could be used to list the most common phrases containing the term &ldquo;rest&rdquo;. These would include, in order of estimated prominence in the ASRS database:  
<table-cwu id="TABLE-US-00014">
<number>14</number>
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="1" colwidth="140PT" align="left"/>
<colspec colname="2" colwidth="77PT" align="left"/>
<thead>
<row>
<entry></entry>
</row>
<row><entry namest="1" nameend="2" align="center" rowsep="1"></entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry>REST FLT (e.g., &ldquo;rest of the flight&rdquo;)</entry>
<entry>REST APCH FLT</entry>
</row>
<row>
<entry>REDUCED REST</entry>
<entry>ACFT REST</entry>
</row>
<row>
<entry>CREW REST</entry>
<entry>ACFT REST FLT</entry>
</row>
<row>
<entry>REST PERIOD</entry>
<entry>ACFT REST APCH</entry>
</row>
<row>
<entry>CAME REST (e.g., &ldquo;came to rest)</entry>
<entry>ACFT CAME REST</entry>
</row>
<row>
<entry>MINIMUM REST</entry>
<entry>ACFT REST APCH FLT</entry>
</row>
<row>
<entry>REST REQUIREMENTS</entry>
<entry>REST TRIP</entry>
</row>
<row>
<entry>REST PERIODS</entry>
<entry>CREW ACFT REST</entry>
</row>
<row>
<entry>REST APCH (e.g., &ldquo;rest of the approach&rdquo;)</entry>
<entry>ADEQUATE REST</entry>
</row>
<row>
<entry>MINIMUM REST APCH</entry>
<entry>Etc.</entry>
</row>
<row><entry namest="1" nameend="2" align="center" rowsep="1"></entry>
</row>
</tbody>
</tgroup>
</table>
</table-cwu>
</paragraph>
<paragraph id="P-0307" lvl="0"><number>&lsqb;0307&rsqb;</number> Given an interest in &ldquo;rest&rdquo; as it relates to fatigue, the user would ignore &ldquo;rest flt&rdquo;, &ldquo;came rest&rdquo;, and other phrases unrelated to fatigue, and would select the fatigue-related phrases. To simplify the selection task, the user could list the terms ACFT, CAME, APCH, TRIP, and perhaps others as additional stopterms and then re-run the phrase generation program. The fatigue-related phrases, such as those shown below, could be used as input to phrase search:  
<table-cwu id="TABLE-US-00015">
<number>15</number>
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="1" colwidth="98PT" align="left"/>
<colspec colname="2" colwidth="119PT" align="left"/>
<thead>
<row>
<entry></entry>
</row>
<row><entry namest="1" nameend="2" align="center" rowsep="1"></entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry>REDUCED REST</entry>
<entry>LEGAL REST</entry>
</row>
<row>
<entry>CREW REST</entry>
<entry>MINIMUM REST REQUIREMENTS</entry>
</row>
<row>
<entry>REST PERIOD</entry>
<entry>COMPENSATORY REST</entry>
</row>
<row>
<entry>MINIMUM REST</entry>
<entry>REST NIGHT</entry>
</row>
<row>
<entry>REST REQUIREMENTS</entry>
<entry>REST BREAK</entry>
</row>
<row>
<entry>REST PERIODS</entry>
<entry>MINIMUM CREW REST</entry>
</row>
<row>
<entry>ADEQUATE REST</entry>
<entry>REQUIRED REST PRIOR</entry>
</row>
<row>
<entry>REQUIRED REST</entry>
<entry>MINIMUM REQUIRED CREW REST</entry>
</row>
<row>
<entry>MINIMUM REQUIRED REST</entry>
<entry>REQUIRED REST PRIOR FLT</entry>
</row>
<row>
<entry>REST OVERNIGHT</entry>
<entry>REQUIRED CREW REST PRIOR</entry>
</row>
<row>
<entry>REQUIRED CREW REST</entry>
<entry>LACK REST</entry>
</row>
<row>
<entry>PROPER REST</entry>
<entry>REST NIGHT PRIOR</entry>
</row>
<row>
<entry>REST PRIOR</entry>
<entry>LACK PROPER REST</entry>
</row>
<row>
<entry>CREW REST PRIOR</entry>
<entry>LACK CREW REST</entry>
</row>
<row>
<entry>SCHEDULED REST</entry>
<entry>LACK ADEQUATE REST</entry>
</row>
<row>
<entry>REST PRIOR FLT</entry>
</row>
<row><entry namest="1" nameend="2" align="center" rowsep="1"></entry>
</row>
</tbody>
</tgroup>
</table>
</table-cwu>
</paragraph>
<paragraph id="P-0308" lvl="0"><number>&lsqb;0308&rsqb;</number> A phrase search on these phrases retrieves narratives containing one or more of them. The most relevant narratives contain a greater variety of the most common phrases. Since phrase generation was used to suggest the list of phrases, it is assured that there are narratives in the database that contain one or more of the phrases on the list. The following are excerpts from some of the narratives that are most relevant to the &ldquo;rest&rdquo; phrases: </paragraph>
<paragraph id="P-0309" lvl="1"><number>&lsqb;0309&rsqb;</number> AFTER A NUMBER OF YRS AS BOTH A MIL AND COMMERCIAL CARRIER PLT I&apos;VE FOUND THAT EVERYONE&apos;S BODY NEEDS A ROUTINE, AND RADICAL CHANGES CAN ADVERSELY AFFECT ONE&apos;S PERF AND ABILITY TO GET ADEQUATE SLEEP DURING THE SUPPOSED REST PERIOD. OUR AIRLINE&apos;S SCHEDULING DEPT OPERATES UNDER CRISIS MGMNT DUE TO OUR MGMNT&apos;S &lsquo;STAFFING STRATEGY,&rsquo; AND THUS REQUIRES MANY RESERVE CREW MEMBERS TO COVER MORE THAN 1 SCHEDULED TRIP IN A CALENDAR DAY AND THUS WE HAVE A LARGE NUMBER OF &lsquo;SCHEDULED REDUCED REST PERIODS&rsquo; WHICH ARE 8 HRS, WHICH DOES NOT INCLUDE TRANSPORTATION LCL IN NATURE, WHICH, IN REALITY, REDUCES YOUR TIME AT A REST FACILITY WELL BELOW 8 HRS, PROVIDED YOU FALL TO SLEEP AS SOON AS YOU ARRIVE AT THE HOTEL. MY TRIP/RERTE FROM HELL STARTED AS A 3 DAY WITH AN 8 HR REST THE FIRST NIGHT WITH AN EARLY RPT. I HAPPENED TO BE COMING OFF A COUPLE OF NIGHT TRIPS AND THE EARLY MORNING RPT HAD ME A LITTLE OUT OF SYNC. WHEN WE ARRIVED AT OUR NEXT OVERNIGHT STATION, WHICH WE WERE SCHEDULED COMPENSATORY REST, I FELL ASLEEP EARLY NOT BEING ACCUSTOMED TO EARLY MORNING RPTS AND THUS WOKE VERY EARLY ON THE THE THIRD DAY . . . THE FAA NEEDS TO RECOGNIZE THE IMPORTANCE OF QUALITY CREW REST AND IMPLEMENT GUIDELINES TO PREVENT SUCH SCHEDULING PRACTICES. (254345) </paragraph>
<paragraph id="P-0310" lvl="1"><number>&lsqb;0310&rsqb;</number> CREW HAD A LEGAL DUTY DAY, BUT LAST 2 DAYS CREW HAD BEEN ON REDUCED REST WITH COMPENSATORY REST TO MINIMUM ALLOWED. CREW WAS EXTREMELY FATIGUED DUE TO MIN LEGAL REST AND RATHER LENGTHY DUTY DAY. CREW HAD BEEN ON DUTY OVER 12 HRS . SUGGESTIVE ACTION: INCREASE REST PERIODS. MIN REST PERIODS ARE ADEQUATE PROVIDED YOU AREN&apos;T FLOWN TO THOSE MINS 6 DAYS IN A ROW. IT&apos;S SIMPLY TOO FATIGUING. THERE WERE MANY SIMPLY MISTAKES MADE THIS FLT, ETC. MISSED CALLS, MISUNDERSTANDING HDG/ALT ASSIGNMENT/FREQ CHANGES. MOST OF THESE ERRORS WERE CAUGHT BY ONE OF THE CREW, THE ALT DEVIATION ON THE LAST LEG OF A 13.2 HR DUTY DAY WITH MINIMUM REQUIRED REST WAS JUST UNAVOIDABLE. PLEASE RESEARCH INCREASED REQUIRED REST PERIODS. (123335) </paragraph>
<paragraph id="P-0311" lvl="1"><number>&lsqb;0311&rsqb;</number> PRIOR TO DEPARTING ON THE LAST FLT OF DAY 2, I BECAME CONCERNED ABOUT THE REQUIRED CREW REST, SINCE WE WERE BEING DELAYED BY MAINT. I KNEW THAT, THOUGH WE HAD 9 HRS REST THE PREVIOUS NIGHT, ONCE WE EXCEEDED 15 HRS DUTY TIME OUR REST FOR THE 24 HR &ldquo;LOOKBACK&rdquo; WOULD BE LESS THAN NORMAL. MY QUESTION WAS THIS: COULD I ACCEPT REDUCED REST ON THE SECOND NIGHT, SINCE I WAS STILL FLYING WHAT WAS SCHEDULED, OR DID WE NEED COMPENSATORY REST BECAUSE OF WHAT WAS ACTUALLY FLOWN&quest; I CALLED OUR COMPANY&apos;S HEAD OF (MY ACFT) TRNING AND EXPLAINED ABOUT MY SIT. HE STATED THAT, WHILE HE FELT I NEEDED COMPENSATORY REST, REPEATED DISCUSSIONS WITH OUR VP OF OPS INDICATED THAT THE COMPANY&apos;S POS WAS THAT REDUCED REST WAS LEGAL. BASED ON THAT, I WENT WITH REDUCED REST. ON COMPLETION OF THE TRIP I TALKED TO OUR DIRECTOR OF OPS, WHO PRODUCED A MEMO FROM OUR VP OF OPS. THE MEMO SUMMARIZED AN FAA RULING DATED JULY 1989 STATING (AGAIN, AS I UNDERSTAND IT) THAT REQUIRED REST IS BASED ON ACTUAL FLT TIME AND DUTY TIME DURING THE PREVIOUS 24 HRS. COMMUTER AIRLINES ROUTINELY USE THE DUTY TIME REGS AS A GOAL TO ACHIEVE MAX UTILIZATION OF PLTS. YET, I HAVE NOT MET A SINGLE LINE PLT THAT FULLY UNDERSTANDS THIS REG. AS AN EXAMPLE, NO LINE PLT I ASKED KNEW THE ANSWER TO MY QUESTION. WHY IS THIS REG SO UNNECESSARILY SUBTLE&quest; (145545) </paragraph>
<paragraph id="P-0312" lvl="0"><number>&lsqb;0312&rsqb;</number> The above narratives contain a variety of the more prominent &ldquo;rest&rdquo; phrases, such as &ldquo;reduced rest&rdquo;, &ldquo;crew rest&rdquo;, and &ldquo;rest periods&rdquo;. In the first of these narratives (254345), the phrases &ldquo;scheduled reduced rest periods&rdquo; and &ldquo;scheduled compensatory rest&rdquo; are also among the highlighted &ldquo;rest&rdquo; phrases, despite the fact that these phrases do not appear in their entirety among the query phrases. Instead, the phrases match several of the query phrases, including &ldquo;scheduled rest&rdquo;, &ldquo;reduced rest&rdquo;, &ldquo;rest periods&rdquo;, and &ldquo;compensatory rest&rdquo;. This indicates the flexibility of phrase search in highlighting larger phrases of interest built up from smaller ones. </paragraph>
<paragraph id="P-0313" lvl="0"><number>&lsqb;0313&rsqb;</number> The combination of phrase generation and phrase search provides the ability to avoid ambiguities in searches. An advantage of this method with a topic like &ldquo;rest&rdquo; is that it can focus on the uses of the term &ldquo;rest&rdquo; that involve fatigue, while avoiding others. A keyterm search would sometimes retrieve narratives involving only &ldquo;rest of the flight&rdquo;, &ldquo;came to rest&rdquo;, etc. Without phrase generation, a user would not know what phrases contained the term &ldquo;rest&rdquo;, and so could not effectively use phrase search to focus on the kinds of &ldquo;rest&rdquo; that are of interest. Using phrase generation, topical phrases can be found for use as queries in phrase search, and thus narratives that are focused on the topic of interest can be found. In even more refined searches, phrases that represent particular nuances of the topic of interest can be selected for use as a query to phrase search. The retrieved narratives will reflect the desired nuances of the topic of interest. </paragraph>
<paragraph id="P-0314" lvl="0"><number>&lsqb;0314&rsqb;</number> Phrase generation also supports domain analysis and taxonomy development by showing prominent variations among topically related phrases. The &ldquo;rest&rdquo; phrases, for example, provide the analyst with a variety of variations on the concept of &ldquo;rest&rdquo;, such as &ldquo;reduced rest&rdquo; and &ldquo;compensatory rest&rdquo;, which, as the third narrative shows, have very particular meanings. With that insight, an analyst could then use phrase search to find other narratives containing &ldquo;reduced rest&rdquo; and/or &ldquo;compensatory rest&rdquo; to further explore the implications of these issues on crew performance and operational safety. </paragraph>
<paragraph id="P-0315" lvl="0"><number>&lsqb;0315&rsqb;</number> Phrase generation is one of several methods that display phrases contained in collections of text as a way to assist a user in domain analysis or query formulation and refinement. Phrase generation, described herein, includes an implicit phrase representation that can provide all possible phrases from the database. In contrast, other methods such as Godby (1994), Gutwin, Paynter, Witten, Nevill-Manning, and Frank (1998), Normore, Bendig, and Godby (1999), Zamir and Etzioni (1999), and Jones and Staveley (1999), maintain explicit and incomplete lists of phrases. In addition, phrase generation can provide the essence of multiple, similar phrases, which can be used as queries in a phrase search. The option of using the flexible matching of phrase search allows the generated query phrases to match both identical and nearly identical phrases in the text. This ensures that inconsequential differences do not spoil the match. </paragraph>
<paragraph id="P-0316" lvl="0"><number>&lsqb;0316&rsqb;</number> Some phrase generation methods such as Church, Gale, Hanks, and Hindle (1991), Gey and Chen (1997), and Godby (1994), use contextual association to identify important word pairs, but do not identify longer phrases, or do not use the same associative method to identify phrases having more than two words. In contrast, phrase generation treats phrases uniformly regardless of their size. </paragraph>
<paragraph id="P-0317" lvl="0"><number>&lsqb;0317&rsqb;</number> Some methods such as Gelbart and Smith (1991), Gutwin, Paynter, Witten, Nevill-Manning, and Frank (1998), and Jones and Staveley (1999), rely on manual identification of phrases at a critical point in the process, while phrase generation is fully automatic. </paragraph>
</section>
<section>
<heading lvl="1">Phrase Discovery </heading>
<paragraph id="P-0318" lvl="0"><number>&lsqb;0318&rsqb;</number> Phrase discovery is a process of identifying short sequences of terms, herein called phrases that are contextually associated within a number of subsets of a database. The phrase discovery process can also identify subsets of a database that contain one or more of the discovered phrases or that contain phrases that are similar to the discovered phrases. These identified subsets can also be sorted according to the extent to which they are representative of the contexts in which the discovered phrases are contextually associated. </paragraph>
<paragraph id="P-0319" lvl="0"><number>&lsqb;0319&rsqb;</number> Phrase discovery is substantially different from phrase generation process described above in FIGS. <highlight><bold>16</bold></highlight>-<highlight><bold>18</bold></highlight>. Phrase discovery derives phrases directly from sequences of terms such as narratives or passages, while phrase generation derives phrases from relational models of databases. Further, phrase discovery does not include a query. Phrase discovery discovers contextually associated phrases that are present in the provided relevant sequence of terms. In contrast, phrase generation includes a query and all generated phrases contain a portion of the query. </paragraph>
<paragraph id="P-0320" lvl="0"><number>&lsqb;0320&rsqb;</number> The process of phrase discovery is initiated by providing a relevant sequence of terms that includes the contexts of interest. In one alternative, the sequence of terms is text. In the following description, the word &ldquo;text&rdquo; is intended to be representative of any sequence of terms. Alternative sequences of terms are described above. A relevant sequence of terms can be obtained by conducting a keyterm search or a phrase search as described above, or by another automated or manual process of selection. </paragraph>
<paragraph id="P-0321" lvl="0"><number>&lsqb;0321&rsqb;</number> Phrase discovery can be used as a method of query expansion. As a query expansion method, one or more terms can be input to keyterm search, or one or more phrases can be input to phrase search, and the retrieved text can provide the relevant text for input to phrase search, and the retrieved text can provide the relevant text for input to phrase discovery, which then produces a list of contextually associated phrases. The relevant text includes contexts of topics of interest, i.e. describes a topic such as &ldquo;fatigue&rdquo; which is of interest to the user, and the contexts include descriptions of issues related to the topic &ldquo;fatigue.&rdquo; This list of contextually associated phrases can then be used as a query in a subsequent phrase search. Thus, an initial query consisting of even a single term or phrase can be expanded into a query consisting of a large number of contextually associated phrases. </paragraph>
<paragraph id="P-0322" lvl="0"><number>&lsqb;0322&rsqb;</number> Phrase discovery can be a single-pass process, directly deriving contextually associated phrases from the provided relevant text. Alternatively, phrase discovery can be applied iteratively. As an iterative process, phrase discovery first derives contextually associated phrases from provided relevant text from any source. The resulting phrases are then provided as a query to phrase search on a database. Based on the query, phrase search then retrieves from the database a new, more focused, and more relevant body of text, and the phrase discovery process then obtains contextually associated phrases from the new relevant text. Phrase discovery can be applied in any number of iterations. Each iteration further focuses the output results. </paragraph>
<paragraph id="P-0323" lvl="0"><number>&lsqb;0323&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 19</cross-reference> shows one embodiment of an overview of the phrase discovery process <highlight><bold>1900</bold></highlight>. The phrase discovery process is described in more detail below. First, a relevant text is provided in block <highlight><bold>1902</bold></highlight>. The provided relevant text can be any text that contains the topic of interest, and preferably text that prominently contains the topic of interest. For example, if the topic of interest is &ldquo;aircrew fatigue&rdquo;, then aircrew fatigue should be among the prominent topics in the provided relevant text. The relevant text can be any quantity of text such as a passage, a paragraph, a narrative, a collection of narratives, or larger selections of text. Phrases are extracted from the provided relevant text in block <highlight><bold>1904</bold></highlight>. The extracted phrases can include all phrases that occur in the relevant text. Alternatively, the extracted phrases can include a selected number of the phrases that occur in the relevant text. The extracted phrases are culled in block <highlight><bold>1906</bold></highlight>. The culled phrases are then input to a gathering process in block <highlight><bold>1908</bold></highlight>. The gathering process gathers phrases that are contextually associated, that is, phrases that are prominent in the local context of the provided relevant text, but are not prominent in the global context of a larger collection of similar text. The phrases resulting from the gathering process <highlight><bold>1908</bold></highlight> are output in block <highlight><bold>1910</bold></highlight>. </paragraph>
<paragraph id="P-0324" lvl="0"><number>&lsqb;0324&rsqb;</number> The process of phrase discovery is initiated by providing relevant text that includes the contexts and topic of interest. That relevant text can be obtained by conducting a keyterm search or a phrase search as described above, or by another automated or manual process. In one alternative, phrase discovery can be preceded by a keyterm search of a database of narratives, which provides a collection of relevant narratives that are relevant to the keyterm search query. A subset of the relevant narratives can then be input to phrase discovery as the provided relevant text. The provided relevant text includes the contexts of the phrases that are subsequently extracted, culled, and gathered by the phrase discovery process. In another alternative, phrase discovery can be preceded by a phrase search of a database of narratives, which provides a collection of narratives that are relevant to the phrase search query. A subset of those relevant narratives can then be input to phrase discovery as the provided relevant text. This text includes the contexts of the phrases that are subsequently extracted, culled, and gathered by the phrase discovery process. In another alternative, a document is identified as being relevant text and the document is provided as input to phrase discovery. In another alternative, passages from a wide variety of documents are gathered by a combination of manual and automated methods to form a database of passages. The database is input to phrase discovery as the provided relevant text. </paragraph>
<paragraph id="P-0325" lvl="0"><number>&lsqb;0325&rsqb;</number> Phrase extraction is a process of identifying and collecting a number of sequences of terms that occur within a larger sequence of terms contained in one or more subsets of a database. One embodiment of phrase extraction obtains phrases from a collection of text. Phrase extraction can identify phrases that occur one or more times in the input sequence of terms without reference to any pre-existing lists of phrases, and without recognition of the grammatical structure of language. Phrase extraction uses each term in the input sequence of terms as a first term in a number of phrases. First, a phrase consisting of a single (1) term is identified. Then, starting with the single term, a phrase of two (2) terms is identified. Processing continues until phrases containing any number of terms, up to a selected number (N) of terms, are identified. Then, a subsequent term is identified in the sequence of terms, and another set of phrases of length 1 to N are identified. The process continues until every term in the input sequence of terms has been used as a starting term for a set of phrases of length 1 to N. In one alternative, a count of the unique phrases is maintained and only one copy of each unique phrase is output along with the corresponding frequency of the unique phrase. </paragraph>
<paragraph id="P-0326" lvl="0"><number>&lsqb;0326&rsqb;</number> In one alternative, phrase extraction can include one or more sets or classes of special terms to determine whether and to what extent a term from one set of special term&apos;s is allowed to appear in a particular position within a phrase. Based on the terms membership in the set of special terms and the term&apos;s presence in the phrase, the phrase may or may not be identified as an acceptable phrase. Only acceptable phrases are then output to the culling process. In one alternative, the special terms include one or more sets of stopterms. In one alternative, the special terms include one or more sets of stopterms. In one alternative, a set of stopterms includes zero or more terms that occur in the relevant text. In another alternative, a set of stopterms can include conventional stopwords such as articles and conjunctions. Stopterms can also include punctuation. </paragraph>
<paragraph id="P-0327" lvl="0"><number>&lsqb;0327&rsqb;</number> The culling process reduces the number of extracted phrases. In one embodiment, the culling process eliminates a phrase that only occurs as part of another, longer phrase within the provided relevant text from which the phrases were obtained. In one alternative, the previously extracted phrases can be input to the culling process. The phrases input to the culling process are collected in a list of candidate phrases. A first phrase from the candidate phrases is selected and the selected phrase is then examined to see if the selected phrase is contained within any of the other candidate phrases in the candidate phrase list. If the selected phrase is contained in another candidate phrase (i.e. a containing phrase) in the candidate phrase list, then the frequencies of the selected phrase and the containing phrase are examined. And if the frequency of the selected phrase is not greater than the frequency of the containing phrase, then the selected phrase only occurs in the provided relevant text as part of the containing phrase. Therefore, the selected phrase is not a stand-alone phrase and is therefore deleted. Each of the phrases in the candidate phrase list are tested as described above. The candidate phrases that remain in the candidate phrase list after the culling process is complete are then output. In one alternative, the phrases are output to a gathering phrases process. </paragraph>
<paragraph id="P-0328" lvl="0"><number>&lsqb;0328&rsqb;</number> The process of gathering related phrases takes a collection of phrases as input, and produces a collection of phrases that are contextually associated. The gathering process can also include sorting the gathered phrases according to the corresponding degrees of contextual association. The gathered phrases having a higher degree of contextual association are more contextually associated locally and less contextually associated globally in a larger collection of similar text. The larger collection of similar text can include some or all of the provided relevant text and also less relevant text, or alternatively can include text that is similar to the provided relevant text and also less relevant text. </paragraph>
<paragraph id="P-0329" lvl="0"><number>&lsqb;0329&rsqb;</number> The gathering phrases process can also be an iterative process. When the gathering phrases process is iterative, each iteration after the first gathering of phrases includes a phrase search where the previously gathered phrases as the input query. The output of the phrase search includes a new body of provided relevant text, from which additional phrases are obtained, as described below. Thus, the iterative process uses feedback of associated phrases to obtain additional contextually associated phrases. The database searched by the phrase search can include the larger collection of similar text, and alternatively, an additional collection of text. The iterative gathering process can also include a process of extracting additional phrases from the new body of provided relevant text, and can also include a culling process to reduce the number of extracted phrases, to produce additional phrases that are contextually associated. The additional phrases can be sorted according to the corresponding degrees of contextual association and combined in sorted order with previously gathered phrases. </paragraph>
<paragraph id="P-0330" lvl="0"><number>&lsqb;0330&rsqb;</number> The phrases resulting from the gathering process are output as the final result of the overall phrase discovery process. In one alternative, the phrases are output in an order according to the corresponding degrees of contextual association, which were determined in the process of gathering phrases. As another alternative, the phrases are output in order of the corresponding frequencies within the provided relevant text. As yet another alternative, when the process of gathering related phrases iterates multiple times and processes multiple relevant texts, the phrases can be output in order of the corresponding highest frequency in any of the multiple relevant texts. In yet another alternative, the phrases are output in an order which is a function of one or more of the corresponding frequencies in relevant texts and one or more of the corresponding rankings according to the degree of contextual association. </paragraph>
<paragraph id="P-0331" lvl="0"><number>&lsqb;0331&rsqb;</number> FIGS. <highlight><bold>20</bold></highlight>-<highlight><bold>20</bold></highlight>E illustrate various embodiments of the phrase extraction process <highlight><bold>1904</bold></highlight>. <cross-reference target="DRAWINGS">FIG. 20</cross-reference> shows an overview of one embodiment of the phrase extraction process <highlight><bold>1904</bold></highlight>. First the phrase starting positions are processed within the relevant text in block <highlight><bold>2002</bold></highlight>. The phrase starting positions include the terms in the relevant text that the process will use to begin each iteration of the phrase extraction process. In one alternative, a number of selected starting position terms are extracted as a number of single-term phrases. Selected multi-term phrases are extracted in block <highlight><bold>2004</bold></highlight>. Multi-term phrases include two or more terms. The first term of each multi-term phrase is one of the phrase starting position terms. The resulting phrase list is output to the next sub-process in block <highlight><bold>2006</bold></highlight>. </paragraph>
<paragraph id="P-0332" lvl="0"><number>&lsqb;0332&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 20A</cross-reference> illustrates one embodiment of the phrase starting positions process <highlight><bold>2002</bold></highlight>. A first term in the relevant text is identified in block <highlight><bold>2010</bold></highlight>. The first term is then identified as both T<highlight><bold>1</bold></highlight> and T<highlight><bold>2</bold></highlight> in block <highlight><bold>2011</bold></highlight>. Next, if there is a term subsequent to T<highlight><bold>1</bold></highlight>, then T<highlight><bold>1</bold></highlight> is not the last term in the relevant text and it is possible that T<highlight><bold>1</bold></highlight> is an acceptable first term in a multi-term phrase, therefore determine if T<highlight><bold>1</bold></highlight> is a stopterm in block <highlight><bold>2013</bold></highlight>, or alternatively, if T<highlight><bold>1</bold></highlight> is a starting stopterm in block <highlight><bold>2013</bold></highlight>A. If T<highlight><bold>1</bold></highlight> is a stopterm in <highlight><bold>2013</bold></highlight>, or if T<highlight><bold>1</bold></highlight> is a starting stopterm in <highlight><bold>2013</bold></highlight>A, then T<highlight><bold>1</bold></highlight> is not an acceptable first term in a multi-term phrase, and therefore identify the term subsequent to T<highlight><bold>1</bold></highlight> as both T<highlight><bold>1</bold></highlight> and T<highlight><bold>2</bold></highlight> in block <highlight><bold>2014</bold></highlight>. The process continues at block <highlight><bold>2012</bold></highlight>. If T<highlight><bold>1</bold></highlight> is not a stopterm in block <highlight><bold>2013</bold></highlight>, or alternatively, if T<highlight><bold>1</bold></highlight> is not a starting stopterm in <highlight><bold>2013</bold></highlight>A, then T<highlight><bold>1</bold></highlight> is an acceptable first term in a multi-term phrase and a potentially acceptable single term phrase, therefore T<highlight><bold>1</bold></highlight> is saved in the phrase list (PL) as a single term phrase in block <highlight><bold>2015</bold></highlight> according to the subprocess shown in <cross-reference target="DRAWINGS">FIG. 20</cross-reference>B, as described below. Next, selected multi-term phrases are extracted at the starting position T<highlight><bold>1</bold></highlight> in block <highlight><bold>2004</bold></highlight> according to the process described in <cross-reference target="DRAWINGS">FIG. 20D</cross-reference> or <cross-reference target="DRAWINGS">FIG. 20</cross-reference>E, as described below. After extracting phrases in block <highlight><bold>2004</bold></highlight>, the phrase extraction process begins at a new starting position by continuing the process at block <highlight><bold>2014</bold></highlight>. </paragraph>
<paragraph id="P-0333" lvl="0"><number>&lsqb;0333&rsqb;</number> If there is not a term subsequent to T<highlight><bold>1</bold></highlight> in the relevant text in block <highlight><bold>2012</bold></highlight>, then T<highlight><bold>1</bold></highlight> is the last term in the relevant text, and the process continues at block <highlight><bold>2017</bold></highlight>. If T<highlight><bold>1</bold></highlight> is a stopterm in block <highlight><bold>2017</bold></highlight>, then T<highlight><bold>1</bold></highlight> is ignored in block <highlight><bold>2019</bold></highlight> and the phrase list is output in block <highlight><bold>2006</bold></highlight>. If T<highlight><bold>1</bold></highlight> is not a stopterm in block <highlight><bold>2017</bold></highlight>, then T<highlight><bold>1</bold></highlight> is a potentially acceptable single-term phrase, therefore T<highlight><bold>1</bold></highlight> is saved in the phrase list as a single-term phrase in block <highlight><bold>2018</bold></highlight> according to the subprocess shown in <cross-reference target="DRAWINGS">FIG. 20</cross-reference>B, as described below. </paragraph>
<paragraph id="P-0334" lvl="0"><number>&lsqb;0334&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 20B</cross-reference> illustrates one embodiment of saving single term phrases. If single term phrases are acceptable in block <highlight><bold>2020</bold></highlight>, then the phrase is saved in block <highlight><bold>2022</bold></highlight>, and then the subprocess illustrated in <cross-reference target="DRAWINGS">FIG. 20B</cross-reference> is ended. If single term phrases are not acceptable in block <highlight><bold>2020</bold></highlight>, then the phrase is not saved, and then the subprocess illustrated in <cross-reference target="DRAWINGS">FIG. 20B</cross-reference> is ended. Single term phrases are acceptable if a user has enabled single term phrases. </paragraph>
<paragraph id="P-0335" lvl="0"><number>&lsqb;0335&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 20C</cross-reference> shows one embodiment of saving a phrase subprocess, block <highlight><bold>2022</bold></highlight> in <cross-reference target="DRAWINGS">FIG. 20</cross-reference>B, of combining the current phrase into the phrase list. If the current phrase is included in the phrase list in block <highlight><bold>2026</bold></highlight> then a frequency counter corresponding to the current phrase in the phrase list is incremented in block <highlight><bold>2028</bold></highlight> and the <cross-reference target="DRAWINGS">FIG. 20C</cross-reference> subprocess ends. If the current phrase is not included in the phrase list in block <highlight><bold>2026</bold></highlight>, then the current phrase is added to the phrase list and a corresponding frequency counter in the phrase list is set to 1 in block <highlight><bold>2030</bold></highlight> and the subprocess ends. </paragraph>
<paragraph id="P-0336" lvl="0"><number>&lsqb;0336&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 20D</cross-reference> illustrates one embodiment of a subprocess of extracting selected multi-term phrases at each starting position in block <highlight><bold>2004</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 20</cross-reference> and <cross-reference target="DRAWINGS">FIG. 20A</cross-reference>. An interior stopterm count is set to zero in block <highlight><bold>2026</bold></highlight>. The initial value of the tuple size is set to 2 in block <highlight><bold>2028</bold></highlight>. For alternative embodiments the initial value of the tuple size can be set to a larger number. The tuple size is the number of terms in the current multi-term phrase. The smallest multi-term phrase has 2 terms, so the initial tuple size is 2. After each current phrase is processed, as described below, the tuple size is incremented in order to process a phrase containing one additional term. Next, the term subsequent to T<highlight><bold>2</bold></highlight> is identified as T<highlight><bold>2</bold></highlight> in block <highlight><bold>2030</bold></highlight>. If the tuple is greater than a pre-selected maximum phrase length in block <highlight><bold>2032</bold></highlight>, then end the subprocess in block <highlight><bold>2034</bold></highlight>, and return to process <highlight><bold>2002</bold></highlight> in <cross-reference target="DRAWINGS">FIG. 20A</cross-reference> at block <highlight><bold>2014</bold></highlight>. If the tuple size is not greater than a pre-selected maximum phrase length in block <highlight><bold>2032</bold></highlight>, then determine if T<highlight><bold>2</bold></highlight> is a stopterm in block <highlight><bold>2036</bold></highlight>. If T<highlight><bold>2</bold></highlight> is not a stopterm then the current phrase is saved in the phrase list in block <highlight><bold>2022</bold></highlight>, as described in <cross-reference target="DRAWINGS">FIG. 20</cross-reference>C, and then the tuple size is incremented in block <highlight><bold>2052</bold></highlight>. If T<highlight><bold>2</bold></highlight> is a stopterm in block <highlight><bold>2036</bold></highlight> then the interior stopterm counter is incremented in block <highlight><bold>2038</bold></highlight> and the number of interior stopterms in the current phrase is compared to a pre-selected number of interior stopterms in block <highlight><bold>2040</bold></highlight>. The preselected number of interior stopterms represents the number of interior stopterms that will be allowed within a phrase. If the number of interior stopterms is greater than the pre-selected number of interior stopterms, then end the subprocess at block <highlight><bold>2034</bold></highlight>, and return to process <highlight><bold>2002</bold></highlight> in <cross-reference target="DRAWINGS">FIG. 20A</cross-reference> at block <highlight><bold>2014</bold></highlight>. If the number of interior stopterms in the current phrase is not greater than the pre-selected number of interior stopterms in block <highlight><bold>2040</bold></highlight>, then the tuple size is incremented in block <highlight><bold>2052</bold></highlight>. Once the tuple size is incremented in block <highlight><bold>2052</bold></highlight>, determine if there is a term subsequent to T<highlight><bold>2</bold></highlight> in the relevant text in block <highlight><bold>2054</bold></highlight>. If there is not a term subsequent to T<highlight><bold>2</bold></highlight> in the relevant text, then end the subprocess at block <highlight><bold>2034</bold></highlight>, and return to process <highlight><bold>2002</bold></highlight> in <cross-reference target="DRAWINGS">FIG. 20A</cross-reference> at block <highlight><bold>2014</bold></highlight>. If there is a term subsequent to T<highlight><bold>2</bold></highlight> in the relevant text in block <highlight><bold>2054</bold></highlight>, then the term subsequent to T<highlight><bold>2</bold></highlight> is identified as T<highlight><bold>2</bold></highlight> in block <highlight><bold>2030</bold></highlight>. The process continues until all acceptable multi-term phrases beginning with T<highlight><bold>1</bold></highlight> are assembled. </paragraph>
<paragraph id="P-0337" lvl="0"><number>&lsqb;0337&rsqb;</number> The process described in <cross-reference target="DRAWINGS">FIG. 20D</cross-reference> uses a single class of stopterms to reject some candidate phrases. In one embodiment, accepted phrases can be limited to those phrases containing no stopterms. For example, if the word &ldquo;the&rdquo; is a stopterm, the phrase &ldquo;call number&rdquo; (the identifier of an aircraft) would be accepted, while the phrase &ldquo;call the number&rdquo; and &ldquo;the call number&rdquo; would be excluded. In an alternative embodiment, accepted phrases can be limited to phrases meeting two conditions: first, the starting and ending terms are not stopterms of the phrases, and second, the phrases have no more than a certain number of interior terms that are stopterms. An interior term is a term that is not a first or a last term in a phrase. For example, using a typical list of stopterms including such words as &ldquo;to&rdquo;, &ldquo;the&rdquo;, and &ldquo;in&rdquo;, and allowing up to two interior stopterms, the phrases &ldquo;approach runway&rdquo;, &ldquo;approach to runway&rdquo;, and &ldquo;approach to the runway&rdquo; would be accepted, while the phrases &ldquo;approach the runway in&rdquo;, &ldquo;approach the runway in the fog&rdquo;, &ldquo;the approach&rdquo;, and &ldquo;approach the&rdquo; would be rejected. </paragraph>
<paragraph id="P-0338" lvl="0"><number>&lsqb;0338&rsqb;</number> Having a single class of stopterms, combined with determination of the position of stopterms within a phrase, may be sufficient for some applications of the phrase extraction process, but having additional classes of terms provides additional control and refinements in extracting phrases having particular forms. A process using multiple classes of terms is illustrated in <cross-reference target="DRAWINGS">FIG. 20</cross-reference>E, described below. <cross-reference target="DRAWINGS">FIG. 20E</cross-reference> illustrates an alternative embodiment of extracting selected multi-term phrases at each starting position in the text. The process of <cross-reference target="DRAWINGS">FIG. 20E</cross-reference> differs from the process of <cross-reference target="DRAWINGS">FIG. 20D</cross-reference> in that the process illustrated in <cross-reference target="DRAWINGS">FIG. 20E</cross-reference> includes use of a number of classes of stopterms and a class of interior-only terms. Three classes of stopterms are illustrated: starting stoptersm, interior stopterms, and ending stopterms. A starting stopterm is a term that may not be the first term of a phrase. An interior stopterm is an interior term that may appear only up to a pre-selected number of times in a phrase (including zero times). An ending stopterm is a term that may not be the last term of a phrase. When distinguishing among the three classes is unnecessary, a stopterm in any class is merely referred to as a stopterm. An interior-only term is a term that is not an interior stopterm and may not be the first or last term of a phrase. </paragraph>
<paragraph id="P-0339" lvl="0"><number>&lsqb;0339&rsqb;</number> Distinguishing starting stopterms from ending stopterms allows, for example, acceptance of phrases such as &ldquo;the autopilot&rdquo; and &ldquo;the mode control panel&rdquo; by not including the word &ldquo;the&rdquo; among the class of starting stopterms, while also excluding phrases such as &ldquo;autopilot the&rdquo; and &ldquo;mode control panel the&rdquo; by including the word &ldquo;the&rdquo; among the class of ending stopterms. Distinguishing the classes of starting stopterms from ending and interior stopterms allows, for example, acceptance of phrases like &ldquo;call number&rdquo;, &ldquo;the call number&rdquo;, and &ldquo;a call number&rdquo; by not including &ldquo;the&rdquo; and &ldquo;a&rdquo; among the class of starting stopterms, while also rejecting phrases such as &ldquo;call a number&rdquo;, &ldquo;call number and&rdquo;, and &ldquo;call number of&rdquo; by allowing no interior stopterms and including &ldquo;a&rdquo;, &ldquo;and&rdquo;, and &ldquo;of&rdquo; among the classes of ending and interior stopterms. Phrases such as &ldquo;and call number&rdquo; and &ldquo;of call number&rdquo; are also rejected by including &ldquo;and&rdquo; and &ldquo;of&rdquo; among the class of starting stopterms. Distinguishing the class interior-only terms from the various classes of stopterms allows, for example, acceptance of phrases such as &ldquo;rate of climb&rdquo;, &ldquo;time of day&rdquo;, and &ldquo;mode control panel&rdquo; by including &ldquo;of&rdquo; among the class of interior-only terms (and conversely not including &ldquo;of&rdquo; among the class of interior stopterms), while also excluding phrases like &ldquo;rate of&rdquo;, &ldquo;rate of the&rdquo;, &ldquo;the rate of climb&rdquo;, and &ldquo;of climb&rdquo;, by including &ldquo;of&rdquo; and &ldquo;the&rdquo; among both the classes of starting and ending stopterms, and also excluding such phrases as &ldquo;cleared the runway&rdquo; and &ldquo;begin to climb&rdquo; by allowing no interior stopterms and including &ldquo;the&rdquo; and &ldquo;to&rdquo; among the class of interior stopterms. </paragraph>
<paragraph id="P-0340" lvl="0"><number>&lsqb;0340&rsqb;</number> In another application, the phrase extraction process can be used for highly targeted phrase extractions, such as finding certain prepositional phrases. In one alternative, highly targeted extractions can be done by defining all vocabulary words except prepositions as starting stopterms, using a conventional stoplist for the ending and interior stopterms, and allowing up to two interior stopterms. Such phrases as &ldquo;on board&rdquo;, &ldquo;in the cockpit&rdquo;, &ldquo;at altitude&rdquo;, and &ldquo;below the other aircraft&rdquo;, would be accepted, while all phrases not starting with a preposition would be rejected. Interior-only terms could be used to further limit the acceptable phrases. Additional general classes of terms, such as ending-only terms, can also be envisioned. </paragraph>
<paragraph id="P-0341" lvl="0"><number>&lsqb;0341&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 20E</cross-reference> illustrates an alternative embodiment of a subprocess of extracting selected multi-term phrases at each starting position in block <highlight><bold>2004</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 20</cross-reference> and <cross-reference target="DRAWINGS">FIG. 20A</cross-reference>. An interior stopterm count is set to zero in block <highlight><bold>2056</bold></highlight>. The initial value of the tuple size is set to 2 in block <highlight><bold>2058</bold></highlight>. Next, the term subsequent to T<highlight><bold>2</bold></highlight> is identified as T<highlight><bold>2</bold></highlight> in block <highlight><bold>2060</bold></highlight>. If the tuple size is greater than a pre-selected maximum phrase length in block <highlight><bold>2062</bold></highlight>, then end the subprocess in block <highlight><bold>2064</bold></highlight>, and return to process <highlight><bold>2002</bold></highlight> in <cross-reference target="DRAWINGS">FIG. 20A</cross-reference> at block <highlight><bold>2014</bold></highlight>. If the tuple size is not greater than a pre-selected maximum phrase length in block <highlight><bold>2062</bold></highlight>, then determine if T<highlight><bold>2</bold></highlight> is an interior stopterm in block <highlight><bold>2066</bold></highlight>. If T<highlight><bold>2</bold></highlight> is an interior stopterm in block <highlight><bold>2066</bold></highlight>, then the interior stopterm counter is incremented in block <highlight><bold>2068</bold></highlight> and the number of interior stopterms in the current phrase is compared to a pre-selected number of interior stopterms in block <highlight><bold>2070</bold></highlight>. If the number of interior stopterms is greater than the pre-selected number of interior stopterms, then end the subprocess in block <highlight><bold>2064</bold></highlight>, and return to process <highlight><bold>2002</bold></highlight> in <cross-reference target="DRAWINGS">FIG. 20A</cross-reference> at block <highlight><bold>2014</bold></highlight>. If the number of interior stopterms in the current phrase is not greater than the pre-selected number of interior stopterms in block <highlight><bold>2070</bold></highlight>, then the tuple size is incremented in block <highlight><bold>2072</bold></highlight>. If T<highlight><bold>2</bold></highlight> is not an interior stop term in block <highlight><bold>2066</bold></highlight>, then determine if T<highlight><bold>2</bold></highlight> is an ending stopterm in block <highlight><bold>2076</bold></highlight>. If T<highlight><bold>2</bold></highlight> is not an ending stopterm then the current phrase is saved in the phrase list in block <highlight><bold>2022</bold></highlight>, as described in <cross-reference target="DRAWINGS">FIG. 20</cross-reference>C, and then the tuple size is incremented in block <highlight><bold>2072</bold></highlight>. If T<highlight><bold>2</bold></highlight> is an ending stopterm in block <highlight><bold>2076</bold></highlight>, then determine if T<highlight><bold>2</bold></highlight> is an interior-only term in block <highlight><bold>2078</bold></highlight>. if T2 is not an interior-only term, then end the subprocess in block <highlight><bold>2064</bold></highlight>, and return to process <highlight><bold>2002</bold></highlight> in <cross-reference target="DRAWINGS">FIG. 20A</cross-reference> at block <highlight><bold>2014</bold></highlight>. If T<highlight><bold>2</bold></highlight> is an interior only term in block <highlight><bold>2078</bold></highlight>, then the tuple size is incremented in block <highlight><bold>2072</bold></highlight>. Once the tuple size is incremented in block <highlight><bold>2072</bold></highlight>, determine if there is a term subsequent to T<highlight><bold>2</bold></highlight> in the relevant text in block <highlight><bold>2074</bold></highlight>. If there is not a term subsequent to T<highlight><bold>2</bold></highlight> in the relevant text, then end the subprocess in block <highlight><bold>2064</bold></highlight>, and return to process <highlight><bold>2002</bold></highlight> in <cross-reference target="DRAWINGS">FIG. 20A</cross-reference> at block <highlight><bold>2014</bold></highlight>. If there is a term subsequent to T<highlight><bold>2</bold></highlight> in the relevant text in block <highlight><bold>2074</bold></highlight>, then the term subsequent to T<highlight><bold>2</bold></highlight> is identified as T<highlight><bold>2</bold></highlight> in block <highlight><bold>2060</bold></highlight>. The phrase processing continues until all acceptable phrases beginning with T<highlight><bold>1</bold></highlight> are assembled. </paragraph>
<paragraph id="P-0342" lvl="0"><number>&lsqb;0342&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 21</cross-reference> illustrates one embodiment of culling the extracted phrases in block <highlight><bold>1906</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 19</cross-reference>. The first phrase from the candidate phrase list (CPL) is identified as P<highlight><bold>1</bold></highlight> in block <highlight><bold>2102</bold></highlight>. Several phrases from the CPL are identified. Each one of the identified phrases includes P<highlight><bold>1</bold></highlight> as a proper subset in block <highlight><bold>2104</bold></highlight> i.e. P<highlight><bold>1</bold></highlight> is only a portion of each one of the phrases. A first one of the phrases is identified as P<highlight><bold>2</bold></highlight> in block <highlight><bold>2106</bold></highlight>. If the frequency of P<highlight><bold>1</bold></highlight> is equal to the frequency of P<highlight><bold>2</bold></highlight> in block <highlight><bold>2108</bold></highlight> then P<highlight><bold>1</bold></highlight> is eliminated from the CPL in block <highlight><bold>2110</bold></highlight> and the process continues at block <highlight><bold>2116</bold></highlight> below. If the frequency of P<highlight><bold>1</bold></highlight> is not equal to the frequency of P<highlight><bold>2</bold></highlight> in block <highlight><bold>2108</bold></highlight>, then a phrase subsequent to P<highlight><bold>2</bold></highlight> is selected as P<highlight><bold>2</bold></highlight> in blocks <highlight><bold>2112</bold></highlight>, <highlight><bold>2114</bold></highlight> and the new P<highlight><bold>2</bold></highlight> is input to block <highlight><bold>2108</bold></highlight> above. If there are no more phrases subsequent to P<highlight><bold>2</bold></highlight> in block <highlight><bold>2112</bold></highlight>, then a phrase subsequent to P<highlight><bold>1</bold></highlight> in the CPL is selected as P<highlight><bold>1</bold></highlight> <highlight><bold>2116</bold></highlight>, <highlight><bold>2118</bold></highlight> and the subsequent P<highlight><bold>1</bold></highlight> is processed beginning with block <highlight><bold>2104</bold></highlight>. If there are no more phrases subsequent to P<highlight><bold>1</bold></highlight> in the CPL then the phrases in the CPL are output to the process of gathering related phrases in block <highlight><bold>1908</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 19</cross-reference>. </paragraph>
<paragraph id="P-0343" lvl="0"><number>&lsqb;0343&rsqb;</number> FIGS. <highlight><bold>22</bold></highlight>-<highlight><bold>22</bold></highlight>D illustrate various embodiments of the process of gathering related phrases in block <highlight><bold>1908</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 19</cross-reference>. In addition to the processes illustrated in FIGS. <highlight><bold>22</bold></highlight>-<highlight><bold>22</bold></highlight>D, related phrases can alternatively be gathered by manually selecting related phrases, or by a single iteration or a multiple iteration of the processes presented in FIGS. <highlight><bold>22</bold></highlight>-<highlight><bold>22</bold></highlight>D. </paragraph>
<paragraph id="P-0344" lvl="0"><number>&lsqb;0344&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 22</cross-reference> illustrates one embodiment of gathering related phrases <highlight><bold>1908</bold></highlight>. A gathered phrase (GPL) list is initialized in block <highlight><bold>2202</bold></highlight>. The phrases output from the most recent extracting and culling process are ranked in order of relevance, in block <highlight><bold>2204</bold></highlight>. The ranked phrases are selected and then combined with the GPL to create a revised GPL in block <highlight><bold>2206</bold></highlight>. A phrase search counter is then incremented in block <highlight><bold>2208</bold></highlight> and evaluated in block <highlight><bold>2210</bold></highlight>. If the phrase search counter is greater than a pre-selected number then the gathered phrase list is output in block <highlight><bold>1910</bold></highlight>. The phrase search counter counts the number of iterations through the gathering related phrases process <highlight><bold>1908</bold></highlight>. Each iteration through the process of gathering related phrases <highlight><bold>1908</bold></highlight> further focuses the discovered phrases on the designed topic. For one embodiment a single iteration is sufficient. For alternative embodiments additional iterations can also be used. </paragraph>
<paragraph id="P-0345" lvl="0"><number>&lsqb;0345&rsqb;</number> If the phrase search counter is not greater than a pre-selected number of phrase searches in block <highlight><bold>2210</bold></highlight> then a phrase search is performed using the gathered phrases as a single query including multiple phrases in block <highlight><bold>2214</bold></highlight>. The phrase search in block <highlight><bold>2214</bold></highlight> is performed on a database having relevant data. This database may or may not include the relevant text provided in block <highlight><bold>1902</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 19</cross-reference> in the initial phrase discovery process, but the database should include a common topic with the relevant text provided in block <highlight><bold>1902</bold></highlight>. </paragraph>
<paragraph id="P-0346" lvl="0"><number>&lsqb;0346&rsqb;</number> The phrase search in block <highlight><bold>2214</bold></highlight> outputs a ranked list of subsets from the database and a selected number of the ranked list of subsets are then designated as the relevant text and input to the extract phrases process described in <cross-reference target="DRAWINGS">FIG. 20</cross-reference> in block <highlight><bold>1904</bold></highlight>. The phrases extracted from the extract phrases process in block <highlight><bold>1904</bold></highlight> are then input to the process of culling the extracted phrases described in <cross-reference target="DRAWINGS">FIG. 21</cross-reference> in block <highlight><bold>1906</bold></highlight>. The phrases output from the process of culling the extracted phrases in block <highlight><bold>1906</bold></highlight> are then ranked at block <highlight><bold>2204</bold></highlight> and the process repeats, until the number in the phrase search counter is greater than the pre-selected number of phrase searches. </paragraph>
<paragraph id="P-0347" lvl="0"><number>&lsqb;0347&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 22A</cross-reference> illustrates one embodiment of ranking the phrases output from the extracting and culling processes of block <highlight><bold>2204</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 22</cross-reference>. First, the relevant text from which the phrases were processed is selected in block <highlight><bold>2224</bold></highlight>. A local model is then created in block <highlight><bold>2226</bold></highlight>. A local model is a contextual model of subsets of the provided relevant text from which the phrases were extracted and culled. All of the relevant text could be modeled in one embodiment. Alternatively, only a selected number of subsets of the provided relevant text that are also the most representative of the provided text are also modeled. One embodiment of a local model includes isolating distinct subsets from one another within the selected relevant text. Another embodiment of a local model includes inserting several non-term &ldquo;buffer terms&rdquo; between distinct subsets. A non-term buffer term includes a set of text designated as space filler. Another embodiment of a local model includes generating a vocabulary list that includes the terms that occur in the selected relevant text and the frequency of each term. </paragraph>
<paragraph id="P-0348" lvl="0"><number>&lsqb;0348&rsqb;</number> Next, a global model is selected in block <highlight><bold>2228</bold></highlight>. A global model can include a contextual model of the entire database or a single relational model of a number of subsets. A global model can also include a single relational model of a number of subsets wherein the number of subsets is greater than the number of subsets used to generate the local model. Alternatively, a global model can include a single relational model of a number of subsets wherein the subsets include the relevant text from which the selected phrases were extracted and culled. A global model can also include a single relational model of subsets wherein the subsets include text that is similar to the relevant text from which the selected phrases were extracted and culled. A global model can also include a number of relational models wherein each model represents one subset. A global model can also include creating a single relational model of a number of subsets by reducing the relations to unique relations. This process is similar to reducing the relations in a query described in keyterm search above, except reducing relations from all of the subset models, not just the subset relations matching a query. For another alternative embodiment a global model also includes limiting unique global model relations to only those relations having the same term pairs as relations in the local model. </paragraph>
<paragraph id="P-0349" lvl="0"><number>&lsqb;0349&rsqb;</number> A number of the phrases that were processed from the relevant text are selected in block <highlight><bold>2230</bold></highlight> and ranked in block <highlight><bold>2232</bold></highlight>. As one alternative, all phrases having a frequency in the relevant text greater than a pre-selected value are selected and ranked. The ranked phrases are then output in block <highlight><bold>2234</bold></highlight>. For an alternative, the output phrases and their corresponding ranking values are output. The output phrases can also be sorted. For one embodiment the output phrases are sorted in an order corresponding to their ranking values. </paragraph>
<paragraph id="P-0350" lvl="0"><number>&lsqb;0350&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 22B</cross-reference> illustrates one embodiment of ranking the selected phrases in block <highlight><bold>2232</bold></highlight>. First the locally relevant relations are emphasized and the globally relevant relations are de-emphasized in block <highlight><bold>2236</bold></highlight>. Next, the locally relevant phrases are emphasized and the globally relevant phrases are de-emphasized in block <highlight><bold>2238</bold></highlight>. </paragraph>
<paragraph id="P-0351" lvl="0"><number>&lsqb;0351&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 22C</cross-reference> illustrates one embodiment of a process of emphasizing the locally relevant relations and de-emphasizing the globally relevant relations in block <highlight><bold>2236</bold></highlight>. First, a first relation is selected in the local model in block <highlight><bold>2240</bold></highlight>. If there is not a relation in the global model having the same term pair as the selected relation in the local model in block <highlight><bold>2242</bold></highlight>, then processing continues at block <highlight><bold>2250</bold></highlight>, described below. If there is a relation in the global model having the same term pair as the selected relation in the local model in block <highlight><bold>2242</bold></highlight>, then the relation having the same term pair as the selected local relation is selected in the global model in block <highlight><bold>2244</bold></highlight>. Next, a new relation is included in a re-weighted model in block <highlight><bold>2246</bold></highlight>. The new relation includes the same term pair as the selected local relation, which is also the same term pair as the selected global relation. The metrics of the new relation are initialized to zero. For each of the types of metrics in the new relation, if the corresponding type of metric in the selected global relation is non-zero, then the corresponding type of metric of the new relation in the re-weighted model is set equal to the result of the corresponding type of metric in the selected relation from the local model divided by the corresponding type of metric in the selected relation from the global model in block <highlight><bold>2248</bold></highlight>. If there is a subsequent relation in the local model in block <highlight><bold>2250</bold></highlight>, that relation is selected in block <highlight><bold>2252</bold></highlight> and processing continues at block <highlight><bold>2242</bold></highlight>. The process continues in blocks <highlight><bold>2242</bold></highlight>-<highlight><bold>2252</bold></highlight> until all relations in the local model are processed. If there is no subsequent relation in the local model in block <highlight><bold>2250</bold></highlight>, then the re-weighted local model is output in block <highlight><bold>2254</bold></highlight>. For one alternative, each type of metric in each relation of the re-weighted model is multiplied by the frequency of the first term of the relation and the frequency of the second term of the relation, as represented in the vocabulary list obtained above with the local model from the selected relevant text. </paragraph>
<paragraph id="P-0352" lvl="0"><number>&lsqb;0352&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 22D</cross-reference> illustrates one embodiment of emphasizing the locally relevant phrases and de-emphasizing the globally relevant phrases in block <highlight><bold>2238</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 22B</cross-reference>. First the re-weighted model is selected in block <highlight><bold>2260</bold></highlight> and the processed phrases are selected in block <highlight><bold>2262</bold></highlight>. Alternatively, a weight could also be determined for each one of the processed phrases. The weight for each one of the processed phrases could also be set to a pre-selected value such as 1. A frequency of occurrence of the phrase within the selected relevant text could also be determined and used as the phrase weight. The selected phrases are then compared to the re-weighted model in block <highlight><bold>2264</bold></highlight>. The selected phrases are then ranked in order of relevance to the re-weighted model in block <highlight><bold>2266</bold></highlight>. The comparison in block <highlight><bold>2264</bold></highlight> can be a process similar to the comparison process in keyterm search described in <cross-reference target="DRAWINGS">FIG. 10</cross-reference> above. Thus, each phrase is modeled as a subset of the database, and the re-weighted model is used as a criterion model. The criterion model (that is, the re-weighted model) is compared with the subset models which represent the phrases to determine the degree of similarity of the criterion model and each of the phrase models. In addition, the ranking of the phrases in block <highlight><bold>2266</bold></highlight> can be done using the process of ranking subsets in keyterm search described above. Thus, the phrases are ranked on their degree of similarity to the re-weighted model. </paragraph>
<paragraph id="P-0353" lvl="0"><number>&lsqb;0353&rsqb;</number> The ranked phrases can also be scaled. For one embodiment the scaling for each one of the processed phrases includes multiplying the ranking value by a function of the phrase frequency. For one embodiment the scaling for each one of the processed phrases includes dividing the ranking value by the number of possible pair-wise, inter-term relations in the phrase. For one embodiment the scaling for each one of the processed phrases includes dividing the ranking value by a function of the largest ranking value. The ranked phrases are then output in block <highlight><bold>2268</bold></highlight>. The output phrases can also include the corresponding ranking value of each one of the ranked phrases. </paragraph>
<paragraph id="P-0354" lvl="0"><number>&lsqb;0354&rsqb;</number> This application is intended to cover any adaptations or variations of the present invention. For example, those of ordinary skill within the art will appreciate that the phrase discovery process can be executed in varying orders instead of being executed in the order as described above. </paragraph>
<paragraph id="P-0355" lvl="0"><number>&lsqb;0355&rsqb;</number> Phrase discovery scans narratives to find phrases that are related to topics of interest. This is very different from phrase generation, which uses phrase models to build likely phrases on a given term or phrase. In the example shown here, phrases related to &ldquo;fatigue&rdquo; are discovered. These include, for example: &ldquo;rest period&rdquo;, &ldquo;continuous duty&rdquo;, &ldquo;crew scheduling&rdquo;, &ldquo;reserve or standby&rdquo;, &ldquo;crew fatigue&rdquo;, and &ldquo;continuous duty overnight&rdquo;. Unlike generated phrases, discovered phrases are not required to contain any of the query terms. For this example, the phrase discovery process began with a keyterm search on the terms: &ldquo;fatigue&rdquo;, &ldquo;fatigued&rdquo;, &ldquo;fatiguing&rdquo;, &ldquo;tired&rdquo;, &ldquo;tiredness&rdquo;, &ldquo;sleep&rdquo;, &ldquo;asleep&rdquo;, &ldquo;sleeping&rdquo;, &ldquo;sleepy&rdquo;, and &ldquo;circadian&rdquo;. The particular forms of these terms were suggested by reviewing the vocabulary used in the narratives of the ASRS database. The phrase discovery process ultimately produced a collection of relevance-ranked narratives and a list of phrases that are topically related to &ldquo;fatigue&rdquo;. </paragraph>
<paragraph id="P-0356" lvl="0"><number>&lsqb;0356&rsqb;</number> The following Table 3.1 shows 50 of 420 phrases related to the topic of fatigue. The 420 phrases were extracted from three sets of 200 narratives that were found to be most relevant to the topic of fatigue. The frequency of each phrase within a set of 200 narratives is shown in the first column. This list shows, for example, that in the context of fatigue, &ldquo;rest period(s)&rdquo;, &ldquo;reduced rest&rdquo;, and &ldquo;crew rest&rdquo; are the most prominent concerns. Further, these are greater concerns than &ldquo;continuous duty&rdquo;, &ldquo;duty period&rdquo;, and &ldquo;crew duty&rdquo;. The list also shows that &ldquo;crew scheduling&rdquo; ranks high among the concerns of the reporters in the context of fatigue. Other prominent concerns include: &ldquo;reserve or standby&rdquo;, &ldquo;rest requirements&rdquo;, &ldquo;crew fatigue&rdquo;, &ldquo;continuous duty overnight(s)&rdquo;, &ldquo;adequate rest&rdquo;, &ldquo;minimum rest&rdquo;, &ldquo;required rest&rdquo;, &ldquo;plt fatigue&rdquo; (i.e., pilot fatigue), and &ldquo;compensatory rest&rdquo;. The prominence of these fatigue-related phrases parallels the prominence of these concerns in the industry.  
<table-cwu id="TABLE-US-00016">
<number>16</number>
<table frame="none" colsep="0" rowsep="0" pgwide="1">
<tgroup align="left" colsep="0" rowsep="0" cols="4">
<colspec colname="1" colwidth="21PT" align="center"/>
<colspec colname="2" colwidth="126PT" align="left"/>
<colspec colname="3" colwidth="21PT" align="center"/>
<colspec colname="4" colwidth="105PT" align="left"/>
<thead>
<row>
<entry namest="1" nameend="4" align="center">TABLE 3.1</entry>
</row>
<row>
<entry></entry>
</row>
<row><entry namest="1" nameend="4" align="center" rowsep="1"></entry>
</row>
<row>
<entry>Freq</entry>
<entry>phrase</entry>
<entry>freq</entry>
<entry>phrase</entry>
</row>
<row><entry namest="1" nameend="4" align="center" rowsep="1"></entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry>152&ensp;</entry>
<entry>REST PERIOD</entry>
<entry>12&ensp;</entry>
<entry>24 HR REST PERIOD</entry>
</row>
<row>
<entry>109&ensp;</entry>
<entry>REDUCED REST</entry>
<entry>12&ensp;</entry>
<entry>CREW SCHEDULER</entry>
</row>
<row>
<entry>79</entry>
<entry>CREW REST</entry>
<entry>12&ensp;</entry>
<entry>FELL ASLEEP</entry>
</row>
<row>
<entry>57</entry>
<entry>CONTINUOUS DUTY</entry>
<entry>12&ensp;</entry>
<entry>LACK OF SLEEP</entry>
</row>
<row>
<entry>46</entry>
<entry>CREW SCHEDULING</entry>
<entry>12&ensp;</entry>
<entry>SCHEDULING PRACTICES</entry>
</row>
<row>
<entry>37</entry>
<entry>DUTY PERIOD</entry>
<entry>11&ensp;</entry>
<entry>ENTIRE CREW</entry>
</row>
<row>
<entry>36</entry>
<entry>REST PERIODS</entry>
<entry>10&ensp;</entry>
<entry>FATIGUE AND STRESS</entry>
</row>
<row>
<entry>34</entry>
<entry>RESERVE OR STANDBY</entry>
<entry>10&ensp;</entry>
<entry>REDUCED REST OVERNIGHT</entry>
</row>
<row>
<entry>30</entry>
<entry>REST REQUIREMENTS</entry>
<entry>9</entry>
<entry>DUTY PERIODS</entry>
</row>
<row>
<entry>28</entry>
<entry>CREW FATIGUE</entry>
<entry>9</entry>
<entry>EARLY AM</entry>
</row>
<row>
<entry>22</entry>
<entry>CREW DUTY</entry>
<entry>9</entry>
<entry>FALL ASLEEP</entry>
</row>
<row>
<entry>20</entry>
<entry>CONTINUOUS DUTY OVERNIGHT</entry>
<entry>9</entry>
<entry>FIRST NIGHT</entry>
</row>
<row>
<entry>19</entry>
<entry>ADEQUATE REST</entry>
<entry>8</entry>
<entry>CIRCADIAN RHYTHMS</entry>
</row>
<row>
<entry>18</entry>
<entry>MINIMUM REST</entry>
<entry>8</entry>
<entry>NOT SLEEP</entry>
</row>
<row>
<entry>18</entry>
<entry>REQUIRED REST</entry>
<entry>8</entry>
<entry>PROPER REST</entry>
</row>
<row>
<entry>17</entry>
<entry>PLT FATIGUE</entry>
<entry>8</entry>
<entry>SCHEDULING DEPT</entry>
</row>
<row>
<entry>16</entry>
<entry>COMPENSATORY REST</entry>
<entry>8</entry>
<entry>SHORT REST</entry>
</row>
<row>
<entry>16</entry>
<entry>STANDBY STATUS</entry>
<entry>8</entry>
<entry>STANDBY PLT</entry>
</row>
<row>
<entry>15</entry>
<entry>REDUCED REST PERIOD</entry>
<entry>7</entry>
<entry>14 HR DUTY</entry>
</row>
<row>
<entry>15</entry>
<entry>SLEEP THE NIGHT</entry>
<entry>7</entry>
<entry>BODY CLOCK</entry>
</row>
<row>
<entry>13</entry>
<entry>CONTINUOUS DUTY OVERNIGHTS</entry>
<entry>7</entry>
<entry>CIRCADIAN RHYTHM</entry>
</row>
<row>
<entry>13</entry>
<entry>EARLY MORNING</entry>
<entry>7</entry>
<entry>CONTEXT OF REST PERIOD</entry>
</row>
<row>
<entry>13</entry>
<entry>LONG DUTY</entry>
<entry>7</entry>
<entry>DEFINITION OF DUTY</entry>
</row>
<row>
<entry>13</entry>
<entry>NIGHT&apos;S SLEEP</entry>
<entry>7</entry>
<entry>DUTY AND REST</entry>
</row>
<row>
<entry>13</entry>
<entry>RESERVE OR STANDBY STATUS</entry>
<entry>7</entry>
<entry>DUTY REGS</entry>
</row>
<row><entry namest="1" nameend="4" align="center" rowsep="1"></entry>
</row>
</tbody>
</tgroup>
</table>
</table-cwu>
</paragraph>
<paragraph id="P-0357" lvl="0"><number>&lsqb;0357&rsqb;</number> It is useful to subdivide the list of topical phrases into groups. One approach, shown below, is based on the prominence of terms in the phrases. To find the prominence of each term among all 420 of the fatigue-related phrases, the frequencies of the term groups containing each term were summed. The top 10 of 304 phrase terms are shown in the following Table 3.2. Table 3.2 shows, for example, that &ldquo;rest&rdquo; is the most prominent term among the phrases.  
<table-cwu id="TABLE-US-00017">
<number>17</number>
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="4">
<colspec colname="1" colwidth="56PT" align="center"/>
<colspec colname="2" colwidth="42PT" align="left"/>
<colspec colname="3" colwidth="49PT" align="center"/>
<colspec colname="4" colwidth="70PT" align="left"/>
<thead>
<row>
<entry namest="1" nameend="4" align="center">TABLE 3.2</entry>
</row>
<row>
<entry></entry>
</row>
<row><entry namest="1" nameend="4" align="center" rowsep="1"></entry>
</row>
<row>
<entry>Sum</entry>
<entry>phrase term</entry>
<entry>sum</entry>
<entry>phrase term</entry>
</row>
<row><entry namest="1" nameend="4" align="center" rowsep="1"></entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry>855</entry>
<entry>REST</entry>
<entry>151</entry>
<entry>FATIGUE</entry>
</row>
<row>
<entry>370</entry>
<entry>DUTY</entry>
<entry>147</entry>
<entry>SLEEP</entry>
</row>
<row>
<entry>304</entry>
<entry>PERIOD</entry>
<entry>135</entry>
<entry>SCHEDULING</entry>
</row>
<row>
<entry>291</entry>
<entry>CREW</entry>
<entry>109</entry>
<entry>NIGHT</entry>
</row>
<row>
<entry>163</entry>
<entry>REDUCED</entry>
<entry>102</entry>
<entry>RESERVE</entry>
</row>
<row><entry namest="1" nameend="4" align="center" rowsep="1"></entry>
</row>
</tbody>
</tgroup>
</table>
</table-cwu>
</paragraph>
<paragraph id="P-0358" lvl="0"><number>&lsqb;0358&rsqb;</number> These terms can be used to group the prominent fatigue-related phrases. For example, one can find all of the phrases containing the prominent term &ldquo;rest&rdquo;. Using this approach, the following 10 tables (Tables 3.3-3.12) show prominent subtopics within the fatigue-related narratives. The frequency of each phrase within 200 fatigue-related narratives is shown in the first column. The following groupings show, for example, that &ldquo;rest period&rdquo; and &ldquo;reduced rest&rdquo; are the most prominent &ldquo;rest&rdquo; phrases. Similarly, &ldquo;continuous duty&rdquo; and &ldquo;duty period&rdquo; are the most prominent &ldquo;duty&rdquo; phrases. Among &ldquo;period&rdquo; phrases, &ldquo;rest period&rdquo; is far more common than &ldquo;duty period&rdquo;, indicating that rest periods are a greater concern than duty periods among the sampled narratives.  
<table-cwu id="TABLE-US-00018">
<number>18</number>
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="1" colwidth="98PT" align="center"/>
<colspec colname="2" colwidth="119PT" align="left"/>
<thead>
<row>
<entry namest="1" nameend="2" align="center">TABLE 3.3</entry>
</row>
<row>
<entry></entry>
</row>
<row><entry namest="1" nameend="2" align="center" rowsep="1"></entry>
</row>
<row>
<entry>freq</entry>
<entry>REST phrases</entry>
</row>
<row><entry namest="1" nameend="2" align="center" rowsep="1"></entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry>152&ensp;</entry>
<entry>REST PERIOD</entry>
</row>
<row>
<entry>109&ensp;</entry>
<entry>REDUCED REST</entry>
</row>
<row>
<entry>79</entry>
<entry>CREW REST</entry>
</row>
<row>
<entry>36</entry>
<entry>REST PERIODS</entry>
</row>
<row>
<entry>30</entry>
<entry>REST REQUIREMENTS</entry>
</row>
<row>
<entry>19</entry>
<entry>ADEQUATE REST</entry>
</row>
<row>
<entry>18</entry>
<entry>MINIMUM REST</entry>
</row>
<row>
<entry>18</entry>
<entry>REQUIRED REST</entry>
</row>
<row>
<entry>16</entry>
<entry>COMPENSATORY REST</entry>
</row>
<row>
<entry>15</entry>
<entry>REDUCED REST PERIOD</entry>
</row>
<row><entry namest="1" nameend="2" align="center" rowsep="1"></entry>
</row>
</tbody>
</tgroup>
</table>
</table-cwu>
</paragraph>
<paragraph id="P-0359" lvl="0"><number>&lsqb;0359&rsqb;</number>  
<table-cwu id="TABLE-US-00019">
<number>19</number>
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="1" colwidth="70PT" align="center"/>
<colspec colname="2" colwidth="147PT" align="left"/>
<thead>
<row>
<entry namest="1" nameend="2" align="center">TABLE 3.4</entry>
</row>
<row>
<entry></entry>
</row>
<row><entry namest="1" nameend="2" align="center" rowsep="1"></entry>
</row>
<row>
<entry>freq</entry>
<entry>DUTY phrases</entry>
</row>
<row><entry namest="1" nameend="2" align="center" rowsep="1"></entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry>57</entry>
<entry>CONTINUOUS DUTY</entry>
</row>
<row>
<entry>37</entry>
<entry>DUTY PERIOD</entry>
</row>
<row>
<entry>22</entry>
<entry>CREW DUTY</entry>
</row>
<row>
<entry>20</entry>
<entry>CONTINUOUS DUTY OVERNIGHT</entry>
</row>
<row>
<entry>13</entry>
<entry>CONTINUOUS DUTY OVERNIGHTS</entry>
</row>
<row>
<entry>13</entry>
<entry>LONG DUTY</entry>
</row>
<row>
<entry>&ensp;9</entry>
<entry>DUTY PERIODS</entry>
</row>
<row>
<entry>&ensp;7</entry>
<entry>14 HR DUTY</entry>
</row>
<row>
<entry>&ensp;7</entry>
<entry>DEFINITION OF DUTY</entry>
</row>
<row>
<entry>&ensp;7</entry>
<entry>DUTY AND REST</entry>
</row>
<row><entry namest="1" nameend="2" align="center" rowsep="1"></entry>
</row>
</tbody>
</tgroup>
</table>
</table-cwu>
</paragraph>
<paragraph id="P-0360" lvl="0"><number>&lsqb;0360&rsqb;</number>  
<table-cwu id="TABLE-US-00020">
<number>20</number>
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="1" colwidth="84PT" align="center"/>
<colspec colname="2" colwidth="133PT" align="left"/>
<thead>
<row>
<entry namest="1" nameend="2" align="center">TABLE 3.5</entry>
</row>
<row>
<entry></entry>
</row>
<row><entry namest="1" nameend="2" align="center" rowsep="1"></entry>
</row>
<row>
<entry>freq</entry>
<entry>PERIOD phrases</entry>
</row>
<row><entry namest="1" nameend="2" align="center" rowsep="1"></entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry>152&ensp;</entry>
<entry>REST PERIOD</entry>
</row>
<row>
<entry>37</entry>
<entry>DUTY PERIOD</entry>
</row>
<row>
<entry>36</entry>
<entry>REST PERIODS</entry>
</row>
<row>
<entry>15</entry>
<entry>REDUCED REST PERIOD</entry>
</row>
<row>
<entry>12</entry>
<entry>24 HR REST PERIOD</entry>
</row>
<row>
<entry>&ensp;9</entry>
<entry>DUTY PERIODS</entry>
</row>
<row>
<entry>&ensp;7</entry>
<entry>CONTEXT OF REST PERIOD</entry>
</row>
<row>
<entry>&ensp;7</entry>
<entry>REQUIRED REST PERIOD</entry>
</row>
<row>
<entry>&ensp;7</entry>
<entry>REST PERIOD EXISTS</entry>
</row>
<row>
<entry>&ensp;7</entry>
<entry>SAID FOR REST PERIODS</entry>
</row>
<row><entry namest="1" nameend="2" align="center" rowsep="1"></entry>
</row>
</tbody>
</tgroup>
</table>
</table-cwu>
</paragraph>
<paragraph id="P-0361" lvl="0"><number>&lsqb;0361&rsqb;</number>  
<table-cwu id="TABLE-US-00021">
<number>21</number>
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="1" colwidth="70PT" align="center"/>
<colspec colname="2" colwidth="147PT" align="left"/>
<thead>
<row>
<entry namest="1" nameend="2" align="center">TABLE 3.6</entry>
</row>
<row>
<entry></entry>
</row>
<row><entry namest="1" nameend="2" align="center" rowsep="1"></entry>
</row>
<row>
<entry>freq</entry>
<entry>CREW phrases</entry>
</row>
<row><entry namest="1" nameend="2" align="center" rowsep="1"></entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry>79</entry>
<entry>CREW REST</entry>
</row>
<row>
<entry>46</entry>
<entry>CREW SCHEDULING</entry>
</row>
<row>
<entry>28</entry>
<entry>CREW FATIGUE</entry>
</row>
<row>
<entry>22</entry>
<entry>CREW DUTY</entry>
</row>
<row>
<entry>12</entry>
<entry>CREW SCHEDULER</entry>
</row>
<row>
<entry>11</entry>
<entry>ENTIRE CREW</entry>
</row>
<row>
<entry>&ensp;7</entry>
<entry>MINIMUM CREW REST</entry>
</row>
<row>
<entry>&ensp;5</entry>
<entry>14 HR CREW DUTY</entry>
</row>
<row>
<entry>&ensp;5</entry>
<entry>CALL FROM CREW SCHEDULING</entry>
</row>
<row>
<entry>&ensp;5</entry>
<entry>CALLED CREW SCHEDULING</entry>
</row>
<row><entry namest="1" nameend="2" align="center" rowsep="1"></entry>
</row>
</tbody>
</tgroup>
</table>
</table-cwu>
</paragraph>
<paragraph id="P-0362" lvl="0"><number>&lsqb;0362&rsqb;</number>  
<table-cwu id="TABLE-US-00022">
<number>22</number>
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="1" colwidth="70PT" align="center"/>
<colspec colname="2" colwidth="147PT" align="left"/>
<thead>
<row>
<entry namest="1" nameend="2" align="center">TABLE 3.7</entry>
</row>
<row>
<entry></entry>
</row>
<row><entry namest="1" nameend="2" align="center" rowsep="1"></entry>
</row>
<row>
<entry>freq</entry>
<entry>REDUCED phrases</entry>
</row>
<row><entry namest="1" nameend="2" align="center" rowsep="1"></entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry>109&emsp;</entry>
<entry>REDUCED REST</entry>
</row>
<row>
<entry>15&ensp;</entry>
<entry>REDUCED REST PERIOD</entry>
</row>
<row>
<entry>10&ensp;</entry>
<entry>REDUCED REST OVERNIGHT</entry>
</row>
<row>
<entry>7</entry>
<entry>SCHEDULED REDUCED REST</entry>
</row>
<row>
<entry>4</entry>
<entry>REDUCED REST PERIODS</entry>
</row>
<row>
<entry>3</entry>
<entry>REDUCED REST SCHEDULES</entry>
</row>
<row>
<entry>3</entry>
<entry>REDUCED REST TRIPS</entry>
</row>
<row>
<entry>2</entry>
<entry>BLOCK-TO-BLOCK REDUCED REST</entry>
</row>
<row>
<entry>2</entry>
<entry>BLOCK REDUCED REST</entry>
</row>
<row>
<entry>2</entry>
<entry>GIVEN A REDUCED REST PERIOD</entry>
</row>
<row><entry namest="1" nameend="2" align="center" rowsep="1"></entry>
</row>
</tbody>
</tgroup>
</table>
</table-cwu>
</paragraph>
<paragraph id="P-0363" lvl="0"><number>&lsqb;0363&rsqb;</number>  
<table-cwu id="TABLE-US-00023">
<number>23</number>
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="1" colwidth="56PT" align="center"/>
<colspec colname="2" colwidth="161PT" align="left"/>
<thead>
<row>
<entry namest="1" nameend="2" align="center">TABLE 3.8</entry>
</row>
<row>
<entry></entry>
</row>
<row><entry namest="1" nameend="2" align="center" rowsep="1"></entry>
</row>
<row>
<entry>freq</entry>
<entry>FATIGUE phrases</entry>
</row>
<row><entry namest="1" nameend="2" align="center" rowsep="1"></entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry>28&ensp;</entry>
<entry>CREW FATIGUE</entry>
</row>
<row>
<entry>17&ensp;</entry>
<entry>PLT FATIGUE</entry>
</row>
<row>
<entry>10&ensp;</entry>
<entry>FATIGUE AND STRESS</entry>
</row>
<row>
<entry>7</entry>
<entry>FATIGUE AND STRESS INDUCED FATIGUE</entry>
</row>
<row>
<entry>5</entry>
<entry>EXTREMELY FATIGUED</entry>
</row>
<row>
<entry>5</entry>
<entry>FATIGUE CAUSED</entry>
</row>
<row>
<entry>4</entry>
<entry>CAUSED BY PLT FATIGUE</entry>
</row>
<row>
<entry>4</entry>
<entry>CHRONIC FATIGUE</entry>
</row>
<row>
<entry>4</entry>
<entry>LEVEL OF FATIGUE</entry>
</row>
<row>
<entry>4</entry>
<entry>SIGNS OF FATIGUE</entry>
</row>
<row><entry namest="1" nameend="2" align="center" rowsep="1"></entry>
</row>
</tbody>
</tgroup>
</table>
</table-cwu>
</paragraph>
<paragraph id="P-0364" lvl="0"><number>&lsqb;0364&rsqb;</number>  
<table-cwu id="TABLE-US-00024">
<number>24</number>
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="1" colwidth="105PT" align="center"/>
<colspec colname="2" colwidth="112PT" align="left"/>
<thead>
<row>
<entry namest="1" nameend="2" align="center">TABLE 3.9</entry>
</row>
<row>
<entry></entry>
</row>
<row><entry namest="1" nameend="2" align="center" rowsep="1"></entry>
</row>
<row>
<entry>freq</entry>
<entry>SLEEP phrases</entry>
</row>
<row><entry namest="1" nameend="2" align="center" rowsep="1"></entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry>15&ensp;</entry>
<entry>SLEEP THE NIGHT</entry>
</row>
<row>
<entry>13&ensp;</entry>
<entry>NIGHT&apos;S SLEEP</entry>
</row>
<row>
<entry>12&ensp;</entry>
<entry>FELL ASLEEP</entry>
</row>
<row>
<entry>12&ensp;</entry>
<entry>LACK OF SLEEP</entry>
</row>
<row>
<entry>9</entry>
<entry>FALL ASLEEP</entry>
</row>
<row>
<entry>8</entry>
<entry>NOT SLEEP</entry>
</row>
<row>
<entry>7</entry>
<entry>SLEEP PATTERNS</entry>
</row>
<row>
<entry>6</entry>
<entry>FALLING ASLEEP</entry>
</row>
<row>
<entry>6</entry>
<entry>SLEEP PRIOR</entry>
</row>
<row>
<entry>5</entry>
<entry>ENOUGH SLEEP</entry>
</row>
<row><entry namest="1" nameend="2" align="center" rowsep="1"></entry>
</row>
</tbody>
</tgroup>
</table>
</table-cwu>
</paragraph>
<paragraph id="P-0365" lvl="0"><number>&lsqb;0365&rsqb;</number>  
<table-cwu id="TABLE-US-00025">
<number>25</number>
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="3">
<colspec colname="OFFSET" colwidth="28PT" align="left"/>
<colspec colname="1" colwidth="42PT" align="left"/>
<colspec colname="2" colwidth="147PT" align="left"/>
<thead>
<row>
<entry></entry>
<entry namest="OFFSET" nameend="2" align="center">TABLE 3.10</entry>
</row>
<row>
<entry></entry>
<entry></entry>
</row>
<row>
<entry></entry>
<entry namest="OFFSET" nameend="2" align="center" rowsep="1"></entry>
</row>
<row>
<entry></entry>
<entry>freq.</entry>
<entry>SCHEDULING phrases</entry>
</row>
<row>
<entry></entry>
<entry namest="OFFSET" nameend="2" align="center" rowsep="1"></entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry></entry>
<entry>46</entry>
<entry>CREW SCHEDULING</entry>
</row>
<row>
<entry></entry>
<entry>12</entry>
<entry>SCHEDULING PRACTICES</entry>
</row>
<row>
<entry></entry>
<entry>8</entry>
<entry>SCHEDULING DEPT</entry>
</row>
<row>
<entry></entry>
<entry>5</entry>
<entry>CALL FROM CREW SCHEDULING</entry>
</row>
<row>
<entry></entry>
<entry>5</entry>
<entry>CALLED CREW SCHEDULING</entry>
</row>
<row>
<entry></entry>
<entry>5</entry>
<entry>TYPE OF SCHEDULING</entry>
</row>
<row>
<entry></entry>
<entry>3</entry>
<entry>CALL SCHEDULING</entry>
</row>
<row>
<entry></entry>
<entry>3</entry>
<entry>CALLED SCHEDULING</entry>
</row>
<row>
<entry></entry>
<entry>3</entry>
<entry>SCHEDULING ASKED</entry>
</row>
<row>
<entry></entry>
<entry>3</entry>
<entry>SCHEDULING CALLED</entry>
</row>
<row>
<entry></entry>
<entry namest="OFFSET" nameend="2" align="center" rowsep="1"></entry>
</row>
</tbody>
</tgroup>
</table>
</table-cwu>
</paragraph>
<paragraph id="P-0366" lvl="0"><number>&lsqb;0366&rsqb;</number>  
<table-cwu id="TABLE-US-00026">
<number>26</number>
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="3">
<colspec colname="OFFSET" colwidth="28PT" align="left"/>
<colspec colname="1" colwidth="42PT" align="left"/>
<colspec colname="2" colwidth="147PT" align="left"/>
<thead>
<row>
<entry></entry>
<entry namest="OFFSET" nameend="2" align="center">TABLE 3.11</entry>
</row>
<row>
<entry></entry>
<entry></entry>
</row>
<row>
<entry></entry>
<entry namest="OFFSET" nameend="2" align="center" rowsep="1"></entry>
</row>
<row>
<entry></entry>
<entry>freq.</entry>
<entry>NIGHT phrases</entry>
</row>
<row>
<entry></entry>
<entry namest="OFFSET" nameend="2" align="center" rowsep="1"></entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry></entry>
<entry>20</entry>
<entry>CONTINUOUS DUTY OVERNIGHT</entry>
</row>
<row>
<entry></entry>
<entry>15</entry>
<entry>SLEEP THE NIGHT</entry>
</row>
<row>
<entry></entry>
<entry>13</entry>
<entry>CONTINUOUS DUTY OVERNIGHTS</entry>
</row>
<row>
<entry></entry>
<entry>13</entry>
<entry>NIGHT&apos;S SLEEP</entry>
</row>
<row>
<entry></entry>
<entry>10</entry>
<entry>REDUCED REST OVERNIGHT</entry>
</row>
<row>
<entry></entry>
<entry>9</entry>
<entry>FIRST NIGHT</entry>
</row>
<row>
<entry></entry>
<entry>7</entry>
<entry>LATE NIGHT</entry>
</row>
<row>
<entry></entry>
<entry>6</entry>
<entry>REST OVERNIGHT</entry>
</row>
<row>
<entry></entry>
<entry>4</entry>
<entry>REST THE NIGHT</entry>
</row>
<row>
<entry></entry>
<entry>3</entry>
<entry>LATE AT NIGHT</entry>
</row>
<row>
<entry></entry>
<entry namest="OFFSET" nameend="2" align="center" rowsep="1"></entry>
</row>
</tbody>
</tgroup>
</table>
</table-cwu>
</paragraph>
<paragraph id="P-0367" lvl="0"><number>&lsqb;0367&rsqb;</number>  
<table-cwu id="TABLE-US-00027">
<number>27</number>
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="3">
<colspec colname="OFFSET" colwidth="28PT" align="left"/>
<colspec colname="1" colwidth="42PT" align="left"/>
<colspec colname="2" colwidth="147PT" align="left"/>
<thead>
<row>
<entry></entry>
<entry namest="OFFSET" nameend="2" align="center">TABLE 3.12</entry>
</row>
<row>
<entry></entry>
<entry></entry>
</row>
<row>
<entry></entry>
<entry namest="OFFSET" nameend="2" align="center" rowsep="1"></entry>
</row>
<row>
<entry></entry>
<entry>freq.</entry>
<entry>RESERVE phrases</entry>
</row>
<row>
<entry></entry>
<entry namest="OFFSET" nameend="2" align="center" rowsep="1"></entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry></entry>
<entry>34</entry>
<entry>RESERVE OR STANDBY</entry>
</row>
<row>
<entry></entry>
<entry>13</entry>
<entry>RESERVE OR STANDBY STATUS</entry>
</row>
<row>
<entry></entry>
<entry>7</entry>
<entry>RESERVE&prime; OR&prime; STANDBY&prime; PLT</entry>
</row>
<row>
<entry></entry>
<entry>7</entry>
<entry>RESERVE OR STANDBY DUTY</entry>
</row>
<row>
<entry></entry>
<entry>7</entry>
<entry>RESERVE OR STANDBY PLT</entry>
</row>
<row>
<entry></entry>
<entry>6</entry>
<entry>RESERVE OR STANDBY FALLS</entry>
</row>
<row>
<entry></entry>
<entry>5</entry>
<entry>CONSISTENT INTERP OF RESERVE</entry>
</row>
<row>
<entry></entry>
<entry>4</entry>
<entry>RESERVE CREW</entry>
</row>
<row>
<entry></entry>
<entry>4</entry>
<entry>RESERVE PLT</entry>
</row>
<row>
<entry></entry>
<entry>3</entry>
<entry>AM A RESERVE CAPT</entry>
</row>
<row>
<entry></entry>
<entry namest="OFFSET" nameend="2" align="center" rowsep="1"></entry>
</row>
</tbody>
</tgroup>
</table>
</table-cwu>
</paragraph>
<paragraph id="P-0368" lvl="0"><number>&lsqb;0368&rsqb;</number> Two very useful by-products of the method used to produce the topically relevant phrases are a display of the most relevant narratives with their matching phrases highlighted, and a relevance-ranked list of the narratives that are relevant to the topic. The following is the most relevant narrative, in its entirety. Although it does not contain any form of the term &ldquo;fatigue&rdquo;, it does contain a diversity of fatigue-related topics. </paragraph>
<paragraph id="P-0369" lvl="1"><number>&lsqb;0369&rsqb;</number> I WORK FOR A LARGE REGIONAL/NATIONAL CARRIER AND CURRENTLY AM A RESERVE CAPT. OUR CURRENT WORKING AGREEMENT HAS VERY LITTLE IN THE WAY OF WORK RULES REGARDING SCHEDULING AND HRS OF SVC, AND THUS, WE ARE SCHEDULED AND FLOWN TO THE MAX ALLOWED BY THE FARS, WHICH WE ALL KNOW LEAVES MUCH TO BE DESIRED WITH THE REALITY OF OUR CIRCADIAN RHYTHMS. MANY PEOPLE THINK THAT CIRCADIAN RHYTHMS ONLY APPLY TO LONG HAUL INTL PLTS. HOWEVER, AFTER A NUMBER OF YRS AS BOTH A MIL AND COMMERCIAL CARRIER PLT I&apos;VE FOUND THAT EVERYONE&apos;S BODY NEEDS A ROUTINE, AND RADICAL CHANGES CAN ADVERSELY AFFECT ONE&apos;S PERF AND ABILITY TO GET ADEQUATE SLEEP DURING THE SUPPOSED REST PERIOD. OUR AIRLINE&apos;S SCHEDULING DEPT OPERATES UNDER CRISIS MGMNT DUE TO OUR MGMNT&apos;S &lsquo;STAFFING STRATEGY,&rsquo; AND THUS REQUIRES MANY RESERVE CREW MEMBERS TO COVER MORE THAN 1 SCHEDULED TRIP IN A CALENDAR DAY AND THUS WE HAVE A LARGE NUMBER OF &lsquo;SCHEDULED REDUCED REST PERIODS&rsquo; WHICH ARE 8 HRS, WHICH DOES NOT INCLUDE TRANSPORTATION LCL IN NATURE, WHICH, IN REALITY, REDUCES YOUR TIME AT A REST FACILITY WELL BELOW 8 HRS, PROVIDED YOU FALL TO SLEEP AS SOON AS YOU ARRIVE AT THE HOTEL. MY TRIP/RERTE FROM HELL STARTED AS A 3 DAY WITH AN 8 HR REST THE FIRST NIGHT WITH AN EARLY RPT. I HAPPENED TO BE COMING OFF A COUPLE OF NIGHT TRIPS AND THE EARLY MORNING RPT HAD ME A LITTLE OUT OF SYNC. WHEN WE ARRIVED AT OUR NEXT OVERNIGHT STATION, WHICH WE WERE SCHEDULED COMPENSATORY REST, I FELL ASLEEP EARLY NOT BEING ACCUSTOMED TO EARLYMORNING RPTS AND THUS WOKE VERY EARLY ON THE THE THIRD DAY. OUR DAY WAS SCHEDULED TO START AT 0450 AND END AT 1358 LCL. WHEN I WENT TO CHKOUT, CREW SCHEDULER INFORMED ME I HAD BEEN REROUTED AND I NOW HAD ADDITIONAL FLTS WITH ANOTHER OVERNIGHT AND MY DUTY DAY NOW WAS GOING TO BE 15:30, LEGAL BUT SAFE&quest; LATER, AS I WAITED TO MAKE THE LAST FLT TO THE OVERNIGHT STATION THEY HAD ME DO AN ADDITIONAL 2 LEGS, WHICH BROUGHT ME UP TO 8 LEGS. AFTER CHKING THE TRIP ON THE SCHEDULING COMPUTER, I FOUND THE SCHEDULER HAD CHANGED THE TRIP TO SHOW A COMBINATION OF ACTUAL TIME FLOWN, AND MARKETING TIMES TO MAKE THE TRIP LEGAL (I.E., UNDER 8 HRS SCHEDULED) AS OPPOSED TO USING THE HISTORIC BLOCK TIMES AS IS CALLED FOR BY BOTH OUR OPS MANUAL AND FAA POI. THE REMAINDER OF THE TRIP WAS MUCH THE SAME. THE FAA NEEDS TO RECOGNIZE THE IMPORTANCE OF QUALITY CREW REST AND IMPLEMENT GUIDELINES TO PREVENT SUCH SCHEDULING PRACTICES. ON THE THIRD AND FOURTH DAY, I WAS FAR FROM BEING AT PEAK PERF AND HAD THERE BEEN A SERIOUS EMER THE OUTCOME MAY HAVE BEEN QUESTIONABLE. THE FAA IS MANDATING MANY ITEMS TO ENHANCE SAFETY SUCH AS TCASII AND GPWS, HOWEVER, THEY SEEM TO FORGET THE MOST CRITICAL AND COMPLEX PIECE OF EQUIP ON THE ACFT: THE PLT&excl; (254345) </paragraph>
<paragraph id="P-0370" lvl="0"><number>&lsqb;0370&rsqb;</number> Numerous fatigue-related phrases are highlighted in this narrative, and most of these appear in the list of 420 fatigue-related phrases produced by phrase discovery. Some phrases that are not on the list are also highlighted. The phrase &ldquo;scheduled compensatory rest&rdquo;, for example, is highlighted because the phrases &ldquo;scheduled rest&rdquo; and &ldquo;compensatory rest&rdquo; are on the list. This approach aids the user in recognizing compound topical phrases in the narratives. The following Table 3.13 shows the accession numbers of the 100 narratives that are most relevant to the fatigue-related phrases. The more relevant narratives appear closer to the top of the list.  
<table-cwu id="TABLE-US-00028">
<number>28</number>
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="5">
<colspec colname="OFFSET" colwidth="14PT" align="left"/>
<colspec colname="1" colwidth="49PT" align="left"/>
<colspec colname="2" colwidth="49PT" align="left"/>
<colspec colname="3" colwidth="49PT" align="left"/>
<colspec colname="4" colwidth="56PT" align="left"/>
<thead>
<row>
<entry></entry>
<entry namest="OFFSET" nameend="4" align="center">TABLE 3.13</entry>
</row>
<row>
<entry></entry>
<entry></entry>
</row>
<row>
<entry></entry>
<entry namest="OFFSET" nameend="4" align="center" rowsep="1"></entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry></entry>
<entry>&ensp;1. 254345</entry>
<entry>26. 340923</entry>
<entry>51. 244901</entry>
<entry>&ensp;76. 190632</entry>
</row>
<row>
<entry></entry>
<entry>&ensp;2. 288683</entry>
<entry>27. 256799</entry>
<entry>52. 80148</entry>
<entry>&ensp;77. 96789</entry>
</row>
<row>
<entry></entry>
<entry>&ensp;3.288893</entry>
<entry>28. 261075</entry>
<entry>53. 307314</entry>
<entry>&ensp;78. 358723</entry>
</row>
<row>
<entry></entry>
<entry>&ensp;4. 288846</entry>
<entry>29. 123541</entry>
<entry>54. 118537</entry>
<entry>&ensp;79. 147013</entry>
</row>
<row>
<entry></entry>
<entry>&ensp;5. 317360</entry>
<entry>30. 206207</entry>
<entry>55. 302099</entry>
<entry>&ensp;80. 298219</entry>
</row>
<row>
<entry></entry>
<entry>&ensp;6. 344664</entry>
<entry>31. 193131</entry>
<entry>56. 245026</entry>
<entry>&ensp;81. 302300</entry>
</row>
<row>
<entry></entry>
<entry>&ensp;7. 295352</entry>
<entry>32. 276356</entry>
<entry>57. 294430</entry>
<entry>&ensp;82. 223012</entry>
</row>
<row>
<entry></entry>
<entry>&ensp;8. 289770</entry>
<entry>33. 367856</entry>
<entry>58. 281395</entry>
<entry>&ensp;83. 172229</entry>
</row>
<row>
<entry></entry>
<entry>&ensp;9. 290921</entry>
<entry>34. 254267</entry>
<entry>59. 142582</entry>
<entry>&ensp;84. 368250</entry>
</row>
<row>
<entry></entry>
<entry>10. 299489</entry>
<entry>35. 294130</entry>
<entry>60. 270256</entry>
<entry>&ensp;85. 206269</entry>
</row>
<row>
<entry></entry>
<entry>11. 362160</entry>
<entry>36. 309408</entry>
<entry>61. 364640</entry>
<entry>&ensp;86. 375952</entry>
</row>
<row>
<entry></entry>
<entry>12. 188837</entry>
<entry>37. 82286</entry>
<entry>62. 146711</entry>
<entry>&ensp;87. 134612</entry>
</row>
<row>
<entry></entry>
<entry>13. 96242</entry>
<entry>38. 145545</entry>
<entry>63. 140005</entry>
<entry>&ensp;88. 280233</entry>
</row>
<row>
<entry></entry>
<entry>14. 277949</entry>
<entry>39. 311602</entry>
<entry>64. 337600</entry>
<entry>&ensp;89. 373770</entry>
</row>
<row>
<entry></entry>
<entry>15. 233057</entry>
<entry>40. 296275</entry>
<entry>65. 258759</entry>
<entry>&ensp;90. 185044</entry>
</row>
<row>
<entry></entry>
<entry>16. 255852</entry>
<entry>41. 205528</entry>
<entry>66. 246248</entry>
<entry>&ensp;91. 261246</entry>
</row>
<row>
<entry></entry>
<entry>17. 297614</entry>
<entry>42. 319125</entry>
<entry>67. 206734</entry>
<entry>&ensp;92. 123033</entry>
</row>
<row>
<entry></entry>
<entry>18. 281704</entry>
<entry>43. 262904</entry>
<entry>68. 254490</entry>
<entry>&ensp;93. 360420</entry>
</row>
<row>
<entry></entry>
<entry>19. 257793</entry>
<entry>44. 367822</entry>
<entry>69. 275586</entry>
<entry>&ensp;94. 345560</entry>
</row>
<row>
<entry></entry>
<entry>20. 219810</entry>
<entry>45. 314510</entry>
<entry>70. 102754</entry>
<entry>&ensp;95. 189506</entry>
</row>
<row>
<entry></entry>
<entry>21. 360800</entry>
<entry>46. 164061</entry>
<entry>71. 218676</entry>
<entry>&ensp;96. 108189</entry>
</row>
<row>
<entry></entry>
<entry>22. 96245</entry>
<entry>47. 184813</entry>
<entry>72. 123335</entry>
<entry>&ensp;97. 356959</entry>
</row>
<row>
<entry></entry>
<entry>23. 273938</entry>
<entry>48. 348901</entry>
<entry>73. 168334</entry>
<entry>&ensp;98. 306800</entry>
</row>
<row>
<entry></entry>
<entry>24. 245003</entry>
<entry>49. 176651</entry>
<entry>74. 301360</entry>
<entry>&ensp;99. 270930</entry>
</row>
<row>
<entry></entry>
<entry>25. 324660</entry>
<entry>50. 143879</entry>
<entry>75. 112090</entry>
<entry>100. 151142</entry>
</row>
<row>
<entry></entry>
<entry namest="OFFSET" nameend="4" align="center" rowsep="1"></entry>
</row>
</tbody>
</tgroup>
</table>
</table-cwu>
</paragraph>
<paragraph id="P-0371" lvl="0"><number>&lsqb;0371&rsqb;</number> This example shows that phrase discovery is useful for finding topically related phrases and narratives that do not necessarily contain the original query terms or phrases. </paragraph>
<paragraph id="P-0372" lvl="0"><number>&lsqb;0372&rsqb;</number> Phrase discovery is somewhat similar to the so-called &ldquo;natural language processing&rdquo; (NLP) methods such as (Godby, 1994); (Jing and Croft, 1994); (Gutwin, Paynter, Witten, Nevill-Manning, and Frank, 1998); (de Lima and Pedersen, 1999); and (Jones and Staveley, 1999), of phrase-finding in that phrase discovery classifies words and requires that candidate word sequences match particular patterns. Most methods, such as (Godby, 1994); (Jing and Croft, 1994); (Gutwin, Paynter, Witten, Nevill-Manning, and Frank, 1998); (de Lima and Pedersen, 1999); and (Jones and Staveley, 1999), however, classify words by part of speech using grammatical taggers and apply a grammar-based set of allowable patterns. These methods typically remove all punctuation and stopterms as a preliminary step, and most then discover only simple or compound nouns leaving all other phrases unrecognizable. In contrast, phrase discovery described herein uses the full text, and applies a simple classification scheme where one categorical distinction is between stopterms and non-stopterms. When phrase discovery is applied to text, stopterms can include punctuation and conventional stopterms. In addition, phrase discovery uses a simple, procedurally defined set of acceptable patterns that can require phrases to begin and end with non-stopterms, can limit the number of interior stopterms, and can allow the &ldquo;-&rdquo; (dash) character to be an interior term. </paragraph>
<paragraph id="P-0373" lvl="0"><number>&lsqb;0373&rsqb;</number> Like Keyphind and Phrasier of Gutwin, Paynter, Witten, Nevill-Manning, and Frank (1998) and Jones and Staveley (1999), phrase discovery described herein identifies phrases in sets of documents. In contrast to Keyphind and Phrasier, however, phrase discovery requires no grammatical tagging, no training phrases, no manual categorization of phrases, and no pre-existing lists of identifiable phrases. Further, phrase discovery identifies a far greater number of the phrases that occur within sets of documents because its method of phrase identification is more powerful. The larger number of phrases identified by phrase discovery also provides much more information for determining the degree of relevance of each document containing one or more of the phrases. </paragraph>
</section>
<section>
<heading lvl="1">OTHER APPLICATIONS </heading>
<paragraph id="P-0374" lvl="0"><number>&lsqb;0374&rsqb;</number> The above described methods and processes of keyterm search, phrase search, phrase generation and phrase discovery have been described and illustrated in terms of information retrieval (IR) embodiments. In IR: terms are symbols or elements of a data set, subsets are collections of symbols, databases are collections of subsets, each relation is binary and links a symbol pair, and quantification of relations is based on contextual associations of symbols within subsets. Further, models are collections of symbol relations, the models can be aggregated, the models can represent subsets, databases, and queries, models can be ranked on similarity to other models, and sequentially grouped terms are derived from models and subsets. </paragraph>
<paragraph id="P-0375" lvl="0"><number>&lsqb;0375&rsqb;</number> It is important to recognize that there are other &ldquo;real world&rdquo; embodiments of these concepts. These real world embodiments are derived from the fact that terms are not limited to being symbols, but can also refer to, or be, entities in the real world, such as people, objects, concepts, actions, attributes, and values. In contrast to the IR embodiment in which symbol collections are subsets, one real-world embodiment includes entity collections such as: occurrences, events, incidents, episodes, circumstances, domains, situations, environments, and objects. Further, any entity collection can be treated as an entity, and any entity can be further elaborated as an entity collection, depending on the observed or desired level of detail. While databases define the total scope of subset collections in the IR embodiments, domains define the total scope of entity collections (e.g., situations) in a real-world embodiment. </paragraph>
<paragraph id="P-0376" lvl="0"><number>&lsqb;0376&rsqb;</number> As with term pair relations in the IR embodiment, quantification of entity pair relations in the real world can also be based on contextual associations. In the real world, the scope of that context is space, time, causality, and thought. Thus, the notion of context is not limited to proximity relations among symbols within a subset. Instead, real-world context is a much broader concept, one that is more fully represented by the term &ldquo;metonymy&rdquo; in the sense developed by Roman Jakobson (Jakobson, R.: &ldquo;Two aspects of language and two types of aphasic disturbances&rdquo; (1956), (pp. 95-114) and &ldquo;Marginal notes on the prose of the poet Pasternak&rdquo; (1935), (pp. 301-317), in K. Pomorska and S. Rudy (Eds.), Language in Literature. Belknap Press, Cambridge, Mass., 1987). Jakobson asserted that the interpretation of a symbol or entity is derived from both its similarity to others and its contextual association with others. Thus, the contextual meaning of a symbol or entity is determined by its connections with others in the same context, that is, by its metonymic relations with others. This notion of metonymy, of contextual meaning, is a fundamental structural component of narrative text, symbol systems, and human behavior, according to Jakobson. </paragraph>
<paragraph id="P-0377" lvl="0"><number>&lsqb;0377&rsqb;</number> This conception of contextual meaning, combined with the fact that symbols typically refer to real-world entities (as when the word &ldquo;autopilot&rdquo; in a narrative refers to the actual system in the real world), suggests that the contextual relations within symbol structures (e.g., narrative text) refer to the metonymic relations within entity structures in the real world (e.g., the situation described in the narrative text). As a practical example, since narrative text is based on real-world situations, the structural relationships among the symbols in narrative text must ultimately be based on the structural relationships among the entities found in the real-world situations described in the narratives. </paragraph>
<paragraph id="P-0378" lvl="0"><number>&lsqb;0378&rsqb;</number> Given a correspondence between entities and symbols, and a correspondence between their metonymic relations, measurements of metonymy within a symbol structure correspond to measurements of metonymy within a corresponding entity structure. For example, a real-world situation can be implicitly or explicitly modeled by an observer and then mapped to an arrangement of words in a narrative describing the situation. The words are symbols corresponding to entities in the situations. Due to the structure of narratives, which maps situational meaning to narrative meaning, contextually related entities tend to be mapped to contextually related words. Consequently, the contextual associations within the narrative (i.e., the symbol structure) created by the observer can be measured as a means of measuring the structure of the situation in the real world. Thus, a metonymic model of the narrative structure is a metonymic model of the structure of the corresponding real-world situation. Similarly, a musical inspiration or experience can be mapped to musical notation that can subsequently be mapped to a metonymic model of the musical inspiration or experience itself. Further, genetic or protein sequences can be represented as symbols that can subsequently be mapped to a metonymic model of the physical entities themselves. </paragraph>
<paragraph id="P-0379" lvl="0"><number>&lsqb;0379&rsqb;</number> It is also possible to directly model entity structures (e.g., situations or environments) without using the intermediary of a narrative or comparable representation. This can be accomplished by identifying each pair of entities and quantifying their degree of metonymic association. The results can be greatly simplified by considering only the most closely associated entity pairs, which can still produce a useful model (Simon, H. A.: The Sciences of the Artificial. MIT Press, Cambridge, Mass., 1969). Further simplification can be achieved by including only those entity pairs that are of particular concern to the observer, such as the essential details of an incident. The resulting models can be structured exactly as are the models of subsets, that is, as collections of relations consisting of pair-wise associations of terms, each quantified by the degree of their metonymic association. The models can then be aggregated, compared, sorted, and otherwise manipulated in a manner similar to those applied to models derived from the IR embodiments described above. </paragraph>
<paragraph id="P-0380" lvl="0"><number>&lsqb;0380&rsqb;</number> Since observers filter observations through their concerns, every model derived directly or indirectly from observations is subjective. Subjective models of entity structures can be called &ldquo;individual situated models&rdquo;, &ldquo;individual domain models&rdquo;, or can in some comparable way be named as a function of the scope of the model. In all cases, these models can be considered to be models of presence, since they represent the pattern and degree of engagement of the observer with a particular subset of the real world. In addition, these models represent the observer&apos;s interpretation of the presence of each entity within the entity structure, since the engagement of each entity with the other entities is also represented in the model. </paragraph>
<paragraph id="P-0381" lvl="0"><number>&lsqb;0381&rsqb;</number> In order to approximate an objective model, multiple models can be aggregated. Aggregations can represent, for example, multiple views of a single situation or multiple views of a single class of situations. Similarly, a large aggregation of situational models approximates a model of the domain of the situations, just as a large aggregation of subset models approximates a model of a database of subsets. Further, a model that aggregates many individual models of presence approximates an objective model of presence. </paragraph>
<paragraph id="P-0382" lvl="0"><number>&lsqb;0382&rsqb;</number> In a real-world embodiment, a query to &ldquo;entity structure search&rdquo; (analogous to one IR embodiment of phrase search) might be a current situation, and the database of situations to be searched could be the set of situations previously encountered. In this sense, a query to find the most similar situations is comparable to recalling prior relevant experience to guide current understanding and action. Similarly, a query might be based on a story told by a person in a conversation. The model of that story could be used to find similar stories known by another person in the conversation. Elements of one or more of these similar stories could form part of the reply of the person hearing the first story. </paragraph>
<paragraph id="P-0383" lvl="0"><number>&lsqb;0383&rsqb;</number> At a finer resolution, a set of phrases in the IR embodiment is analogous to a set of sequentially related entities in the real-world embodiment. Examples of such related entities include, for example: sequences of actions, chains of circumstances, and sequences of causes and effects. A search using sequences like these as a query to &ldquo;entity sequence discovery&rdquo; (analogous to phrase discovery) consists of finding related sequences of actions, related chains of circumstances, or related sequences of causes and effects. Applying the queries to &ldquo;entity sequence search&rdquo; (analogous to one IR embodiment of phrase search) or &ldquo;entity sequence discovery&rdquo; (analogous to phrase discovery) could also find the broader contexts of the observed sequences among the situations or other entity structures of prior experience, as well as related sequences within those contexts. This embodiment is also like having a snippet of a song remind one of other lines of the song or of circumstances in which that song was particularly salient. </paragraph>
<paragraph id="P-0384" lvl="0"><number>&lsqb;0384&rsqb;</number> A collection of individual entities observed in the real world could serve as a query to a real-world form of &ldquo;key-entity search&rdquo; (analogous to keyterm search), and that search could find previously encountered entity structures containing some or all of the observed entities in their most typical or salient contexts. This is another form of reminding based on contextual memory, where that memory is embodied as an ability to search a collection of contextual models. </paragraph>
<paragraph id="P-0385" lvl="0"><number>&lsqb;0385&rsqb;</number> Similarly, one or more entities can be a query to &ldquo;entity sequence generation&rdquo; (analogous to phrase generation) in order to find entity sequences that are prominent in contextual memory and contain one or more entities from the query. An example of this is placing an observed event into previously observed sequences of events of particular significance. </paragraph>
<paragraph id="P-0386" lvl="0"><number>&lsqb;0386&rsqb;</number> In summary, the formal structures of terms, relations, metrics, models, and model manipulations apply equally well to information retrieval (IR) embodiments and to real-world embodiments. Further, formal structures in an IR embodiment can correspond to, and represent, those in a real-world embodiment, and vice versa. </paragraph>
<paragraph id="P-0387" lvl="0"><number>&lsqb;0387&rsqb;</number> This correspondence allows these embodiments to be very useful in the design of software and systems based on models of real-world domains, situations, environments, etc., by enabling the real-world models readily to map to computer-based models, such as those used in the information retrieval embodiment. This ease of mapping directly supports methods such as object-oriented analysis, modeling, and design, and allows any combination of real-world and symbolic analysis and modeling to contribute to the ultimate design of software and systems. For example, document analysis and modeling can be used to guide fieldwork in the real world so as to refine, extend, and validate the models, leading to the final design. All computer software and system design intended to support real-world activities can benefit from use of these techniques, including not only the design of traditional applications, but also the design of virtual reality software and systems. Using the methods described, the analysis and modeling of application domains, situations, and environments can be based on collections of symbols such as documents as well as real-world entities such as people at work in their everyday working environments, and the results can then be directly mapped to computable representations. </paragraph>
<paragraph id="P-0388" lvl="0"><number>&lsqb;0388&rsqb;</number> It is also contemplated that the various embodiments described above can also be practiced in the context of a computer system, computer software, computer hardware and combinations thereof. <cross-reference target="DRAWINGS">FIG. 23</cross-reference> shows a high-level block diagram of a computer system upon which the above described embodiments may be executed in the form of computer software and hardware. As shown, the computer system <highlight><bold>2300</bold></highlight> includes a processor <highlight><bold>2302</bold></highlight>, ROM <highlight><bold>2304</bold></highlight>, and RAM <highlight><bold>2306</bold></highlight>, each connected to a bus system <highlight><bold>2308</bold></highlight>. The bus system <highlight><bold>2308</bold></highlight> may include one or more buses connected to each other through various bridges, controllers and/or adapters, such as are well known in the art. For example, the bus system <highlight><bold>2308</bold></highlight> may include a &ldquo;system bus&rdquo; that is connected through an adapter to one or more expansion buses, such as a Peripheral Component Interconnect (PCI) bus. Also coupled to the bus system <highlight><bold>2308</bold></highlight> are a mass storage device <highlight><bold>2310</bold></highlight>, a network interface <highlight><bold>2312</bold></highlight>, and a number (N) of input/output (I/O) devices <highlight><bold>2316</bold></highlight>-<highlight><bold>1</bold></highlight> through <highlight><bold>2316</bold></highlight>-N. </paragraph>
<paragraph id="P-0389" lvl="0"><number>&lsqb;0389&rsqb;</number> I/O devices <highlight><bold>2316</bold></highlight>-<highlight><bold>1</bold></highlight> through <highlight><bold>2316</bold></highlight>-N may include, for example, a keyboard, a pointing device, a display device and/or other conventional I/O devices. Mass storage device <highlight><bold>2310</bold></highlight> may include any suitable device for storing large volumes of data, such as a magnetic disk or tape, magneto-optical (MO) storage device, or any of various types of Digital Versatile Disk (DVD) or Compact Disk (CD) based storage. </paragraph>
<paragraph id="P-0390" lvl="0"><number>&lsqb;0390&rsqb;</number> Network interface <highlight><bold>2312</bold></highlight> provides data communication between the computer system and other computer systems on a network. Hence, network interface <highlight><bold>2312</bold></highlight> may be any device suitable for or enabling the computer system <highlight><bold>2300</bold></highlight> to communicate data with a remote processing system over a data communication link, such as a conventional telephone modem, an Integrated Services Digital Network (ISDN) adapter, a Digital Subscriber Line (DSL) adapter, a cable modem, a satellite transceiver, an Ethernet adapter, or the like. </paragraph>
<paragraph id="P-0391" lvl="0"><number>&lsqb;0391&rsqb;</number> Of course, many variations upon the architecture shown in <cross-reference target="DRAWINGS">FIG. 23</cross-reference> can be made to suit the particular needs of a given system. Thus, certain components may be added to that shown in <cross-reference target="DRAWINGS">FIG. 23</cross-reference> for given system, or certain components shown in <cross-reference target="DRAWINGS">FIG. 23</cross-reference> may be omitted from the given system. </paragraph>
<paragraph id="P-0392" lvl="0"><number>&lsqb;0392&rsqb;</number> Given this description, one skilled in the art will readily appreciate that the described techniques can be practiced with other computer system configurations, including multiprocessor systems, minicomputers, mainframe computers, and the like. It will also be appreciated that any of a variety of programming languages may be used to implement the embodiments as described herein. Furthermore, it is common in the art to speak of software, in one form or another (e.g., program, procedure, process, application, module, logic . . . ), as taking an action or causing a result. Such expressions are merely a shorthand way of saying that execution of the software by a computer causes the processor of the computer to perform an action or produce a result. It will also be appreciated that the above-described processes can be implemented in software or hardwired in a computer system or combinations thereof. Therefore, the description of any of the embodiments described herein is not limited to any particular combination of hardware and/or software. </paragraph>
<paragraph id="P-0393" lvl="0"><number>&lsqb;0393&rsqb;</number> In the foregoing specification, the invention has been described with reference to specific exemplary embodiments thereof. It will be evident that various modifications may be made thereto without departing from the broader spirit and scope of the invention as set forth in the following claims. The specification and drawings are, accordingly, to be regarded in an illustrative sense rather than a restrictive sense. Further the use of section headings is not to be construed as being limiting in any manner but rather to ease the organization and understanding of an otherwise complex presentation of information. </paragraph>
</section>
</detailed-description>
</subdoc-description>
<subdoc-claims>
<heading lvl="1">What is claimed is: </heading>
<claim id="CLM-00001">
<claim-text><highlight><bold>1</bold></highlight>. A method of searching a database comprising: 
<claim-text>providing a plurality of relational models, each one of the plurality of relational models includes one relational model of one subset of a database; </claim-text>
<claim-text>inputting a first query; </claim-text>
<claim-text>creating a relational model of the first query, wherein the relational model of the query includes a plurality of relations, each one of the plurality of relations has a first query term pair and a first query plurality of types of relational summation metrics (RSMs); </claim-text>
<claim-text>comparing the relational model of the first query to each one of the plurality of relational models of the subsets; and </claim-text>
<claim-text>outputting a first plurality of identifiers of the subsets relevant to the first query. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00002">
<claim-text><highlight><bold>2</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference> wherein, each one of the plurality of relational models of the subsets of the database include a plurality of relations. </claim-text>
</claim>
<claim id="CLM-00003">
<claim-text><highlight><bold>3</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00002">claim 2</dependent-claim-reference> wherein, each one of the plurality of relations has a subset term pair and a subset plurality of types of RSMs. </claim-text>
</claim>
<claim id="CLM-00004">
<claim-text><highlight><bold>4</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00003">claim 3</dependent-claim-reference> wherein, an order of the plurality of types of RSMs corresponds to an order of the term pair. </claim-text>
</claim>
<claim id="CLM-00005">
<claim-text><highlight><bold>5</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00003">claim 3</dependent-claim-reference> further comprising, providing a plurality of stopterms. </claim-text>
</claim>
<claim id="CLM-00006">
<claim-text><highlight><bold>6</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00005">claim 5</dependent-claim-reference> wherein, if either a first term in the subset term pair or a second term in the subset term pair is one of the plurality of stopterms then the relation is not included in the relational model of the subset. </claim-text>
</claim>
<claim id="CLM-00007">
<claim-text><highlight><bold>7</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00005">claim 5</dependent-claim-reference>, wherein, if a first stopterm is included in the first query, then the first stopterm is no longer included in the plurality of stopterms. </claim-text>
</claim>
<claim id="CLM-00008">
<claim-text><highlight><bold>8</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00005">claim 5</dependent-claim-reference>, wherein, if a first stopterm is included in the first query, then the plurality of stopterms no longer includes any stopterms. </claim-text>
</claim>
<claim id="CLM-00009">
<claim-text><highlight><bold>9</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00003">claim 3</dependent-claim-reference> wherein, each one of the subset plurality of types of RSMs includes a summation of the corresponding types of relational metrics of each one of a plurality of occurrences of the subset term pair within a context window within the subset. </claim-text>
</claim>
<claim id="CLM-00010">
<claim-text><highlight><bold>10</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00009">claim 9</dependent-claim-reference> wherein, the types of RSMs include a non-directional contextual metric (NDCM). </claim-text>
</claim>
<claim id="CLM-00011">
<claim-text><highlight><bold>11</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00011">claim 10</dependent-claim-reference> wherein: 
<claim-text>the value of the NDCM for a single occurrence of a term pair (T<highlight><bold>1</bold></highlight>, T<highlight><bold>2</bold></highlight>) in the subset is NDCM(T<highlight><bold>1</bold></highlight>, T<highlight><bold>2</bold></highlight>)&equals;C&minus;1&minus;N, wherein: 
<claim-text>T<highlight><bold>1</bold></highlight> is a first term in the term pair; </claim-text>
<claim-text>T<highlight><bold>2</bold></highlight> is a second term in the term pair; </claim-text>
<claim-text>C is equal to a number of terms in the context window; and </claim-text>
<claim-text>N is equal to a number of terms occurring between T<highlight><bold>1</bold></highlight> and T<highlight><bold>2</bold></highlight>. </claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00012">
<claim-text><highlight><bold>12</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00009">claim 9</dependent-claim-reference> wherein, the types of relational metrics include a right contextual metric (RCM). </claim-text>
</claim>
<claim id="CLM-00013">
<claim-text><highlight><bold>13</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00011">claim 12</dependent-claim-reference> wherein: 
<claim-text>the RCM for a single occurrence of a term pair (T<highlight><bold>1</bold></highlight>, T<highlight><bold>2</bold></highlight>) in the subset is RCM(T<highlight><bold>1</bold></highlight>, T<highlight><bold>2</bold></highlight>), wherein: 
<claim-text>T<highlight><bold>1</bold></highlight> is a first term in the term pair; </claim-text>
<claim-text>T<highlight><bold>2</bold></highlight> is a second term in the term pair; </claim-text>
<claim-text>RCM(T<highlight><bold>1</bold></highlight>, T<highlight><bold>2</bold></highlight>)&equals;0, if T<highlight><bold>2</bold></highlight> precedes T<highlight><bold>1</bold></highlight>; and </claim-text>
<claim-text>RCM(T<highlight><bold>1</bold></highlight>, T<highlight><bold>2</bold></highlight>)&equals;C&minus;1&minus;N, if T<highlight><bold>1</bold></highlight> precedes T<highlight><bold>2</bold></highlight>, wherein 
<claim-text>C is equal to a number of terms in the context window; and </claim-text>
<claim-text>N is equal to a number of terms occurring between T<highlight><bold>1</bold></highlight> and T<highlight><bold>2</bold></highlight>. </claim-text>
</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00014">
<claim-text><highlight><bold>14</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00009">claim 9</dependent-claim-reference> wherein, the types of relational metrics include a left contextual metric (LCM). </claim-text>
</claim>
<claim id="CLM-00015">
<claim-text><highlight><bold>15</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00011">claim 14</dependent-claim-reference> wherein: 
<claim-text>the LCM for a single occurrence of a term pair (T<highlight><bold>1</bold></highlight>, T<highlight><bold>2</bold></highlight>) in the subset is LCM(T<highlight><bold>1</bold></highlight>, T<highlight><bold>2</bold></highlight>), wherein: 
<claim-text>T<highlight><bold>1</bold></highlight> is a first term in the term pair; </claim-text>
<claim-text>T<highlight><bold>2</bold></highlight> is a second term in the term pair; </claim-text>
<claim-text>LCM(T<highlight><bold>1</bold></highlight>, T<highlight><bold>2</bold></highlight>)&equals;0, if T<highlight><bold>2</bold></highlight> precedes T<highlight><bold>1</bold></highlight>; and </claim-text>
<claim-text>LCM(T<highlight><bold>1</bold></highlight>, T<highlight><bold>2</bold></highlight>)&equals;C&minus;1&minus;N, if T<highlight><bold>1</bold></highlight> precedes T<highlight><bold>2</bold></highlight>, wherein 
<claim-text>C is equal to a number of terms in the context window; and </claim-text>
<claim-text>N is equal to a number of terms occurring between T<highlight><bold>1</bold></highlight> and T<highlight><bold>2</bold></highlight>. </claim-text>
</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00016">
<claim-text><highlight><bold>16</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00009">claim 9</dependent-claim-reference> wherein, the types of relational metrics include a directional contextual metric (DCM). </claim-text>
</claim>
<claim id="CLM-00017">
<claim-text><highlight><bold>17</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00011">claim 16</dependent-claim-reference> wherein: 
<claim-text>the DCM for a single occurrence of a term pair (T<highlight><bold>1</bold></highlight>, T<highlight><bold>2</bold></highlight>) in the subset is DCM(T<highlight><bold>1</bold></highlight>, T<highlight><bold>2</bold></highlight>), wherein: 
<claim-text>T<highlight><bold>1</bold></highlight> is a first term in the term pair; </claim-text>
<claim-text>T<highlight><bold>2</bold></highlight> is a second term in the term pair; </claim-text>
<claim-text>DCM(T<highlight><bold>1</bold></highlight>, T<highlight><bold>2</bold></highlight>)&equals;RCM(T<highlight><bold>1</bold></highlight>, T<highlight><bold>2</bold></highlight>)&minus;LCM(T<highlight><bold>1</bold></highlight>, T<highlight><bold>2</bold></highlight>), wherein: 
<claim-text>RCM(T<highlight><bold>1</bold></highlight>, T<highlight><bold>2</bold></highlight>) is a right contextual metric for the single occurrence of a term pair (T<highlight><bold>1</bold></highlight>, T<highlight><bold>2</bold></highlight>) in the subset; </claim-text>
<claim-text>LCM(T<highlight><bold>1</bold></highlight>, T<highlight><bold>2</bold></highlight>) is a left contextual metric for the single occurrence of a term pair (T<highlight><bold>1</bold></highlight>, T<highlight><bold>2</bold></highlight>) in the subset; and </claim-text>
<claim-text>RCM (T<highlight><bold>1</bold></highlight>, T<highlight><bold>2</bold></highlight>)&gE;LCM(T<highlight><bold>1</bold></highlight>, T<highlight><bold>2</bold></highlight>) </claim-text>
</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00018">
<claim-text><highlight><bold>18</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00009">claim 9</dependent-claim-reference> wherein, the context window includes a number of terms equal to a function of an average sentence length. </claim-text>
</claim>
<claim id="CLM-00019">
<claim-text><highlight><bold>19</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00009">claim 9</dependent-claim-reference> wherein, the context window includes a number of terms equal to a function of an average paragraph length. </claim-text>
</claim>
<claim id="CLM-00020">
<claim-text><highlight><bold>20</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00009">claim 9</dependent-claim-reference> wherein, the context window includes a number of terms equal to a pre-selected number. </claim-text>
</claim>
<claim id="CLM-00021">
<claim-text><highlight><bold>21</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00009">claim 9</dependent-claim-reference> further comprising: 
<claim-text>selecting a relation threshold value for a selected one of the subset plurality of types of RSMs; and </claim-text>
<claim-text>eliminating all relations having a value of the selected type of RSM less than the relation threshold value. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00022">
<claim-text><highlight><bold>22</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00009">claim 9</dependent-claim-reference> further comprising: 
<claim-text>selecting one of the subset plurality of types of RSMs; and </claim-text>
<claim-text>selecting a pre-selected number of relations having a greatest value of the selected type of RSM from each one of the plurality of relational models of the subsets. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00023">
<claim-text><highlight><bold>23</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference> wherein, the first query includes a plurality of query fields. </claim-text>
</claim>
<claim id="CLM-00024">
<claim-text><highlight><bold>24</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00022">claim 23</dependent-claim-reference> wherein, creating a relational model of the first query includes: 
<claim-text>creating a plurality of relational models of the plurality of query fields wherein, each one of the plurality of relational models of the plurality of query fields includes one relational model of one of the plurality of query fields in the first query, wherein each one of the plurality of relational models of the plurality of query fields has a plurality of relations; and </claim-text>
<claim-text>combining the plurality of relational models of the plurality of query fields in the first query into a first query relational model. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00025">
<claim-text><highlight><bold>25</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00022">claim 24</dependent-claim-reference> wherein, combining the plurality of relational models of the plurality of query fields in the first query into a first query relational model comprises: 
<claim-text>analyzing a first one of the plurality of relational models of the plurality of query fields including: 
<claim-text>determining if a first relation from the first one of the plurality of relational models of the plurality of query fields is included in the first query model comprising: 
<claim-text>selecting a first relation from the first one of the plurality of relational models of the plurality of query fields, wherein the selected first relation includes a first term pair; </claim-text>
<claim-text>determining if the first term pair is included in one of the plurality of relations in the first query model; </claim-text>
<claim-text>if the first term pair is not included in one of the plurality of relations in the first query model, then including the selected first relation in the first query model; and </claim-text>
<claim-text>if the first term pair is included in one of the plurality of relations in the first query model, then comparing a first order of the first term pair in the selected first relation with a second order of the first term pair in the relation from the first query model containing the first term pair; </claim-text>
<claim-text>if the first order and the second order are the same, combining a plurality of types of RSMs of the selected first relation in the first query field model, with a corresponding plurality of types of RSMs of the relation containing the first term pair in the first query model, </claim-text>
<claim-text>if the first order and the second order are not the same, then: 
<claim-text>reversing the order of the term pair in the selected first relation; </claim-text>
<claim-text>exchanging a right directional RSM of the selected first relation with a left directional RSM of the selected first relation; and </claim-text>
<claim-text>combining a plurality of types of RSMs of the selected first relation in the first query field model, with a corresponding plurality of types of RSMs of the relation containing the first term pair in the first query model; and </claim-text>
</claim-text>
<claim-text>determining if a subsequent relation from the first one of the plurality of relational models of the plurality of query fields is included in the first query model; and </claim-text>
</claim-text>
</claim-text>
<claim-text>analyzing a subsequent one of the plurality of relational models of the plurality of query fields. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00026">
<claim-text><highlight><bold>26</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00022">claim 25</dependent-claim-reference> wherein, combining a plurality of types of RSMs of the selected first relation in the first query field model, with the corresponding plurality of types of RSMs of the relation containing the first term pair in the first query model includes: 
<claim-text>selecting one of the plurality of types of RSMs; </claim-text>
<claim-text>selecting the relation from either of the first query field model or the first query model, wherein the selected relation has the greatest magnitude of the selected type of RSM; and </claim-text>
<claim-text>replacing the relation containing the first term pair in the first query model with the selected relation. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00027">
<claim-text><highlight><bold>27</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00022">claim 26</dependent-claim-reference> wherein, the selected one of the plurality of the types of relevance metrics includes at least one of a group consisting of: 
<claim-text>a combination of types of relevance metrics; </claim-text>
<claim-text>a weighted sum of types of relevance metrics; and </claim-text>
<claim-text>a weighted product of types of relevance metrics. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00028">
<claim-text><highlight><bold>28</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00022">claim 25</dependent-claim-reference> wherein, combining a plurality of types of RSMs of the selected first relation in a first query field model, with a corresponding plurality of types of RSMs of the relation containing the first term pair in the first query model includes: 
<claim-text>calculating a summation of the corresponding types of RSMs from the relation containing the first term pair in both the first query field model and the first query model; and </claim-text>
<claim-text>replacing the plurality of types of RSMs for the relation containing the first term pair in the first query model with the summation of the corresponding types of RSMs. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00029">
<claim-text><highlight><bold>29</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00022">claim 24</dependent-claim-reference> wherein, the first query further includes: 
<claim-text>selecting at least one of the plurality of query fields in the first query; </claim-text>
<claim-text>assigning a weight to the selected query field, wherein each one of the plurality of RSMs corresponding to the selected query field are scaled by a factor determined by the weight. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00030">
<claim-text><highlight><bold>30</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00022">claim 24</dependent-claim-reference> wherein, each one of the plurality of relations in each one of the plurality of relational models of the plurality of first query fields includes a summation of the corresponding types of relational metrics of each one of a plurality of occurrences of a first query term pair within the query field, wherein, the plurality of types of relational metrics include a non-directional contextual metric (NDCM), a right contextual metric (RCM), a left contextual metric (LCM), and a directional contextual metric (DCM). </claim-text>
</claim>
<claim id="CLM-00031">
<claim-text><highlight><bold>31</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference> further comprising, providing a plurality of stopterms, wherein, if neither a first term in the first query term pair nor a second term in the first query term pair is one of the plurality of stopterms then the RSMs are increased. </claim-text>
</claim>
<claim id="CLM-00032">
<claim-text><highlight><bold>32</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference> further comprising, providing a plurality of stopterms, wherein, if both a first term in the first query term pair and a second term in the first query term pair are included in the plurality of stopterms then the RSMs are decreased. </claim-text>
</claim>
<claim id="CLM-00033">
<claim-text><highlight><bold>33</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference> further comprising, providing a plurality of stopterms, wherein, if either but not both a first term in the first query term pair or a second term in the first query term pair is one of the plurality of stopterms then the RSMs are unchanged. </claim-text>
</claim>
<claim id="CLM-00034">
<claim-text><highlight><bold>34</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference> further comprising, providing a plurality of emphasis terms, wherein, if neither a first term in the first query term pair nor a second term in the first query term pair is one of the plurality of emphasis terms then the RSMs are decreased. </claim-text>
</claim>
<claim id="CLM-00035">
<claim-text><highlight><bold>35</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference> further comprising, providing a plurality of emphasis terms, wherein, if both a first term in the first query term pair and a second term in the first query term pair are included in the plurality of emphasis terms then the RSMs are increased. </claim-text>
</claim>
<claim id="CLM-00036">
<claim-text><highlight><bold>36</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference> further comprising, providing a plurality of emphasis terms, wherein, if either but not both a first term in the first query term pair or a second term in the first query term pair is one of the plurality of emphasis terms then the RSMs are unchanged. </claim-text>
</claim>
<claim id="CLM-00037">
<claim-text><highlight><bold>37</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference> further comprising: 
<claim-text>providing a plurality of stop relations, wherein each one of the plurality of stop relations includes a first term and a second term and a plurality of types of relational metrics; and </claim-text>
<claim-text>eliminating the plurality of stop relations from the relational model of the first query. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00038">
<claim-text><highlight><bold>38</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference> wherein, inputting a first query includes transforming the first query. </claim-text>
</claim>
<claim id="CLM-00039">
<claim-text><highlight><bold>39</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00033">claim 38</dependent-claim-reference> wherein, transforming the first query includes at least one of a group consisting of: 
<claim-text>not changing the first query; and </claim-text>
<claim-text>replacing a portion of the first query with an alternate portion. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00040">
<claim-text><highlight><bold>40</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00044">claim 49</dependent-claim-reference> wherein, the alternate portion is cross referenced to the portion of the first query in a look-up table. </claim-text>
</claim>
<claim id="CLM-00041">
<claim-text><highlight><bold>41</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00044">claim 40</dependent-claim-reference> wherein, the look-up table comprises: 
<claim-text>a plurality of non-empty hash chains, wherein each one of the plurality of non-empty hash chains corresponds to a first section of the portion of the first query and each one of the plurality of hash chains has a plurality of phrases beginning with the first section of the portion of the first query; and </claim-text>
<claim-text>a plurality of alternate portions, wherein each one of the plurality of alternate portions corresponds to one of the plurality of phrases. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00042">
<claim-text><highlight><bold>42</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference> wherein, comparing the relational model of the first query includes: 
<claim-text>calculating a plurality of first relevance metrics for a first one of the plurality of relational models of the subsets comprising: 
<claim-text>determining an intersection model of the relational model of the first query and a first one of the plurality of relational models of the subsets including: 
<claim-text>determining a plurality of intersection relations, wherein each one of the plurality of intersection relations has: 
<claim-text>a shared term pair, wherein a shared term pair includes a term pair present in at least one relation in each one of the first query relational model and the first one of the plurality of the relational models of the subsets; and </claim-text>
<claim-text>a plurality of intersection metrics (IM) wherein, each one of the plurality of IM&equals;fct(RSM<highlight><subscript>Q1</subscript></highlight>, RSM<highlight><subscript>S1</subscript></highlight>), wherein: 
<claim-text>RSM<highlight><subscript>Q1 </subscript></highlight>is a type of RSM in the relational model of the first query; and </claim-text>
<claim-text>RSM<highlight><subscript>S1 </subscript></highlight>is a corresponding type of RSM in the relational model of the first one of the plurality of relational models of the subsets; and </claim-text>
</claim-text>
</claim-text>
</claim-text>
<claim-text>calculating a first relevance metric for each one of the plurality of types of RSMs equal to a function of the plurality of corresponding IMs of all intersection relations; and </claim-text>
</claim-text>
<claim-text>determining a subsequent plurality of first relevance metrics corresponding to each subsequent one of the plurality of relational models of the subsets. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00043">
<claim-text><highlight><bold>43</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00044">claim 42</dependent-claim-reference> wherein, determining a plurality of intersection relations further has: 
<claim-text>determining a first order of the shared term pair in the first query relational model and a second order of the shared term pair in the first one of the plurality of the relational models of the subsets; and </claim-text>
<claim-text>reversing the second order and exchanging an RCM and an LCM of the relation having the term pair in the first one of the plurality of the relational models of the subsets, if the first order and second order are not equal. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00044">
<claim-text><highlight><bold>44</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00044">claim 42</dependent-claim-reference> wherein, the fct(RSM<highlight><subscript>Q1</subscript></highlight>, RSM<highlight><subscript>S1</subscript></highlight>)&equals;&lsqb;RSM<highlight><subscript>Q1</subscript></highlight>&rsqb;*&lsqb;RSM<highlight><subscript>S1</subscript></highlight>&rsqb;. </claim-text>
</claim>
<claim id="CLM-00045">
<claim-text><highlight><bold>45</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00044">claim 42</dependent-claim-reference> wherein, a scale factor is applied to the fct(RSM<highlight><subscript>Q1</subscript></highlight>, RSM<highlight><subscript>S1</subscript></highlight>). </claim-text>
</claim>
<claim id="CLM-00046">
<claim-text><highlight><bold>46</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00044">claim 42</dependent-claim-reference> wherein, the function of the plurality of corresponding IMs of all intersection relations includes a summation of the plurality of corresponding IMs of all intersection relations. </claim-text>
</claim>
<claim id="CLM-00047">
<claim-text><highlight><bold>47</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00044">claim 42</dependent-claim-reference> wherein, the function of the plurality of corresponding IMs of all intersection relations includes a summation of all of the plurality of RSM<highlight><subscript>Q1 </subscript></highlight>of each one of the plurality of first query relations which are included in the plurality of intersection relations. </claim-text>
</claim>
<claim id="CLM-00048">
<claim-text><highlight><bold>48</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00044">claim 42</dependent-claim-reference> wherein, calculating a plurality of first relevance metrics for a first one of the plurality of relational models of the subsets further comprises assigning a zero relevance to the first one of the plurality of subsets if all term pairs of the relational model of the first query are not included in the relational model of the first subset. </claim-text>
</claim>
<claim id="CLM-00049">
<claim-text><highlight><bold>49</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00044">claim 42</dependent-claim-reference> wherein, determining an intersection model further has: 
<claim-text>applying a scaling factor to the function of the plurality of corresponding intersection metrics. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00050">
<claim-text><highlight><bold>50</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00044">claim 49</dependent-claim-reference> wherein, the scaling factor is a subset emphasis factor (SEF)&equals;S<highlight><subscript>S</subscript></highlight>/R, wherein S<highlight><subscript>S </subscript></highlight>is equal to a summation of a selected type of relational metrics from the subset for all shared relations and R is equal to a summation of the selected type of relational metric in the subset. </claim-text>
</claim>
<claim id="CLM-00051">
<claim-text><highlight><bold>51</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00044">claim 49</dependent-claim-reference> wherein, the scaling factor is a query emphasis factor (QEF)&equals;S<highlight><subscript>Q</subscript></highlight>/Q, wherein S<highlight><subscript>Q </subscript></highlight>is equal to a summation of a selected type of relational metrics from the query for all shared relations and Q is equal to a summation of the selected type of relational metric in the relevance model of the first query. </claim-text>
</claim>
<claim id="CLM-00052">
<claim-text><highlight><bold>52</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00044">claim 49</dependent-claim-reference> wherein, the scaling factor is a length emphasis factor (LEF)&equals;L<highlight><subscript>s</subscript></highlight>/T wherein, L<highlight><subscript>s </subscript></highlight>is equal to a number of terms in the subset and T is equal to a number greater than a number of terms in a largest subset of the database. </claim-text>
</claim>
<claim id="CLM-00053">
<claim-text><highlight><bold>53</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00044">claim 49</dependent-claim-reference> wherein, the scaling factor is an alternate length emphasis factor (LEF<highlight><subscript>alt</subscript></highlight>)&equals;L<highlight><subscript>cap</subscript></highlight>/T wherein, L<highlight><subscript>cap </subscript></highlight>is equal to the lesser of either a number of terms in the subset or an average number of terms in each one of the plurality of subsets, and T is equal to a number greater than a number of terms in a largest subset of the database. </claim-text>
</claim>
<claim id="CLM-00054">
<claim-text><highlight><bold>54</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00044">claim 42</dependent-claim-reference> wherein, outputting a plurality of identifiers of subsets relevant to the first query includes: 
<claim-text>outputting a plurality of types of relevance metrics corresponding to each one of the plurality of subsets; </claim-text>
<claim-text>selecting one of the plurality of types of relevance metrics; and </claim-text>
<claim-text>sorting the plurality of identifiers of subsets in order of magnitude of the selected one of the plurality of types of relevance metrics. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00055">
<claim-text><highlight><bold>55</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00055">claim 54</dependent-claim-reference> wherein, the selected one of the plurality of the types of relevance metrics includes at least one of a group consisting of: 
<claim-text>a combination of types of relevance metrics; </claim-text>
<claim-text>a weighted sum of types of relevance metrics; and </claim-text>
<claim-text>a weighted product of types of relevance metrics. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00056">
<claim-text><highlight><bold>56</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00055">claim 54</dependent-claim-reference> further including, normalizing each one of the plurality of corresponding intersection metrics of all intersection relations. </claim-text>
</claim>
<claim id="CLM-00057">
<claim-text><highlight><bold>57</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00055">claim 54</dependent-claim-reference> further including, outputting the relational model of the first query. </claim-text>
</claim>
<claim id="CLM-00058">
<claim-text><highlight><bold>58</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00055">claim 54</dependent-claim-reference> further including, displaying a pre-selected number of subsets in order of magnitude of the selected type of relevance metric. </claim-text>
</claim>
<claim id="CLM-00059">
<claim-text><highlight><bold>59</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00055">claim 58</dependent-claim-reference> further including, highlighting a plurality of shared term pairs in each one of the plurality of subsets, wherein the terms within each one of the plurality of shared term pairs, occur within a context window. </claim-text>
</claim>
<claim id="CLM-00060">
<claim-text><highlight><bold>60</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00055">claim 59</dependent-claim-reference> wherein, the plurality of shared term pairs consists of a plurality of shared term pairs having a greatest magnitude of a selected type of relevance metric. </claim-text>
</claim>
<claim id="CLM-00061">
<claim-text><highlight><bold>61</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00055">claim 59</dependent-claim-reference> further including, determining a typical order of the plurality of shared term pairs comprising: 
<claim-text>comparing a magnitude of an RCM of the shared term pair to a magnitude of an LCM of the shared term pair; </claim-text>
<claim-text>if the RCM is larger then the shared term pair is in the typical order; </claim-text>
<claim-text>if the LCM is larger then reverse the order of the shared term pair and exchange the RCM and the LCM. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00062">
<claim-text><highlight><bold>62</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00055">claim 54</dependent-claim-reference> further including, displaying a plurality of shared term pairs in each one of the plurality of subsets, wherein the terms within each one of the plurality of shared term pairs, occur within a context window. </claim-text>
</claim>
<claim id="CLM-00063">
<claim-text><highlight><bold>63</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00066">claim 62</dependent-claim-reference> further including, for each one of the plurality of shared term pairs, displaying a feedback metric of the query (FBM<highlight><subscript>Q1</subscript></highlight>) equal to a combination of an LCM<highlight><subscript>Q1 </subscript></highlight>and an RCM<highlight><subscript>Q1 </subscript></highlight>and displaying a feedback metric of the subset FBM<highlight><subscript>S1 </subscript></highlight>equal to a combination of an LCM<highlight><subscript>S1 </subscript></highlight>and an RCM<highlight><subscript>S1 </subscript></highlight>and a product equal to &lsqb;FBM<highlight><subscript>Q1</subscript></highlight>&rsqb;*&lsqb;FBM<highlight><subscript>S1</subscript></highlight>&rsqb;, wherein the LCM<highlight><subscript>Q1 </subscript></highlight>is equal to a left contextual metric of the shared term pair in the query, the RCM<highlight><subscript>Q1 </subscript></highlight>is equal to a right contextual metric of the shared term pair in the query, LCM<highlight><subscript>S1 </subscript></highlight>is equal to a left contextual metric of the shared term pair in the subset and the RCM<highlight><subscript>S1 </subscript></highlight>is equal to a right contextual metric of the shared term pair in the subset. </claim-text>
</claim>
<claim id="CLM-00064">
<claim-text><highlight><bold>64</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00066">claim 62</dependent-claim-reference> wherein, the plurality of shared term pairs consists of a plurality of shared term pairs having a greatest magnitude of a selected type of relevance metric. </claim-text>
</claim>
<claim id="CLM-00065">
<claim-text><highlight><bold>65</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00044">claim 42</dependent-claim-reference> further including: 
<claim-text>inputting a second query; </claim-text>
<claim-text>creating a relational model of the second query; </claim-text>
<claim-text>comparing the relational model of the second query to each one of the plurality of relational models of the subsets; </claim-text>
<claim-text>outputting a second plurality of identifiers of the subsets relevant to the second query; and </claim-text>
<claim-text>combining a second plurality of relevance metrics for the second query with the plurality of relevance metrics for the first query. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00066">
<claim-text><highlight><bold>66</bold></highlight>. A method as recited in <dependent-claim-reference depends_on="CLM-00066">claim 65</dependent-claim-reference> further including, determining a third plurality of identifiers of the subsets consisting of identifiers of the subsets present in both the first and second pluralities of subsets, wherein the combined relevance metrics for each one of the identifiers of the subsets in the first plurality of identifiers of the subsets and the second plurality of identifiers of the subsets is greater than zero. </claim-text>
</claim>
<claim id="CLM-00067">
<claim-text><highlight><bold>67</bold></highlight>. A method as recited in <dependent-claim-reference depends_on="CLM-00066">claim 66</dependent-claim-reference> wherein, combining includes calculating a product of a first type of first relevance metric and a first type of a second relevance metric. </claim-text>
</claim>
<claim id="CLM-00068">
<claim-text><highlight><bold>68</bold></highlight>. A method as recited in <dependent-claim-reference depends_on="CLM-00066">claim 65</dependent-claim-reference> further including, determining a third plurality of identifiers of the subsets consisting of identifiers of the subsets present in either the first and second pluralities of subsets, wherein the combined relevance metrics for each one of the identifiers of the subsets in the first plurality of identifiers of the subsets and the second plurality of identifiers of the subsets is greater than zero. </claim-text>
</claim>
<claim id="CLM-00069">
<claim-text><highlight><bold>69</bold></highlight>. A method as recited in <dependent-claim-reference depends_on="CLM-00066">claim 68</dependent-claim-reference> wherein, combining includes calculating a summation of a first type of first relevance metric and a first type of a second relevance metric. </claim-text>
</claim>
<claim id="CLM-00070">
<claim-text><highlight><bold>70</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference> wherein, each one of the plurality of identifiers of subsets corresponds to one of the plurality of subsets of the database. </claim-text>
</claim>
<claim id="CLM-00071">
<claim-text><highlight><bold>71</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference> wherein, the database includes at least one of a group consisting of: text, narratives, reports, literature, punctuation, messages, electronic mail, internet text, web site information, linguistic patterns, grammatical tags, alphabetic data, alphabetic strings, numeric data, numeric strings, alphanumeric data, alphanumeric strings, sound, music, voice, audio data, audio encoding, vocal encoding, biological information, biological data, biological representations, biological analogs, medical information, medical data, medical representations, medical sequences, medical patterns, genetic sequences, genetic representations, genetic analogs, protein sequences, protein representations, protein analogs, computer software, computer hardware, computer firmware, computer input, computer internal information, computer output, computer representations, computer analogs, sequential symbols, sequential data, sequential items, sequential objects, sequential events, sequential causes, sequential time spans, sequential actions, sequential attributes, sequential entities, sequential relations, sequential representations, patterned symbols, patterned data, patterned items, patterned objects, patterned events, patterned causes, patterned time spans, patterned actions, patterned attributes, patterned entities, patterned relations, and patterned representations.</claim-text>
</claim>
</subdoc-claims>
<subdoc-drawings id="DRAWINGS">
<heading lvl="0" align="CENTER">Drawings</heading>
<representative-figure>1</representative-figure>
<figure id="figure-D00000">
<image id="EMI-D00000" file="US20030004914A1-20030102-D00000.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00001">
<image id="EMI-D00001" file="US20030004914A1-20030102-D00001.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00002">
<image id="EMI-D00002" file="US20030004914A1-20030102-D00002.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00003">
<image id="EMI-D00003" file="US20030004914A1-20030102-D00003.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00004">
<image id="EMI-D00004" file="US20030004914A1-20030102-D00004.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00005">
<image id="EMI-D00005" file="US20030004914A1-20030102-D00005.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00006">
<image id="EMI-D00006" file="US20030004914A1-20030102-D00006.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00007">
<image id="EMI-D00007" file="US20030004914A1-20030102-D00007.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00008">
<image id="EMI-D00008" file="US20030004914A1-20030102-D00008.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00009">
<image id="EMI-D00009" file="US20030004914A1-20030102-D00009.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00010">
<image id="EMI-D00010" file="US20030004914A1-20030102-D00010.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00011">
<image id="EMI-D00011" file="US20030004914A1-20030102-D00011.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00012">
<image id="EMI-D00012" file="US20030004914A1-20030102-D00012.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00013">
<image id="EMI-D00013" file="US20030004914A1-20030102-D00013.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00014">
<image id="EMI-D00014" file="US20030004914A1-20030102-D00014.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00015">
<image id="EMI-D00015" file="US20030004914A1-20030102-D00015.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00016">
<image id="EMI-D00016" file="US20030004914A1-20030102-D00016.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00017">
<image id="EMI-D00017" file="US20030004914A1-20030102-D00017.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00018">
<image id="EMI-D00018" file="US20030004914A1-20030102-D00018.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00019">
<image id="EMI-D00019" file="US20030004914A1-20030102-D00019.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00020">
<image id="EMI-D00020" file="US20030004914A1-20030102-D00020.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00021">
<image id="EMI-D00021" file="US20030004914A1-20030102-D00021.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00022">
<image id="EMI-D00022" file="US20030004914A1-20030102-D00022.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00023">
<image id="EMI-D00023" file="US20030004914A1-20030102-D00023.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00024">
<image id="EMI-D00024" file="US20030004914A1-20030102-D00024.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00025">
<image id="EMI-D00025" file="US20030004914A1-20030102-D00025.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00026">
<image id="EMI-D00026" file="US20030004914A1-20030102-D00026.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00027">
<image id="EMI-D00027" file="US20030004914A1-20030102-D00027.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00028">
<image id="EMI-D00028" file="US20030004914A1-20030102-D00028.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00029">
<image id="EMI-D00029" file="US20030004914A1-20030102-D00029.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00030">
<image id="EMI-D00030" file="US20030004914A1-20030102-D00030.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00031">
<image id="EMI-D00031" file="US20030004914A1-20030102-D00031.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00032">
<image id="EMI-D00032" file="US20030004914A1-20030102-D00032.TIF" imf="TIFF" ti="DR"/>
</figure>
</subdoc-drawings>
</patent-application-publication>
