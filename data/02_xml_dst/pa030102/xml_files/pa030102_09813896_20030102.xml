<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE patent-application-publication SYSTEM "pap-v16-2002-01-01.dtd" [
<!ENTITY US20030005258A1-20030102-M00001.NB SYSTEM "US20030005258A1-20030102-M00001.NB" NDATA NB>
<!ENTITY US20030005258A1-20030102-M00001.TIF SYSTEM "US20030005258A1-20030102-M00001.TIF" NDATA TIF>
<!ENTITY US20030005258A1-20030102-M00002.NB SYSTEM "US20030005258A1-20030102-M00002.NB" NDATA NB>
<!ENTITY US20030005258A1-20030102-M00002.TIF SYSTEM "US20030005258A1-20030102-M00002.TIF" NDATA TIF>
<!ENTITY US20030005258A1-20030102-M00003.NB SYSTEM "US20030005258A1-20030102-M00003.NB" NDATA NB>
<!ENTITY US20030005258A1-20030102-M00003.TIF SYSTEM "US20030005258A1-20030102-M00003.TIF" NDATA TIF>
<!ENTITY US20030005258A1-20030102-M00004.NB SYSTEM "US20030005258A1-20030102-M00004.NB" NDATA NB>
<!ENTITY US20030005258A1-20030102-M00004.TIF SYSTEM "US20030005258A1-20030102-M00004.TIF" NDATA TIF>
<!ENTITY US20030005258A1-20030102-M00005.NB SYSTEM "US20030005258A1-20030102-M00005.NB" NDATA NB>
<!ENTITY US20030005258A1-20030102-M00005.TIF SYSTEM "US20030005258A1-20030102-M00005.TIF" NDATA TIF>
<!ENTITY US20030005258A1-20030102-M00006.NB SYSTEM "US20030005258A1-20030102-M00006.NB" NDATA NB>
<!ENTITY US20030005258A1-20030102-M00006.TIF SYSTEM "US20030005258A1-20030102-M00006.TIF" NDATA TIF>
<!ENTITY US20030005258A1-20030102-M00007.NB SYSTEM "US20030005258A1-20030102-M00007.NB" NDATA NB>
<!ENTITY US20030005258A1-20030102-M00007.TIF SYSTEM "US20030005258A1-20030102-M00007.TIF" NDATA TIF>
<!ENTITY US20030005258A1-20030102-M00008.NB SYSTEM "US20030005258A1-20030102-M00008.NB" NDATA NB>
<!ENTITY US20030005258A1-20030102-M00008.TIF SYSTEM "US20030005258A1-20030102-M00008.TIF" NDATA TIF>
<!ENTITY US20030005258A1-20030102-M00009.NB SYSTEM "US20030005258A1-20030102-M00009.NB" NDATA NB>
<!ENTITY US20030005258A1-20030102-M00009.TIF SYSTEM "US20030005258A1-20030102-M00009.TIF" NDATA TIF>
<!ENTITY US20030005258A1-20030102-M00010.NB SYSTEM "US20030005258A1-20030102-M00010.NB" NDATA NB>
<!ENTITY US20030005258A1-20030102-M00010.TIF SYSTEM "US20030005258A1-20030102-M00010.TIF" NDATA TIF>
<!ENTITY US20030005258A1-20030102-M00011.NB SYSTEM "US20030005258A1-20030102-M00011.NB" NDATA NB>
<!ENTITY US20030005258A1-20030102-M00011.TIF SYSTEM "US20030005258A1-20030102-M00011.TIF" NDATA TIF>
<!ENTITY US20030005258A1-20030102-M00012.NB SYSTEM "US20030005258A1-20030102-M00012.NB" NDATA NB>
<!ENTITY US20030005258A1-20030102-M00012.TIF SYSTEM "US20030005258A1-20030102-M00012.TIF" NDATA TIF>
<!ENTITY US20030005258A1-20030102-M00013.NB SYSTEM "US20030005258A1-20030102-M00013.NB" NDATA NB>
<!ENTITY US20030005258A1-20030102-M00013.TIF SYSTEM "US20030005258A1-20030102-M00013.TIF" NDATA TIF>
<!ENTITY US20030005258A1-20030102-M00014.NB SYSTEM "US20030005258A1-20030102-M00014.NB" NDATA NB>
<!ENTITY US20030005258A1-20030102-M00014.TIF SYSTEM "US20030005258A1-20030102-M00014.TIF" NDATA TIF>
<!ENTITY US20030005258A1-20030102-P00900.TIF SYSTEM "US20030005258A1-20030102-P00900.TIF" NDATA TIF>
<!ENTITY US20030005258A1-20030102-D00000.TIF SYSTEM "US20030005258A1-20030102-D00000.TIF" NDATA TIF>
<!ENTITY US20030005258A1-20030102-D00001.TIF SYSTEM "US20030005258A1-20030102-D00001.TIF" NDATA TIF>
<!ENTITY US20030005258A1-20030102-D00002.TIF SYSTEM "US20030005258A1-20030102-D00002.TIF" NDATA TIF>
<!ENTITY US20030005258A1-20030102-D00003.TIF SYSTEM "US20030005258A1-20030102-D00003.TIF" NDATA TIF>
<!ENTITY US20030005258A1-20030102-D00004.TIF SYSTEM "US20030005258A1-20030102-D00004.TIF" NDATA TIF>
<!ENTITY US20030005258A1-20030102-D00005.TIF SYSTEM "US20030005258A1-20030102-D00005.TIF" NDATA TIF>
<!ENTITY US20030005258A1-20030102-D00006.TIF SYSTEM "US20030005258A1-20030102-D00006.TIF" NDATA TIF>
<!ENTITY US20030005258A1-20030102-D00007.TIF SYSTEM "US20030005258A1-20030102-D00007.TIF" NDATA TIF>
<!ENTITY US20030005258A1-20030102-D00008.TIF SYSTEM "US20030005258A1-20030102-D00008.TIF" NDATA TIF>
]>
<patent-application-publication>
<subdoc-bibliographic-information>
<document-id>
<doc-number>20030005258</doc-number>
<kind-code>A1</kind-code>
<document-date>20030102</document-date>
</document-id>
<publication-filing-type>new</publication-filing-type>
<domestic-filing-data>
<application-number>
<doc-number>09813896</doc-number>
</application-number>
<application-number-series-code>09</application-number-series-code>
<filing-date>20010322</filing-date>
</domestic-filing-data>
<technical-information>
<classification-ipc>
<classification-ipc-primary>
<ipc>G06F015/00</ipc>
</classification-ipc-primary>
<classification-ipc-edition>07</classification-ipc-edition>
</classification-ipc>
<classification-us>
<classification-us-primary>
<uspc>
<class>712</class>
<subclass>001000</subclass>
</uspc>
</classification-us-primary>
</classification-us>
<title-of-invention>Feature weighting in k-means clustering</title-of-invention>
</technical-information>
<inventors>
<first-named-inventor>
<name>
<given-name>Dharmendra</given-name>
<middle-name>Shantilal</middle-name>
<family-name>Modha</family-name>
</name>
<residence>
<residence-us>
<city>San Jose</city>
<state>CA</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</first-named-inventor>
<inventor>
<name>
<given-name>William</given-name>
<middle-name>Scott</middle-name>
<family-name>Spangler</family-name>
</name>
<residence>
<residence-us>
<city>San Martin</city>
<state>CA</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
</inventors>
<correspondence-address>
<name-1>MCGINN &amp; GIBB, PLLC</name-1>
<name-2></name-2>
<address>
<address-1>8321 OLD COURTHOUSE ROAD</address-1>
<address-2>SUITE 200</address-2>
<city>VIENNA</city>
<state>VA</state>
<postalcode>22182-3817</postalcode>
<country>
<country-code>US</country-code>
</country>
</address>
</correspondence-address>
</subdoc-bibliographic-information>
<subdoc-abstract>
<paragraph id="A-0001" lvl="0">A method and system is provided for integrating multiple feature spaces in a k-means clustering algorithm when analyzing data records having multiple, heterogeneous feature spaces. The method assigns different relative weights to these various features spaces. Optimal feature weights are also determined that lead to a clustering that simultaneously minimizes the average intra-cluster dispersion and maximizes the average inter-cluster dispersion along all the feature spaces. Examples are provided that empirically demonstrate the effectiveness of feature weighting in clustering using two different feature domains. </paragraph>
</subdoc-abstract>
<subdoc-description>
<summary-of-invention>
<section>
<heading lvl="1">BACKGROUND OF THE INVENTION </heading>
<paragraph id="P-0001" lvl="0"><number>&lsqb;0001&rsqb;</number> 1. Field of the Invention </paragraph>
<paragraph id="P-0002" lvl="0"><number>&lsqb;0002&rsqb;</number> The present invention generally relates to data clustering and in particular, concerns a method and system for providing a framework for integrating multiple, heterogeneous feature spaces in a k-means clustering algorithm. </paragraph>
<paragraph id="P-0003" lvl="0"><number>&lsqb;0003&rsqb;</number> 2. Description of the Related Art </paragraph>
<paragraph id="P-0004" lvl="0"><number>&lsqb;0004&rsqb;</number> Clustering, the grouping together of similar data points in a data set, is a widely used procedure for analyzing data for &ldquo;data mining&rdquo; applications. Such applications of clustering include unsupervised classification and taxonomy generation, nearest-neighbor searching, scientific discovery, vector quantization, text analysis and navigation, data reduction and summarization, supermarket database analysis, customer/market segmentation, and time series analysis. </paragraph>
<paragraph id="P-0005" lvl="0"><number>&lsqb;0005&rsqb;</number> One of the more popular techniques for clustering data of a set of data records includes partitioning operations (also referred to as finding &ldquo;pattern vectors&rdquo;) of the data using a k-means clustering algorithm which generates a minimum variance grouping of data by minimizing the sum of squared Euclidean distances from cluster centroids. The popularity of the k-means clustering algorithm is based on its ease of interpretation, simplicity of use, scalability, speed of convergence, parallelizability, adaptability to sparse data, and ease of out-of-core use. </paragraph>
<paragraph id="P-0006" lvl="0"><number>&lsqb;0006&rsqb;</number> The k-means clustering algorithm functions to reduce data. Initial cluster centers are chosen arbitrarily. Records from the database are then distributed among the chosen cluster domains based on minimum distances. After records are distributed, the cluster centers are updated to reflect the means of all the records in the respective cluster domains. This process is iterated so long as the cluster centers continue to move and converge and remain static. Performance of this algorithm is influenced by the number and location of the initial cluster centers, and by the order in which pattern samples are passed through the program. </paragraph>
<paragraph id="P-0007" lvl="0"><number>&lsqb;0007&rsqb;</number> Initial use of the k-means clustering algorithm typically requires a user or an external algorithm to define the number of clusters. Second, all the data points within the data set are loaded into the function. Preferably, the data points are indexed according to a numeric field value and a record number. Third, a cluster center is initialized for each of the predefined number of clusters. Each cluster center contains a random normalized valued for each field within the cluster. Thus, initial centers are typically randomly defined. Alternatively, initial cluster center values may be predetermined based on equal divisions of the range within a field. In a fourth step, a routine is performed for each of the records in the database. For each record number from one to the current record number, the cluster center closest to the current record is determined. The record is then assigned to that closest cluster by adding the record number to the list of records previously assigned to the cluster. In a fifth step, after all of the records have been assigned to a cluster, the cluster center for each cluster is adjusted to reflect the averages of data values contained in the records assigned to the cluster. The steps of assigning records to clusters and then adjusting the cluster centers is repeated until the cluster centers move less than a predetermined epsilon value. At this point the cluster centers are viewed as being static. </paragraph>
<paragraph id="P-0008" lvl="0"><number>&lsqb;0008&rsqb;</number> A fundamental starting point for machine learning, multivariate statistics, or &ldquo;data mining,&rdquo; a data record can be represented as a high-dimensional feature vector. In many traditional applications, all the features are essentially of the same &ldquo;type.&rdquo; However, many emerging data sets are often have many different feature spaces, for example: </paragraph>
<paragraph id="P-0009" lvl="2"><number>&lsqb;0009&rsqb;</number> Image indexing and searching systems use at least four different types of features: color, texture, shape, and location. </paragraph>
<paragraph id="P-0010" lvl="2"><number>&lsqb;0010&rsqb;</number> Hypertext documents contains at least three different types of features: the words, the out-links, and the in-links. </paragraph>
<paragraph id="P-0011" lvl="2"><number>&lsqb;0011&rsqb;</number> XML (www.xml.org) has become a standard way to represent data records; such records may have a number of different textual, referential, graphical, numerical, and categorical features. </paragraph>
<paragraph id="P-0012" lvl="2"><number>&lsqb;0012&rsqb;</number> Profile of a typical Amazon.com customer may contain purchased books, music, DVD/video, software, toys, etc. These above examples illustrate that data sets with multiple, heterogeneous features are indeed natural and common. In addition, many data sets on the University of California Irvine Machine Learning and Knowledge Discovery and Data Mining repositories contain data records with heterogeneous features. Data clustering is an unsupervised learning operation whose output provides fundamental techniques in machine learning and statistics. Statistical and computational issues associated with the k-means clustering algorithm have extensively been used for these clustering operations. The same cannot be said, however, for another key ingredient for multidimensional data analysis: clustering data records having multiple, heterogeneous feature spaces. </paragraph>
</section>
<section>
<heading lvl="1">SUMMARY OF THE INVENTION </heading>
<paragraph id="P-0013" lvl="0"><number>&lsqb;0013&rsqb;</number> It is, therefore, an object of the present invention to provide a method and system for integrating multiple, heterogeneous feature spaces in a k-means clustering algorithm. The method of the invention adaptively selects the relative weights assigned to various features spaces, which simultaneously attains good separation along all the feature spaces. </paragraph>
<paragraph id="P-0014" lvl="0"><number>&lsqb;0014&rsqb;</number> The invention integrates multiple feature spaces in a k-means clustering algorithm by assigning different relative weights to these various features spaces. Optimal feature weights are also determined that can be incorporated with this algorithm that lead to a clustering that simultaneously minimizes the average intra-cluster dispersion and maximizes the average inter-cluster dispersion along all the feature spaces.</paragraph>
</section>
</summary-of-invention>
<brief-description-of-drawings>
<section>
<heading lvl="1">DESCRIPTION OF THE DRAWINGS </heading>
<paragraph id="P-0015" lvl="0"><number>&lsqb;0015&rsqb;</number> The foregoing and other objects, aspects and advantages will be better understood from the following detailed description of preferred embodiments of the invention with reference to the drawings, in which: </paragraph>
<paragraph id="P-0016" lvl="0"><number>&lsqb;0016&rsqb;</number> <cross-reference target="DRAWINGS">FIGS. 1</cross-reference><highlight><italic>a </italic></highlight>and <highlight><bold>1</bold></highlight><highlight><italic>b </italic></highlight>show a data computing system and method of the invention respectively; </paragraph>
<paragraph id="P-0017" lvl="0"><number>&lsqb;0017&rsqb;</number> <cross-reference target="DRAWINGS">FIGS. 2</cross-reference><highlight><italic>a</italic></highlight>, <highlight><bold>2</bold></highlight><highlight><italic>b</italic></highlight>, <highlight><bold>2</bold></highlight><highlight><italic>c </italic></highlight>and <highlight><bold>2</bold></highlight><highlight><italic>d </italic></highlight>show graphs of a first example using the invention wherein the HEART (resp. ADULT) data, in <cross-reference target="DRAWINGS">FIGS. 2</cross-reference><highlight><italic>a </italic></highlight>and <highlight><bold>2</bold></highlight><highlight><italic>b </italic></highlight>respectively, how a plot of the &ldquo;objective&rdquo; function Q1&times;Q2 in equation (6) versus the weight c&tilde;x. The HEART (resp. ADULT) data, the <cross-reference target="DRAWINGS">FIGS. 2</cross-reference><highlight><italic>c </italic></highlight>and <highlight><bold>2</bold></highlight><highlight><italic>d </italic></highlight>respectively, show a plot of macro-p (resp. micro-p, macro-p, and macro-r) versus the weight &agr;<highlight><subscript>1</subscript></highlight>; </paragraph>
<paragraph id="P-0018" lvl="0"><number>&lsqb;0018&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 3</cross-reference> shows the feasible weights for the second exemplary use of the invention wherein when the feature space is 3, and the triangular region formed by the intersection of the plane at &agr;<highlight><subscript>1</subscript></highlight>&plus;&agr;<highlight><subscript>2</subscript></highlight>&plus;&agr;<highlight><subscript>3</subscript></highlight>&equals;1 with the nonnegative orthant of <custom-character file="US20030005258A1-20030102-P00900.TIF" wi="20" he="20" id="custom-character-00001"/><highlight><superscript>3</superscript></highlight>; and </paragraph>
<paragraph id="P-0019" lvl="0"><number>&lsqb;0019&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 4</cross-reference> shows a newsgroups data set, in which plot macro-p versus the &ldquo;objective&rdquo; function Q<highlight><subscript>1</subscript></highlight>&times;Q<highlight><subscript>2</subscript></highlight>&times;Q<highlight><subscript>3 </subscript></highlight>for various different weight tuples. </paragraph>
</section>
</brief-description-of-drawings>
<detailed-description>
<section>
<heading lvl="1">DETAILED DESCRIPTION OF PREFERRED EMBODIMENTS OF THE INVENTION </heading>
<paragraph id="P-0020" lvl="0"><number>&lsqb;0020&rsqb;</number> 1. Introduction </paragraph>
<paragraph id="P-0021" lvl="0"><number>&lsqb;0021&rsqb;</number> While the invention is primarily disclosed as a method, it will be understood by a person of ordinary skill in the art that an apparatus, such as a conventional data processor, including a CPU, memory, I/O, program storage, a connecting bus, and other appropriate components, could be programmed or otherwise designed to facilitate the practice of the method of the invention. Such a processor would include appropriate program means for executing the method of the invention. Also, an article of manufacture, such as a pre-recorded disk or other similar computer program product, for use with a data processing system, could include a storage medium and program means recorded thereon for directing the data processing system to facilitate the practice of the method of the invention. It will be understood that such apparatus and articles of manufacture also fall within the spirit and scope of the invention. </paragraph>
<paragraph id="P-0022" lvl="0"><number>&lsqb;0022&rsqb;</number> While the invention is primarily disclosed as a method, it will be understood by a person of ordinary skill in the art that an apparatus, such as a conventional data processor, including a CPU, memory, I/O, program storage, a connecting bus, and other appropriate components, could be programmed or otherwise designed to facilitate the practice of the method of the invention. Such a processor would include appropriate program means for executing the method of the invention. Also, an article of manufacture, such as a pre-recorded disk or other similar computer program product, for use with a data processing system, could include a storage medium and program means recorded thereon for directing the data processing system to facilitate the practice of the method of the invention. It will be understood that such apparatus and articles of manufacture also fall within the spirit and scope of the invention. </paragraph>
<paragraph id="P-0023" lvl="0"><number>&lsqb;0023&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 1</cross-reference><highlight><italic>a </italic></highlight>shows an exemplary data processing system for practicing the disclosed feature weighted K-means data clustering analysis methodology that includes a computing device in the form of a conventional computer <highlight><bold>20</bold></highlight>, including one or more processing units <highlight><bold>21</bold></highlight>, a system memory <highlight><bold>22</bold></highlight>, and a system bus <highlight><bold>23</bold></highlight> that couples various system components including the system memory to the processing unit <highlight><bold>21</bold></highlight>. The system bus <highlight><bold>23</bold></highlight> may be any of several types of bus structures including a memory bus or memory controller, a peripheral bus, and a local bus using any of a variety of bus architectures. </paragraph>
<paragraph id="P-0024" lvl="0"><number>&lsqb;0024&rsqb;</number> The system memory includes read only memory (ROM) <highlight><bold>24</bold></highlight> and random access memory (RAM) <highlight><bold>25</bold></highlight>. A basic input/output system <highlight><bold>26</bold></highlight> (BIOS), containing the basic routines that helps to transfer information between elements within the computer <highlight><bold>20</bold></highlight>, such as during start-up, is stored in ROM <highlight><bold>24</bold></highlight>. </paragraph>
<paragraph id="P-0025" lvl="0"><number>&lsqb;0025&rsqb;</number> The computer <highlight><bold>20</bold></highlight> further includes a hard disk drive <highlight><bold>27</bold></highlight> for reading from and writing to a hard disk, not shown, a magnetic disk drive <highlight><bold>28</bold></highlight> for reading from or writing to a removable magnetic disk <highlight><bold>29</bold></highlight>, and an optical disk drive <highlight><bold>30</bold></highlight> for reading from or writing to a removable optical disk <highlight><bold>31</bold></highlight> such as a CD-ROM or other optical media. The hard disk drive <highlight><bold>27</bold></highlight>, magnetic disk drive <highlight><bold>28</bold></highlight>, and optical disk drive <highlight><bold>30</bold></highlight> are connected to the system bus <highlight><bold>23</bold></highlight> by a hard disk drive interface <highlight><bold>32</bold></highlight>, a magnetic disk drive interface <highlight><bold>33</bold></highlight>, and an optical drive interface <highlight><bold>34</bold></highlight>, respectively. The drives and their associated computer-readable media provide nonvolatile storage of computer readable instructions, data structures, program modules and other data for the computer <highlight><bold>20</bold></highlight>. Although the exemplary environment described herein employs a hard disk, a removable magnetic disk <highlight><bold>29</bold></highlight> and a removable optical disk <highlight><bold>31</bold></highlight>, it should be appreciated by those skilled in the art that other types of computer readable media which can store data that is accessible by a computer, such as magnetic cassettes, flash memory cards, digital video disks, Bernoulli cartridges, random access memories (RAMs), read only memories (ROM), and the like, may also be used in the exemplary operating environment. Data and program instructions can be in the storage area that is readable by a machine, and that tangibly embodies a program of instructions executable by the machine for performing the method of the present invention described herein for data mining applications. </paragraph>
<paragraph id="P-0026" lvl="0"><number>&lsqb;0026&rsqb;</number> A number of program modules may be stored on the hard disk, magnetic disk <highlight><bold>29</bold></highlight>, optical disk <highlight><bold>31</bold></highlight>, ROM <highlight><bold>24</bold></highlight> or RAM <highlight><bold>25</bold></highlight>, including an operating system <highlight><bold>35</bold></highlight>, one or more application programs <highlight><bold>36</bold></highlight>, other program modules <highlight><bold>37</bold></highlight>, and program data <highlight><bold>38</bold></highlight>. A user may enter commands and information into the computer <highlight><bold>20</bold></highlight> through input devices such as a keyboard <highlight><bold>40</bold></highlight> and pointing device 42. Other input devices (not shown) may include a microphone, joystick, game pad, satellite dish, scanner, or the like. These and other input devices are often connected to the processing unit <highlight><bold>21</bold></highlight> through a serial port interface <highlight><bold>46</bold></highlight> that is coupled to the system bus, but may be connected by other interfaces, such as a parallel port, game port or a universal serial bus (USB). A monitor <highlight><bold>47</bold></highlight> or other type of display device is also connected to the system bus <highlight><bold>23</bold></highlight> via an interface, such as a video adapter <highlight><bold>48</bold></highlight>. In addition to the monitor, personal computers typically include other peripheral output devices (not shown), such as speakers and printers. The computer <highlight><bold>20</bold></highlight> may operate in a networked environment using logical connections to one or more remote computers, such as a remote computer <highlight><bold>49</bold></highlight>. The remote computer <highlight><bold>49</bold></highlight> may be another personal computer, a server, a router, a network PC, a peer device or other common network node, and typically includes many or all of the elements described above relative to the computer <highlight><bold>20</bold></highlight>, although only a memory storage device <highlight><bold>50</bold></highlight> has been illustrated in <cross-reference target="DRAWINGS">FIG. 1</cross-reference><highlight><italic>a</italic></highlight>. The logical connections depicted in <cross-reference target="DRAWINGS">FIG. 1</cross-reference><highlight><italic>a </italic></highlight>include a local area network (LAN) <highlight><bold>51</bold></highlight> and a wide area network (WAN) <highlight><bold>52</bold></highlight>. Such networking environments are commonplace in offices, enterprise-wide computer networks, intranets and the Internet. </paragraph>
<paragraph id="P-0027" lvl="0"><number>&lsqb;0027&rsqb;</number> When used in a LAN networking environment, the computer <highlight><bold>20</bold></highlight> is connected to the local network <highlight><bold>51</bold></highlight> through a network interface or adapter <highlight><bold>53</bold></highlight>. When used in a WAN networking environment, the computer <highlight><bold>20</bold></highlight> typically includes a modem <highlight><bold>54</bold></highlight> or other means for establishing communications over the wide area network <highlight><bold>52</bold></highlight>, such as the Internet. The modem <highlight><bold>54</bold></highlight>, which may be internal or external, is connected to the system bus <highlight><bold>23</bold></highlight> via the serial port interface <highlight><bold>46</bold></highlight>. In a networked environment, program modules depicted relative to the computer <highlight><bold>20</bold></highlight>, or portions thereof, may be stored in the remote memory storage device. It will be appreciated that the network connections shown are exemplary and other means of establishing a communications link between the computers may be used. </paragraph>
<paragraph id="P-0028" lvl="0"><number>&lsqb;0028&rsqb;</number> The method of the invention as shown in general form in <cross-reference target="DRAWINGS">FIG. 1</cross-reference><highlight><italic>b, </italic></highlight>may be implemented using standard programming and/or engineering techniques using computer programming software, firmware, hardware or any combination or subcombination thereof. Any such resulting program(s), having computer readable program code means, may be embodied or provided within one or more computer readable or usable media such as fixed (hard) drives, disk, diskettes, optical disks, magnetic tape, semiconductor memories such as read-only memory (ROM), etc., or any transmitting/receiving medium such as the Internet or other communication network or link, thereby making a computer program product, i.e., an article of manufacture, according to the invention. The article of manufacture containing the computer programming code may be made and/or used by executing the code directly from one medium, by copying the code from one medium to another medium, or by transmitting the code over a network. </paragraph>
<paragraph id="P-0029" lvl="0"><number>&lsqb;0029&rsqb;</number> The computing system for implementing the method of the invention can be in the form of software, firmware, hardware or any combination or subcombination thereof, which embody the invention. One skilled in the art of computer science will easily be able to combine the software created as described with appropriate general purpose or special purpose computer hardware to create a computer system and/or computer subcomponents embodying the invention and to create a computer system and/or computer subcomponents for carrying out the method of the invention. </paragraph>
<paragraph id="P-0030" lvl="0"><number>&lsqb;0030&rsqb;</number> The method of the invention is for clustering data, by establishing a starting point at step 1 as shown in <cross-reference target="DRAWINGS">FIG. 1</cross-reference><highlight><italic>b </italic></highlight>wherein, a given data set having m feature spaces, and each data object (record) is represented as a tuple of m feature vectors. To cluster, a measure of &ldquo;distortion&rdquo; between two data records is needed. Since, different types of features may have radically different statistical distributions, in general, it is unnatural to disregard fundamental differences between various different types of features and to impose a uniform, un-weighted distortion measure across disparate feature spaces. In Section 2 below, a distortion between two data records as a weighted sum of suitable distortion measures on individual component feature vectors is provided; where the distortions on individual components are allowed to be different. In Section 3 below, using a &ldquo;convex programming&rdquo; formulation, the classical Euclidean k-means algorithm is generalized to use the weighted distortion measure. In Section 4 below, optimal feature weights are selected that lead to a clustering that simultaneously minimizes the average intra-cluster dispersion and maximizes the average inter-cluster dispersion along all the feature spaces. In Section 5, an outline evaluation strategy is provided. In Sections 6 and 7, two exemplary uses of the invention are provided for a) clustering data sets with numerical and categorical features; and b) clustering text data sets with words, 2-phrases, and 3-phrases respectively. Using data sets with a known ground truth classification, the clusterings are empirically demonstrated that correspond to the optimal feature weights deliver nearly optimal precision/recall performance. </paragraph>
<paragraph id="P-0031" lvl="0"><number>&lsqb;0031&rsqb;</number> Feature weighting may be thought of as a generalization of feature selection where the latter restricts attention to weights that are either 1 (retain the feature) or 0 (eliminate the feature), see Wettschereck et al., <highlight><italic>Artificial Intelligence Review </italic></highlight>in the article entitled &ldquo;A review and empirical evaluation of feature weighting methods for a class of lazy learning algorithms,&rdquo; Vol. 11, pps. 273-314, 1997. Feature selection in the context of supervised learning has a long history in machine learning, see, for example, for example, see Blum et al., <highlight><italic>Artificial Intelligence, </italic></highlight>&ldquo;Selection of relevant features and examples in machine learning,&rdquo; Vol. 97, pps. 245-271, 1997. Feature selection in the context of unsupervised learning has only recently been systematically studied. </paragraph>
<paragraph id="P-0032" lvl="7"><number>&lsqb;0032&rsqb;</number> 2. Data Model and a Distortion Measure </paragraph>
<paragraph id="P-0033" lvl="0"><number>&lsqb;0033&rsqb;</number> 2.1 Data Model: </paragraph>
<paragraph id="P-0034" lvl="0"><number>&lsqb;0034&rsqb;</number> Assume that a set of data records where each object is a tuple of m component feature vectors are given. A typical data object is written as: x&equals;(F<highlight><subscript>1</subscript></highlight>, F<highlight><subscript>2</subscript></highlight>, . . . F<highlight><subscript>m</subscript></highlight>), where the l-th component feature vector F<highlight><subscript>l,</subscript></highlight>1&lE;1&lE;m, is to be thought of as a column vector and lies in some (abstract) feature space F<highlight><subscript>l</subscript></highlight>. The data object x lies on the m-fold product feature space F&equals;F<highlight><subscript>1</subscript></highlight>&times;F<highlight><subscript>2</subscript></highlight>&times; . . . F<highlight><subscript>m</subscript></highlight>. The feature spaces &lcub;F<highlight><subscript>1</subscript></highlight>&rcub;<highlight><subscript>l&equals;1</subscript></highlight><highlight><superscript>m </superscript></highlight>can be different dimensional and possess different topologies, hence, the data model accommodates heterogeneous types of features. There are two examples of feature spaces that include: </paragraph>
<paragraph id="P-0035" lvl="2"><number>&lsqb;0035&rsqb;</number> Euclidean Case: F<highlight><subscript>l </subscript></highlight>is either <custom-character file="US20030005258A1-20030102-P00900.TIF" wi="20" he="20" id="custom-character-00002"/>f<highlight><subscript>l</subscript></highlight>&gE;1, or some compact submanifold thereof. </paragraph>
<paragraph id="P-0036" lvl="2"><number>&lsqb;0036&rsqb;</number> Spherical Case: F<highlight><subscript>l </subscript></highlight>is the intersection of the f<highlight><subscript>l</subscript></highlight>-dimensional, f<highlight><subscript>l</subscript></highlight>&gE;1, unit sphere with the non-negative orthant of <custom-character file="US20030005258A1-20030102-P00900.TIF" wi="20" he="20" id="custom-character-00003"/><highlight><superscript>f</superscript></highlight><highlight><superscript><highlight><subscript>l</subscript></highlight></superscript></highlight>. </paragraph>
<paragraph id="P-0037" lvl="0"><number>&lsqb;0037&rsqb;</number> 2.2 A Weighted Distortion Measure: </paragraph>
<paragraph id="P-0038" lvl="0"><number>&lsqb;0038&rsqb;</number> Measuring distortion between two given two data records x&equals;(F<highlight><subscript>1</subscript></highlight>, F<highlight><subscript>2</subscript></highlight>, . . . F<highlight><subscript>m</subscript></highlight>) and {tilde over (x)}&equals;({tilde over (F)}<highlight><subscript>1</subscript></highlight>, {tilde over (F)}<highlight><subscript>2</subscript></highlight>, . . . {tilde over (F)}<highlight><subscript>m</subscript></highlight>). For 1&lE;l&lE;m, let D<highlight><subscript>l </subscript></highlight>denote a distortion measure between the corresponding component feature vectors F<highlight><subscript>l </subscript></highlight>and F<highlight><subscript>l</subscript></highlight>. Mathematically, only two needed properties of the distortion function: </paragraph>
<paragraph id="P-0039" lvl="2"><number>&lsqb;0039&rsqb;</number> D<highlight><subscript>l</subscript></highlight>:F<highlight><subscript>l</subscript></highlight>&times;F<highlight><subscript>l</subscript></highlight>&rarr;(0, &infin;). </paragraph>
<paragraph id="P-0040" lvl="2"><number>&lsqb;0040&rsqb;</number> For a fixed F<highlight><subscript>l</subscript></highlight>, D<highlight><subscript>l </subscript></highlight>is convex in {tilde over (F)}<highlight><subscript>l</subscript></highlight>. </paragraph>
<paragraph id="P-0041" lvl="7"><number>&lsqb;0041&rsqb;</number> Euclidean Case </paragraph>
<paragraph id="P-0042" lvl="0"><number>&lsqb;0042&rsqb;</number> The squared-Euclidean distance: </paragraph>
<paragraph lvl="0"><in-line-formula><highlight><italic>D</italic></highlight><highlight><subscript>l</subscript></highlight>(<highlight><italic>F</italic></highlight><highlight><subscript>l</subscript></highlight><highlight><italic>{tilde over (F)}</italic></highlight><highlight><subscript>l</subscript></highlight>)&equals;(<highlight><italic>F</italic></highlight><highlight><subscript>l</subscript></highlight><highlight><italic>&minus;{tilde over (F)}</italic></highlight><highlight><subscript>l</subscript></highlight>)<highlight><superscript>T</superscript></highlight>(<highlight><italic>F</italic></highlight><highlight><subscript>l</subscript></highlight><highlight><italic>&minus;{tilde over (F)}</italic></highlight><highlight><subscript>l</subscript></highlight>) </in-line-formula></paragraph>
<paragraph id="P-0043" lvl="7"><number>&lsqb;0043&rsqb;</number> trivially satisfies the non-negativity and, for &lgr;&isin;&lsqb;0,1&rsqb;, the convexity follows from: D<highlight><subscript>l</subscript></highlight>(F<highlight><subscript>l</subscript></highlight>,&lgr;{tilde over (F)}&prime;<highlight><subscript>l</subscript></highlight>&divide;(1&minus;&lgr;){tilde over (F)}&Prime;<highlight><subscript>l</subscript></highlight>)&lE;&lgr;D<highlight><subscript>l</subscript></highlight>(F<highlight><subscript>l</subscript></highlight>,{tilde over (F)}&prime;<highlight><subscript>l</subscript></highlight>)&plus;(1&minus;&lgr;)D<highlight><subscript>l</subscript></highlight>(F<highlight><subscript>l</subscript></highlight>,{tilde over (F)}&Prime;<highlight><subscript>l</subscript></highlight>). </paragraph>
<paragraph id="P-0044" lvl="7"><number>&lsqb;0044&rsqb;</number> Spherical Case </paragraph>
<paragraph id="P-0045" lvl="0"><number>&lsqb;0045&rsqb;</number> The cosine distance D<highlight><subscript>l</subscript></highlight>(F<highlight><subscript>l</subscript></highlight>, {tilde over (F)}<highlight><subscript>l</subscript></highlight>&equals;1&minus;F<highlight><subscript>l</subscript></highlight><highlight><superscript>T</superscript></highlight>{tilde over (F)}<highlight><subscript>l </subscript></highlight>trivially satisfies the non-negativity and, for &lgr;&isin;&lsqb;0,1&rsqb;, the convexity follows from:  
<math-cwu id="MATH-US-00001">
<number>1</number>
<math>
<mrow>
  <mrow>
    <mrow>
      <msub>
        <mi>D</mi>
        <mi>l</mi>
      </msub>
      <mo>&it;</mo>
      <mrow>
        <mo>(</mo>
        <mrow>
          <msub>
            <mi>F</mi>
            <mi>l</mi>
          </msub>
          <mo>,</mo>
          <mfrac>
            <mrow>
              <mrow>
                <mi>&lambda;</mi>
                <mo>&it;</mo>
                <mstyle>
                  <mtext>&emsp;</mtext>
                </mstyle>
                <mo>&it;</mo>
                <msub>
                  <mover>
                    <mi>F</mi>
                    <mrow>
                      <mo>~</mo>
                      <mi>&prime;</mi>
                    </mrow>
                  </mover>
                  <mi>l</mi>
                </msub>
              </mrow>
              <mo>+</mo>
              <mrow>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <mn>1</mn>
                    <mo>-</mo>
                    <mi>&lambda;</mi>
                  </mrow>
                  <mo>)</mo>
                </mrow>
                <mo>&it;</mo>
                <msub>
                  <mover>
                    <mi>F</mi>
                    <mrow>
                      <mo>~</mo>
                      <mi>&Prime;</mi>
                    </mrow>
                  </mover>
                  <mi>l</mi>
                </msub>
              </mrow>
            </mrow>
            <mrow>
              <mo>&LeftDoubleBracketingBar;</mo>
              <mrow>
                <mrow>
                  <mi>&lambda;</mi>
                  <mo>&it;</mo>
                  <mstyle>
                    <mtext>&emsp;</mtext>
                  </mstyle>
                  <mo>&it;</mo>
                  <msub>
                    <mover>
                      <mi>F</mi>
                      <mrow>
                        <mo>~</mo>
                        <mi>&prime;</mi>
                      </mrow>
                    </mover>
                    <mi>l</mi>
                  </msub>
                </mrow>
                <mo>+</mo>
                <mrow>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <mn>1</mn>
                      <mo>-</mo>
                      <mi>&lambda;</mi>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                  <mo>&it;</mo>
                  <msub>
                    <mover>
                      <mi>F</mi>
                      <mrow>
                        <mo>~</mo>
                        <mi>&Prime;</mi>
                      </mrow>
                    </mover>
                    <mi>l</mi>
                  </msub>
                </mrow>
              </mrow>
              <mo>&RightDoubleBracketingBar;</mo>
            </mrow>
          </mfrac>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mrow>
    <mo>&leq;</mo>
    <mrow>
      <mrow>
        <mi>&lambda;</mi>
        <mo>&it;</mo>
        <mstyle>
          <mtext>&emsp;</mtext>
        </mstyle>
        <mo>&it;</mo>
        <mrow>
          <msub>
            <mi>D</mi>
            <mi>l</mi>
          </msub>
          <mo>&it;</mo>
          <mrow>
            <mo>(</mo>
            <mrow>
              <msub>
                <mi>F</mi>
                <mi>l</mi>
              </msub>
              <mo>,</mo>
              <mover>
                <msub>
                  <mi>F</mi>
                  <mi>l</mi>
                </msub>
                <msup>
                  <mo>~</mo>
                  <mi>&prime;</mi>
                </msup>
              </mover>
            </mrow>
            <mo>)</mo>
          </mrow>
        </mrow>
      </mrow>
      <mo>+</mo>
      <mrow>
        <mrow>
          <mo>(</mo>
          <mrow>
            <mn>1</mn>
            <mo>-</mo>
            <mi>&lambda;</mi>
          </mrow>
          <mo>)</mo>
        </mrow>
        <mo>&it;</mo>
        <mrow>
          <msub>
            <mi>D</mi>
            <mi>l</mi>
          </msub>
          <mo>&it;</mo>
          <mrow>
            <mo>(</mo>
            <mrow>
              <msub>
                <mi>F</mi>
                <mi>l</mi>
              </msub>
              <mo>,</mo>
              <mover>
                <msub>
                  <mi>F</mi>
                  <mi>l</mi>
                </msub>
                <msup>
                  <mo>~</mo>
                  <mi>&Prime;</mi>
                </msup>
              </mover>
            </mrow>
            <mo>)</mo>
          </mrow>
        </mrow>
      </mrow>
    </mrow>
  </mrow>
  <mo>,</mo>
</mrow>
</math>
<mathematica-file id="MATHEMATICA-00001" file="US20030005258A1-20030102-M00001.NB"/>
<image id="EMI-M00001" wi="216.027" he="31.9221" file="US20030005258A1-20030102-M00001.TIF" imf="TIFF" ti="MF"/>
</math-cwu>
</paragraph>
<paragraph id="P-0046" lvl="7"><number>&lsqb;0046&rsqb;</number> where &par; . . . &par; denotes the Euclidean-norm. The division by: &par;&lgr;{tilde over (F)}&prime;<highlight><subscript>l</subscript></highlight>&plus;(1&minus;&lgr;){tilde over (F)}&prime;<highlight><subscript>1</subscript></highlight>&par; ensures that the second argument of D<highlight><subscript>l </subscript></highlight>is unit vector. Geometrically, the convexity along the geodesic are connecting the two unit vectors {tilde over (F)}&prime;<highlight><subscript>l </subscript></highlight>and {tilde over (F)}&Prime;<highlight><subscript>l </subscript></highlight>and not along the chord connecting two are defined. Given m valid distortion measures &lcub;D<highlight><subscript>l</subscript></highlight>&rcub;<highlight><subscript>l&equals;1</subscript></highlight><highlight><superscript>m </superscript></highlight>between the corresponding m component feature vectors of x and {tilde over (x)}, a weighted distortion measure between x and {tilde over (x)} is defined as:  
<math-cwu id="MATH-US-00002">
<number>2</number>
<math>
<mrow>
  <mrow>
    <mrow>
      <msup>
        <mi>D</mi>
        <mi>&alpha;</mi>
      </msup>
      <mo>&it;</mo>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mi>x</mi>
          <mo>,</mo>
          <mover>
            <mi>x</mi>
            <mo>~</mo>
          </mover>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
      <munderover>
        <mo>&Sum;</mo>
        <mrow>
          <mi>l</mi>
          <mo>=</mo>
          <mn>1</mn>
        </mrow>
        <mi>m</mi>
      </munderover>
      <mo>&it;</mo>
      <mrow>
        <msub>
          <mi>&alpha;</mi>
          <mi>l</mi>
        </msub>
        <mo>&it;</mo>
        <mrow>
          <msub>
            <mi>D</mi>
            <mi>l</mi>
          </msub>
          <mo>&it;</mo>
          <mrow>
            <mo>(</mo>
            <mrow>
              <msub>
                <mi>F</mi>
                <mrow>
                  <mi>l</mi>
                  <mo>,</mo>
                </mrow>
              </msub>
              <mo>&it;</mo>
              <mover>
                <msub>
                  <mi>F</mi>
                  <mi>l</mi>
                </msub>
                <mo>~</mo>
              </mover>
            </mrow>
            <mo>)</mo>
          </mrow>
        </mrow>
      </mrow>
    </mrow>
  </mrow>
  <mo>,</mo>
</mrow>
</math>
<mathematica-file id="MATHEMATICA-00002" file="US20030005258A1-20030102-M00002.NB"/>
<image id="EMI-M00002" wi="216.027" he="24.01245" file="US20030005258A1-20030102-M00002.TIF" imf="TIFF" ti="MF"/>
</math-cwu>
</paragraph>
<paragraph id="P-0047" lvl="7"><number>&lsqb;0047&rsqb;</number> where the feature weights &lcub;&agr;<highlight><subscript>l</subscript></highlight>&rcub;<highlight><subscript>l&equals;1</subscript></highlight><highlight><superscript>m </superscript></highlight>are non-negative and sum to 1 and &agr;&equals;(&agr;<highlight><subscript>l</subscript></highlight>, &agr;<highlight><subscript>&part;</subscript></highlight>, . . . &agr;<highlight><subscript>m</subscript></highlight>). The weighted distortion D<highlight><superscript>&agr;</superscript></highlight> is a convex combination of convex distortion measures, and hence, for a fixed x,D<highlight><superscript>&agr;</superscript></highlight> is the convex in {tilde over (x)}. The feature weights &lcub;&agr;<highlight><subscript>l</subscript></highlight>&rcub;<highlight><subscript>l&equals;1</subscript></highlight><highlight><superscript>m </superscript></highlight>are enabled in the method, and are used to assign different relative importance to component feature vectors. In Section 4 below, appropriate choice of these parameters is made. </paragraph>
<paragraph id="P-0048" lvl="7"><number>&lsqb;0048&rsqb;</number> 3. k-Means with Weighted Distortion </paragraph>
<paragraph id="P-0049" lvl="0"><number>&lsqb;0049&rsqb;</number> 3.1. The Problem: </paragraph>
<paragraph id="P-0050" lvl="0"><number>&lsqb;0050&rsqb;</number> Suppose that n-data records are given such that </paragraph>
<paragraph lvl="0"><in-line-formula><highlight><italic>x,&equals;</italic></highlight>(<highlight><italic>F</italic></highlight><highlight><subscript>(i,1)</subscript></highlight><highlight><italic>,F</italic></highlight><highlight><subscript>(i,2</subscript></highlight>)<highlight><italic>, . . . F</italic></highlight><highlight><subscript>(l, m)</subscript></highlight>), 1&lE;<highlight><italic>i&lE;n, </italic></highlight></in-line-formula></paragraph>
<paragraph id="P-0051" lvl="7"><number>&lsqb;0051&rsqb;</number> where the l-th, 1&lE;l&lE;m, component feature vector of every data record is in the feature space F<highlight><subscript>l</subscript></highlight>. Partitioning of the data set &lcub;x<highlight><subscript>i</subscript></highlight>&rcub;<highlight><subscript>i&equals;1</subscript></highlight><highlight><superscript>n </superscript></highlight>is sought into k-disjoint clusters &lcub;&pgr;<highlight><subscript>u</subscript></highlight>&rcub;<highlight><subscript>u&equals;1</subscript></highlight><highlight><superscript>l</superscript></highlight>. </paragraph>
<paragraph id="P-0052" lvl="0"><number>&lsqb;0052&rsqb;</number> 3.2 Generalized Centroids: </paragraph>
<paragraph id="P-0053" lvl="0"><number>&lsqb;0053&rsqb;</number> Given a partitioning &lcub;&lgr;<highlight><subscript>u</subscript></highlight>&rcub;<highlight><subscript>u&equals;1</subscript></highlight><highlight><superscript>k</superscript></highlight>, for each partition &pgr;<highlight><subscript>u</subscript></highlight>, write the corresponding generalized centroid as </paragraph>
<paragraph lvl="0"><in-line-formula><highlight><italic>c</italic></highlight><highlight><subscript>u</subscript></highlight>&equals;(<highlight><italic>c</italic></highlight><highlight><subscript>(u,1)</subscript></highlight><highlight><italic>,c</italic></highlight><highlight><subscript>(u,2)</subscript></highlight><highlight><italic>, . . . c</italic></highlight><highlight><subscript>(u,m)</subscript></highlight>), </in-line-formula></paragraph>
<paragraph id="P-0054" lvl="7"><number>&lsqb;0054&rsqb;</number> where, for 1&lE;l&lE;m, the l-th component c<highlight><subscript>(u,l) </subscript></highlight>is in F<highlight><subscript>l</subscript></highlight>.c<highlight><subscript>u </subscript></highlight>as the solution of the following &ldquo;convex programming&rdquo; problem is defined as:  
<math-cwu id="MATH-US-00003">
<number>3</number>
<math>
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <msub>
          <mi>c</mi>
          <mi>u</mi>
        </msub>
        <mo>=</mo>
        <mrow>
          <mrow>
            <munder>
              <mrow>
                <mi>arg</mi>
                <mo>&it;</mo>
                <mi>min</mi>
              </mrow>
              <mrow>
                <mover>
                  <mi>x</mi>
                  <mo>~</mo>
                </mover>
                <mo>&it;</mo>
                <mstyle>
                  <mtext>&emsp;</mtext>
                </mstyle>
                <mo>&Element;</mo>
                <mstyle>
                  <mtext>&emsp;</mtext>
                </mstyle>
                <mo>&it;</mo>
                <mi>f</mi>
              </mrow>
            </munder>
            <mo>&af;</mo>
            <mrow>
              <mo>(</mo>
              <mrow>
                <munder>
                  <mo>&Sum;</mo>
                  <mrow>
                    <mi>x</mi>
                    <mo>&it;</mo>
                    <mstyle>
                      <mtext>&emsp;</mtext>
                    </mstyle>
                    <mo>&Element;</mo>
                    <mstyle>
                      <mtext>&emsp;</mtext>
                    </mstyle>
                    <mo>&it;</mo>
                    <msub>
                      <mi>&pi;</mi>
                      <mi>u</mi>
                    </msub>
                  </mrow>
                </munder>
                <mo>&it;</mo>
                <msup>
                  <mi>D</mi>
                  <mrow>
                    <mi>&alpha;</mi>
                    <mo>&af;</mo>
                    <mrow>
                      <mo>(</mo>
                      <mrow>
                        <mi>x</mi>
                        <mo>,</mo>
                        <mover>
                          <mi>x</mi>
                          <mo>~</mo>
                        </mover>
                      </mrow>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                </msup>
              </mrow>
              <mo>)</mo>
            </mrow>
          </mrow>
          <mo>.</mo>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>1</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
<mathematica-file id="MATHEMATICA-00003" file="US20030005258A1-20030102-M00003.NB"/>
<image id="EMI-M00003" wi="216.027" he="28.09485" file="US20030005258A1-20030102-M00003.TIF" imf="TIFF" ti="MF"/>
</math-cwu>
</paragraph>
<paragraph id="P-0055" lvl="7"><number>&lsqb;0055&rsqb;</number> In an empirical average sense, the generalized centroid may be thought of as being the closest in D<highlight><superscript>&agr;</superscript></highlight> to all the data records in the cluster &pgr;<highlight><subscript>u</subscript></highlight>. The key to solving (1) is to observe that D<highlight><superscript>&agr;</superscript></highlight> is component-wise-convex, and, hence, equation (1) can be solved by separately solving for each of its m components c<highlight><subscript>(u,l)</subscript></highlight>, 1&lE;1&lE;m. In other words, the following m &ldquo;convex programming&rdquo; problems is solved:  
<math-cwu id="MATH-US-00004">
<number>4</number>
<math>
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <msub>
          <mi>c</mi>
          <mrow>
            <mo>(</mo>
            <mrow>
              <mi>u</mi>
              <mo>,</mo>
              <mi>l</mi>
            </mrow>
            <mo>)</mo>
          </mrow>
        </msub>
        <mo>=</mo>
        <mrow>
          <mrow>
            <munder>
              <mi>argmin</mi>
              <mrow>
                <msub>
                  <mover>
                    <mi>F</mi>
                    <mo>~</mo>
                  </mover>
                  <mi>l</mi>
                </msub>
                <mo>&it;</mo>
                <mstyle>
                  <mtext>&emsp;</mtext>
                </mstyle>
                <mo>&Element;</mo>
                <mstyle>
                  <mtext>&emsp;</mtext>
                </mstyle>
                <mo>&it;</mo>
                <msub>
                  <mi>F</mi>
                  <mi>l</mi>
                </msub>
              </mrow>
            </munder>
            <mo>&af;</mo>
            <mrow>
              <mo>(</mo>
              <mrow>
                <munder>
                  <mo>&Sum;</mo>
                  <mrow>
                    <mi>x</mi>
                    <mo>&it;</mo>
                    <mstyle>
                      <mtext>&emsp;</mtext>
                    </mstyle>
                    <mo>&Element;</mo>
                    <mstyle>
                      <mtext>&emsp;</mtext>
                    </mstyle>
                    <mo>&it;</mo>
                    <mi>&pi;</mi>
                  </mrow>
                </munder>
                <mo>&it;</mo>
                <mrow>
                  <msub>
                    <mi>D</mi>
                    <mi>l</mi>
                  </msub>
                  <mo>&af;</mo>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <msub>
                        <mi>F</mi>
                        <mi>l</mi>
                      </msub>
                      <mo>,</mo>
                      <msub>
                        <mover>
                          <mi>F</mi>
                          <mo>~</mo>
                        </mover>
                        <mi>l</mi>
                      </msub>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                </mrow>
              </mrow>
              <mo>)</mo>
            </mrow>
          </mrow>
          <mo>.</mo>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>2</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
<mathematica-file id="MATHEMATICA-00004" file="US20030005258A1-20030102-M00004.NB"/>
<image id="EMI-M00004" wi="216.027" he="28.09485" file="US20030005258A1-20030102-M00004.TIF" imf="TIFF" ti="MF"/>
</math-cwu>
</paragraph>
<paragraph id="P-0056" lvl="7"><number>&lsqb;0056&rsqb;</number> For the two feature spaces of interest (others as well), the solution of equation (2) can be written in a closed form using a Euclidean and Spherical case, respectively:  
<math-cwu id="MATH-US-00005">
<number>5</number>
<math>
<mrow>
  <msub>
    <mi>c</mi>
    <mrow>
      <mo>(</mo>
      <mrow>
        <mi>u</mi>
        <mo>,</mo>
        <mi>l</mi>
      </mrow>
      <mo>)</mo>
    </mrow>
  </msub>
  <mo>=</mo>
  <mrow>
    <mo>{</mo>
    <mtable>
      <mtr>
        <mtd>
          <mrow>
            <mfrac>
              <mn>1</mn>
              <munder>
                <mo>&Sum;</mo>
                <mrow>
                  <mi>&aleph;</mi>
                  <mo>&it;</mo>
                  <mstyle>
                    <mtext>&emsp;</mtext>
                  </mstyle>
                  <mo>&Element;</mo>
                  <mstyle>
                    <mtext>&emsp;</mtext>
                  </mstyle>
                  <mo>&it;</mo>
                  <msubsup>
                    <mi>&pi;</mi>
                    <mi>u</mi>
                    <mn>1</mn>
                  </msubsup>
                </mrow>
              </munder>
            </mfrac>
            <mo>&it;</mo>
            <mrow>
              <munder>
                <mo>&Sum;</mo>
                <mrow>
                  <mi>&aleph;</mi>
                  <mo>&it;</mo>
                  <mstyle>
                    <mtext>&emsp;</mtext>
                  </mstyle>
                  <mo>&Element;</mo>
                  <mstyle>
                    <mtext>&emsp;</mtext>
                  </mstyle>
                  <mo>&it;</mo>
                  <msub>
                    <mi>&pi;</mi>
                    <mi>u</mi>
                  </msub>
                </mrow>
              </munder>
              <mo>&it;</mo>
              <msub>
                <mi>F</mi>
                <mi>l</mi>
              </msub>
            </mrow>
          </mrow>
        </mtd>
      </mtr>
      <mtr>
        <mtd>
          <mfrac>
            <mrow>
              <msub>
                <mo>&Sum;</mo>
                <mrow>
                  <mi>&aleph;</mi>
                  <mo>&it;</mo>
                  <mstyle>
                    <mtext>&emsp;</mtext>
                  </mstyle>
                  <mo>&Element;</mo>
                  <mstyle>
                    <mtext>&emsp;</mtext>
                  </mstyle>
                  <mo>&it;</mo>
                  <msubsup>
                    <mi>&pi;</mi>
                    <mi>u</mi>
                    <mn>1</mn>
                  </msubsup>
                </mrow>
              </msub>
              <mo>&it;</mo>
              <msub>
                <mi>F</mi>
                <mi>l</mi>
              </msub>
            </mrow>
            <mrow>
              <mo>&LeftDoubleBracketingBar;</mo>
              <mrow>
                <munder>
                  <mo>&Sum;</mo>
                  <mrow>
                    <mi>&aleph;</mi>
                    <mo>&it;</mo>
                    <mstyle>
                      <mtext>&emsp;</mtext>
                    </mstyle>
                    <mo>&Element;</mo>
                    <mstyle>
                      <mtext>&emsp;</mtext>
                    </mstyle>
                    <mo>&it;</mo>
                    <msub>
                      <mi>&pi;</mi>
                      <mi>u</mi>
                    </msub>
                  </mrow>
                </munder>
                <mo>&it;</mo>
                <msub>
                  <mi>F</mi>
                  <mi>l</mi>
                </msub>
              </mrow>
              <mo>&RightDoubleBracketingBar;</mo>
            </mrow>
          </mfrac>
        </mtd>
      </mtr>
    </mtable>
  </mrow>
</mrow>
</math>
<mathematica-file id="MATHEMATICA-00005" file="US20030005258A1-20030102-M00005.NB"/>
<image id="EMI-M00005" wi="216.027" he="66.9627" file="US20030005258A1-20030102-M00005.TIF" imf="TIFF" ti="MF"/>
</math-cwu>
</paragraph>
<paragraph id="P-0057" lvl="7"><number>&lsqb;0057&rsqb;</number> where x&equals;(F<highlight><subscript>1</subscript></highlight>, F<highlight><subscript>2</subscript></highlight>, . . . , F<highlight><subscript>m</subscript></highlight>) </paragraph>
<paragraph id="P-0058" lvl="0"><number>&lsqb;0058&rsqb;</number> 3.3 The Method: </paragraph>
<paragraph id="P-0059" lvl="0"><number>&lsqb;0059&rsqb;</number> Referring to <cross-reference target="DRAWINGS">FIG. 1</cross-reference><highlight><italic>b, </italic></highlight>the method of the invention uses the formulation of equation (1) using the steps below, wherein the distortion is measured of each individual cluster &pgr;<highlight><subscript>u</subscript></highlight>,1&lE;u&lE;k, as:  
<math-cwu id="MATH-US-00006">
<number>6</number>
<math>
<mrow>
  <mrow>
    <munder>
      <mo>&Sum;</mo>
      <mrow>
        <mi>x</mi>
        <mo>&it;</mo>
        <mstyle>
          <mtext>&emsp;</mtext>
        </mstyle>
        <mo>&Element;</mo>
        <mstyle>
          <mtext>&emsp;</mtext>
        </mstyle>
        <mo>&it;</mo>
        <msub>
          <mi>&pi;</mi>
          <mi>u</mi>
        </msub>
      </mrow>
    </munder>
    <mo>&it;</mo>
    <mrow>
      <msup>
        <mi>D</mi>
        <mi>a</mi>
      </msup>
      <mo>&it;</mo>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mi>x</mi>
          <mo>,</mo>
          <msub>
            <mi>c</mi>
            <mi>u</mi>
          </msub>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mrow>
  </mrow>
  <mo>,</mo>
</mrow>
</math>
<mathematica-file id="MATHEMATICA-00006" file="US20030005258A1-20030102-M00006.NB"/>
<image id="EMI-M00006" wi="216.027" he="21.12075" file="US20030005258A1-20030102-M00006.TIF" imf="TIFF" ti="MF"/>
</math-cwu>
</paragraph>
<paragraph id="P-0060" lvl="7"><number>&lsqb;0060&rsqb;</number> and the quality of the entire partitioning &lcub;&pgr;<highlight><subscript>u</subscript></highlight>&rcub;<highlight><subscript>u&equals;1</subscript></highlight><highlight><superscript>k </superscript></highlight>as the combined distortion of all the k clusters:  
<math-cwu id="MATH-US-00007">
<number>7</number>
<math>
<mrow>
  <munderover>
    <mo>&Sum;</mo>
    <mrow>
      <mi>u</mi>
      <mo>=</mo>
      <mn>1</mn>
    </mrow>
    <mi>k</mi>
  </munderover>
  <mo>&it;</mo>
  <mrow>
    <munder>
      <mo>&Sum;</mo>
      <mrow>
        <mi>x</mi>
        <mo>&it;</mo>
        <mstyle>
          <mtext>&emsp;</mtext>
        </mstyle>
        <mo>&Element;</mo>
        <mstyle>
          <mtext>&emsp;</mtext>
        </mstyle>
        <mo>&it;</mo>
        <msub>
          <mi>&pi;</mi>
          <mi>u</mi>
        </msub>
      </mrow>
    </munder>
    <mo>&it;</mo>
    <mrow>
      <msup>
        <mi>D</mi>
        <mi>&alpha;</mi>
      </msup>
      <mo>&it;</mo>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mi>x</mi>
          <mo>,</mo>
          <msub>
            <mi>c</mi>
            <mi>u</mi>
          </msub>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mrow>
  </mrow>
</mrow>
</math>
<mathematica-file id="MATHEMATICA-00007" file="US20030005258A1-20030102-M00007.NB"/>
<image id="EMI-M00007" wi="216.027" he="25.9119" file="US20030005258A1-20030102-M00007.TIF" imf="TIFF" ti="MF"/>
</math-cwu>
</paragraph>
<paragraph id="P-0061" lvl="7"><number>&lsqb;0061&rsqb;</number> What is sought is k-disjoint clusters such that equation (3) as follows is minimized wherein these k-disjoint clusters are: </paragraph>
<paragraph lvl="0"><in-line-formula>&pgr;<highlight><subscript>1</subscript></highlight><highlight><superscript>&dagger;</superscript></highlight>, &pgr;<highlight><subscript>2</subscript></highlight><highlight><superscript>&dagger;</superscript></highlight>; . . . ,&pgr;<highlight><subscript>k</subscript></highlight><highlight><superscript>&dagger;</superscript></highlight>, and </in-line-formula> 
<math-cwu id="MATH-US-00008">
<number>8</number>
<math>
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <msubsup>
            <mrow>
              <mo>{</mo>
              <msubsup>
                <mi>&pi;</mi>
                <mi>u</mi>
                <mi>&dagger;</mi>
              </msubsup>
              <mo>}</mo>
            </mrow>
            <mrow>
              <mi>u</mi>
              <mo>=</mo>
              <mn>1</mn>
            </mrow>
            <mi>k</mi>
          </msubsup>
          <mo>=</mo>
          <mrow>
            <munder>
              <mi>argmin</mi>
              <msubsup>
                <mrow>
                  <mo>{</mo>
                  <msub>
                    <mi>&pi;</mi>
                    <mi>u</mi>
                  </msub>
                  <mo>}</mo>
                </mrow>
                <mrow>
                  <mi>u</mi>
                  <mo>=</mo>
                  <mn>1</mn>
                </mrow>
                <mi>k</mi>
              </msubsup>
            </munder>
            <mo>&af;</mo>
            <mrow>
              <mo>(</mo>
              <mrow>
                <munderover>
                  <mo>&Sum;</mo>
                  <mrow>
                    <mi>u</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                  </mrow>
                  <mi>k</mi>
                </munderover>
                <mo>&it;</mo>
                <mrow>
                  <munder>
                    <mo>&Sum;</mo>
                    <mrow>
                      <mi>x</mi>
                      <mo>&it;</mo>
                      <mstyle>
                        <mtext>&emsp;</mtext>
                      </mstyle>
                      <mo>&Element;</mo>
                      <mstyle>
                        <mtext>&emsp;</mtext>
                      </mstyle>
                      <mo>&it;</mo>
                      <msub>
                        <mi>&pi;</mi>
                        <mi>u</mi>
                      </msub>
                    </mrow>
                  </munder>
                  <mo>&it;</mo>
                  <mrow>
                    <msup>
                      <mi>D</mi>
                      <mi>a</mi>
                    </msup>
                    <mo>&af;</mo>
                    <mrow>
                      <mo>(</mo>
                      <mrow>
                        <mi>x</mi>
                        <mo>,</mo>
                        <msub>
                          <mi>c</mi>
                          <mi>u</mi>
                        </msub>
                      </mrow>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                </mrow>
              </mrow>
              <mo>)</mo>
            </mrow>
          </mrow>
        </mrow>
        <mo>,</mo>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>3</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
<mathematica-file id="MATHEMATICA-00008" file="US20030005258A1-20030102-M00008.NB"/>
<image id="EMI-M00008" wi="216.027" he="27.13095" file="US20030005258A1-20030102-M00008.TIF" imf="TIFF" ti="MF"/>
</math-cwu>
</paragraph>
<paragraph id="P-0062" lvl="7"><number>&lsqb;0062&rsqb;</number> where the feature weights &agr;&equals;(&agr;<highlight><subscript>1</subscript></highlight>,&agr;<highlight><subscript>2</subscript></highlight>, . . . &agr;<highlight><subscript>m</subscript></highlight>) are fixed. When only one of the weights &lcub;&agr;<highlight><subscript>l</subscript></highlight>&rcub;<highlight><subscript>l&equals;1</subscript></highlight><highlight><superscript>m </superscript></highlight>is nonzero, the maximization problem (3) is known to be NP-complete, meaning no known algorithm exists for solving the problem in polynomial time. K-means is used, which is an efficient and effective data clustering algorithm. Moreover, k-means can be thought of as a gradient ascent method, and, hence, never increases the &ldquo;objective&rdquo; function and eventually converges to a local minima. </paragraph>
<paragraph id="P-0063" lvl="0"><number>&lsqb;0063&rsqb;</number> In <cross-reference target="DRAWINGS">FIG. 1</cross-reference><highlight><italic>b, </italic></highlight>an overview of the processing components is shown for clustering data. These processing components perform data clustering on data records stored on a storage medium such as the computer system&apos;s hard disk drive <highlight><bold>27</bold></highlight>. The data records typically are made up of a number of data fields or attributes. Examples of such data records is discussed below in the two examples of the invention. </paragraph>
<paragraph id="P-0064" lvl="0"><number>&lsqb;0064&rsqb;</number> The components that perform the clustering require three inputs: the number of clusters K, a set of K initial starting points, and the data records to be clustered. The clustering of data by these components produces a final solution as shown in step 5 as an output. Each of the K clusters of this final solution is represented by its mean (centroid) where each mean has d components equal to the number of attributes of the data records and a fixed feature weight of the m-feature spaces. </paragraph>
<paragraph id="P-0065" lvl="0"><number>&lsqb;0065&rsqb;</number> A refinement of feature weights in step 4 below produces &ldquo;better&rdquo; clustering from the data records to be clustered using the methodology of the invention. A most favorable refined starting point produced using a &ldquo;good&rdquo; approximation of an initial starting point is discussed below that would move the set of starting points that are closer to the modes of the data distribution. </paragraph>
<paragraph id="P-0066" lvl="0"><number>&lsqb;0066&rsqb;</number> At Step 1: </paragraph>
<paragraph id="P-0067" lvl="0"><number>&lsqb;0067&rsqb;</number> An initial point with an arbitrary partitioning of the data records of the data records to be evaluated is provided, wherein, &lcub;&pgr;<highlight><subscript>u</subscript></highlight><highlight><superscript>(o)</superscript></highlight>&rcub;<highlight><subscript>u&equals;1</subscript></highlight><highlight><superscript>k</superscript></highlight>. Let &lcub;c<highlight><subscript>u</subscript></highlight><highlight><superscript>(o)</superscript></highlight>&rcub;<highlight><subscript>u&equals;1</subscript></highlight><highlight><superscript>k </superscript></highlight>denote the generalized centroids associated with the given partitioning. Set the index of iteration t&equals;0. A choice of the initial partitioning is quite crucial to finding a &ldquo;good&rdquo; local minima; to achieve this, see a method for doing this technique as taught in U.S. Pat. No. 6,115,708 hereby incorporated by reference. </paragraph>
<paragraph id="P-0068" lvl="0"><number>&lsqb;0068&rsqb;</number> At Step 2: </paragraph>
<paragraph id="P-0069" lvl="0"><number>&lsqb;0069&rsqb;</number> For each data record x<highlight><subscript>i</subscript></highlight>,1&lE;i&lE;n, find the generalized centroid that is closest to x<highlight><subscript>i</subscript></highlight>. Now, for 1&lE;u&lE;k, compute the new partitioning &lcub;&pgr;<highlight><subscript>u</subscript></highlight><highlight><superscript>(t&plus;1)</superscript></highlight>&rcub;<highlight><subscript>u&equals;1</subscript></highlight><highlight><superscript>k </superscript></highlight>induced by the old generalized centroids: </paragraph>
<paragraph lvl="0"><in-line-formula>&lcub;<highlight><italic>c</italic></highlight><highlight><subscript>u</subscript></highlight><highlight><superscript>(t)</superscript></highlight>&rcub;<highlight><subscript>u&equals;1</subscript></highlight><highlight><superscript>k</superscript></highlight>:&pgr;<highlight><subscript>u</subscript></highlight><highlight><superscript>(t&plus;1)</superscript></highlight><highlight><italic>&equals;&lcub;x&isin;&lcub;x</italic></highlight><highlight><subscript>i</subscript></highlight>&rcub;<highlight><subscript>i&equals;1</subscript></highlight><highlight><superscript>n</superscript></highlight><highlight><italic>:D</italic></highlight><highlight><superscript>&agr;</superscript></highlight>(<highlight><italic>x,c</italic></highlight><highlight><subscript>u</subscript></highlight><highlight><superscript>(t)</superscript></highlight>)&lE;<highlight><italic>D</italic></highlight><highlight><superscript>&agr;</superscript></highlight>(<highlight><italic>x,c</italic></highlight><highlight><subscript>v</subscript></highlight><highlight><superscript>(t)</superscript></highlight>),1<highlight><italic>&lE;v&lE;k&rcub;.</italic></highlight>&emsp;&emsp;(4) </in-line-formula></paragraph>
<paragraph id="P-0070" lvl="7"><number>&lsqb;0070&rsqb;</number> In words, &pgr;<highlight><subscript>u</subscript></highlight><highlight><superscript>(t&plus;1) </superscript></highlight>is the set of all data records that are closest to the generalized centroid c<highlight><subscript>u</subscript></highlight><highlight><superscript>(t)</superscript></highlight>. If some data object is simultaneously closest to more than one generalized centroid, then it is randomly assigned to one of the clusters. Clusters defined using equation (4) are known as Voronoi or Dirichlet partitions. </paragraph>
<paragraph id="P-0071" lvl="0"><number>&lsqb;0071&rsqb;</number> Step 3: </paragraph>
<paragraph id="P-0072" lvl="0"><number>&lsqb;0072&rsqb;</number> Compute the new generalized centroids &lcub;c<highlight><subscript>u</subscript></highlight><highlight><superscript>(t&plus;1)</superscript></highlight>&rcub;<highlight><subscript>u&equals;1</subscript></highlight><highlight><superscript>k </superscript></highlight>&pgr;<highlight><subscript>u</subscript></highlight><highlight><superscript>&dagger;</superscript></highlight>&equals;&pgr;<highlight><subscript>u</subscript></highlight><highlight><superscript>(t&plus;1) </superscript></highlight>corresponding to the partitioning computed in equation (4) by using equations (1)-(2) where instead of &pgr;<highlight><subscript>u</subscript></highlight>, the following is used: &pgr;<highlight><subscript>u</subscript></highlight><highlight><superscript>(t&plus;1)</superscript></highlight>. </paragraph>
<paragraph id="P-0073" lvl="0"><number>&lsqb;0073&rsqb;</number> Step 4: </paragraph>
<paragraph id="P-0074" lvl="0"><number>&lsqb;0074&rsqb;</number> Optionally refine the features weights and store in memory using the method discussed in Section 4 below. </paragraph>
<paragraph id="P-0075" lvl="0"><number>&lsqb;0075&rsqb;</number> Step 5: </paragraph>
<paragraph id="P-0076" lvl="0"><number>&lsqb;0076&rsqb;</number> If some &ldquo;stopping criterion&rdquo; is met, then set and set c<highlight><subscript>u</subscript></highlight><highlight><superscript>&dagger;</superscript></highlight>&equals;c<highlight><subscript>u</subscript></highlight><highlight><superscript>(t&plus;1) </superscript></highlight>for 1&lE;u&lE;k, and exit at step 5 as the final clustering solution. Otherwise, increment t by 1, and go to step 2 above and repeat the process. An example of a stopping criterion is: Stop if the change in the &ldquo;objective&rdquo; function as defined in equation (6) below, between two successive iterations, is less than a specified threshold, for example the generalized centroids do not move and being less than a small floating point number. </paragraph>
<paragraph id="P-0077" lvl="7"><number>&lsqb;0077&rsqb;</number> 4. Choice of the Feature Weights: </paragraph>
<paragraph id="P-0078" lvl="0"><number>&lsqb;0078&rsqb;</number> Throughout this section, fix the number of clusters k&lE;2 and fix the initial &pgr;<highlight><subscript>u</subscript></highlight><highlight><superscript>&dagger;</superscript></highlight>&equals;&pgr;<highlight><subscript>u</subscript></highlight><highlight><superscript>(t&plus;1) </superscript></highlight>partitioning used by the k-means algorithm in Step 1 above. Let:  
<math-cwu id="MATH-US-00009">
<number>9</number>
<math>
<mrow>
  <mrow>
    <mi>&Delta;</mi>
    <mo>=</mo>
    <mrow>
      <mo>{</mo>
      <mrow>
        <mi>&alpha;</mi>
        <mo>&it;</mo>
        <mrow>
          <mo>&LeftBracketingBar;</mo>
          <mrow>
            <mrow>
              <mrow>
                <munderover>
                  <mo>&Sum;</mo>
                  <mrow>
                    <mi>l</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                  </mrow>
                  <mi>m</mi>
                </munderover>
                <mo>&it;</mo>
                <msub>
                  <mi>&alpha;</mi>
                  <mi>l</mi>
                </msub>
              </mrow>
              <mo>=</mo>
              <mn>1</mn>
            </mrow>
            <mo>,</mo>
            <mrow>
              <msub>
                <mi>&alpha;</mi>
                <mi>l</mi>
              </msub>
              <mo>&GreaterEqual;</mo>
              <mn>0</mn>
            </mrow>
            <mo>,</mo>
            <mrow>
              <mn>1</mn>
              <mo>&leq;</mo>
              <mi>l</mi>
              <mo>&leq;</mo>
              <mi>m</mi>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mrow>
  </mrow>
  <mo>}</mo>
</mrow>
</math>
<mathematica-file id="MATHEMATICA-00009" file="US20030005258A1-20030102-M00009.NB"/>
<image id="EMI-M00009" wi="216.027" he="24.01245" file="US20030005258A1-20030102-M00009.TIF" imf="TIFF" ti="MF"/>
</math-cwu>
</paragraph>
<paragraph id="P-0079" lvl="7"><number>&lsqb;0079&rsqb;</number> denote the set of all possible feature weights. Given a feature weight &agr;&isin;&Dgr;, &pgr;(&agr;)&equals;&lcub;&pgr;<highlight><subscript>u</subscript></highlight><highlight><superscript>t</superscript></highlight>&rcub;<highlight><subscript>u&equals;1</subscript></highlight><highlight><superscript>k </superscript></highlight>denote the partitioning obtained by running the k-means algorithm with the fixed initial partitioning and the given feature weight. In principle, the k-means algorithm for every possible feature weight in &Dgr; is run. From the set of all possible clusterings &lcub;&pgr;(&agr;)&verbar;&agr;&isin;&Dgr;&rcub;, by selecting a clustering that is in some sense the best, by introducing a figure-of-merit to compare various clusterings. Fix a partitioning &Pi;(&agr;). By focusing on how well this partitioning clusters along the l-th, 1&lE;l&lE;m, component feature vector, the average within clusters distortion can be defined and average between cluster distortion along the l-th component feature vector, respectively, as:  
<math-cwu id="MATH-US-00010">
<number>10</number>
<math>
  <mrow>
    <mrow>
      <msub>
        <mi>&Gamma;</mi>
        <mi>l</mi>
      </msub>
      <mo>&it;</mo>
      <mrow>
        <mo>(</mo>
        <mi>&alpha;</mi>
        <mo>)</mo>
      </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
      <munderover>
        <mo>&Sum;</mo>
        <mrow>
          <mi>u</mi>
          <mo>=</mo>
          <mn>1</mn>
        </mrow>
        <mi>k</mi>
      </munderover>
      <mo>&it;</mo>
      <mrow>
        <munder>
          <mo>&Sum;</mo>
          <mrow>
            <mi>x</mi>
            <mo>&it;</mo>
            <mstyle>
              <mtext>&emsp;</mtext>
            </mstyle>
            <mo>&Element;</mo>
            <mstyle>
              <mtext>&emsp;</mtext>
            </mstyle>
            <mo>&it;</mo>
            <msub>
              <mi>&pi;</mi>
              <mi>u</mi>
            </msub>
          </mrow>
        </munder>
        <mo>&it;</mo>
        <mrow>
          <msub>
            <mi>D</mi>
            <mi>l</mi>
          </msub>
          <mo>&it;</mo>
          <mrow>
            <mo>(</mo>
            <mrow>
              <msub>
                <mi>F</mi>
                <mi>l</mi>
              </msub>
              <mo>,</mo>
              <msubsup>
                <mi>c</mi>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <mi>u</mi>
                    <mo>,</mo>
                    <mi>l</mi>
                  </mrow>
                  <mo>)</mo>
                </mrow>
                <mi>&dagger;</mi>
              </msubsup>
            </mrow>
            <mo>)</mo>
          </mrow>
        </mrow>
      </mrow>
    </mrow>
  </mrow>
</math>
<math>
  <mrow>
    <mrow>
      <mrow>
        <msub>
          <mi>A</mi>
          <mi>l</mi>
        </msub>
        <mo>&it;</mo>
        <mrow>
          <mo>(</mo>
          <mi>&alpha;</mi>
          <mo>)</mo>
        </mrow>
      </mrow>
      <mo>=</mo>
      <mrow>
        <mfrac>
          <mi>l</mi>
          <mrow>
            <mi>k</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
        </mfrac>
        <mo>&it;</mo>
        <mrow>
          <munderover>
            <mo>&Sum;</mo>
            <mrow>
              <mi>u</mi>
              <mo>=</mo>
              <mn>1</mn>
            </mrow>
            <mi>k</mi>
          </munderover>
          <mo>&it;</mo>
          <mrow>
            <munder>
              <mo>&Sum;</mo>
              <mrow>
                <mi>x</mi>
                <mo>&it;</mo>
                <mstyle>
                  <mtext>&emsp;</mtext>
                </mstyle>
                <mo>&Element;</mo>
                <mstyle>
                  <mtext>&emsp;</mtext>
                </mstyle>
                <mo>&it;</mo>
                <msub>
                  <mi>&pi;</mi>
                  <mi>u</mi>
                </msub>
              </mrow>
            </munder>
            <mo>&it;</mo>
            <mrow>
              <munderover>
                <mo>&Sum;</mo>
                <mrow>
                  <mrow>
                    <mi>v</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                  </mrow>
                  <mo>,</mo>
                  <mrow>
                    <mi>v</mi>
                    <mo>&NotEqual;</mo>
                    <mi>u</mi>
                  </mrow>
                </mrow>
                <mi>k</mi>
              </munderover>
              <mo>&it;</mo>
              <mrow>
                <msub>
                  <mi>D</mi>
                  <mi>l</mi>
                </msub>
                <mo>&it;</mo>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <msub>
                      <mi>F</mi>
                      <mi>l</mi>
                    </msub>
                    <mo>,</mo>
                    <msubsup>
                      <mi>c</mi>
                      <mrow>
                        <mo>(</mo>
                        <mrow>
                          <mi>u</mi>
                          <mo>,</mo>
                          <mi>l</mi>
                        </mrow>
                        <mo>)</mo>
                      </mrow>
                      <mi>&dagger;</mi>
                    </msubsup>
                  </mrow>
                  <mo>)</mo>
                </mrow>
              </mrow>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mrow>
    <mo>,</mo>
  </mrow>
</math>
<mathematica-file id="MATHEMATICA-00010" file="US20030005258A1-20030102-M00010.NB"/>
<image id="EMI-M00010" wi="216.027" he="54.97065" file="US20030005258A1-20030102-M00010.TIF" imf="TIFF" ti="MF"/>
</math-cwu>
</paragraph>
<paragraph id="P-0080" lvl="7"><number>&lsqb;0080&rsqb;</number> where x&equals;(F<highlight><subscript>1</subscript></highlight>, F<highlight><subscript>2</subscript></highlight>, . . . F<highlight><subscript>m</subscript></highlight>). It is desirable to minimize &Ggr;<highlight><subscript>l</subscript></highlight>(&agr;) and to maximize A<highlight><subscript>l</subscript></highlight>(&agr;), (i.e., coherent clusters that are well-separated from each other is desirable). Hence, minimize:  
<math-cwu id="MATH-US-00011">
<number>11</number>
<math>
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <msub>
            <mi>Q</mi>
            <mi>l</mi>
          </msub>
          <mo>&af;</mo>
          <mrow>
            <mo>(</mo>
            <mi>&alpha;</mi>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>=</mo>
        <msup>
          <mrow>
            <mo>(</mo>
            <mfrac>
              <mrow>
                <msub>
                  <mi>&Gamma;</mi>
                  <mi>l</mi>
                </msub>
                <mo>&af;</mo>
                <mrow>
                  <mo>(</mo>
                  <mi>&alpha;</mi>
                  <mo>)</mo>
                </mrow>
              </mrow>
              <mrow>
                <msub>
                  <mi>A</mi>
                  <mi>l</mi>
                </msub>
                <mo>&af;</mo>
                <mrow>
                  <mo>(</mo>
                  <mi>&alpha;</mi>
                  <mo>)</mo>
                </mrow>
              </mrow>
            </mfrac>
            <mo>)</mo>
          </mrow>
          <mrow>
            <msub>
              <mi>n</mi>
              <mn>1</mn>
            </msub>
            <mo>/</mo>
            <mi>n</mi>
          </mrow>
        </msup>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>5</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
<mathematica-file id="MATHEMATICA-00011" file="US20030005258A1-20030102-M00011.NB"/>
<image id="EMI-M00011" wi="216.027" he="19.93005" file="US20030005258A1-20030102-M00011.TIF" imf="TIFF" ti="MF"/>
</math-cwu>
</paragraph>
<paragraph id="P-0081" lvl="7"><number>&lsqb;0081&rsqb;</number> where n<highlight><subscript>l </subscript></highlight>denotes the number of data records that have a non-zero feature vector along the l-th component. The quantity n<highlight><subscript>l </subscript></highlight>is introduced to accommodate sparse data sets. If the l-th feature space is simply <custom-character file="US20030005258A1-20030102-P00900.TIF" wi="20" he="20" id="custom-character-00004"/><highlight><superscript>f1 </superscript></highlight>Q<highlight><subscript>l</subscript></highlight>(&agr;) and D<highlight><subscript>l </subscript></highlight>is the squared-Euclidean distance, the &Ggr;<highlight><subscript>l</subscript></highlight>(&agr;) is simply the trace of the within-class covariance matrix and A<highlight><subscript>l</subscript></highlight>(&agr;) is the trace of the between class covariance matrix. In this case, </paragraph>
<paragraph id="P-0082" lvl="7"><number>&lsqb;0082&rsqb;</number> is the ratio used in determining the quality of a given classification, and as the &ldquo;objective&rdquo; function underlying his multiple discriminant analysis. Minimizing Q<highlight><subscript>l</subscript></highlight>(&agr;) leads to a good discrimination along the l-the component feature space. Since it is desirable to simultaneously attain good discrimination along all the m feature spaces, the optimal feature weights &agr;<highlight><superscript>&dagger;</superscript></highlight> are selected as:  
<math-cwu id="MATH-US-00012">
<number>12</number>
<math>
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <msup>
          <mi>&alpha;</mi>
          <mi>&dagger;</mi>
        </msup>
        <mo>=</mo>
        <mrow>
          <mrow>
            <munder>
              <mi>argmin</mi>
              <mrow>
                <mi>&alpha;</mi>
                <mo>&it;</mo>
                <mstyle>
                  <mtext>&emsp;</mtext>
                </mstyle>
                <mo>&Element;</mo>
                <mstyle>
                  <mtext>&emsp;</mtext>
                </mstyle>
                <mo>&it;</mo>
                <mi>&Delta;</mi>
              </mrow>
            </munder>
            <mo>[</mo>
            <mrow>
              <munderover>
                <mo>&Product;</mo>
                <mrow>
                  <mi>l</mi>
                  <mo>=</mo>
                  <mn>1</mn>
                </mrow>
                <mi>m</mi>
              </munderover>
              <mo>&it;</mo>
              <mrow>
                <msub>
                  <mi>Q</mi>
                  <mi>l</mi>
                </msub>
                <mo>&af;</mo>
                <mrow>
                  <mo>(</mo>
                  <mi>&alpha;</mi>
                  <mo>)</mo>
                </mrow>
              </mrow>
            </mrow>
            <mo>]</mo>
          </mrow>
          <mo>.</mo>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>6</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
<mathematica-file id="MATHEMATICA-00012" file="US20030005258A1-20030102-M00012.NB"/>
<image id="EMI-M00012" wi="216.027" he="24.97635" file="US20030005258A1-20030102-M00012.TIF" imf="TIFF" ti="MF"/>
</math-cwu>
</paragraph>
<paragraph id="P-0083" lvl="7"><number>&lsqb;0083&rsqb;</number> 5. Evaluating Method Effectiveness: </paragraph>
<paragraph id="P-0084" lvl="0"><number>&lsqb;0084&rsqb;</number> In assuming that the optimal weight tuple &agr;<highlight><superscript>&dagger;</superscript></highlight> by minimizing the &ldquo;objective&rdquo; function in (6) has been selected. How good is the clustering corresponding to the optimal feature weight tuple&quest; To answer this, assume that pre-classified data is given and benchmark the precision/recall performance of various clusterings against the given ground truth. Precision/recall numbers measure the &ldquo;overlap&rdquo; between a given clustering and the ground truth classification. This precision/recall numbers are not used in the selection of the optimal weight tuple, and intended only to provide a way of evaluate utility of feature weighting. By using the precision/recall numbers to only compare partitionings with a fixed number of clusters k, that is, partitionings with the same &ldquo;model complexity&rdquo;, a measure of effectiveness is provided demonstrated. </paragraph>
<paragraph id="P-0085" lvl="0"><number>&lsqb;0085&rsqb;</number> To meaningfully define precision/recall, conversion of the clusterings into classification using the following simple rule is made by identifying each cluster with the class that has the largest overlap with the cluster, and assign every element in that cluster to the found class. The rule allows multiple clusters to be assigned to a single class, but never assigns a single cluster to multiple classes. Suppose there are c classes &lcub;&ohgr;<highlight><subscript>t</subscript></highlight>&rcub;<highlight><subscript>t&equals;1</subscript></highlight><highlight><superscript>c </superscript></highlight>in the ground truth classification. For a given clustering, by using the above rule, let &agr;<highlight><subscript>t </subscript></highlight>denote the number of data records that are correctly assigned to the class w<highlight><subscript>t</subscript></highlight>, and let b<highlight><subscript>t </subscript></highlight>denote the data records that are incorrectly rejected from the class &ohgr;<highlight><subscript>t</subscript></highlight>, let b<highlight><subscript>l </subscript></highlight>denote the data records that are incorrectly rejected from the class &ohgr;<highlight><subscript>t</subscript></highlight>. Precision and recall are defined as:  
<math-cwu id="MATH-US-00013">
<number>13</number>
<math>
<mrow>
  <mrow>
    <mi>Pt</mi>
    <mo>=</mo>
    <mrow>
      <mfrac>
        <msub>
          <mi>a</mi>
          <mi>t</mi>
        </msub>
        <mrow>
          <msub>
            <mi>a</mi>
            <mi>t</mi>
          </msub>
          <mo>+</mo>
          <msub>
            <mi>b</mi>
            <mi>c</mi>
          </msub>
        </mrow>
      </mfrac>
      <mo>=</mo>
      <mrow>
        <msub>
          <mi>r</mi>
          <mi>t</mi>
        </msub>
        <mo>=</mo>
        <mfrac>
          <msub>
            <mi>a</mi>
            <mi>t</mi>
          </msub>
          <mrow>
            <msub>
              <mi>a</mi>
              <mi>t</mi>
            </msub>
            <mo>+</mo>
            <msub>
              <mi>c</mi>
              <mi>t</mi>
            </msub>
          </mrow>
        </mfrac>
      </mrow>
    </mrow>
  </mrow>
  <mo>,</mo>
  <mrow>
    <mn>1</mn>
    <mo>&leq;</mo>
    <mi>t</mi>
    <mo>&leq;</mo>
    <mi>c</mi>
  </mrow>
  <mo>,</mo>
</mrow>
</math>
<mathematica-file id="MATHEMATICA-00013" file="US20030005258A1-20030102-M00013.NB"/>
<image id="EMI-M00013" wi="216.027" he="17.03835" file="US20030005258A1-20030102-M00013.TIF" imf="TIFF" ti="MF"/>
</math-cwu>
</paragraph>
<paragraph id="P-0086" lvl="7"><number>&lsqb;0086&rsqb;</number> The precision and recall are defined per class. Next, the performance averages across classes using macro-precision (macro-p), macro-recall (macro-r), micro-precision (micro-p), and micro-recall (micro-r) are captured by:  
<math-cwu id="MATH-US-00014">
<number>14</number>
<math>
  <mrow>
    <mrow>
      <mi>macro</mi>
      <mo>&it;</mo>
      <mstyle>
        <mtext>-</mtext>
      </mstyle>
      <mo>&it;</mo>
      <mi>p</mi>
    </mrow>
    <mo>=</mo>
    <mrow>
      <mrow>
        <mfrac>
          <mn>1</mn>
          <mi>c</mi>
        </mfrac>
        <mo>&it;</mo>
        <mrow>
          <munderover>
            <mo>&Sum;</mo>
            <mrow>
              <mi>t</mi>
              <mo>=</mo>
              <mn>1</mn>
            </mrow>
            <mi>c</mi>
          </munderover>
          <mo>&it;</mo>
          <mrow>
            <msub>
              <mi>p</mi>
              <mi>t</mi>
            </msub>
            <mo>&it;</mo>
            <mstyle>
              <mtext>&emsp;</mtext>
            </mstyle>
            <mo>&it;</mo>
            <mi>and</mi>
            <mo>&it;</mo>
            <mstyle>
              <mtext>&emsp;</mtext>
            </mstyle>
            <mo>&it;</mo>
            <mi>macro</mi>
            <mo>&it;</mo>
            <mstyle>
              <mtext>-</mtext>
            </mstyle>
            <mo>&it;</mo>
            <mi>r</mi>
          </mrow>
        </mrow>
      </mrow>
      <mo>=</mo>
      <mrow>
        <mfrac>
          <mn>1</mn>
          <mi>c</mi>
        </mfrac>
        <mo>&it;</mo>
        <mrow>
          <munderover>
            <mo>&Sum;</mo>
            <mrow>
              <mi>t</mi>
              <mo>=</mo>
              <mn>1</mn>
            </mrow>
            <mi>c</mi>
          </munderover>
          <mo>&it;</mo>
          <msub>
            <mi>r</mi>
            <mi>t</mi>
          </msub>
        </mrow>
      </mrow>
    </mrow>
  </mrow>
</math>
<math>
  <mrow>
    <mrow>
      <mrow>
        <mi>micro</mi>
        <mo>&it;</mo>
        <mstyle>
          <mtext>-</mtext>
        </mstyle>
        <mo>&it;</mo>
        <mi>p</mi>
      </mrow>
      <mo>&it;</mo>
      <mover>
        <mo>=</mo>
        <mrow>
          <mo>(</mo>
          <mi>a</mi>
          <mo>)</mo>
        </mrow>
      </mover>
      <mo>&it;</mo>
      <mrow>
        <mrow>
          <mi>micro</mi>
          <mo>&it;</mo>
          <mstyle>
            <mtext>-</mtext>
          </mstyle>
          <mo>&it;</mo>
          <mi>r</mi>
        </mrow>
        <mo>=</mo>
        <mrow>
          <mfrac>
            <mn>1</mn>
            <mi>n</mi>
          </mfrac>
          <mo>&it;</mo>
          <mrow>
            <munderover>
              <mo>&Sum;</mo>
              <mrow>
                <mi>t</mi>
                <mo>=</mo>
                <mn>1</mn>
              </mrow>
              <mi>c</mi>
            </munderover>
            <mo>&it;</mo>
            <msub>
              <mi>a</mi>
              <mi>t</mi>
            </msub>
          </mrow>
        </mrow>
      </mrow>
    </mrow>
    <mo>,</mo>
  </mrow>
</math>
<mathematica-file id="MATHEMATICA-00014" file="US20030005258A1-20030102-M00014.NB"/>
<image id="EMI-M00014" wi="216.027" he="51.11505" file="US20030005258A1-20030102-M00014.TIF" imf="TIFF" ti="MF"/>
</math-cwu>
</paragraph>
<paragraph id="P-0087" lvl="7"><number>&lsqb;0087&rsqb;</number> where (a) follows since, in this case, &Sgr;<highlight><subscript>t&equals;1</subscript></highlight><highlight><superscript>c</superscript></highlight>(a<highlight><subscript>t</subscript></highlight>&plus;b<highlight><subscript>t</subscript></highlight>)&equals;&Sgr;&equals;<highlight><subscript>t&equals;1</subscript></highlight><highlight><superscript>c</superscript></highlight>(a<highlight><subscript>t</subscript></highlight>&plus;c<highlight><subscript>t</subscript></highlight>)&equals;n. </paragraph>
<paragraph id="P-0088" lvl="7"><number>&lsqb;0088&rsqb;</number> 6. Examples of Use of the Method of Clustering Data Sets with Numerical and Categorical Attributes </paragraph>
<paragraph id="P-0089" lvl="0"><number>&lsqb;0089&rsqb;</number> 6.1 Data Model: </paragraph>
<paragraph id="P-0090" lvl="0"><number>&lsqb;0090&rsqb;</number> Suppose a data set with both numerical and categorical features is given. By &ldquo;linearly scaling&rdquo; each numerical feature, that is, subtracting the mean and divide by the square-root of the variance. All linearly scaled numerical features into one feature space are clubbed, and, for this feature vector, the squared-Euclidean distance is used, by representing each q-ary categorical feature using a 1-in-q representation, and club all the categorical features into a single feature space. Assuming no missing values, all the categorical feature vector have the same norm, by only retaining the &ldquo;direction&rdquo; of the categorical feature vectors, that is, and normalizing each categorical feature vector to have an unit Euclidean norm, and use the cosine distance. Essentially, each data object x is represented as a m-tuple, m&equals;2, of feature vectors (F<highlight><subscript>1</subscript></highlight>, F<highlight><subscript>2</subscript></highlight>). </paragraph>
<paragraph id="P-0091" lvl="0"><number>&lsqb;0091&rsqb;</number> 6.2 Heart and Adult Data Sets: </paragraph>
<paragraph id="P-0092" lvl="0"><number>&lsqb;0092&rsqb;</number> <cross-reference target="DRAWINGS">FIGS. 2</cross-reference><highlight><italic>a, </italic></highlight><highlight><bold>2</bold></highlight><highlight><italic>b, </italic></highlight><highlight><bold>2</bold></highlight><highlight><italic>c </italic></highlight>and <highlight><bold>2</bold></highlight><highlight><italic>d </italic></highlight>show graphs of a first example using the method of the invention wherein the HEART (resp. ADULT) data, <cross-reference target="DRAWINGS">FIGS. 1</cross-reference><highlight><italic>a </italic></highlight>and <highlight><bold>1</bold></highlight><highlight><italic>b </italic></highlight>respectively, each show a plot of the &ldquo;objective&rdquo; function Q1&times;Q2 in equation (6) versus the weight c&tilde;x. The vertical lines in <cross-reference target="DRAWINGS">FIGS. 2</cross-reference><highlight><italic>a, </italic></highlight><highlight><bold>2</bold></highlight><highlight><italic>b, </italic></highlight><highlight><bold>2</bold></highlight><highlight><italic>c </italic></highlight>and <highlight><bold>2</bold></highlight><highlight><italic>d </italic></highlight>indicate the position of the optimal weight tuples. For the HEART (resp. ADULT) data, the <cross-reference target="DRAWINGS">FIGS. 2</cross-reference><highlight><italic>c </italic></highlight>and <highlight><bold>2</bold></highlight><highlight><italic>d </italic></highlight>respectively, each shows a plot of macro-p (resp. micro-p, macro-p, and macro-r) versus the weight &agr;<highlight><subscript>1</subscript></highlight>. For the HEART data, macro-p, macro-r, and micro-p numbers are very close to each other, thus, to avoid visual clutter, only plotted macro-p numbers are shown. For the ADULT data, the top, the middle, and bottom plots in <cross-reference target="DRAWINGS">FIG. 2</cross-reference><highlight><italic>d </italic></highlight>are micro-p, macro-p, and macro-r. </paragraph>
<paragraph id="P-0093" lvl="0"><number>&lsqb;0093&rsqb;</number> The HEART data set consists of n&equals;270 instances, and can be obtained from the STATLOG repository: http://www. ncc.up.pt/liacc/ML/statlog/. Every instance consists of 7 numerical and 6 categorical features. The data set has two classes: absence and presence of heart disease; 55.56% (resp. 44.44%) instances were in the former (resp. later) class. </paragraph>
<paragraph id="P-0094" lvl="7"><number>&lsqb;0094&rsqb;</number> The ADULT data set consists of n&equals;32561 instances that were extracted from the 1994 Census database. Every instance consists of 6 numerical and 8 categorical features. The data set has two classes: those with income less than or equal to $50,000 and those with income more than $50,000; 75.22% (resp. 24.78%) instances were in the former (resp. later) class. </paragraph>
<paragraph id="P-0095" lvl="0"><number>&lsqb;0095&rsqb;</number> 6.3 The Optimal Weights </paragraph>
<paragraph id="P-0096" lvl="0"><number>&lsqb;0096&rsqb;</number> In this case, the set of feasible weights is &Dgr;&equals;&lcub;(&agr;<highlight><subscript>1</subscript></highlight>,&agr;<highlight><subscript>2</subscript></highlight>)&verbar;&agr;<highlight><subscript>1</subscript></highlight>&plus;&agr;<highlight><subscript>2</subscript></highlight>&equals;1,&agr;<highlight><subscript>1</subscript></highlight>,&agr;<highlight><subscript>2&gE;0</subscript></highlight>&rcub;. The number of clusters k&equals;8is selected, and a binary search on the weight &agr;<highlight><subscript>1</subscript></highlight>&isin;&lsqb;0,1&rsqb; is done to minimize the &ldquo;objective&rdquo; function in equation (6). </paragraph>
<paragraph id="P-0097" lvl="0"><number>&lsqb;0097&rsqb;</number> For the HEART (resp. ADULT) data, the top-left (resp. top-right) panel in <cross-reference target="DRAWINGS">FIG. 1</cross-reference> shows a plot of the &ldquo;objective&rdquo; function Q<highlight><subscript>1</subscript></highlight>&times;Q<highlight><subscript>2 </subscript></highlight>in (6) versus the weight al. For the HEART and ADULT data sets, the &ldquo;objective&rdquo; function is minimized by the weights (0.12, 0.88) and (0.11, 0.89), respectively. </paragraph>
<paragraph id="P-0098" lvl="0"><number>&lsqb;0098&rsqb;</number> For the HEART (resp. ADULT) data, the bottom-left (resp. bottom-right) panel in <cross-reference target="DRAWINGS">FIGS. 2</cross-reference><highlight><italic>a</italic></highlight>-<highlight><italic>d </italic></highlight>shows a plot of macro-p (resp. micro-p, macro-p, and macro-r) versus the weight &agr;<highlight><subscript>1 </subscript></highlight>By comparing the top-left (resp. top-right) panel with the bottom-left (resp. bottom-right) panel, it can be seen that, roughly, macro-p (resp. micro-p, macro-p, and macro-r) are negatively correlated with the &ldquo;objective&rdquo; function Q<highlight><subscript>1</subscript></highlight>&times;Q<highlight><subscript>2 </subscript></highlight>and that, in fact, the optimal weight tuples achieve nearly optimal precision and recall. In conclusion, optimizing the &ldquo;objective&rdquo; function Q<highlight><subscript>1</subscript></highlight>&times;Q<highlight><subscript>2 </subscript></highlight>leads, reassuringly, to optimizing the precision/recall performance, thus leads to good clusterings and a final solution. </paragraph>
<paragraph id="P-0099" lvl="7"><number>&lsqb;0099&rsqb;</number> 7. Second Example of Using the Method of Clustering Text Data using Words and Phrases as the Feature Spaces </paragraph>
<paragraph id="P-0100" lvl="0"><number>&lsqb;0100&rsqb;</number> 7.1. Phrases in Information Retrieval: </paragraph>
<paragraph id="P-0101" lvl="0"><number>&lsqb;0101&rsqb;</number> Vector space models, represent each document as a vector of certain (possibly weighted and normalized)term frequencies. Typically, terms are single words. However, to capture word ordering, it is intuitive to also include multi-word sequences, namely, phrases, as terms. The use of phrases as terms in vector space models has been well studied. In the example as follows, the phrases along with the words in a single vector space model are not a club. For information retrieval, when single words are also simultaneously used, it is known that natural language phrases do not perform significantly better than statistical phrases. Hence, the focus is on statistical phrases which are simpler to extract, see, for example, see Agrawal et al. in &ldquo;Mining sequential patterns,&rdquo; <highlight><italic>Proc. Int. Conf. Data Eng., </italic></highlight>(1995). Also, see Miadenic et al., &ldquo;Word sequences as features in text-learning&rdquo; in <highlight><italic>Proc. </italic></highlight>7<highlight><italic>th Electrotech. Computer. Science Conference, Ljubljana, Slovenia, </italic></highlight>pages145-148, (1998) found that while adding 2-and 3-word phrases improved the classifier performance, longer phrases did not. Hence, the example illustrates single words, 2-word phrases and 3-word phrases. </paragraph>
<paragraph id="P-0102" lvl="0"><number>&lsqb;0102&rsqb;</number> 7.2 Data Model: </paragraph>
<paragraph id="P-0103" lvl="0"><number>&lsqb;0103&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 3</cross-reference> shows the feasible weights for the second exemplary use of the invention wherein when m&equals;3, &Dgr; is the triangular region formed by the intersection of the plane at &agr;<highlight><subscript>1</subscript></highlight>&plus;&agr;<highlight><subscript>2</subscript></highlight>&plus;&agr;<highlight><subscript>3</subscript></highlight>&equals;1 with the nonnegative orthant of <custom-character file="US20030005258A1-20030102-P00900.TIF" wi="20" he="20" id="custom-character-00005"/><highlight><superscript>3</superscript></highlight>. The left-vertex, the right-vertex, and the top-vertex of the triangle corresponds to the points (1, 0, 0), (0, 1, 0), and (0, 0, 1), respectively. Each document is represented as a triplet of three vectors: a vector of word frequencies, a vector of 2-word phrase frequencies, and a vector of 3-word frequencies, that is, &agr;&equals;(F<highlight><subscript>1</subscript></highlight>, F<highlight><subscript>2, </subscript></highlight>F<highlight><subscript>3</subscript></highlight>). It is now shown how to compute such representations for every document in a given corpus. This creation of the first feature vector is a standard exercise in information retrieval. The basic idea is to construct a word dictionary of all the words that appear in any of the documents in the corpus, and to prune or eliminate stop word from this dictionary. For the present application, also eliminated are those low-frequency words which appeared in less that 0.64% of the documents. Suppose F<highlight><subscript>l </subscript></highlight>unique words remain in the dictionary after such elimination. Assign an unique identifier from 1 to f<highlight><subscript>l </subscript></highlight>to each of these words. Now, for each document x in the corpus, the first vector f<highlight><subscript>l </subscript></highlight>in the triplet will be a f<highlight><subscript>l </subscript></highlight>dimensional vector. The jth column entry, 1&lE;j&lE;f<highlight><subscript>2</subscript></highlight>, of F<highlight><subscript>l </subscript></highlight>is the number of occurrences of the jth word in the document x. Creation of the second (resp. third) feature vector is essentially the same as the first, except that the low-frequency 2-word (resp. 3-word) phrase elimination threshold to one-half (resp. 3-word) phrase is generally less likely than a single word. Let f<highlight><subscript>2 </subscript></highlight>(resp. f<highlight><subscript>3</subscript></highlight>) denote the dimensionalities of the second (resp. third) feature vector. Finally, each of the three components F<highlight><subscript>1</subscript></highlight>, F<highlight><subscript>2</subscript></highlight>, F<highlight><subscript>3 </subscript></highlight>is normalized to have a unit Euclidean norm, that is, their directions are retained and their lengths are discarded. There are a large number of term-weighting schemes in information retrieval for assigning different relative importance to various terms in the feature vectors. These feature vectors correspond to a popular scheme known as normalized term frequency. The distortion measures D<highlight><subscript>1</subscript></highlight>, D<highlight><subscript>2</subscript></highlight>, and D<highlight><subscript>3 </subscript></highlight>are the cosine distances. </paragraph>
<paragraph id="P-0104" lvl="0"><number>&lsqb;0104&rsqb;</number> 7.3 Newsgroups Data: </paragraph>
<paragraph id="P-0105" lvl="0"><number>&lsqb;0105&rsqb;</number> Picked out of the following 10 newsgroups from the &ldquo;Newsgroups data&rdquo; from a newspaper to illustrate the invention: misc.forsale  
<table-cwu id="TABLE-US-00001">
<number>1</number>
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="4">
<colspec colname="OFFSET" colwidth="14PT" align="left"/>
<colspec colname="1" colwidth="49PT" align="left"/>
<colspec colname="2" colwidth="70PT" align="left"/>
<colspec colname="3" colwidth="84PT" align="left"/>
<thead>
<row>
<entry></entry>
<entry></entry>
</row>
<row>
<entry></entry>
<entry namest="OFFSET" nameend="3" align="center" rowsep="1"></entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry></entry>
<entry>sci.crypt</entry>
<entry>comp.windows.x</entry>
<entry>Comp.sys.mac.hardware</entry>
</row>
<row>
<entry></entry>
<entry>rec.autos</entry>
<entry>rec.sport.baseball</entry>
<entry>soc.religion.christian</entry>
</row>
<row>
<entry></entry>
<entry>sci.space</entry>
<entry>talk.politics.guns</entry>
<entry>talk.politics.mideast</entry>
</row>
<row>
<entry></entry>
<entry namest="OFFSET" nameend="3" align="center" rowsep="1"></entry>
</row>
</tbody>
</tgroup>
</table>
</table-cwu>
</paragraph>
<paragraph id="P-0106" lvl="7"><number>&lsqb;0106&rsqb;</number> Each newsgroup contains 1000 documents; after removing empty documents, a total of n&equals;9961 documents exist. For this data set, the unpruned word (resp. 2-word phrase and 3-word phrase) dictionary had size 72586 (resp. 429604 and 461132) out of which f<highlight><subscript>l</subscript></highlight>&equals;2583 (resp. f<highlight><subscript>2</subscript></highlight>&equals;2144 and f<highlight><subscript>2</subscript></highlight>&equals;2268) elements that appeared in at least 64 (resp. 32 and 16) documents was retained. All the three features spaces were highly sparse; on an average, after pruning, each document had only 50 (resp. 7.19 and 8.34) words (resp. 2-word and 3-word phrases). Finally, n<highlight><subscript>l</subscript></highlight>&equals;n&equals;9961 (resp. n<highlight><subscript>2</subscript></highlight>&equals;8639 and n<highlight><subscript>3</subscript></highlight>&equals;4664) documents had at least one word (resp. 2-word phrase and 3-word phrase). Note that the numbers n<highlight><subscript>1</subscript></highlight>, n<highlight><subscript>2</subscript></highlight>, and n<highlight><subscript>3 </subscript></highlight>are used in equation (5) above. </paragraph>
<paragraph id="P-0107" lvl="0"><number>&lsqb;0107&rsqb;</number> 7.4 The Optimal Weights: </paragraph>
<paragraph id="P-0108" lvl="0"><number>&lsqb;0108&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 4</cross-reference> shows a newsgroups data set, in which plot macro-p versus the &ldquo;objective&rdquo; function Q<highlight><subscript>1</subscript></highlight>&times;Q<highlight><subscript>2</subscript></highlight>&times;Q<highlight><subscript>3 </subscript></highlight>for various different weight tuples. The macro-p value corresponding to the optimal weight tuple is shown using the symbol &square;, and others are shown using the symbol &bull;. The &ldquo;negative correlation&rdquo; between macro-p and the &ldquo;objective&rdquo; function is evident from the plot. Macro-p, macro-r, and micro-p numbers are very close to each other, thus, to avoid visual clutter, only plotted macro-p numbers are shown. In this case, the set of feasible weights is the triangular region shown in <cross-reference target="DRAWINGS">FIG. 4</cross-reference>. The k-means algorithm with k&equals;10 on the Newsgroups data set with 31 different feature weights that are shown using the symbol &bull; in <cross-reference target="DRAWINGS">FIG. 4</cross-reference>. The &ldquo;objective&rdquo; function in equation (6) is minimized by a weight tuple (0.50, 0.25, 0.25) was run. </paragraph>
<paragraph id="P-0109" lvl="0"><number>&lsqb;0109&rsqb;</number> Further, <cross-reference target="DRAWINGS">FIG. 4</cross-reference> roughly shows that as the &ldquo;objective&rdquo; function decreases, macro-p increases. The &ldquo;objective&rdquo; function corresponding to the optimal weight tuple is plotted using the symbol; by definition, this is the left-most point on the plot. It can be seen that the optimal weight tuple has a smaller macro-p value than only one other weight tuple, namely, (0.495, 0.010, 0.495). Although macro-r and micro-p results are not shown, they lead to the same conclusions. Use of the precision/recall numbers reveals that optimal feature weighting provides good final data clustering. </paragraph>
<paragraph id="P-0110" lvl="0"><number>&lsqb;0110&rsqb;</number> As discussed above, it has been assumed that the number of clusters k is given; however, an important problem that the invention can be used is to automatically determine the number of clusters in an adaptive or data-driven fashion using information-theoretic criteria such as the MDL principle. A computationally efficient gradient descent procedure for computing the optimal parameter tuple a &dagger; can be used. This entails combining the optimization problems in equation (6) and in equation (3) into a single problem that can be solved using an iterative gradient descent heuristics for the squared-Euclidean distance. In the method of the invention, the new weighted distortion measure D<highlight><superscript>&agr;</superscript></highlight> has been used in the k-means algorithm; it may also be possible to use this weighted distortion with a graph-based algorithm such as the complete link method or with hierarchical agglomerative clustering algorithms. </paragraph>
<paragraph id="P-0111" lvl="0"><number>&lsqb;0111&rsqb;</number> In summary, the invention provides a method for obtaining good data clustering by integrating multiple, heterogeneous feature spaces in the k-means clustering algorithm by adaptively selecting relative weights assigned to various features spaces while simultaneously attaining good separation along all the feature spaces. </paragraph>
<paragraph id="P-0112" lvl="0"><number>&lsqb;0112&rsqb;</number> While the preferred embodiment of the present invention has been illustrated in detail, it should be apparent that modifications and adaptations to that embodiment may occur to one skilled in the art without departing from the spirit or scope of the present invention as set forth in the following claims. </paragraph>
</section>
</detailed-description>
</subdoc-description>
<subdoc-claims>
<heading lvl="1">What is claimed is: </heading>
<claim id="CLM-00001">
<claim-text><highlight><bold>1</bold></highlight>. A method for evaluating and outputting a final clustering solution for a plurality of multi-dimensional data records, said data records having multiple, heterogeneous feature spaces represented by feature vectors, said method comprising: 
<claim-text>defining a distortion between two feature vectors as a weighted sum of distortion measures on components of said feature vector; </claim-text>
<claim-text>clustering said multi-dimensional data records into k-clusters using a &ldquo;convex programming&rdquo; formulation; and </claim-text>
<claim-text>selecting feature weights of said feature vectors. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00002">
<claim-text><highlight><bold>2</bold></highlight>. The method according to <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein said selecting of feature weights are optimized by an &ldquo;objective&rdquo; function to produce said solution of a final clustering that simultaneously minimizes average intra-cluster dispersion and maximizes average inter-cluster dispersion along all said feature spaces. </claim-text>
</claim>
<claim id="CLM-00003">
<claim-text><highlight><bold>3</bold></highlight>. The method according to <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein said clustering includes initially applying a local minima of said clustering. </claim-text>
</claim>
<claim id="CLM-00004">
<claim-text><highlight><bold>4</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein said clustering comprises a k-means clustering algorithm. </claim-text>
</claim>
<claim id="CLM-00005">
<claim-text><highlight><bold>5</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00002">claim 2</dependent-claim-reference>, wherein said minimizing distortion of individual clusters includes taking said data records and iteratively determining Voronoi partitions until said &ldquo;objective&rdquo; function, between two successive iterations, is less than a specified threshold. </claim-text>
</claim>
<claim id="CLM-00006">
<claim-text><highlight><bold>6</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein said clustering comprises analyzing word data, and said feature vectors comprise multiple-word frequencies of said data records. </claim-text>
</claim>
<claim id="CLM-00007">
<claim-text><highlight><bold>7</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein said clustering comprises analyzing data records having numerical and categorical attributes, and said feature vectors comprise linearly-scaled numerical attributes and each q-ary categorical feature using a 1-in-q representation of said data records. </claim-text>
</claim>
<claim id="CLM-00008">
<claim-text><highlight><bold>8</bold></highlight>. A method for evaluating and outputting a clustering solution for a plurality of multi-dimensional data records, said data records having multiple, heterogeneous feature spaces represented by feature vectors, said method comprising: 
<claim-text>defining a distortion between two said feature vectors as a weighted sum of distortion measures on components of said feature vector; </claim-text>
<claim-text>clustering said multi-dimensional data records into k-clusters using a &ldquo;convex programming&rdquo; formulation of a generalized k-means clustering function; and </claim-text>
<claim-text>selecting optimal feature weights of said feature vectors by an &ldquo;objective&rdquo; function to produce said solution of a final clustering that simultaneously minimizes average intra-cluster dispersion and maximizes average inter-cluster dispersion along all said feature spaces. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00009">
<claim-text><highlight><bold>9</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00008">claim 8</dependent-claim-reference>, wherein said clustering includes initially applying a local minima of said clustering. </claim-text>
</claim>
<claim id="CLM-00010">
<claim-text><highlight><bold>10</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00008">claim 8</dependent-claim-reference>, wherein said minimizing distortion of individual clusters includes taking said data records and iteratively determining Voronoi partitions until said &ldquo;objective&rdquo; function, between two successive iterations, is less than a specified threshold. </claim-text>
</claim>
<claim id="CLM-00011">
<claim-text><highlight><bold>11</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00008">claim 8</dependent-claim-reference>, wherein said clustering comprises analyzing word data, and said feature vectors comprise multiple-word frequencies of said data records. </claim-text>
</claim>
<claim id="CLM-00012">
<claim-text><highlight><bold>12</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00008">claim 8</dependent-claim-reference>, wherein said clustering comprises analyzing data records having numerical and categorical attributes, and said feature vectors comprise linearly-scaled numerical attributes and each q-ary categorical feature using a 1-in-q representation of said data records. </claim-text>
</claim>
<claim id="CLM-00013">
<claim-text><highlight><bold>13</bold></highlight>. A computer system for data mining and outputting a final clustering solution, wherein said system includes a memory for storing a database having a plurality of multi-dimensional data records, each having multiple, heterogeneous feature spaces represented by feature vectors, said system including a processor for executing instructions comprising: 
<claim-text>defining a distortion between two feature vectors as a weighted sum of distortion measures on components of said feature vector; </claim-text>
<claim-text>clustering said multi-dimensional data records into k-clusters using a &ldquo;convex programming&rdquo; formulation; and </claim-text>
<claim-text>selecting feature weights of said feature vectors. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00014">
<claim-text><highlight><bold>14</bold></highlight>. The system of <dependent-claim-reference depends_on="CLM-00011">claim 13</dependent-claim-reference>, wherein said instruction for selecting of said feature weights are optimized by implementing an &ldquo;objective&rdquo; function to produce said solution of a final clustering that simultaneously minimizes average intra-cluster dispersion and maximizes average inter-cluster dispersion along all said feature spaces. </claim-text>
</claim>
<claim id="CLM-00015">
<claim-text><highlight><bold>15</bold></highlight>. The system of <dependent-claim-reference depends_on="CLM-00011">claim 13</dependent-claim-reference>, wherein said instruction of said clustering includes an instruction for initially applying a local minima of said clustering. </claim-text>
</claim>
<claim id="CLM-00016">
<claim-text><highlight><bold>16</bold></highlight>. The system of <dependent-claim-reference depends_on="CLM-00011">claim 13</dependent-claim-reference>, wherein said instruction for clustering includes instructions for implementing a k-means clustering algorithm. </claim-text>
</claim>
<claim id="CLM-00017">
<claim-text><highlight><bold>17</bold></highlight>. The system of <dependent-claim-reference depends_on="CLM-00011">claim 14</dependent-claim-reference>, wherein said instruction for minimizing distortion of individual clusters includes taking said data records and iteratively determining Voronoi partitions until said &ldquo;objective&rdquo; function, between two successive iterations, is less than a specified threshold. </claim-text>
</claim>
<claim id="CLM-00018">
<claim-text><highlight><bold>18</bold></highlight>. The system of <dependent-claim-reference depends_on="CLM-00011">claim 13</dependent-claim-reference>, wherein said instruction for clustering includes instructions for analyzing word data. </claim-text>
</claim>
<claim id="CLM-00019">
<claim-text><highlight><bold>19</bold></highlight>. The system of <dependent-claim-reference depends_on="CLM-00011">claim 13</dependent-claim-reference>, wherein said instruction for clustering includes instructions for analyzing data records having numerical and categorical attributes. </claim-text>
</claim>
<claim id="CLM-00020">
<claim-text><highlight><bold>20</bold></highlight>. A program storage device readable by machine, tangibly embodying a program of instructions executable by said machine to perform a method for evaluating and outputting a final clustering solution from a set of data records having multiple, heterogeneous feature spaces represented as feature vectors, said method comprising: 
<claim-text>defining a distortion between two feature vectors as a weighted sum of distortion measures on components of said feature vector; </claim-text>
<claim-text>clustering said multi-dimensional data records into k-clusters using a &ldquo;convex programming&rdquo; formulation; and </claim-text>
<claim-text>selecting feature weights of said feature vectors. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00021">
<claim-text><highlight><bold>21</bold></highlight>. The device of <dependent-claim-reference depends_on="CLM-00022">claim 20</dependent-claim-reference>, wherein said selecting of feature weights are optimized by an &ldquo;objective&rdquo; function to produce said solution of a final clustering that simultaneously minimizes average intra-cluster dispersion and maximizes average inter-cluster dispersion along all said feature spaces. </claim-text>
</claim>
<claim id="CLM-00022">
<claim-text><highlight><bold>22</bold></highlight>. The device of <dependent-claim-reference depends_on="CLM-00022">claim 20</dependent-claim-reference>, wherein said clustering includes initially applying a local minima of said clustering. </claim-text>
</claim>
<claim id="CLM-00023">
<claim-text><highlight><bold>23</bold></highlight>. The device of <dependent-claim-reference depends_on="CLM-00022">claim 20</dependent-claim-reference>, wherein said clustering comprises a k-means clustering algorithm. </claim-text>
</claim>
<claim id="CLM-00024">
<claim-text><highlight><bold>24</bold></highlight>. The device of <dependent-claim-reference depends_on="CLM-00022">claim 21</dependent-claim-reference>, wherein said minimizing distortion of individual clusters includes taking said data records and iteratively determining Voronoi partitions until said &ldquo;objective&rdquo; function, between two successive iterations, is less than a specified threshold. </claim-text>
</claim>
<claim id="CLM-00025">
<claim-text><highlight><bold>25</bold></highlight>. The device of <dependent-claim-reference depends_on="CLM-00022">claim 20</dependent-claim-reference>, wherein said clustering comprises analyzing word data, and said feature vectors comprise multiple-word frequencies of said data records. </claim-text>
</claim>
<claim id="CLM-00026">
<claim-text><highlight><bold>26</bold></highlight>. The device of <dependent-claim-reference depends_on="CLM-00022">claim 20</dependent-claim-reference>, wherein said clustering comprises analyzing data records having numerical and categorical attributes, and said feature vectors comprise linearly-scaled numerical attributes and each q-ary categorical feature using a 1-in-q representation of said data records.</claim-text>
</claim>
</subdoc-claims>
<subdoc-drawings id="DRAWINGS">
<heading lvl="0" align="CENTER">Drawings</heading>
<representative-figure>1</representative-figure>
<figure id="figure-D00000">
<image id="EMI-D00000" file="US20030005258A1-20030102-D00000.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00001">
<image id="EMI-D00001" file="US20030005258A1-20030102-D00001.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00002">
<image id="EMI-D00002" file="US20030005258A1-20030102-D00002.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00003">
<image id="EMI-D00003" file="US20030005258A1-20030102-D00003.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00004">
<image id="EMI-D00004" file="US20030005258A1-20030102-D00004.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00005">
<image id="EMI-D00005" file="US20030005258A1-20030102-D00005.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00006">
<image id="EMI-D00006" file="US20030005258A1-20030102-D00006.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00007">
<image id="EMI-D00007" file="US20030005258A1-20030102-D00007.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00008">
<image id="EMI-D00008" file="US20030005258A1-20030102-D00008.TIF" imf="TIFF" ti="DR"/>
</figure>
</subdoc-drawings>
</patent-application-publication>
