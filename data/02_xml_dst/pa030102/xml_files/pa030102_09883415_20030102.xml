<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE patent-application-publication SYSTEM "pap-v16-2002-01-01.dtd" [
<!ENTITY US20030004966A1-20030102-D00000.TIF SYSTEM "US20030004966A1-20030102-D00000.TIF" NDATA TIF>
<!ENTITY US20030004966A1-20030102-D00001.TIF SYSTEM "US20030004966A1-20030102-D00001.TIF" NDATA TIF>
<!ENTITY US20030004966A1-20030102-D00002.TIF SYSTEM "US20030004966A1-20030102-D00002.TIF" NDATA TIF>
<!ENTITY US20030004966A1-20030102-D00003.TIF SYSTEM "US20030004966A1-20030102-D00003.TIF" NDATA TIF>
<!ENTITY US20030004966A1-20030102-D00004.TIF SYSTEM "US20030004966A1-20030102-D00004.TIF" NDATA TIF>
<!ENTITY US20030004966A1-20030102-D00005.TIF SYSTEM "US20030004966A1-20030102-D00005.TIF" NDATA TIF>
<!ENTITY US20030004966A1-20030102-D00006.TIF SYSTEM "US20030004966A1-20030102-D00006.TIF" NDATA TIF>
<!ENTITY US20030004966A1-20030102-D00007.TIF SYSTEM "US20030004966A1-20030102-D00007.TIF" NDATA TIF>
<!ENTITY US20030004966A1-20030102-D00008.TIF SYSTEM "US20030004966A1-20030102-D00008.TIF" NDATA TIF>
<!ENTITY US20030004966A1-20030102-D00009.TIF SYSTEM "US20030004966A1-20030102-D00009.TIF" NDATA TIF>
<!ENTITY US20030004966A1-20030102-D00010.TIF SYSTEM "US20030004966A1-20030102-D00010.TIF" NDATA TIF>
<!ENTITY US20030004966A1-20030102-D00011.TIF SYSTEM "US20030004966A1-20030102-D00011.TIF" NDATA TIF>
<!ENTITY US20030004966A1-20030102-D00012.TIF SYSTEM "US20030004966A1-20030102-D00012.TIF" NDATA TIF>
<!ENTITY US20030004966A1-20030102-D00013.TIF SYSTEM "US20030004966A1-20030102-D00013.TIF" NDATA TIF>
<!ENTITY US20030004966A1-20030102-D00014.TIF SYSTEM "US20030004966A1-20030102-D00014.TIF" NDATA TIF>
<!ENTITY US20030004966A1-20030102-D00015.TIF SYSTEM "US20030004966A1-20030102-D00015.TIF" NDATA TIF>
<!ENTITY US20030004966A1-20030102-D00016.TIF SYSTEM "US20030004966A1-20030102-D00016.TIF" NDATA TIF>
<!ENTITY US20030004966A1-20030102-D00017.TIF SYSTEM "US20030004966A1-20030102-D00017.TIF" NDATA TIF>
<!ENTITY US20030004966A1-20030102-D00018.TIF SYSTEM "US20030004966A1-20030102-D00018.TIF" NDATA TIF>
<!ENTITY US20030004966A1-20030102-D00019.TIF SYSTEM "US20030004966A1-20030102-D00019.TIF" NDATA TIF>
<!ENTITY US20030004966A1-20030102-D00020.TIF SYSTEM "US20030004966A1-20030102-D00020.TIF" NDATA TIF>
<!ENTITY US20030004966A1-20030102-D00021.TIF SYSTEM "US20030004966A1-20030102-D00021.TIF" NDATA TIF>
<!ENTITY US20030004966A1-20030102-D00022.TIF SYSTEM "US20030004966A1-20030102-D00022.TIF" NDATA TIF>
<!ENTITY US20030004966A1-20030102-D00023.TIF SYSTEM "US20030004966A1-20030102-D00023.TIF" NDATA TIF>
<!ENTITY US20030004966A1-20030102-D00024.TIF SYSTEM "US20030004966A1-20030102-D00024.TIF" NDATA TIF>
<!ENTITY US20030004966A1-20030102-D00025.TIF SYSTEM "US20030004966A1-20030102-D00025.TIF" NDATA TIF>
<!ENTITY US20030004966A1-20030102-D00026.TIF SYSTEM "US20030004966A1-20030102-D00026.TIF" NDATA TIF>
<!ENTITY US20030004966A1-20030102-D00027.TIF SYSTEM "US20030004966A1-20030102-D00027.TIF" NDATA TIF>
<!ENTITY US20030004966A1-20030102-D00028.TIF SYSTEM "US20030004966A1-20030102-D00028.TIF" NDATA TIF>
<!ENTITY US20030004966A1-20030102-D00029.TIF SYSTEM "US20030004966A1-20030102-D00029.TIF" NDATA TIF>
<!ENTITY US20030004966A1-20030102-D00030.TIF SYSTEM "US20030004966A1-20030102-D00030.TIF" NDATA TIF>
<!ENTITY US20030004966A1-20030102-D00031.TIF SYSTEM "US20030004966A1-20030102-D00031.TIF" NDATA TIF>
<!ENTITY US20030004966A1-20030102-D00032.TIF SYSTEM "US20030004966A1-20030102-D00032.TIF" NDATA TIF>
<!ENTITY US20030004966A1-20030102-D00033.TIF SYSTEM "US20030004966A1-20030102-D00033.TIF" NDATA TIF>
]>
<patent-application-publication>
<subdoc-bibliographic-information>
<document-id>
<doc-number>20030004966</doc-number>
<kind-code>A1</kind-code>
<document-date>20030102</document-date>
</document-id>
<publication-filing-type>new</publication-filing-type>
<domestic-filing-data>
<application-number>
<doc-number>09883415</doc-number>
</application-number>
<application-number-series-code>09</application-number-series-code>
<filing-date>20010618</filing-date>
</domestic-filing-data>
<technical-information>
<classification-ipc>
<classification-ipc-primary>
<ipc>G06F007/00</ipc>
</classification-ipc-primary>
<classification-ipc-edition>07</classification-ipc-edition>
</classification-ipc>
<classification-us>
<classification-us-primary>
<uspc>
<class>707</class>
<subclass>104100</subclass>
</uspc>
</classification-us-primary>
</classification-us>
<title-of-invention>Business method and apparatus for employing induced multimedia classifiers based on unified representation of features reflecting disparate modalities</title-of-invention>
</technical-information>
<inventors>
<first-named-inventor>
<name>
<given-name>Rudolf</given-name>
<middle-name>M.</middle-name>
<family-name>Bolle</family-name>
</name>
<residence>
<residence-us>
<city>Bedford Hills</city>
<state>NY</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</first-named-inventor>
<inventor>
<name>
<given-name>Norman</given-name>
<family-name>Haas</family-name>
</name>
<residence>
<residence-us>
<city>Mount Kisco</city>
<state>NY</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
<inventor>
<name>
<given-name>Frank</given-name>
<middle-name>J.</middle-name>
<family-name>Oles</family-name>
</name>
<residence>
<residence-us>
<city>Peekskill</city>
<state>NY</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
<inventor>
<name>
<given-name>Tong</given-name>
<family-name>Zhang</family-name>
</name>
<residence>
<residence-us>
<city>Tuckahoe</city>
<state>NY</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
</inventors>
<assignee>
<organization-name>International Business Machines Corporation</organization-name>
<address>
<city>Armonk</city>
<state>NY</state>
<country>
<country-code>US</country-code>
</country>
</address>
<assignee-type>02</assignee-type>
</assignee>
<correspondence-address>
<name-1>Louis J. Percello</name-1>
<name-2>Intellectual Property Law Dept.</name-2>
<address>
<address-1>IBM Corporation</address-1>
<address-2>P.O. Box 218</address-2>
<city>Yorktown Heights</city>
<state>NY</state>
<postalcode>10598</postalcode>
<country>
<country-code>US</country-code>
</country>
</address>
</correspondence-address>
</subdoc-bibliographic-information>
<subdoc-abstract>
<paragraph id="A-0001" lvl="0">This invention is a business system and method to perform categorization (classification) of multimedia items and to make business decisions based on the categorization of the item. The multimedia items are comprised of a multitude of disparate information sources, in particular, visual information and textual information. Classifiers are induced based on combining textual and visual feature vectors. Textual features are the traditional ones. Visual features include, but are not limited to, color properties of key intervals and motion properties of key intervals. The visual feature vectors are determined in such a fashion that the vectors are sparse. The text and the visual representation vectors are combined in a systematic and coherent fashion. This vector representation of a media item lends itself to well-established learning techniques and can be used for multimedia item categorization. The resulting business system, subject of this invention, can be used for many purposes. An example here are enforcement of copyright, trademark, intellectual property, parental guidance and common decency restrictions. Other uses are multimedia item classifier to determine routing of incoming items or building user profiles based on user multimedia preferences. </paragraph>
</subdoc-abstract>
<subdoc-description>
<summary-of-invention>
<section>
<heading lvl="1">FIELD OF THE INVENTION </heading>
<paragraph id="P-0001" lvl="0"><number>&lsqb;0001&rsqb;</number> This invention relates to the business of handling multimedia information (media items), such as video and images that have audio associated with it or possibly have text associated with it in the form of captions. More specifically, the invention relates to the business of handling video and audio by processing the video and audio for supervised and unsupervised machine learning of categorization techniques based on disparate information sources such as visual information and speech transcript. The invention also relates to combining these disparate information sources in a coherent fashion to make business decisions. </paragraph>
</section>
<section>
<heading lvl="1">BACKGROUND OF THE INVENTION </heading>
<paragraph id="P-0002" lvl="0"><number>&lsqb;0002&rsqb;</number> Beyond data that can be represented in machine readable tabular form and, of course, machine readable text documents, many other forms of media are transitioning to machine readable digital form. For example, visual data such as images and video are increasingly being produced in digital form or converted to digital form. Large collections and catalogues of these media objects need to be organized, similarly to structured text data, but using categorization technology enhanced with new technologies that allow for convenient categorization based on visual and audio content of the media. Such collections of media are managed using multimedia databases where the data that are stored are combinations of numerical, textual, auditory and visual data. </paragraph>
<paragraph id="P-0003" lvl="0"><number>&lsqb;0003&rsqb;</number> Video is a special, peculiar type of data object in the sense that there is a notion of time associated with this data. These types of data are referred to as streamed information, streamed multimedia data or temporal media. When transporting this data from one location to some other location for viewing purposes, it is important that the data arrives in the right order and at the right time. In other words, if frame n of a video is displayed at time t, frame n&plus;1 has to be at the viewing location at time t plus {fraction (1/30)}th of a second. Of course, if the media are moved or transported for other purposes, there is no such requirement. </paragraph>
<paragraph id="P-0004" lvl="0"><number>&lsqb;0004&rsqb;</number> Similarly to text documents, which can be segmented into sections, paragraphs and sentences, temporal media data can be divided up into smaller more or less meaningful time-continuous chunks. For video data, these chunks are often referred to as scenes, segments and shots, where a shot is the continuous depiction of space by a single camera between the time the camera is switched on and switched off, i.e., it is an image of continuous space-time. In this disclosure, we refer to these temporal, time-continuous (but not necessarily space-continuous) chunks of media as media items. These media items include image and video, with associated audio or text and, in general, information stream items composed of disparate sources of information. Examples of media items are commercial segments (or groups) broadcast at regular time intervals on almost every TV channel; a single commercial is another example of a media item or video segment. </paragraph>
<paragraph id="P-0005" lvl="0"><number>&lsqb;0005&rsqb;</number> Multimedia databases may contain collections of such temporal media items in addition to non-streamed media objects such as still images and text documents. Associated with the media items may be global textual or parametric data, such as the name of the director of the video/music (audio) or the date of recording. Categorization of these media items into classes can be accomplished through supervised and unsupervised clustering and decision tree generation based on the text and, possibly, parametric data. </paragraph>
<paragraph id="P-0006" lvl="0"><number>&lsqb;0006&rsqb;</number> Multimedia collections may also be categorized based on data content, such as the amount of green or red in images or video and sound frequency components of audio segments. The media item collections have to be then preprocessed and the results have to be somehow categorized based on the visual properties. Categorizing media items based on semantic content, the actual meaning (subjects and objects) of the media items, on the other hand, is a difficult issue. For video, speech may be categorized or recognized to some extent, but beyond that, the situation is much more complicated because of the rudimentary state of the art in machine-interpretation of visual data. </paragraph>
<paragraph id="P-0007" lvl="0"><number>&lsqb;0007&rsqb;</number> Determining whether a given media item is equal to (a piece of) one of, or is similar to (a piece of) one of, a plurality of temporal media items; or, determining whether it is equal or similar to a media item or equal or similar to a sub segment in a media item collection is another important multimedia categorization problem. A variant here is the issue of determining if a given temporal input media item contains a segment which is equal or similar to one of a plurality of temporal media stream segments or determining if the input stream contains a segment which is equal or similar to a media item in a multimedia database. To achieve this one needs to somehow compare a temporal media item to a plurality of temporal media items or databases of such items. This problem arises when certain media items need to be selected or deselected in a given temporal media item or in a plurality of temporal media items. An example here is the problem of deselecting or suppressing repetitive media items in a television broadcast program. Such repetitive media items can be commercials or commercial segments or groups which are suppressed either by muting the sound channel or by both muting the sound channel and blanking the visual channel. </paragraph>
<paragraph id="P-0008" lvl="0"><number>&lsqb;0008&rsqb;</number> To develop a procedure for identifying media items as belonging to particular classes or categories, (or for any classification or pattern recognition task, for that matter) supervised learning technology can be based on decision trees, on logical rules, or on other mathematical techniques such as linear discriminant methods (including perceptrons, support vector machines, and related variants), nearest neighbor methods, Bayesian inference, etc. We can generically refer to the output of such supervised learning systems as classifiers. </paragraph>
<paragraph id="P-0009" lvl="0"><number>&lsqb;0009&rsqb;</number> Supervised learning technology requires a training set consisting of labeled data, that is, representations of previously categorized media segments, to enable a computer to induce patterns that allow it to categorize hitherto unseen media segments. Generally, there is also a test set, also consisting of labeled data, that is used to evaluate whatever specific categorization procedure is developed. In academic exercises, the test set is usually disjoint from the training set to compensate for the phenomenon of overfitting. In practice, it may be difficult to get large amounts of labeled data of high quality. If the labeled data set is small, the only way to get any useful results at all may be to use all the available data in both the training set and the test set. </paragraph>
<paragraph id="P-0010" lvl="0"><number>&lsqb;0010&rsqb;</number> To apply standard approaches to supervised learning, the media segments in both the training set and the test set must be represented in terms of numbers derived from counting occurrences of features. The relationship between features extracted for the purposes of supervised learning and the content of a media segment has an important impact on the success of the enterprise, so it has to be addressed, but it is not part of supervised learning per se. </paragraph>
<paragraph id="P-0011" lvl="0"><number>&lsqb;0011&rsqb;</number> From these feature vectors, the computer induces classifiers based on patterns or properties that characterize when a media segment belongs to a particular category. The term &ldquo;pattern&rdquo; is meant to be very general. These patterns or properties may be presented as rules, which may sometimes be easily understood by a human being, or in other, less accessible formats, such as a weight vector and threshold used to partition a vector space with a hyperplane. Exactly what constitutes a pattern or property in a classifier depends on the particular machine learning technology employed. To use a classifier to categorize incoming hitherto unseen media segments, the newly arriving data must not only be put into a format corresponding to the original format of the training data, but it must then undergo a further transformation based on the list of features extracted from the training data in the training phase, so that it finally possesses a representation as a feature vector that permits the presence or absence of the relevant patterns or properties to be determined. </paragraph>
<paragraph id="P-0012" lvl="0"><number>&lsqb;0012&rsqb;</number> The assignment of more than one category to an item is called multiple categorization. Some supervised learning techniques (for example, a few, but not all, approaches using decision trees) do not support multiple categorization. They make the assumption that each item categorized will belong to at most one category, which may not be adequate in some applications. Some supervised learning systems may return a ranked list of possibilities instead of a single category, but this is still slightly deficient for some applications, because such a system might assign categories even to items that should be placed in no category. What are usually most useful are those supervised learning methods that give realistic confidence levels with each assigned category. </paragraph>
<paragraph id="P-0013" lvl="0"><number>&lsqb;0013&rsqb;</number> The idea behind text feature selection is that the occurrence of the selected features in text associated with an unclassified data item will be a useful ingredient for the development of an automated classification system designed to assign one or more categories to the data item. For text data, the first processing step that must be done is tokenization, i.e., the segmentation of a string of characters into a string of words or tokens. However, the representation of an item of text data as a string of arbitrary words, with all of the meaningful linguistic structures it implicitly contains, is often simply too complicated and rich for a computer to handle. Even if one does no parsing of the text, there may well be too many potential features, in which case some distillation is needed. Luckily, single words themselves have been seen to comprise an adequate set of features for many supervised learning problems. Sometimes it is useful to identify the part of speech of each word, thus distinguishing between an instance of the verb walk and the noun walk. (This is called part-of-speech tagging.) This only scratches the surface. Modern techniques of computational linguistics permit the identification of complex features in text, but with rising complexity comes vast numbers of features. At any rate, after the training set is prepared, and after the text associated with it is identified, a list of those text features deemed particularly relevant to the particular classification task at hand is automatically extracted. Call the features in this list the extracted text features, and call the process of building the list text feature extraction. There is an issue in regard to whether a single list of features, called in this setting a global dictionary, is created or whether there is a separate list for each category, called in this context local dictionaries. The resolution of this issue can depend on the details of the supervised learning technique employed, but, in applications related to text, local dictionaries generally give better performance. There are a variety of criteria for judging relevance during feature extraction. A simple one is to use absolute or normalized frequency to compile a list of a fixed number n of the most frequent features for each category, taking into account the fact that small categories may be so underpopulated that the total number of features in them may be less than n. More sophisticated techniques for judging relevance involve the use of information-theoretic measures such as entropy or the use of statistical methods such as principal component analysis. </paragraph>
<paragraph id="P-0014" lvl="0"><number>&lsqb;0014&rsqb;</number> After text feature extraction, a new vector representation of each text item associated with the training data is then extracted in terms of how frequently each selected feature occurs in that item. The vector representation may be binary, simply indicating the presence or absence of each feature, or it may be numeric in which each numeric value is derived from a count of the number of occurrences of each feature. </paragraph>
<paragraph id="P-0015" lvl="0"><number>&lsqb;0015&rsqb;</number> A large body of prior art of video processing for video identification, detection, categorization, and classification is concerned with the detection of commercials in a video stream, i.e., the media item is a commercial or a sequence of commercials. This is not a categorization problem per se, but rather a detection problem. The detection of one class (or category) of interest, though, is in itself a categorization problem, where the categories are &ldquo;category-of-interest&rdquo; and &ldquo;unknown.&rdquo;</paragraph>
<paragraph id="P-0016" lvl="0"><number>&lsqb;0016&rsqb;</number> Many methods rely on the fact that commercials are often surrounded by blank frames, changes in audio/brightness level, simple representations of intermediate frames and more global dynamic properties that typically hold for commercials. An example of a method and apparatus for detection and identification of portions of temporal video streams containing commercials is described in U.S. Pat. No. 5,151,788 to Blum. Here, a blank frame is detected in the video stream and the video stream is tested for &ldquo;activity&rdquo; (properties such as sound level, brightness level and average shot length). U.S. Pat. No. 5,696,866 to Iggulden et al. extend the idea to detecting a &ldquo;flat&rdquo; frame. In addition to a frame being flat at the beginning and end of a commercial, they include that the frame has to be silent. Additional features, such as changes in the audio power or amplitude and changes in brightness of the luminance signal between program and commercial segments, of the video signal are used in U.S. Pat. No. 5,343,251 to Nafeh. </paragraph>
<paragraph id="P-0017" lvl="0"><number>&lsqb;0017&rsqb;</number> Many techniques for detecting commercials, reduce commercials to a small set of representative frames, or key frames, and then use image matching schemes to match the key frames. Here, each particular commercial has some representation, instead of using generic attributes above that describe the category of commercials. For example, U.S. Pat. No. 5,708,477 to S. J. Forbes et al. uses the notion of a list of abbreviated frames for representing commercial video segments. An abbreviated frame is an array of digital values representing the average intensities of the pixels in a particular portion of the video frame. Upon detection of a scene change in the live video stream, computed and stored abbreviated frames are matched and commercial is detected and classified (if present in memory). A technique that uses more sophisticated frame representations is presented in reference: </paragraph>
<paragraph id="P-0018" lvl="2"><number>&lsqb;0018&rsqb;</number> J. M. Sanchez, X. Binefa, J. Vitria, and P. Radeva, Local color analysis for scene break detection applied to TV commercial recognition, Third International Conference, Visual&apos;99, Amsterdam, June 1999, pp. 237-244. </paragraph>
<paragraph id="P-0019" lvl="0"><number>&lsqb;0019&rsqb;</number> (This reference is incorporated herein in its entirety.) Each commercial in the database is represented by a number of color histograms, or color frequency vectors, for a representative frame for each shot in the commercial. The shot boundaries of a commercial are detected by some shot boundary detection algorithm (finding scene breaks). Commercials are detected in a live video stream by comparing all the color histograms of all the commercials to the color histograms representing a shot in video stream. No temporal information is incorporated in the representation of the commercial. </paragraph>
<paragraph id="P-0020" lvl="0"><number>&lsqb;0020&rsqb;</number> All this prior art falls in the realm of detection of video copies. The use of image feature histograms, where the images are particular video frames, like shot boundaries, have been popularized in the area of image recognition, and, later on in the area of image search. Color histograms (color frequency distributions) are the most widely used, in particular, the Red Green Blue (RGB) and the Hue Saturation and Intensity (HSI). Other color spaces that could be used are those defined by the CIE (Commission Internationale de l&apos;Eclairage&mdash;the International Committee for Illumination). These spaces are CIE L*u*v* hue angle and saturation and CIE L*a*v* hue angle and saturation. Ratios of color components such as the red response divided by the green response (after appropriate gamma correction) also yield intensity independent color measures. Another popular method is to divide each response by the average response across all spectral bands, such as Rn&equals;R/(R&plus;G&plus;B), to produce a set of fractional color components (which sum to one). </paragraph>
<paragraph id="P-0021" lvl="0"><number>&lsqb;0021&rsqb;</number> A particular instance of image database search, is image classification, or image content recognition. In an image classification problem, typically, the number of classes is smaller than the number of images in an image database. An example of image classification is found in: </paragraph>
<paragraph id="P-0022" lvl="2"><number>&lsqb;0022&rsqb;</number> R. Bolle, J. Connell, G. Taubin, N. Haas, R. Mohan, &ldquo;VeggieVision: A produce recognition system,&rdquo; in Proc. Third IEEE Workshop on Applications of Computer Vision, pp. 244-251, December 1996. </paragraph>
<paragraph id="P-0023" lvl="0"><number>&lsqb;0023&rsqb;</number> This reference is incorporated herein in its entirety. Color histograms are used in this work, but the use of color frequency distributions is extended to the use of histograms to represent other features that are used for image/object classification. Histograms are a compact representation of a reference image that do not depend on the location or orientation of the object in the image, or, at least, depend only a little because of quantization effects. For example, visual texture is a feature used in &ldquo;VeggieVision&rdquo; to Bolle et al. As opposed to color, texture is a visual feature that is much more difficult to describe and to capture computationally. It is also a feature that cannot be attributed to a single pixel but rather is attributed to a patch of image data. The texture of an image patch is a description of the spatial brightness variation in that patch. This can be a repetitive pattern of primitives (texels), or, can be more random, i.e., structural textures and statistical textures. Computational texture measures are either region-based or edge-based, trying to capture structural textures and statistical textures, respectively. In &ldquo;VeggieVision&rdquo; to Bolle et al., a texture representation of an image, image class, or image category, then, is a one-dimensional histogram of local texture feature values. Shape can also be represented in terms of frequency distribution. The information available to work with is the two-dimensional boundary of (say) a segmented image. Boundary shape is a feature of multiple boundary pixels and is expressed by a local computational feature, for example, curvature. Local curvature is estimated by fitting a circle at each point of the boundary. After smoothing, this boundary shape feature is quantized and a histogram is computed. Instead of over an area, such as for color histograms, these histograms are computed from a collection of image pixels that form the boundary of the object image. Finally, size of image segments is another feature of the images that is important in &ldquo;VeggieVision&rdquo; to Bolle et al. A method that computes area from many collections of three boundary points is proposed. Three points determine a circle and, hence, a diameter D. A histogram of these diameter estimates is then used as a representation for objects (in the image) size. </paragraph>
<paragraph id="P-0024" lvl="0"><number>&lsqb;0024&rsqb;</number> Many video copy detection solutions use some spatial representation of frames or images (spatial representations as described above) and some temporal representation of the times between the frames, i.e., a spatial-temporal representation. Indyk et al. have proposed a method for video copy detection, solely using the distance (time) between shot breaks in the video as the feature of the video. </paragraph>
<paragraph id="P-0025" lvl="2"><number>&lsqb;0025&rsqb;</number> P. Indyk, G. Iyengar and N. Shivakumar, Finding pirated video sequences on the Internet. tech. rep., Stanford Infolab, February 1999. </paragraph>
<paragraph id="P-0026" lvl="0"><number>&lsqb;0026&rsqb;</number> This method (incorporated herein by reference) is somewhat limited in the richness of the representation. Other video copy detection algorithms use some form of image matching (visual data) combined with temporal evidence integration. A method for detecting arbitrary video sequences, including commercials, is described in (incorporated herein by reference): </paragraph>
<paragraph id="P-0027" lvl="2"><number>&lsqb;0027&rsqb;</number> R. Mohan, &ldquo;Video Sequence Matching&rdquo;, International Conference on Acoustics, Speech and Signal Processing, (ICASSP), May 1998. </paragraph>
<paragraph id="P-0028" lvl="0"><number>&lsqb;0028&rsqb;</number> Mohan defines that there is a match between a given video sequence and some segment of a database video sequence if each frame in the given sequence matches the corresponding frame in the database video segment. That is, the matching sequences are of the same temporal length; matching slow-motion sequences is performed by temporal sub-sampling of the database segments. The representation of a video segment is a vector of representations of the constituent frames in the form of an ordinal measure of a reduced intensity image of each frame. Before matching, the database is prepared for video sequence by computing the ordinal measure for each frame in each video segment in the database. Finding a match between some given action video sequence and the databases then amounts to sequentially matching the input sequence against each sub-sequence in the database and detecting minimums. This method introduces the temporal aspects of the video media items. </paragraph>
<paragraph id="P-0029" lvl="0"><number>&lsqb;0029&rsqb;</number> All these color-based image methods are subject to digitizing and encoding artifacts, like color variations. See A. Hampapur and R. M. Bolle, Feature based Indexing for Media Tracking. In Proc. of Int. Conf. on Multimedia and Expo, August 2000, pp. 67-70 (Hampapur et al.). To circumvent color variations Hampapur et al. have, instead, used other features that are invariant to color variations. In a first, off-line indexing phase representations for a set of known reference media items are computed and stored in an index structure. For each segment, a set of intervals is determined and from each key interval, a set of feature values is extracted from portions of the video frames. The values are quantized and index tables are built where feature values point to the reference media items. In the search and detection phase, a real-time process of computing and quantizing features from a target media stream is done in the same fashion. Additionally, counters are initialized for each of the known media items. When computed feature values point to a known media item, the corresponding counter is incremented. High values of the counter indicate the presence of a known media item in the target stream. An interesting thing to note here is that any feature type, such as, color, edges or motion, can be used in this method. Further, features are not computed on a frame basis (as in the above methods) but rather from regions within the frame and even regions of consecutive frames (local optical) flow. Detecting media items is further accomplished with a computational complexity that is sub-linear. </paragraph>
<paragraph id="P-0030" lvl="0"><number>&lsqb;0030&rsqb;</number> Reference Hampapur et al. is incorporated herein by reference. </paragraph>
<paragraph id="P-0031" lvl="0"><number>&lsqb;0031&rsqb;</number> What all these above mentioned references have in common is that the visual features extracted from the video do not have a whole lot of semantic meaning, e.g., a color, in and of itself, does not say much about the semantic content of the image or video. See Lienhart, C. Kuhmunch and W. Effelsberg, &ldquo;On the detection and recognition of television commercials.&rdquo; In Proc. of the IEEE Conf. on Multimedia Computing and Systems, 1997 (Lienhart et al.). Lienhart et al. take things a step further. They describe a system for performing both feature based detection and recognition of known commercials. The visual features that are used have spatial-temporal aspects. They use directly measurable features, such as, a spot being no longer than 30 seconds, spots being separated by a short break of 5-12 monochrome frames, and the volume of the audio signal being turned up. In addition, they use indirectly measurable features, like the fact that spots are full of motion, animated, and full of action. In addition, commercial spots have many still frames and many of them contain textual information. It is important to note that these are the beginnings of image processing techniques for extracting semantic information, such as action and motion, from video frames. </paragraph>
<paragraph id="P-0032" lvl="0"><number>&lsqb;0032&rsqb;</number> Reference Lienhart et al. is incorporated herein by reference. </paragraph>
<paragraph id="P-0033" lvl="0"><number>&lsqb;0033&rsqb;</number> Now consider B- T Truong, S. Venkatesh and C. Dorai, &ldquo;Automatic Genre Identification for Content-Based Categorization,&rdquo; in Proc. Int. Conf. On Pattern Recognition, September 2000, pp. 230-233 (B- T Truong et al.), incorporated herein in its entirety. The authors take the use of extracted semantic features a step further. The extracted visual features have cinematographic meaning, such as, fades, dissolves and motion features. Motion features are incorporated in terms of &ldquo;quiet&rdquo; visual scenes (the absence of motion) and &ldquo;motion runs,&rdquo; unbroken sequences of motion, where motion is defined in terms of luminance differences between frames. In addition, the authors use color features in terms of color coherence over time, high brightness and high saturation. The authors used the well-known C4.5 decision tree induction program to build a classifier for genre labeling. </paragraph>
<paragraph id="P-0034" lvl="0"><number>&lsqb;0034&rsqb;</number> Another technique for video categorization is described in </paragraph>
<paragraph id="P-0035" lvl="2"><number>&lsqb;0035&rsqb;</number> N. Dimitrova, L. Agnihotri and G. Wei, &ldquo;Video classification based on HMM using text and faces,&rdquo; (Dimitrova et al.). </paragraph>
<paragraph id="P-0036" lvl="0"><number>&lsqb;0036&rsqb;</number> Herein, first fifteen labels defined based on these visual features (by text, the authors, mean superimposed text in the video) are defined, examples are &ldquo;talking head&rdquo; and &ldquo;one text line.&rdquo; A technique using Hidden Markov models (HMM) is described to classify a given media item into predefined categories, namely, commercial, news, sitcom and soap. An HMM takes these labels as input and has observation symbols as output. The system consists of two phases, a training and a classification stage. Reference Dimitrova et al. is incorporated herein in its entirety. </paragraph>
<paragraph id="P-0037" lvl="0"><number>&lsqb;0037&rsqb;</number> It is important to note that Dimitrova et al. does not use text in machine (ASCII) readable form, it uses the presence or absence of text block(s) in the video frames. </paragraph>
<paragraph id="P-0038" lvl="0"><number>&lsqb;0038&rsqb;</number> On the other hand, such machine-readable ASCII text, along with, visual features is used for video categorization in M. A. Smith and T. Kanade, &ldquo;Video skimming for quick browsing based on audio and image characterization,&rdquo; Carnegie Mellon University, Tech. Rep. CMU-CS-95-186, June 1995 (Smith et al.). </paragraph>
<paragraph id="P-0039" lvl="0"><number>&lsqb;0039&rsqb;</number> Reference Smith et al. is incorporated herein in its entirety. A sophisticated video database browsing systems is described, the authors refer to browsing as &ldquo;skimming.&rdquo; Much emphasis is placed on visual analysis for video interpretation and video summarization (the construction of two-dimensional depictions of the video to allow for nonlinear access). Visual analysis include scene break detection, camera motion analysis, and object detection (faces and superimposed text). The audio transcript is used to identify keywords in it. Term frequency inverse document frequency techniques are used to identify critical words. Words that appear frequently in a particular video segment but occur infrequently in standard corpuses receive the highest weight. In Smith et al. the speech recognition is not automated yet, and closed-captioning is used instead. Video search is accomplished through the use of the extracted words as search keys, browsing of video summaries then allows for quickly finding the video of interest. </paragraph>
<paragraph id="P-0040" lvl="0"><number>&lsqb;0040&rsqb;</number> A content-based video browsing system that applies linguistic analysis to the closed captioning is described in I. Mani, D. House, M. Maybury, M. Green, &ldquo;Towards content-based browsing of broadcast news video,&rdquo; in Intelligent Multimedia Info Retrieval, Issue 1997, M. T. Maybury (ed.), pp 241- 258. AAAI Press/The MIT Press (Mani et al.). </paragraph>
<paragraph id="P-0041" lvl="0"><number>&lsqb;0041&rsqb;</number> The reference Mani et al. is incorporated herein in its entirety. </paragraph>
<paragraph id="P-0042" lvl="0"><number>&lsqb;0042&rsqb;</number> Emphasis in Mani et al. is placed on topic and story segmentation. Assuming that one could associate terms in a document with subjects in a thesaurus, the authors hypothesize that as topics change, the associated thesaural subjects change as well. The work is based on a thesaurus of 124 subject categories, with text summaries represented in a 124-dimensional space. Well-known subject similarity measures as the angle between subject vectors are used. The issue then is detecting a change in topic by detecting a change in angle. The subject vector, however, has to be computed over a certain video time interval, the authors refer to this as a block. The block size is important here. The authors do not arrive at a universally usable block size and contemplate adjustable block size. Further, the authors consider the use of cues that closed-captioners insert, in particular &ldquo;&gt;&gt;&rdquo; indicates a change of speaker, while &ldquo;&gt;&gt;&gt;&rdquo; indicates a change in topic. These cues were found to be unreliable. Therefore, the authors investigate the use of what they call &ldquo;sign off&rdquo; cues. These are specific textual cues that indicate a change in topic, as &ldquo;Goodnight Jim&rdquo; in the MacNeil-Lehrer NewsHour shown in the past on PBS. The authors use no visual cues to detect story boundaries. </paragraph>
<paragraph id="P-0043" lvl="0"><number>&lsqb;0043&rsqb;</number> Finally, the use of automated speech recognition of the audio track to determine story and topic is being used more and more since speech recognition technology is steadily improving. The use of automated speech recognition can be classified as (1) dictation applications, (2) conversational or transactional applications, and (3) indexing applications. A comprehensive and excellent overview of the latter application is presented in Coden et al.: </paragraph>
<paragraph id="P-0044" lvl="2"><number>&lsqb;0044&rsqb;</number> A. R. Coden, E. W. Brown, &ldquo;Speech Transcript Analysis for Automatic Search,&rdquo; IBM Research Tech. Rep., IBM Research Tech. Rep., (Coden et al.). </paragraph>
<paragraph id="P-0045" lvl="0"><number>&lsqb;0045&rsqb;</number> This reference (Coden et al.) is incorporated herein by reference. All of the video indexing, video summarization, video segmentation, and video categorization and subject detection technologies based on automated speech recognition, described in Coden et al., use no or very little visual information. </paragraph>
<paragraph id="P-0046" lvl="0"><number>&lsqb;0046&rsqb;</number> There is also quite some prior art dealing with segmenting documents (discourse) into portions corresponding to topics. This is typically referred to as &ldquo;discourse segmentation&rdquo; to distinguish it from character segmentation from image or video for optical character recognition (OCR). The term &ldquo;discourse,&rdquo; further is more general because it includes spoken language, which is transcribed from wave forms to text (e.g., ASCII) for analysis purposes. In the following discussion, we will use the terms interchangeably. </paragraph>
<paragraph id="P-0047" lvl="0"><number>&lsqb;0047&rsqb;</number> One popular recurring idea is to partition the discourse into fragments, and to measure the &ldquo;similarity&rdquo; of one fragment to another, using the cosine metric, which is the dot product of the word occurrence frequencies. (Morphological analysis is usually employed first, to reduce inflected, declined, etc., words to their base forms&mdash;&ldquo;stemming&rdquo; or &ldquo;lemmatization&rdquo;). </paragraph>
<paragraph id="P-0048" lvl="0"><number>&lsqb;0048&rsqb;</number> Reference Hearst, M. A., Multi-paragraph segmentation of expository text. Proceedings of the 32nd Annual Conference of the Association for Computational Linguistics, Las Cruces, N.Mex., 1994, pp. 9-16. (Hearst), is incorporated herein by reference. </paragraph>
<paragraph id="P-0049" lvl="0"><number>&lsqb;0049&rsqb;</number> Hearst does this by partitioning the entire document into tiles of more or less uniform size, the size being on the order of a paragraph. She then plots C (j, j&plus;1) versus j, for j&equals;1, . . . , N&minus;1, where N is the number of tiles in the document, and C is the inter-tile co-occurrence (or similarity) coefficient. After smoothing of this curve, local minimal values indicate discourse boundaries, since minimal similarity indicates probable different topics of the tiles. </paragraph>
<paragraph id="P-0050" lvl="0"><number>&lsqb;0050&rsqb;</number> Also incorporated by reference is J. C. Reynar, &ldquo;An automated method of finding topic boundaries,&rdquo; Proceedings of the 32nd Annual Conference of the Association for Computational Linguistics, student session, Las Cruces, N.Mex., 1994, pp. 331-333, (Reynar). Reynar divides a discourse at a very fine grain: the individual word. He then records the correspondences (0 or 1) with every other word in an N&times;N matrix, where N is the document size in words. Then any choice of discourse boundaries defines a set of square sub-matrices of the matrix lying along the main diagonal, each sub-matrix representing the intra-segment co-occurrence values. Reynar defines the best discourse segmentation to be the one that minimizes the density of 1&apos;s in the extra-segmental co-occurrence regions of the matrix. Here the extra-segmental regions are all matrix entries not lying in the intra-segmental sub-matrices. He calls his technique dotplotting. </paragraph>
<paragraph id="P-0051" lvl="0"><number>&lsqb;0051&rsqb;</number> Further references Ponte and Croft, and Kozima are incorporated herein by reference: </paragraph>
<paragraph id="P-0052" lvl="1"><number>&lsqb;0052&rsqb;</number> Ponte J. M. and Croft W. B. 1997, Text Segmentation by Topic, in Proceedings of the First European Conference on Research and Advanced Technology for Digital Libraries, pp. 120-129. (Ponte and Croft) </paragraph>
<paragraph id="P-0053" lvl="1"><number>&lsqb;0053&rsqb;</number> Kozima, H. 1993 Text Segmentation based on similarity between words. In Proceedings of the 31st Annual Conference of the Association for Computational Linguistics, Columbus, Ohio. pp. 286-288, (Kozima) </paragraph>
<paragraph id="P-0054" lvl="0"><number>&lsqb;0054&rsqb;</number> Ponte and Croft, use a similar technique, except that they &ldquo;expand&rdquo; each word in a partition by looking it up in a &ldquo;thesaurus&rdquo; and taking all of the words in the same concept group that the seed word was in. (This is an attempt to overcome co-ocurrence, or correspondence, failures due to the use of synonyms or hypernyms, when really the same underlying concept is being referenced.) Ponte and Croft bootstrap the correspondences by developing a document-specific thesaurus, using &ldquo;local context analysis&rdquo; of labeled documents. Then, to find the best co-occurence sub-matrices, instead of exhaustively considering all possibilities, they use a dynamic programming technique, minimizing a cost function. Kozima et al. perform a similar word &ldquo;expansion,&rdquo; by means of &ldquo;spreading activation&rdquo; in a linguistic semantic net. Two words are considered to be co-occurrences of, or corresponding to, each other if and only if each can be reached from the other by less than m steps in the semantic net, for some arbitrarily chosen value of m. </paragraph>
<paragraph id="P-0055" lvl="0"><number>&lsqb;0055&rsqb;</number> There are feature-based approaches, too, that do not rely on word co-occurrence or correspondences, for example, Litman and Passoneau. Here a set of word features is developed. These features are derived from multiple knowledge sources: prosodic features, cue phrase features, noun phrase features, combined features. A decision tree, expressed in terms of these features, is then evaluated at each potential discourse segment boundary to decide if it is truly a discourse segmentation point or not. The decision expression can be hand-crafted or automatically produced by feeding training data to a learning system such as the well-known C4.5 decision tree classification scheme </paragraph>
<paragraph id="P-0056" lvl="0"><number>&lsqb;0056&rsqb;</number> Reference &lsqb;Litman D. J. and Passoneau R. J. 1995. Combining multiple knowledge sources for discourse segmentation. In Proceedings of the 33rd Annual Conference of the Association for Computational Linguistics, Cambridge, Mass.&rsqb;, (Litman and Passoneau), is incorporated herein by reference. </paragraph>
<paragraph id="P-0057" lvl="0"><number>&lsqb;0057&rsqb;</number> Now consider &lsqb;D. Beeferman, A. Berger and J. Lafferty, Text Segmentation Using Exponential Models, CMU Tech Rep&rsqb;, (Beeferman, et al.) that is incorporated herein by reference and introduces a feature-based discourse segmentation technique for documents. The idea is to assign to each position in the data stream a probability that a discourse boundary occurs. Central to the approach is a pair of tools: a short- and a long-range model of language. The short-term model is a trigram model, the conditional probability of a word based on the two preceding words. The long-term model is obtained by retaining a cache of recently seen trigrams. Determining a discourse boundary in statistical terms is cast by formulating the probability of a boundary both in terms of the short- and the long-term model. Maximal values of this probability then indicate discourse boundaries. Beeferman et al. touch upon, but do not implement, multimedia document (containing audio, text and video) discourse segmentation. Examples of short- long-term features that they propose are: &ldquo;is there a sharp change in video stream in the last 20 frames,&rdquo; &ldquo;is there, a blank video frame nearby,&rdquo; and &ldquo;is the a match between the spectrum of the current image and the spectrum of the image near the last segment boundary.&rdquo;</paragraph>
<paragraph id="P-0058" lvl="0"><number>&lsqb;0058&rsqb;</number> In sum, we can (roughly) distinguish the following approaches to media item categorization and media item subject detection; or, more generally, media item classification. The approaches are classified based on the features that are used. The features are derived from the raw analog signal, visual features computed from digitized media items frames (images), textual features directly decoded from the closed-caption, and textual features obtained from automatically computed speech transcripts. Here is a list of common kinds of features used to classify multimedia items: </paragraph>
<paragraph id="P-0059" lvl="2"><number>&lsqb;0059&rsqb;</number> Raw analog visual and audio signals. </paragraph>
<paragraph id="P-0060" lvl="2"><number>&lsqb;0060&rsqb;</number> Visual features computed from individual frames. </paragraph>
<paragraph id="P-0061" lvl="2"><number>&lsqb;0061&rsqb;</number> Visual features computed from individual frames plus temporal features. </paragraph>
<paragraph id="P-0062" lvl="2"><number>&lsqb;0062&rsqb;</number> Visual features computed from individual frames, temporal features plus audio features. </paragraph>
<paragraph id="P-0063" lvl="2"><number>&lsqb;0063&rsqb;</number> Semantic visual features computed from individual frames plus temporal features. </paragraph>
<paragraph id="P-0064" lvl="2"><number>&lsqb;0064&rsqb;</number> Semantic visual features computed from multiple frames and temporal features. </paragraph>
<paragraph id="P-0065" lvl="2"><number>&lsqb;0065&rsqb;</number> Closed-captioning (predetermined keyword spotting) plus visual features. </paragraph>
<paragraph id="P-0066" lvl="2"><number>&lsqb;0066&rsqb;</number> Speech transcript (predetermined keyword spotting) plus visual features. </paragraph>
<paragraph id="P-0067" lvl="2"><number>&lsqb;0067&rsqb;</number> Using only textual data, either speech transcript or closed-captioning. </paragraph>
<paragraph id="P-0068" lvl="2"><number>&lsqb;0068&rsqb;</number> Speech transcript computed from audio track. </paragraph>
<paragraph id="P-0069" lvl="2"><number>&lsqb;0069&rsqb;</number> Speech transcript computed from audio track plus rudimentary visual features. </paragraph>
<paragraph id="P-0070" lvl="2"><number>&lsqb;0070&rsqb;</number> Text document analysis. </paragraph>
</section>
<section>
<heading lvl="1">PROBLEMS WITH THE PRIOR ART </heading>
<paragraph id="P-0071" lvl="0"><number>&lsqb;0071&rsqb;</number> Some of the problems with the prior art are now presented. </paragraph>
<paragraph id="P-0072" lvl="0"><number>&lsqb;0072&rsqb;</number> 1. The prior art media item categorization techniques for business decisions are based either only on visual information or only on textual information. </paragraph>
<paragraph id="P-0073" lvl="0"><number>&lsqb;0073&rsqb;</number> 2. The media item representations are not designed to handle both textual and visual features that can be extracted from media items. This inherently limits the number of media items that can be distinguished, i.e., the discrimination power of the representations will not extend beyond a certain (not very large) number of different video categories and the number of business decisions, consequently, is then smaller because of prior art limitations. </paragraph>
<paragraph id="P-0074" lvl="0"><number>&lsqb;0074&rsqb;</number> 3. The media item representations are not designed to handle in a coherent fashion both textual and visual features that can be extracted from media items and hence business decisions cannot be made in a coherent fashion. </paragraph>
</section>
<section>
<heading lvl="1">OBJECTS OF THE INVENTION </heading>
<paragraph id="P-0075" lvl="0"><number>&lsqb;0075&rsqb;</number> An object of this invention is an improved business system and method for categorizing multimedia items. </paragraph>
<paragraph id="P-0076" lvl="0"><number>&lsqb;0076&rsqb;</number> An object of this invention is an improved business system and method for categorizing multimedia items using both textual and visual features. </paragraph>
<paragraph id="P-0077" lvl="0"><number>&lsqb;0077&rsqb;</number> An object of this invention is an improved business system and method for categorizing multimedia items while handling both textual and visual features coherently. </paragraph>
<paragraph id="P-0078" lvl="0"><number>&lsqb;0078&rsqb;</number> An object of this invention is a business system for performing categorizing of multimedia items in a large number of categories (classes). </paragraph>
</section>
<section>
<heading lvl="1">SUMMARY OF THE INVENTION </heading>
<paragraph id="P-0079" lvl="0"><number>&lsqb;0079&rsqb;</number> The present invention relates to business processes based on a categorization methodology for categorizing multimedia items. Examples include video and images with captions. There are two phases: a learning phase, whose purpose is to induce a classifier, and categorization phase, in which the classifier may be applied in business processes. </paragraph>
<paragraph id="P-0080" lvl="0"><number>&lsqb;0080&rsqb;</number> In the learning phase, off-line supervised or unsupervised learning, using a training set of labeled or unlabeled multimedia items as appropriate, is employed to induce a classifier based on patterns found in a unified representation as a single feature vector of disparate kinds of features, linguistic and visual, found in a media item under consideration. The unified representation of disparate features in a single feature vector will enable a classifier to make use of more complicated patterns for categorization, patterns that simultaneously involve linguistic and visual aspects of the media, resulting in superior performance as compared with other less sophisticated techniques. This allows for better designed and more precisily performing business processes. First, for each media item, the accompanying text is represented by a sparse textual feature vector. Secondly, for each media item, a set of key frames or key intervals (key intervals, for short) is determined, which can either be regularly sampled in time or based on the information content. From each key interval, a set of features is extracted from a number of regions in the key intervals. These regions can be different for each feature. The extracted features are coarsely quantized. Hence, each key interval is encoded by a sparse textual feature vector and a sparse visual feature vector. The sparse textual feature vectors and the sparse visual feature vectors may optionally need to be further transformed to assure their compatibility in various ways, such as (1) with respect to range of the values appearing in the two kinds of vectors or (2) with respect to the competitive sizes of the two kinds of vectors with respect to some norm or measure. The textual feature vector and the visual feature vector are combined by concatenation to produce a unified representation in a single vector of disparate kinds of features. Having created the unified representation of the training data, standard methods of classifier induction are then used, followed by appropriate business processes and business decisions. </paragraph>
<paragraph id="P-0081" lvl="0"><number>&lsqb;0081&rsqb;</number> In categorization phase, the process of computing sparse textual feature vectors and sparse visual feature vectors for a media item is repeated. The classifier induced in the learning phase is used to identify the class (category) of a media item and appropriate business processes and business decisions might also be learned. </paragraph>
<paragraph id="P-0082" lvl="0"><number>&lsqb;0082&rsqb;</number> A standard method of classifier induction is the construction of a k-nearest-neighbor classifier based on training data whose elements are labeled with the classes to which they belong. A k-nearest-neighbor classifier is one that classifies vectors using some measure of similarity. It assigns to a vector whose class is unknown the class to which a majority of its k nearest neighbors in the training data belong. The simplest kind of a k-nearest-neighbor classifier is one in which k is taken to be 1. The categorization phase for this simplest kind of k-nearest-neighbor classifier amounts to, for an item whose class is unknown, finding its nearest neighbor, which is the most similar item in the training data to that item according to a similarity measure, and assigning to the unknown item the class of its nearest neighbor. This invention explicitly includes the use of these fairly simple classifiers (as well as their more complex classifiers, such as support vector machines and various classifiers based on statistical analysis of the vectors representing the training data) among the standard methods of classifier induction to which reference was made above. In particular, this invention includes the solution of the problem of finding the media item in a reference collection that is most similar to a hitherto unconsidered media item by ascertaining the degree of similarity between the vector representations of the hitherto unconsidered media item and each reference media item, as long as the vector representations that are compared are constructed as described above, thereby providing a unified representation of disparate modalities of the media items being compared. Based on the multimedia item(s) further appropriate business processes and business decisions can be applied.</paragraph>
</section>
</summary-of-invention>
<brief-description-of-drawings>
<section>
<heading lvl="1">BRIEF DESCRIPTION OF THE FIGURES </heading>
<paragraph id="P-0083" lvl="0"><number>&lsqb;0083&rsqb;</number> The foregoing and other objects, aspects and advantages will be better understood from the following non limiting detailed description of preferred embodiments of the invention with reference to the drawings that include the following: </paragraph>
<paragraph id="P-0084" lvl="0"><number>&lsqb;0084&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 1</cross-reference> is a prior art block diagram of the state-of-the-art categorization technology for document categorization. </paragraph>
<paragraph id="P-0085" lvl="0"><number>&lsqb;0085&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 2</cross-reference> is a flowchart showing the disparate sources of information that are used for media item categorization </paragraph>
<paragraph id="P-0086" lvl="0"><number>&lsqb;0086&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 3</cross-reference> is a more specific flowchart describing the combined computation of the disparate sources of information from a media item. </paragraph>
<paragraph id="P-0087" lvl="0"><number>&lsqb;0087&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 4</cross-reference> is a detailed flowchart for inducing classifiers for categorizing media items that comprise of disparate sources of information. </paragraph>
<paragraph id="P-0088" lvl="0"><number>&lsqb;0088&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 5</cross-reference> is an example of a decision rule induced by the training processes that combines visual and textual information. </paragraph>
<paragraph id="P-0089" lvl="0"><number>&lsqb;0089&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 6A</cross-reference> is a block diagram of the learning/training phase of one preferred system embodying the present invention. </paragraph>
<paragraph id="P-0090" lvl="0"><number>&lsqb;0090&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 6B</cross-reference> is a block diagram of the classification phase of one preferred system embodying the present invention. </paragraph>
<paragraph id="P-0091" lvl="0"><number>&lsqb;0091&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 7A</cross-reference> is a flowchart of the visual feature extraction process for media items, where first media item regions are selected and then the data within the regions is transformed. </paragraph>
<paragraph id="P-0092" lvl="0"><number>&lsqb;0092&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 7B</cross-reference> is a flow chart of a visual feature extraction process for media items, where first the media item is transformed and then regions in the feature domain are selected. </paragraph>
<paragraph id="P-0093" lvl="0"><number>&lsqb;0093&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 8</cross-reference> is a sketch of an media item frame with two arbitrary regions from which the features are computed. </paragraph>
<paragraph id="P-0094" lvl="0"><number>&lsqb;0094&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 9</cross-reference> is a video frame where the regions are rectangular windows (regions). </paragraph>
<paragraph id="P-0095" lvl="0"><number>&lsqb;0095&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 9A</cross-reference> is an example video frame with rectangular regions that show the data that is used for feature computation. </paragraph>
<paragraph id="P-0096" lvl="0"><number>&lsqb;0096&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 10</cross-reference> shows a flow diagram of a video transformation where the regions span one or more frames and the media item visual feature is optical flow. </paragraph>
<paragraph id="P-0097" lvl="0"><number>&lsqb;0097&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 11A</cross-reference> is an example flow diagram of visual hue feature computation for a key frame with rectangular windows. </paragraph>
<paragraph id="P-0098" lvl="0"><number>&lsqb;0098&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 11B</cross-reference> gives an example quantization of hue space to define visual hue codes. </paragraph>
<paragraph id="P-0099" lvl="0"><number>&lsqb;0099&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 12</cross-reference> is a flowchart of the visual feature vector computation from the visual part of the media item. </paragraph>
<paragraph id="P-0100" lvl="0"><number>&lsqb;0100&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 13A</cross-reference> shows a method for computing visual feature vectors from the visual part of the media item where both temporal and spatial properties of the media item are preserved. </paragraph>
<paragraph id="P-0101" lvl="0"><number>&lsqb;0101&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 13B</cross-reference> shows a method for computing visual feature vectors from the visual part of the media item where only the temporal properties of the media item are preserved. </paragraph>
<paragraph id="P-0102" lvl="0"><number>&lsqb;0102&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 13C</cross-reference> shows a method for computing visual feature vectors from the visual part of the media item where only the spatial properties of the media item are preserved. </paragraph>
<paragraph id="P-0103" lvl="0"><number>&lsqb;0103&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 13D</cross-reference> shows a preferred specific method for computing visual feature vectors from the visual part of the media item where the key intervals of the media item are ordered based on one or more features of the key intervals. </paragraph>
<paragraph id="P-0104" lvl="0"><number>&lsqb;0104&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 14A</cross-reference> is a flowchart of the process for computing a first visual feature vector in <cross-reference target="DRAWINGS">FIG. 13D</cross-reference>. </paragraph>
<paragraph id="P-0105" lvl="0"><number>&lsqb;0105&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 14B</cross-reference> is a flowchart of the process of shortening the first visual feature vector to a standard length vector in <cross-reference target="DRAWINGS">FIG. 13D</cross-reference>. </paragraph>
<paragraph id="P-0106" lvl="0"><number>&lsqb;0106&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 15</cross-reference> shows the process of combining visual and textual feature vectors to obtain a vector representing the disparate sources of information in the media item. </paragraph>
<paragraph id="P-0107" lvl="0"><number>&lsqb;0107&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 16</cross-reference> is a detailed system block diagram of the categorization system, illustrating how different formats of media items are handled in the categorization process. </paragraph>
<paragraph id="P-0108" lvl="0"><number>&lsqb;0108&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 17</cross-reference> introduces the idea of associating a feature vector with a continuous portion of the media item as opposed to associating the feature vector with the entire media item. </paragraph>
<paragraph id="P-0109" lvl="0"><number>&lsqb;0109&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 18</cross-reference> is a flowchart of a system that uses the media item classifier for segmenting a temporal media item into contiguous segments, where each segment is of one or more categories. </paragraph>
<paragraph id="P-0110" lvl="0"><number>&lsqb;0110&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 19</cross-reference> is a one-dimensional depiction of the categorization as a function of time function that points out problems with temporal media item categorization. </paragraph>
<paragraph id="P-0111" lvl="0"><number>&lsqb;0111&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 20A</cross-reference> gives heuristic aggregation rules based on category boundaries. </paragraph>
<paragraph id="P-0112" lvl="0"><number>&lsqb;0112&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 20B</cross-reference> gives heuristic aggregation rules based on category regions. </paragraph>
<paragraph id="P-0113" lvl="0"><number>&lsqb;0113&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 21A</cross-reference> is an instance of the flowchart of <cross-reference target="DRAWINGS">FIG. 18</cross-reference> where the optimal category aggregation is determined by optimizing a cost function. </paragraph>
<paragraph id="P-0114" lvl="0"><number>&lsqb;0114&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 21B</cross-reference> is a system for learning aggregation rules based on labeled training data and optional heuristic rules. </paragraph>
<paragraph id="P-0115" lvl="0"><number>&lsqb;0115&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 22</cross-reference> shows two examples of a lowest-cost interpretation rules and the application of the rules to a short sequence of categories. </paragraph>
<paragraph id="P-0116" lvl="0"><number>&lsqb;0116&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 23</cross-reference> is a business method for determining profiles of those multimedia item categories of particular interest to various different users retrieving multimedia content from a private and/or public network. </paragraph>
<paragraph id="P-0117" lvl="0"><number>&lsqb;0117&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 24A</cross-reference> is a business application of the multimedia item categorizer for building index tables to organize a multimedia item data base in terms of categories or indices. </paragraph>
<paragraph id="P-0118" lvl="0"><number>&lsqb;0118&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 24B</cross-reference> is a business application for retrieving of multimedia items from database based on the database organization and database indexing described in <cross-reference target="DRAWINGS">FIG. 24A</cross-reference>. </paragraph>
<paragraph id="P-0119" lvl="0"><number>&lsqb;0119&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 25</cross-reference> is a business application where one or more of multimedia items are received and the items are routed according to the multimedia items categories. </paragraph>
<paragraph id="P-0120" lvl="0"><number>&lsqb;0120&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 26</cross-reference> is a business application where one or more multimedia items are received and for each item a decision based on the category of the multimedia item is made. </paragraph>
<paragraph id="P-0121" lvl="0"><number>&lsqb;0121&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 27</cross-reference> is a business application where one or more multimedia items on a private and/or public network are examined and for each found multimedia item a decision based on the category of the multimedia item is made.</paragraph>
</section>
</brief-description-of-drawings>
<detailed-description>
<section>
<heading lvl="1">DETAILED DESCRIPTION OF THE INVENTION </heading>
<paragraph id="P-0122" lvl="0"><number>&lsqb;0122&rsqb;</number> This system categorizing media items has two distinct aspects. The first aspect is called the training phase which builds representations of the reference media items; the second phase is called the categorization phase, where instances media items are categorized. The training phase is an off-line process that involves processing of the reference media items to form a set of one or more categories. The categorization phase classifies a media item in a collection of such items by processing the media item to extract audio and visual features and using the media item class representations. </paragraph>
<paragraph id="P-0123" lvl="0"><number>&lsqb;0123&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 1</cross-reference> shows a prior art flowchart for a system <highlight><bold>100</bold></highlight> for categorizing text documents. In step <highlight><bold>110</bold></highlight>, a set of text documents is input to the system. Each text document is labeled as belong to a class S&equals;c<highlight><subscript>1</subscript></highlight>, i&equals;1, . . . , C. The classes S can be hierarchical, in the sense that each class S, can be recursively divided up into a number of subclasses, S&equals;S<highlight><subscript>subclass1</subscript></highlight>, S<highlight><subscript>subclass2</subscript></highlight>, . . . , S<highlight><subscript>subclassN</subscript></highlight>. In <highlight><bold>120</bold></highlight> a single vector is computing representing the text in each document in D. Such a vector V is a large-dimensional vector with entry n equal to 1 or 0, respectively, if word n is present, or not, in the document; or, such a vector V can be a large-dimensional vector with entry n equal to f where f is the number of times word n is present in the document. Examples of source of text vectors <highlight><bold>120</bold></highlight> include: close captions, open captions, captions, speech recognition applied to one or more audio input, semantic meanings derived from one or more audio streams, and global text information associated with the media item. In step <highlight><bold>130</bold></highlight> (<cross-reference target="DRAWINGS">FIG. 1</cross-reference>) each vector V is labeled the same as the corresponding document. Step <highlight><bold>140</bold></highlight> induces machine-learned classification methods for classifying unseen vectors V representing new unclassified documents. Finally, Box <highlight><bold>150</bold></highlight>, infers classification method to classify (categorize) unknown documents D, represented by feature vectors V. </paragraph>
<paragraph id="P-0124" lvl="0"><number>&lsqb;0124&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 2</cross-reference> shows a diagram of the various disparate input sources to the categorization process. The general multimedia input sources is media item <highlight><bold>200</bold></highlight>. This input source consists of audio <highlight><bold>210</bold></highlight>, text documents <highlight><bold>220</bold></highlight> and visual input <highlight><bold>230</bold></highlight>. Using the audio input, a speech transcript in computed (<highlight><bold>240</bold></highlight>) applying well-known techniques in the prior art. Possibly, if needed and available, a closed- or open-captioning (<highlight><bold>220</bold></highlight>) of the visual footage (<highlight><bold>230</bold></highlight>) is used to generate textual features <highlight><bold>120</bold></highlight> (very much in line with step <highlight><bold>120</bold></highlight> in <cross-reference target="DRAWINGS">FIG. 1</cross-reference>). Thirdly, the visual track <highlight><bold>230</bold></highlight> is transformed into characteristic visual feature spaces (<highlight><bold>250</bold></highlight>). The current invention puts these disparate sources of information, i.e., speech transcript representation <highlight><bold>240</bold></highlight>, closed- caption representation (general textual features) <highlight><bold>120</bold></highlight>, and visual features transformed into characteristic spaces <highlight><bold>250</bold></highlight> together in a unified framework. This framework allows the treatment of these sources of information in a coherent fashion. </paragraph>
<paragraph id="P-0125" lvl="0"><number>&lsqb;0125&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 3</cross-reference> shows a more specific way of processing the multimedia media item for categorization purposes. The input data, media item (<highlight><bold>300</bold></highlight>), is processed separately in terms of the visual track, <highlight><bold>305</bold></highlight>, and the audio track, <highlight><bold>310</bold></highlight>. The visual track <highlight><bold>305</bold></highlight> and the audio track are processed independently and concurrently. From the visual track (<highlight><bold>305</bold></highlight>), characteristic key frames or key intervals are selected <highlight><bold>320</bold></highlight>. These characteristic pieces of video are transformed into characteristic visual spaces <highlight><bold>330</bold></highlight> that in some way characterize the video clip in terms of visual features associated with the video categories. These visual space representations are transformed into sparse visual feature vectors (<highlight><bold>335</bold></highlight>). The audio track <highlight><bold>310</bold></highlight> is automatically transcribed into a precise as possible transcript. Additionally, possible available closed- and open-captioning is included in the textual description of the media item <highlight><bold>300</bold></highlight>. In <highlight><bold>360</bold></highlight> the textual content associated with the current media item <highlight><bold>330</bold></highlight> is tokenized (i.e., the sequence of characters in the text is divided into words and other meaningful tokens) and, optionally, stemmed (i.e., tokens are replaced by standard forms, roots, or morphological stems in order to reduce the size of the feature space ultimately constructed, such as replacing the word cats by cat, the word men by man, the work came by come, etc. Step <highlight><bold>365</bold></highlight> computes a textual representation vector of the current media item <highlight><bold>300</bold></highlight> under consideration. In output <highlight><bold>370</bold></highlight>, a collective coherent textual and visual representation of the media item is accomplished established. An out put vector <highlight><bold>380</bold></highlight> is the result of this process. </paragraph>
<paragraph id="P-0126" lvl="0"><number>&lsqb;0126&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 4</cross-reference>, flowchart <highlight><bold>400</bold></highlight>, shows, when supervised learning is employed, the complete integration of disparate media modules in the learning phase, i.e., the induction from labeled data of a classifier whose purpose is media item categorization. In the initial step <highlight><bold>410</bold></highlight>, the system accepts as input a data set D consisting of media items, each labeled as belonging to 0 or more classes from a set or hierarchy of classes S. Steps <highlight><bold>420</bold></highlight> and <highlight><bold>430</bold></highlight> may be permuted or carried out simultaneously. In <highlight><bold>420</bold></highlight>, the system constructs a single vector representation of text features and/or audio features extracted or associated with each media item in D. These features may be present in a transcript produced by voice recognition software, in close-captioned text, or in open-captioned text. Some features may indicate the presence of or the character, appropriately quantized, of other audible characteristics of the media segment, such as music, silence, and loud noises. In <highlight><bold>430</bold></highlight>, the system constructs a single vector representation of the visual features extracted or associated with each media item in D. In <highlight><bold>440</bold></highlight>, for each labeled media item in the data set D, the system constructs a training set T(D) by combining the two vector representations of that media segment (constructed in <highlight><bold>420</bold></highlight> and <highlight><bold>430</bold></highlight>) into a single composite feature vector, with the resulting vector labeled by the same set of classes used to label the media item. Optionally, in <highlight><bold>440</bold></highlight>, before combining the vector representations, the system may uniformly transform one or both of those set of representations in order to assure compatibility. Among the ways that incompatibility may arise may be (1) a marked difference in the number of values that may appear as components of the vectors and (2) a marked difference in the norms or sizes of the vectors present in the two sets. The exact criterion for what constitutes a marked difference between to the sets of vectors will depend in practice on the particular technique of supervised learning being employed, and it may idiosyncratically depend on the data set D. Thus, in practice, the criterion may be experimentally determined by the evaluation of different classifiers induced under different assumptions. At any rate, in <highlight><bold>440</bold></highlight>, the system ultimately produces, normally by concatenation of the (possibly transformed) feature vectors produced in <highlight><bold>420</bold></highlight> and <highlight><bold>430</bold></highlight>, a composite labeled feature vector is ultimately produced for each media item in D. In <highlight><bold>450</bold></highlight>, the system uses a supervised learning technique&mdash;a wide variety of them exist&mdash;with T(D) as training data to induce a classifier that can be used to assign classes in S to a hitherto unseen feature vector with the same structure as those in T(D). In <highlight><bold>460</bold></highlight>, the system outputs the classifier induced in <highlight><bold>450</bold></highlight>, as well as any parameters, information or settings needed to represent hitherto unseen media items as unified feature vectors with exactly the same format and structure as those in T(D), so that the classifier induced in <highlight><bold>450</bold></highlight> can be legitimately applied to those vectors. </paragraph>
<paragraph id="P-0127" lvl="0"><number>&lsqb;0127&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 5</cross-reference> shows a typical example of video classification rules derived from disparate information sources such as the visual and audio track. These sources are visual, <highlight><bold>510</bold></highlight> and <highlight><bold>516</bold></highlight>, and auditory, <highlight><bold>512</bold></highlight> and <highlight><bold>514</bold></highlight>. In the speech track the words &ldquo;golf&rdquo; and &ldquo;grass&rdquo; (<highlight><bold>512</bold></highlight> &amp; <highlight><bold>514</bold></highlight>) are detected. The combination of these two words point <highlight><bold>520</bold></highlight> point to the fact that we are dealing with a piece of video footage of category sports <highlight><bold>525</bold></highlight>, with the subject golf <highlight><bold>530</bold></highlight>. However, the visual circumstances in the topic of golf <highlight><bold>510</bold></highlight>, where the actual game of golf is played, and a relatively static scene <highlight><bold>516</bold></highlight> of a &ldquo;talking head&rdquo; indicating a golf interview. Items <highlight><bold>510</bold></highlight> and <highlight><bold>520</bold></highlight> together infer that the medium items has category &ldquo;sports&rdquo; <highlight><bold>545</bold></highlight> with subject &ldquo;golf game&rdquo; <highlight><bold>548</bold></highlight>. On the other hand, when the footage is relatively static, <highlight><bold>516</bold></highlight>, it is clear that the category of the footage is sports again &ldquo;<highlight><bold>555</bold></highlight>&rdquo; but that the subjects is an interview about golf &ldquo;<highlight><bold>558</bold></highlight>.&rdquo;</paragraph>
<paragraph id="P-0128" lvl="0"><number>&lsqb;0128&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 6</cross-reference> shows a block diagram of the system architecture. The top portion (<cross-reference target="DRAWINGS">FIG. 6A</cross-reference>) shows the supervised/unsupervised phase <highlight><bold>610</bold></highlight>, i.e., the part of the system that handles the computation of classes and learns a representation (i.e., class/category representations <highlight><bold>675</bold></highlight>) of the reference media items <highlight><bold>610</bold></highlight>. The learning is achieved through training engine <highlight><bold>620</bold></highlight>, which computes the class representations <highlight><bold>675</bold></highlight>. The bottom portion (<cross-reference target="DRAWINGS">FIG. 6B</cross-reference>) shows the classification/categorization phase <highlight><bold>615</bold></highlight> using the classification/categorization engine <highlight><bold>650</bold></highlight>. Here features are computed from the target incoming medium item M <highlight><bold>660</bold></highlight>. The class/category of medium item <highlight><bold>660</bold></highlight> is reported to <highlight><bold>685</bold></highlight>. Each of these phases is described in detail below. </paragraph>
<paragraph id="P-0129" lvl="0"><number>&lsqb;0129&rsqb;</number> The training phase, as shown in <cross-reference target="DRAWINGS">FIG. 6</cross-reference>A, includes the key frame or key interval (hereafter referred to as key interval) selection step, the visual feature generation step, and the visual feature vector population step. Each of these steps is discussed below. </paragraph>
<paragraph id="P-0130" lvl="0"><number>&lsqb;0130&rsqb;</number> Visual features <highlight><bold>775</bold></highlight> are computed from key intervals. Two methods for visual feature computation are described in <cross-reference target="DRAWINGS">FIGS. 7A and 7B</cross-reference>. The media item <highlight><bold>750</bold></highlight> in this case has a time dimension <highlight><bold>755</bold></highlight>. Characteristic key frames, e.g., a frame a t<highlight><subscript>0 </subscript></highlight><highlight><bold>760</bold></highlight>, or characteristic key intervals <highlight><bold>770</bold></highlight> are selected. A key interval could be a window &lsqb;delta<highlight><subscript>t1</subscript></highlight>, delta<highlight><subscript>t2</subscript></highlight>&rsqb; around frame a t<highlight><subscript>0 </subscript></highlight><highlight><bold>760</bold></highlight>. So far, the methods described in <cross-reference target="DRAWINGS">FIG. 7A and 7B</cross-reference> are the same. Key intervals <highlight><bold>770</bold></highlight> or key frames <highlight><bold>760</bold></highlight> are also selected in the same fashion for both methods. These key frames or intervals could be in the middle of shots, they could be at the beginning and end of shots, or the could be equally spaced over the shots. Alternatively, the key intervals can be selected based on visual properties of the media item, such as, temporal regions of minimal motion or temporal regions of maximal motion. </paragraph>
<paragraph id="P-0131" lvl="0"><number>&lsqb;0131&rsqb;</number> In <cross-reference target="DRAWINGS">FIG. 7</cross-reference>A, the visual feature computation <highlight><bold>705</bold></highlight> proceeds as follows. Given the key frame or key interval <highlight><bold>770</bold></highlight>, this interval is quantized into regions in step <highlight><bold>706</bold></highlight>. In step <highlight><bold>707</bold></highlight>, the data in each region are transformed into a different feature space. (Such a transformation could, for example, be visual flow computation for each region.) Following this, in step <highlight><bold>708</bold></highlight>, region features are computed. (In the visual flow example this could be average flow in a region.) The visual features <highlight><bold>775</bold></highlight> are output of visual feature computation process <highlight><bold>705</bold></highlight>. In <cross-reference target="DRAWINGS">FIG. 7</cross-reference>A, the visual feature computation <highlight><bold>705</bold></highlight> proceeds differently. Now, in step <highlight><bold>717</bold></highlight> first the data in the key frame or key interval is transformed into a different feature space. (Such a transformation could, for instance, be a Fourier transform.) In step <highlight><bold>716</bold></highlight>, the values of this transformation are regarded as a domain and this domain is quantized into regions. (If the transformation is the Fourier transform, the domain is the frequency spectrum.) For each of the regions in that domain, in step <highlight><bold>708</bold></highlight>, region features are computed. The visual features <highlight><bold>775</bold></highlight> are output of visual feature computation process <highlight><bold>705</bold></highlight>. The visual features are coarsely quantized into codes (see, for example, <cross-reference target="DRAWINGS">FIG. 11B</cross-reference>) and these codes are used to populate feature vectors as in <cross-reference target="DRAWINGS">FIG. 13</cross-reference>. </paragraph>
<paragraph id="P-0132" lvl="0"><number>&lsqb;0132&rsqb;</number> The feature transformation and feature extraction steps are dependent on the type of similarity to be measured. For example, image color based coding processes have been discussed in Smith et al. There are several other techniques of media transformation and feature computation that are well known in the art. See Smith and Chang, Tools and Techniques for Color Image Retrieval, In IS&amp;T/SPIE Proc Vol. 2670, Storage and Retrieval for Image and Video Databases. </paragraph>
<paragraph id="P-0133" lvl="0"><number>&lsqb;0133&rsqb;</number> The feature-based vector generation process is applied to all the key intervals selected by the key-framing process. The media item M in <cross-reference target="DRAWINGS">FIGS. 7A and B</cross-reference> is depicted by a horizontal rectangle <highlight><bold>750</bold></highlight>. </paragraph>
<paragraph id="P-0134" lvl="0"><number>&lsqb;0134&rsqb;</number> Time running from time 0 to time T seconds is indicated by the time axis <highlight><bold>755</bold></highlight>. A key frame is selected at time is t<highlight><subscript>0 </subscript></highlight>(<highlight><bold>760</bold></highlight>) or, alternatively, a key interval (<highlight><bold>770</bold></highlight>) is selected between times delta<highlight><subscript>t1 </subscript></highlight>and delta<highlight><subscript>t2</subscript></highlight>. </paragraph>
<paragraph id="P-0135" lvl="0"><number>&lsqb;0135&rsqb;</number> In general, the visual feature computation is comprised of a number of steps. The complexity of each of the steps is dependent on the feature being considered. In this invention, for media items we describe in more or less detail two visual features, namely, image color-based codes and optical flow-based codes. As indicated above, this feature code generation process, that generates feature values <highlight><bold>775</bold></highlight>, is comprised of two steps, namely, the step where features are extracted <highlight><bold>705</bold></highlight> and a quantization or code generation step as in <cross-reference target="DRAWINGS">FIG. 13</cross-reference>. Each of these steps is discussed below. </paragraph>
<paragraph id="P-0136" lvl="0"><number>&lsqb;0136&rsqb;</number> Referring again to <cross-reference target="DRAWINGS">FIGS. 7A and B</cross-reference>, in the feature vector computation step <highlight><bold>705</bold></highlight>, the data in the key intervals are processed to extract different types of measurements or visual features. These measurements may be based on both global and local spatial or spatio-temporal properties of a key interval. These measurements may also be extracted from predetermined portions (regions) of the key intervals. </paragraph>
<paragraph id="P-0137" lvl="0"><number>&lsqb;0137&rsqb;</number> For example, in <cross-reference target="DRAWINGS">FIG. 8 a</cross-reference> part of a media item <highlight><bold>810</bold></highlight> (e.g., key interval) is shown with two arbitrary shaped regions, <highlight><bold>820</bold></highlight> and <highlight><bold>830</bold></highlight>, region (1) and region (2), respectively. Only from the visual data in these regions, the feature values <highlight><bold>775</bold></highlight> for (say) feature F<highlight><subscript>j </subscript></highlight>are computed. The regions, <highlight><bold>820</bold></highlight> . . . <highlight><bold>830</bold></highlight>, here can cover the complete key interval and the regions can differ the features F<highlight><subscript>j</subscript></highlight>, j&equals;1, 2, . . . that are used. Each feature F<highlight><subscript>j </subscript></highlight>may have different regions W<highlight><subscript>j </subscript></highlight>and a different number of these local regions. </paragraph>
<paragraph id="P-0138" lvl="0"><number>&lsqb;0138&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 9</cross-reference> shows an example where the media item key frames <highlight><bold>900</bold></highlight> for feature F<highlight><subscript>j </subscript></highlight>are divided up into 16 rectangular windows (regions) <highlight><bold>901</bold></highlight>, <highlight><bold>902</bold></highlight>, . . . , <highlight><bold>903</bold></highlight>, . . . , <highlight><bold>904</bold></highlight> (the local regions). Other windowing schemes are easily contemplated by persons skilled in the art. </paragraph>
<paragraph id="P-0139" lvl="0"><number>&lsqb;0139&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 9A</cross-reference> gives an example of a video frame <highlight><bold>950</bold></highlight> (part of the media item) with the window structure <highlight><bold>901</bold></highlight>, <highlight><bold>902</bold></highlight>, . . . , <highlight><bold>903</bold></highlight>, . . . , <highlight><bold>904</bold></highlight> (as in <cross-reference target="DRAWINGS">FIG. 9</cross-reference>) overlaid. Each window <highlight><bold>960</bold></highlight> contains data which is used for visual feature vector computation; the media transformation <highlight><bold>705</bold></highlight> would use data in the whole frame <highlight><bold>950</bold></highlight>. In the case where the domain regions are windows, with w<highlight><subscript>1 </subscript></highlight>windows horizontally and w<highlight><subscript>2 </subscript></highlight>windows vertically we have W<highlight><subscript>j</subscript></highlight>&equals;w<highlight><subscript>1</subscript></highlight>&times;w<highlight><subscript>2 </subscript></highlight>regions. The media item region selection (<highlight><bold>706</bold></highlight>) for each visual feature j which has W<highlight><subscript>j </subscript></highlight>quantized elements. These features are then stored in feature vector <highlight><bold>755</bold></highlight>. </paragraph>
<paragraph id="P-0140" lvl="0"><number>&lsqb;0140&rsqb;</number> Referring to <cross-reference target="DRAWINGS">FIG. 7</cross-reference>A, media regions are selected <highlight><bold>706</bold></highlight> every time t<highlight><subscript>0 </subscript></highlight>(<highlight><bold>760</bold></highlight>), i.e., a key interval, is encountered. Each feature F<highlight><subscript>j</subscript></highlight>, j&equals;1, . . . , m is computed from the data in the appropriate regions for he different features F<highlight><subscript>j</subscript></highlight>. The region transformation <highlight><bold>707</bold></highlight> depicts the transformation of media into feature space, such features can be color space transformation, edge detection, optical flow, etc. In general, the resulting feature values are not directly used as input to the feature range quantization step (<highlight><bold>715</bold></highlight>). Rather, an intermediate transform <highlight><bold>708</bold></highlight> is applied that maps the features obtained by region transformation <highlight><bold>707</bold></highlight> into a smaller number of feature values for each F<highlight><subscript>j</subscript></highlight>, j&equals;1, . . . , m. An example is to compute the average hue for the regions <highlight><bold>901</bold></highlight>, <highlight><bold>902</bold></highlight>, . . . , <highlight><bold>903</bold></highlight>, . . . , <highlight><bold>904</bold></highlight> in <cross-reference target="DRAWINGS">FIG. 9</cross-reference>. Or, it maps a number of high-contrast edge element located in the form of video text into the coordinates of the window that contains the edge elements. In essence, the transformation <highlight><bold>708</bold></highlight> is a data reduction step so that the vector representations for the media items S&equals;s<highlight><subscript>1</subscript></highlight>, . . . , s<highlight><subscript>n </subscript></highlight>are as sparse as possible and yet the reference streams can still be distinguished. </paragraph>
<paragraph id="P-0141" lvl="0"><number>&lsqb;0141&rsqb;</number> Referring to <cross-reference target="DRAWINGS">FIG. 7</cross-reference>B, it is alternatively possible that all the data in the key interval can be transformed through a media item transform <highlight><bold>717</bold></highlight> into a feature space G. Hence, a mapping <highlight><bold>717</bold></highlight> is performed of all the data in a key interval into a feature space G. (An example here is a Fourier transform of the data in the key interval, with G one- or two-dimensional frequency spaces.) Then local regions of feature space G (feature regions), obtained through feature region selection <highlight><bold>716</bold></highlight>, are used as input to the region feature computation step <highlight><bold>708</bold></highlight>. </paragraph>
<paragraph id="P-0142" lvl="0"><number>&lsqb;0142&rsqb;</number> An example of this case is the computation of features from the audio track. The key (time) interval now could be &lsqb;delta<highlight><subscript>t1</subscript></highlight>, delta<highlight><subscript>t2</subscript></highlight>&rsqb; (<highlight><bold>770</bold></highlight> in <cross-reference target="DRAWINGS">FIG. 7B</cross-reference>) and one of the features F can be the Fourier transform of this time interval. This gives the distribution of audio frequencies, from low to high, in the time interval &lsqb;delta<highlight><subscript>t1</subscript></highlight>, delta<highlight><subscript>t2</subscript></highlight>&rsqb;. Regions in the domain &lsqb;0,f<highlight><subscript>max</subscript></highlight>) of G of the frequency distribution function are used to compute audio features (rather than the domain being the key interval). Here f<highlight><subscript>max </subscript></highlight>is the maximum frequency in the key interval. For instance, one could use a subdivision of &lsqb;0, f<highlight><subscript>max</subscript></highlight>) into equal sub intervals (regions), and the output of transformation <highlight><bold>708</bold></highlight> (region-based-feature-computation) could be simply the average value or energy in the sub intervals. </paragraph>
<paragraph id="P-0143" lvl="0"><number>&lsqb;0143&rsqb;</number> The visual feature extraction can encompass many different transformations and quantizations. In the case of visual media, it includes, computing the color histogram of a key frame, computing color histogram of selected spatial regions in the key frame, computing the pixel difference between two or more temporally displaced key frames, computing a measure to detect the presence of high-contrast regions in a key frame (like scene-text), or computing the optical flow or other spatial displacements between two subsequent frames (or possible frames spaced further apart) within a key interval. <cross-reference target="DRAWINGS">FIG. 10</cross-reference> shows this last case. A video stream <highlight><bold>1010</bold></highlight> is the input to a video transformation <highlight><bold>1020</bold></highlight> that computes the optical flow <highlight><bold>1030</bold></highlight> (transformed media). A possible way to implement this is to select key intervals of frames <highlight><bold>1002</bold></highlight> . . . <highlight><bold>1008</bold></highlight> in video stream <highlight><bold>1010</bold></highlight> as input to the video transformation <highlight><bold>1020</bold></highlight>. Each key interval is transformed into a feature domain which in this case are individual frames <highlight><bold>1032</bold></highlight> . . . <highlight><bold>1038</bold></highlight> of optical flow. Every pixel in these frame contains two values, x and y, that together represent the optical flow at the pixel in vector form (x, y)<highlight><superscript>t</superscript></highlight>. The optical flow is computed by comparing and matching two or more consecutive frames in the key intervals. Optical flow computation is well known in the prior art. Besides optical flow, any other transformation that has as domain two or more frames, be it consecutive frames or frames spaced apart, and maps this data into a function on some other domain that can be used as features for categorizing video sequences. </paragraph>
<paragraph id="P-0144" lvl="0"><number>&lsqb;0144&rsqb;</number> To put it simply, the domain is the data space from which the visual features are computed. The domain can be the video frame itself, which amounts to the red, green and blue channels; or, e.g., luminance. On the other hand, the domain could be the a functional transformation of two or more frames into a different domain, as optical flow between the frames. In that case, the domain is space of optical flow from which the visual features are computed. </paragraph>
<paragraph id="P-0145" lvl="0"><number>&lsqb;0145&rsqb;</number> Categorization segments of a media item M, is aided by using some color quantization, for example, the following frame color codes. The color space of frames (images) has been extensively used for indexing and searching based on image content. Application of hue color code, a preferred embodiment of this invention, is comprised of a number of steps (see <cross-reference target="DRAWINGS">FIG. 11A</cross-reference>). </paragraph>
<paragraph id="P-0146" lvl="0"><number>&lsqb;0146&rsqb;</number> Color is a good feature for video categorization. In particular, the hue component of color contains much information. Hue is the portion of color information that indicates which color a particular pixel in frame <highlight><bold>1110</bold></highlight> has. The colors range from red, green to magenta (see <cross-reference target="DRAWINGS">FIG. 11B</cross-reference>.) Hence, from the video signal, which is as input to the system is typically in an RGB or YIQ format, the hue component has to be extracted.. This requires a transformation from the original color space (RGB or YIQ) of the media item to the HSV color space. This is achieved by using the standard algorithms <highlight><bold>1115</bold></highlight> for color space conversions (e.g., Foley &amp; van Dam, Chapter 13). Once the image is transformed in to the HSV model, the hue channel is separated from the HSV model, as the code generation is based on the hue values of the pixels in the key frames. </paragraph>
<paragraph id="P-0147" lvl="0"><number>&lsqb;0147&rsqb;</number> Refer now to the block diagram of <cross-reference target="DRAWINGS">FIG. 11A</cross-reference>. Let media item <highlight><bold>750</bold></highlight> be the input and frame <highlight><bold>1110</bold></highlight> at time t<highlight><subscript>0 </subscript></highlight><highlight><bold>760</bold></highlight> be the current key frame. This frame <highlight><bold>1110</bold></highlight> could be in YIQ format and is denoted as YIQ(frame) in <cross-reference target="DRAWINGS">FIG. 11A</cross-reference>. Then in block <highlight><bold>1175</bold></highlight> the feature (hue) vectors <highlight><bold>1155</bold></highlight> for this frame are computed. The regions for the frame are rectangular windows <highlight><bold>1130</bold></highlight>. The output of process <highlight><bold>1175</bold></highlight> therefore is a vector F&equals;(f<highlight><subscript>11</subscript></highlight>, f<highlight><subscript>12</subscript></highlight>, . . . , f<highlight><subscript>1n</subscript></highlight>, f<highlight><subscript>i1</subscript></highlight>, f<highlight><subscript>i2</subscript></highlight>, . . . , f<highlight><subscript>in</subscript></highlight>, f<highlight><subscript>N1</subscript></highlight>, f<highlight><subscript>N2</subscript></highlight>, . . . , f<highlight><subscript>Nn</subscript></highlight>) with N the number of windows and n the number of color tokens per window. </paragraph>
<paragraph id="P-0148" lvl="0"><number>&lsqb;0148&rsqb;</number> The first step in <highlight><bold>1175</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 11</cross-reference>A, is to convert the the YIQ encoded frame into an HSV encoded frame <highlight><bold>1115</bold></highlight>. This is well known in the prior art. The output then of <highlight><bold>1115</bold></highlight> is the hue part of the color information of the frame, denoted as Hue(frame) <highlight><bold>1120</bold></highlight>. In <cross-reference target="DRAWINGS">FIG. 8A</cross-reference> the frame is divided up into windows <highlight><bold>1130</bold></highlight>. (Note that these need not be rectangular, they can be arbitrarily shaped as show in <cross-reference target="DRAWINGS">FIGS. 8 &amp; 8A</cross-reference>. Rectangles are a specific case of an arbitrary shaped window.) Block <highlight><bold>1130</bold></highlight> outputs Hue(window) <highlight><bold>1135</bold></highlight> for each window <highlight><bold>901</bold></highlight>, <highlight><bold>902</bold></highlight>, . . . , <highlight><bold>903</bold></highlight>, . . . , <highlight><bold>904</bold></highlight> in the current frame. Subsequently, block <highlight><bold>1140</bold></highlight> determines the average hue, Average_hue(window), in the windows. The averaging is the first data reduction step. (Note that other averaging methods are contemplated. For example in one embodiment, the median of the window instead of the average is used and it is more robust to noise.) The second data reduction step in <highlight><bold>1175</bold></highlight>, is block <highlight><bold>1150</bold></highlight> which quantizes Average_hue(window) into a small number of quantization levels per window and assigns codes to each quantization level. The mechanics of this quantization and code assignment is explained in <cross-reference target="DRAWINGS">FIG. 11B</cross-reference>. As noted above, the output of step <highlight><bold>1175</bold></highlight> is a feature vector F &equals;(f<highlight><subscript>11</subscript></highlight>, f<highlight><subscript>12</subscript></highlight>, . . . , f<highlight><subscript>1n</subscript></highlight>, f<highlight><subscript>l1</subscript></highlight>, f<highlight><subscript>l2</subscript></highlight>, . . . , f<highlight><subscript>ln</subscript></highlight>, f<highlight><subscript>N1</subscript></highlight>, f<highlight><subscript>N2</subscript></highlight>, . . . , f<highlight><subscript>Nn</subscript></highlight>) <highlight><bold>1155</bold></highlight>. </paragraph>
<paragraph id="P-0149" lvl="0"><number>&lsqb;0149&rsqb;</number> There are a number of different ways of extracting feature values Average_hue(window) <highlight><bold>1145</bold></highlight>. For example, at one extreme case the hue value at each pixel can be considered as a feature (i.e., the windows are the pixels) or at the other extreme the hue value of all the pixels in the frame can be averaged to generate the feature value (i.e., the window is the frame). In a preferred embodiment, as indicated above, the frame is divided into w<highlight><subscript>1 </subscript></highlight>windows in one dimension of the image and w<highlight><subscript>2 </subscript></highlight>windows along the other dimension as in <cross-reference target="DRAWINGS">FIG. 9</cross-reference>. An average hue value is computed based on the pixels in each window. Thus the hue color for a video key frame is a set of w<highlight><subscript>1</subscript></highlight>&times;w<highlight><subscript>2 </subscript></highlight>average hue values. Quantized these give a feature vector C for each frame. </paragraph>
<paragraph id="P-0150" lvl="0"><number>&lsqb;0150&rsqb;</number> In <cross-reference target="DRAWINGS">FIG. 11B, 1180</cross-reference> is the hue value of a pixel. The hue values can range from 0 degrees to 360 degrees, in <cross-reference target="DRAWINGS">FIG. 11B</cross-reference> indicated by <highlight><bold>1160</bold></highlight> (0 degrees), <highlight><bold>1161</bold></highlight> (30 degrees), <highlight><bold>1162</bold></highlight> (60 degrees), . . . , <highlight><bold>1163</bold></highlight> (180 degrees), . . . , <highlight><bold>1164</bold></highlight> (330 degrees), <highlight><bold>1165</bold></highlight> (360 degrees). The hue values from 330 degrees (<highlight><bold>1164</bold></highlight>) through 30 degrees (<highlight><bold>1161</bold></highlight>) are centered around the color pure red (<highlight><bold>1170</bold></highlight>), from 30 degrees (<highlight><bold>1161</bold></highlight>) through 90 degrees around the color pure yellow (<highlight><bold>1171</bold></highlight>) etc. To arrive at hue feature vectors <highlight><bold>1155</bold></highlight>, the hue value outputs of the averaging operation <highlight><bold>1140</bold></highlight> need to be coarsely quantized. This quantization, and code assignment, to obtain feature vectors <highlight><bold>1155</bold></highlight> is performed in step <highlight><bold>1150</bold></highlight>. <cross-reference target="DRAWINGS">FIG. 11B</cross-reference> gives a possible hue quantization, code assignment, table <highlight><bold>1180</bold></highlight>. Coding could be performed according to the following table  
<table-cwu id="TABLE-US-00001">
<number>1</number>
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="4">
<colspec colname="1" colwidth="119PT" align="center"/>
<colspec colname="2" colwidth="21PT" align="center"/>
<colspec colname="3" colwidth="35PT" align="center"/>
<colspec colname="4" colwidth="42PT" align="center"/>
<thead>
<row>
<entry></entry>
</row>
<row><entry namest="1" nameend="4" align="center" rowsep="1"></entry>
</row>
<row>
<entry></entry>
<entry></entry>
<entry></entry>
<entry>Number in</entry>
</row>
<row>
<entry>Color range</entry>
<entry>Code</entry>
<entry>Color</entry>
<entry>Figure</entry>
</row>
<row><entry namest="1" nameend="4" align="center" rowsep="1"></entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry>330 &lt; Average_hue (window) &lt;&equals; 30&ensp;</entry>
<entry>0</entry>
<entry>Red</entry>
<entry>1170</entry>
</row>
<row>
<entry>30 &lt; Average_hue (window) &lt;&equals; 90</entry>
<entry>1</entry>
<entry>Yellow</entry>
<entry>1171</entry>
</row>
<row>
<entry>&ensp;90 &lt; Average_hue (window) &lt;&equals; 150</entry>
<entry>2</entry>
<entry>Green</entry>
<entry>1172</entry>
</row>
<row>
<entry>150 &lt; Average_hue (window) &lt;&equals; 210</entry>
<entry>3</entry>
<entry>Cyan</entry>
<entry>1173</entry>
</row>
<row>
<entry>210 &lt; Average_hue (window) &lt;&equals; 270</entry>
<entry>4</entry>
<entry>Blue</entry>
<entry>1174</entry>
</row>
<row>
<entry>270 &lt; Average_hue (window) &lt;&equals; 330</entry>
<entry>5</entry>
<entry>Magenta</entry>
<entry>1175</entry>
</row>
<row><entry namest="1" nameend="4" align="center" rowsep="1"></entry>
</row>
</tbody>
</tgroup>
</table>
</table-cwu>
</paragraph>
<paragraph id="P-0151" lvl="0"><number>&lsqb;0151&rsqb;</number> There are several different ways in which a feature can be quantized. The choice of quantization can affect the categorization processes. </paragraph>
<paragraph id="P-0152" lvl="0"><number>&lsqb;0152&rsqb;</number> The feature-based code generation steps discussed above have been separated out as steps for clarity of presentation. However, these steps are combined to minimize the computation required to generate these feature vectors. </paragraph>
<paragraph id="P-0153" lvl="0"><number>&lsqb;0153&rsqb;</number> The feature vector extraction and coding process described above is one specific method of generating the feature vectors. Depending on the kind of similarity metric being used, the feature extraction and coding process can be significantly different. The vector representation mechanism and its efficiency in performing media item categorization are not significantly affected by the coding scheme itself. For example, one possible metric of similarity is the motion similarity of image sequences, that is, here video sequences are compared based on flow rather than color. Such a coding and similarity measurement scheme can be used within the frame work proposed in this invention (see R. Mohan, &ldquo;Video Sequence Matching&rdquo;, cited above). </paragraph>
<paragraph id="P-0154" lvl="0"><number>&lsqb;0154&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 12</cross-reference> is a flowchart <highlight><bold>1200</bold></highlight> of the process of the visual feature vector computation from the visual part of the media item. Input to this process is the visual part of the media item <highlight><bold>1210</bold></highlight>. Following this step is the spatial and temporal quantization <highlight><bold>1220</bold></highlight> of the visual part in the media item. The temporal quantization is the process of selecting key frames or key intervals as in <cross-reference target="DRAWINGS">FIGS. 7A and 7B</cross-reference>. The spatial quantization is the process described in <cross-reference target="DRAWINGS">FIGS. 8 and 9</cross-reference>. From this spatio-temporally (both in image/frame space and in time) quantized visual data, features are computed <highlight><bold>1230</bold></highlight>, such as flow as in <cross-reference target="DRAWINGS">FIG. 10</cross-reference> and hue as in <cross-reference target="DRAWINGS">FIG. 11</cross-reference>. In step <highlight><bold>1240</bold></highlight>, these feature values are coarsely quantized <highlight><bold>1240</bold></highlight> into a small number of discrete values, or codes (e.g., the quantization <highlight><bold>1180</bold></highlight> of the hue values in <cross-reference target="DRAWINGS">FIG. 11B</cross-reference>). A process of mapping or counting <highlight><bold>1250</bold></highlight> (described in <cross-reference target="DRAWINGS">FIG. 13</cross-reference>) derives a visual feature vector, F<highlight><subscript>v</subscript></highlight>, <highlight><bold>1260</bold></highlight>. </paragraph>
<paragraph id="P-0155" lvl="0"><number>&lsqb;0155&rsqb;</number> Moving on to <cross-reference target="DRAWINGS">FIG. 13</cross-reference>, here three possible methods for constructing visual feature vectors F<highlight><subscript>v </subscript></highlight>are given. The visual part of the media item <highlight><bold>1310</bold></highlight> consists in this figure of key frames or key intervals <highlight><bold>1303</bold></highlight>, <highlight><bold>1306</bold></highlight>, and <highlight><bold>1309</bold></highlight>. Let N be the number of key frames or key intervals, W the number of windows (regions) per key frame/interval, and C the number of codes for a visual feature. </paragraph>
<paragraph id="P-0156" lvl="0"><number>&lsqb;0156&rsqb;</number> A first method to determine a visual feature vector is given in <cross-reference target="DRAWINGS">FIG. 13A</cross-reference>. The visual feature vector <highlight><bold>1320</bold></highlight> here is determined by a mappings <highlight><bold>1325</bold></highlight>, <highlight><bold>1327</bold></highlight> from quantized feature values c<highlight><subscript>1</subscript></highlight>, c<highlight><subscript>2</subscript></highlight>, c<highlight><subscript>3 </subscript></highlight>of key intervals to the feature vector <highlight><bold>1320</bold></highlight>. The coded feature values c<highlight><subscript>1</subscript></highlight>, c<highlight><subscript>2</subscript></highlight>, c<highlight><subscript>3 </subscript></highlight>are mapped <highlight><bold>1325</bold></highlight> to entries of the visual feature vector <highlight><bold>1320</bold></highlight>. In that case, for a specific feature, the visual feature vector will be of length W&times;N, where W is the number of regions per key interval and N is the number of key intervals. Alternatively, the codes c<highlight><subscript>1</subscript></highlight>, c<highlight><subscript>2</subscript></highlight>, c<highlight><subscript>3 </subscript></highlight>could represent the absence &lsquo;c&equals;0&rsquo; or presence &lsquo;c&equals;1&rsquo; of a certain feature; or, the codes c<highlight><subscript>1</subscript></highlight>, c<highlight><subscript>2</subscript></highlight>, c<highlight><subscript>3 </subscript></highlight>could represent the absence &lsquo;c&equals;0&rsquo; or the amount of presence &lsquo;c&equals;x&rsquo;, with 1&lt;x&lt;C where C is some upper bound. An example of the former could be the color feature red, an example of the latter could be a visual motion feature. Note that the length of the visual feature vector F<highlight><subscript>v </subscript></highlight>depends on the number of key intervals N, the number of regions W, and the number of codes C. That is, the length is N&times;W. Both spatial and temporal information about the key interval is preserved in the visual feature vector F<highlight><subscript>v</subscript></highlight>. </paragraph>
<paragraph id="P-0157" lvl="0"><number>&lsqb;0157&rsqb;</number> A second method to determine a visual feature vector is given in <cross-reference target="DRAWINGS">FIG. 13B</cross-reference>. The visual feature vector <highlight><bold>1330</bold></highlight> here is determined by a counting <highlight><bold>1335</bold></highlight>, <highlight><bold>1337</bold></highlight> the occurrences of feature code values in the key intervals. Key interval <highlight><bold>1303</bold></highlight> has codes c&equals;1, 2, . . . , 6 (in this case the number of codes C is 6) associated with each of the spatial regions. The first 6 elements of the visual feature vector F<highlight><subscript>v </subscript></highlight><highlight><bold>1330</bold></highlight> are determined by counting the codes in the first key interval <highlight><bold>1303</bold></highlight>. Code 1 is associated with 8 spatial regions, hence F<highlight><subscript>v</subscript></highlight>(1)&equals;8, code 2 is associated with 4 spatial regions, hence F<highlight><subscript>v</subscript></highlight>(2)&equals;4, and so forth. The next 6 elements of the visual feature vector F<highlight><subscript>v </subscript></highlight><highlight><bold>1330</bold></highlight> are determined by counting <highlight><bold>1337</bold></highlight> the codes in the second key interval <highlight><bold>1306</bold></highlight>. This process is repeated for all key intervals and all visual features. Note that again the length of the visual feature vector F<highlight><subscript>v </subscript></highlight>depends on the number of key intervals N but not on the number regions W. In particular, the length of the visual feature vector is N&times;C. Also note that the information of the spatial arrangement of the codes in the key intervals is lost. </paragraph>
<paragraph id="P-0158" lvl="0"><number>&lsqb;0158&rsqb;</number> A third method to determine a visual feature vector is given in <cross-reference target="DRAWINGS">FIG. 13C</cross-reference>. The visual feature vector <highlight><bold>1340</bold></highlight> . . . <highlight><bold>1350</bold></highlight> here is again determined by a counting <highlight><bold>1345</bold></highlight>, <highlight><bold>1355</bold></highlight> the occurrences of feature code values in the key intervals <highlight><bold>1303</bold></highlight>, <highlight><bold>1306</bold></highlight>, <highlight><bold>1309</bold></highlight>, . . . In this case, the counting <highlight><bold>1345</bold></highlight>, <highlight><bold>1355</bold></highlight> is performed by determining the number of times a code occurs over corresponding regions in the key intervals. The first part <highlight><bold>1340</bold></highlight> of the visual feature vector F<highlight><subscript>v </subscript></highlight>is determined by the first region W(1,1) (the upper-left window) of the key intervals. The first element of the visual feature vector is the number of times that c&equals;1 appears in the regions W(1,1), in this case 2 times. The second element of the visual feature vector is the number of times that c&equals;2 appears in the regions W(1,1), in this case 0 times. This counting is done for all feature codes c&equals;1, . . . , C, where C is the largest code number. The second part <highlight><bold>1350</bold></highlight> of the visual feature vector F<highlight><subscript>v </subscript></highlight>is determined by the regions W(1,2) (second region in first row) of the key intervals. The first element of this part <highlight><bold>1350</bold></highlight> of the visual feature vector is the number of times that c&equals;1 appears in W(1,2), in this case 2 times. The second element of the visual feature vector is the number of times that c&equals;2 appears in W(1,2), in this case 1 times. This counting is done for all feature codes c&equals;1, . . . , C, where C is the largest code number. Note that the length of the visual feature vector for this particular visual feature is now equal to W&times;C. Also note that for this type of feature code occurrence counting the temporal information is lost. </paragraph>
<paragraph id="P-0159" lvl="0"><number>&lsqb;0159&rsqb;</number> The methods in <cross-reference target="DRAWINGS">FIG. 13A and 13B</cross-reference> preserve temporal information and, therefore, the length of vector F<highlight><subscript>v </subscript></highlight>depends on N. For the method in <cross-reference target="DRAWINGS">FIG. 13</cross-reference>C, the length of vector F<highlight><subscript>v </subscript></highlight>does not depend on N but the spatial information is lost. It is desirable that the length of vector F<highlight><subscript>v </subscript></highlight>does not depend on N; that the temporal information is discarded; and, that the spatial information is maintained. </paragraph>
<paragraph id="P-0160" lvl="0"><number>&lsqb;0160&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 13D</cross-reference> shows a preferred specific methods for determining a visual feature vectors F<highlight><subscript>v </subscript></highlight>from the visual part of the media item <highlight><bold>1310</bold></highlight>. The key frames or key intervals <highlight><bold>1356</bold></highlight>, <highlight><bold>1353</bold></highlight>, <highlight><bold>1360</bold></highlight>, . . . , <highlight><bold>1359</bold></highlight> are selected from a media item. However, by rearranging these key frames or intervals, <highlight><bold>1383</bold></highlight>, <highlight><bold>1386</bold></highlight>, etc., these key frames or intervals are ordered into a new sequence <highlight><bold>1320</bold></highlight> of key intervals <highlight><bold>1353</bold></highlight>, <highlight><bold>1356</bold></highlight>, <highlight><bold>1359</bold></highlight>, . . . , <highlight><bold>1360</bold></highlight> according to the value of visual feature that is to be encoded in the visual feature vector F<highlight><subscript>v</subscript></highlight>. For example, the visual feature of average frame brightness can be ordered from high to low in the sequence <highlight><bold>1320</bold></highlight>. Or, the visual feature of average optical flow in key intervals can be ordered from high to low in <highlight><bold>1320</bold></highlight>. The effect and purpose of this ordering is that the temporal information in the video stream is discarded. (This is analogous to discarding word location information in using word frequency vectors, e.g., F<highlight><subscript>t</subscript></highlight>, in text document analysis.) The visual feature value codes (quantized feature values) in the regions of the key frames or intervals are then mapped <highlight><bold>1365</bold></highlight> into a first visual feature vector F<highlight><subscript>v </subscript></highlight><highlight><bold>1370</bold></highlight>. Assume that there are n key frames or intervals in media item <highlight><bold>1310</bold></highlight> with W regions per key frame this gives a feature vector F<highlight><subscript>v </subscript></highlight><highlight><bold>1370</bold></highlight> of length N&equals;n&times;W. So we have the first ordered key frame or interval <highlight><bold>1353</bold></highlight> through the n-th ordered key frame or interval <highlight><bold>1360</bold></highlight>. Code c<highlight><subscript>11 </subscript></highlight>in region W(11) of the first key interval <highlight><bold>1353</bold></highlight> is mapped to the first element of F<highlight><subscript>v </subscript></highlight><highlight><bold>1370</bold></highlight>, code c<highlight><subscript>21 </subscript></highlight>in region W(1,1) of the second key interval <highlight><bold>1356</bold></highlight> is mapped to the second element of F<highlight><subscript>v </subscript></highlight><highlight><bold>1370</bold></highlight>, code c<highlight><subscript>31 </subscript></highlight>in region W(1,1) of the third key interval <highlight><bold>1359</bold></highlight> is mapped to the third element of F<highlight><subscript>v </subscript></highlight><highlight><bold>1370</bold></highlight>, and code c<highlight><subscript>n1 </subscript></highlight>in region W(1,1) of the n-th region <highlight><bold>1360</bold></highlight> is mapped to the n-th element of F<highlight><subscript>v </subscript></highlight><highlight><bold>1370</bold></highlight>. Then, code c<highlight><subscript>12 </subscript></highlight>in region W(1,2) of the first key interval <highlight><bold>1353</bold></highlight> is mapped to the (n&plus;1)-th element of F<highlight><subscript>v </subscript></highlight><highlight><bold>1370</bold></highlight>, code c<highlight><subscript>22 </subscript></highlight>in region W(1,2) of the second key interval <highlight><bold>1356</bold></highlight> is mapped to the (n&plus;2)-th element of F<highlight><subscript>v </subscript></highlight><highlight><bold>1370</bold></highlight>, code c<highlight><subscript>32 </subscript></highlight>in region W(1,2) of the third key interval <highlight><bold>1359</bold></highlight> is mapped to the (n&plus;3)-th element of F<highlight><subscript>v </subscript></highlight><highlight><bold>1370</bold></highlight>, and code c<highlight><subscript>N2 </subscript></highlight>in region W(1,2) of the n-th key interval <highlight><bold>1360</bold></highlight> is mapped to the 2n-th element of F<highlight><subscript>v </subscript></highlight><highlight><bold>1370</bold></highlight>. This process is repeated till all codes in all key intervals are mapped into vector F<highlight><subscript>v </subscript></highlight><highlight><bold>1370</bold></highlight>. Note that the length <highlight><bold>1367</bold></highlight> of the resulting visual feature vector <highlight><bold>1370</bold></highlight> F<highlight><subscript>v </subscript></highlight>is N&equals;n&times;W (<highlight><bold>1371</bold></highlight>). This process is further described in <cross-reference target="DRAWINGS">FIG. 14A</cross-reference>. </paragraph>
<paragraph id="P-0161" lvl="0"><number>&lsqb;0161&rsqb;</number> The last step in <cross-reference target="DRAWINGS">FIG. 13D</cross-reference> is to map <highlight><bold>1375</bold></highlight> visual feature vector <highlight><bold>1370</bold></highlight> F<highlight><subscript>v </subscript></highlight>to a fixed-length visual feature vector F<highlight><subscript>v </subscript></highlight><highlight><bold>1380</bold></highlight>. The length of this vector is M, F<highlight><subscript>v</subscript></highlight>&equals;(k<highlight><subscript>1</subscript></highlight>, k<highlight><subscript>2</subscript></highlight>, . . . , k<highlight><subscript>M</subscript></highlight>). We require here that M&equals;m<highlight><subscript>s</subscript></highlight>&times;W&lt;N&equals;n&times;W. </paragraph>
<paragraph id="P-0162" lvl="0"><number>&lsqb;0162&rsqb;</number> There two preferred methods to achieve this, sub sampling vector F<highlight><subscript>v </subscript></highlight>and averaging components of F<highlight><subscript>v</subscript></highlight>. These are described below and as process <highlight><bold>1440</bold></highlight> and process <highlight><bold>1470</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 14</cross-reference>B, respectively. For sub sampling m<highlight><subscript>s</subscript></highlight>&equals;m<highlight><subscript>c</subscript></highlight>, for averaging, m<highlight><subscript>s</subscript></highlight>&equals;m<highlight><subscript>f</subscript></highlight>. </paragraph>
<paragraph id="P-0163" lvl="0"><number>&lsqb;0163&rsqb;</number> A first method is sub-sampling F<highlight><subscript>v</subscript></highlight>. Let m<highlight><subscript>c</subscript></highlight>&equals;&boxdr;N/M&boxdl;, the ceiling of N divided by M, the sampling factor of vector F<highlight><subscript>v </subscript></highlight>to obtain F<highlight><subscript>v</subscript></highlight>. The constant M should be an integer multiple of W, let m&equals;M/W. Sub sampling this vector F<highlight><subscript>v </subscript></highlight>is achieved as follows. Let F<highlight><subscript>v</subscript></highlight>&equals;(R(1), R(2), . . . , R(W)), where each R(i), i&equals;1, . . . , W, R(i)&equals;(r(i,1), r(i, 2), r(i, 3), . . . , r(i, n)) of length n represents a key interval region W(i). Then F<highlight><subscript>v</subscript></highlight>&equals;(r(1,1), r(1,2), . . . , r(1,n), . . . , r(i,1), . . . , r(i,n), r(W,1), . . . , r(W,n)). The vector F<highlight><subscript>v </subscript></highlight>is then (R(1), R(2), . . . , R(W)), where each R(i) is obtained by sub-sampling R(i). That is, R(i)&equals;(r(i,1), r(i, 1&plus;m<highlight><subscript>c</subscript></highlight>), . . . , r(i, m)). This is further explained as process <highlight><bold>1440</bold></highlight> in <cross-reference target="DRAWINGS">FIG. 14B</cross-reference>. </paragraph>
<paragraph id="P-0164" lvl="0"><number>&lsqb;0164&rsqb;</number> A second method to shorten F<highlight><subscript>v </subscript></highlight>is to average components in F<highlight><subscript>v </subscript></highlight>vector to obtain a vector F<highlight><subscript>v </subscript></highlight>of fixed length M. Let m<highlight><subscript>f</subscript></highlight>&equals;&boxur;N/M&boxul;, the floor of N divided by M, the number of components of vector F<highlight><subscript>v </subscript></highlight>to obtain F<highlight><subscript>v</subscript></highlight>. The constant M is again an integer multiple of W, let m&equals;M/W. Again, F<highlight><subscript>v</subscript></highlight>&equals;(R(1), R(2), . . . , R(W)), with each R(i), i&equals;1, . . . , W, representing a region in a key interval. That is, R(1) represents the first region W(1,1), also denoted as W(1), an so on. The vector components of F<highlight><subscript>v </subscript></highlight>are R(i)&equals;(r(i, 1), r(i, 2), r(i, 3), . . . , r(i, n)) of length n represent a region W(i) of the key intervals. Let the shortened vector F<highlight><subscript>v</subscript></highlight>&equals;(R(1), R(2), . . . , R(W)), with R(i)&equals;(r(i,1), r(i, 2), r(i, 3), . . . , r(i, m)). Then each vector R(i) is mapped into a corresponding shorter vector R(i) as: </paragraph>
<paragraph lvl="0"><in-line-formula><highlight><italic>R</italic></highlight>(<highlight><italic>i</italic></highlight>)&equals;(<highlight><italic>r</italic></highlight>(<highlight><italic>i, </italic></highlight>1), . . . , <highlight><italic>r</italic></highlight>(<highlight><italic>i, m</italic></highlight>)); </in-line-formula></paragraph>
<paragraph id="P-0165" lvl="7"><number>&lsqb;0165&rsqb;</number> with </paragraph>
<paragraph lvl="0"><in-line-formula><highlight><italic>r</italic></highlight>(<highlight><italic>i, j</italic></highlight>)&equals;avg&lsqb;<highlight><italic>r</italic></highlight>(<highlight><italic>i, </italic></highlight>(<highlight><italic>j&minus;</italic></highlight>1)<highlight><italic>m</italic></highlight><highlight><subscript>f</subscript></highlight>&plus;1), <highlight><italic>r</italic></highlight>(<highlight><italic>i, </italic></highlight>(<highlight><italic>j&minus;</italic></highlight>1)<highlight><italic>m</italic></highlight><highlight><subscript>f</subscript></highlight>&plus;2), . . . , <highlight><italic>r</italic></highlight>(<highlight><italic>i, </italic></highlight>(<highlight><italic>j&minus;</italic></highlight>1)<highlight><italic>m</italic></highlight><highlight><subscript>f</subscript></highlight><highlight><italic>&plus;m</italic></highlight>)&rsqb;, </in-line-formula></paragraph>
<paragraph id="P-0166" lvl="0"><number>&lsqb;0166&rsqb;</number> This shortening of the vector F<highlight><subscript>v </subscript></highlight>to the vector F<highlight><subscript>v </subscript></highlight>is described in the flowchart process <highlight><bold>1470</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 14B</cross-reference>. </paragraph>
<paragraph id="P-0167" lvl="0"><number>&lsqb;0167&rsqb;</number> In <cross-reference target="DRAWINGS">FIG. 14</cross-reference>A, is a flowchart of the process for computing the first visual feature vector F<highlight><subscript>v </subscript></highlight><highlight><bold>1370</bold></highlight> from the media item <highlight><bold>1310</bold></highlight> is shown. Input to this process is media item <highlight><bold>1310</bold></highlight> and in step <highlight><bold>1401</bold></highlight>, key frames or key intervals are selected. (This process is described in <cross-reference target="DRAWINGS">FIGS. 7A and B</cross-reference>.) The output is a set of key intervals labeled k<highlight><subscript>i</subscript></highlight>, i&equals;1, . . . , n (<highlight><bold>1402</bold></highlight>). The number of key intervals n depends on the length of the media item. Process <highlight><bold>1403</bold></highlight> sorts these key intervals based on a visual property. As noted above, the visual feature of average frame brightness can be ordered from high to low <highlight><bold>1380</bold></highlight>. Or, the visual feature of average optical flow in key intervals can be ordered from high to low <highlight><bold>1380</bold></highlight>. The effect and purpose of this ordering is tat the temporal information in the video stream is discarded. (This is analogous to discarding word location information in using word frequency vectors, e.g., F<highlight><subscript>t</subscript></highlight>, in text document analysis.) The output <highlight><bold>1320</bold></highlight> of process <highlight><bold>1403</bold></highlight> is a set of key intervals labeled k<highlight><subscript>j</subscript></highlight>, j&equals;1, . . . , n (<highlight><bold>1320</bold></highlight>). This is the set of n key intervals, <highlight><bold>1353</bold></highlight>, <highlight><bold>1356</bold></highlight>, <highlight><bold>1359</bold></highlight>, . . . , <highlight><bold>1360</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 13D</cross-reference>. The process, then, of populating the first visual feature F<highlight><subscript>v </subscript></highlight><highlight><bold>1370</bold></highlight> with the elements c<highlight><subscript>ij </subscript></highlight>of the ordered key intervals <highlight><bold>1353</bold></highlight>, <highlight><bold>1356</bold></highlight>, <highlight><bold>1359</bold></highlight>, . . . starts at step <highlight><bold>1410</bold></highlight>. First, in step <highlight><bold>1412</bold></highlight>, the pointers i, j, k, are set to 1 i&equals;j&equals;k&equals;1. in step <highlight><bold>1414</bold></highlight>, F<highlight><subscript>v </subscript></highlight>(k) is set to c<highlight><subscript>ij</subscript></highlight>. Subsequently, both i and k are incremented in <highlight><bold>1416</bold></highlight>. The next step is to check if i&gt;n, if not <highlight><bold>1418</bold></highlight>, the next element of F<highlight><subscript>v </subscript></highlight>(k) is set to the element c<highlight><subscript>ij </subscript></highlight>of the next key interval. If i&gt;n <highlight><bold>1420</bold></highlight>, j is incremented <highlight><bold>1422</bold></highlight>, meaning that the next window of the key intervals will be handled. A test is performed first to determine if j&gt;W. If not <highlight><bold>1424</bold></highlight>, on the other hand, the c<highlight><subscript>ij </subscript></highlight>of the next window are entered into visual feature vector F<highlight><subscript>v</subscript></highlight>. if yes <highlight><bold>1426</bold></highlight>, this means that all window visual feature vectors are entered into F<highlight><subscript>v </subscript></highlight>and the process ends <highlight><bold>1428</bold></highlight>. </paragraph>
<paragraph id="P-0168" lvl="0"><number>&lsqb;0168&rsqb;</number> The flowcharts in <cross-reference target="DRAWINGS">FIG. 14B</cross-reference> describe two shortening processes <highlight><bold>4132</bold></highlight> of the vector F<highlight><subscript>v </subscript></highlight><highlight><bold>1430</bold></highlight> to the vector F<highlight><subscript>v </subscript></highlight><highlight><bold>1434</bold></highlight>. Two processes for shortening the vector F<highlight><subscript>v </subscript></highlight><highlight><bold>1430</bold></highlight> are described in this figure, the process of sub sampling F<highlight><subscript>v </subscript></highlight><highlight><bold>1440</bold></highlight> (on the left) and the process of averaging F<highlight><subscript>v </subscript></highlight><highlight><bold>1470</bold></highlight> (on the right). </paragraph>
<paragraph id="P-0169" lvl="0"><number>&lsqb;0169&rsqb;</number> The process <highlight><bold>1440</bold></highlight> starts at step <highlight><bold>1441</bold></highlight>. In step <highlight><bold>1442</bold></highlight>, the variables W (the number of regions per key interval), N (length of F<highlight><subscript>v</subscript></highlight>), M (length of F<highlight><subscript>v</subscript></highlight>), m<highlight><subscript>c</subscript></highlight>&equals;&boxdr;N/M&boxdl; (the sub sample rate), m&equals;M/W, and the vector F<highlight><subscript>v </subscript></highlight>are input to the system. Additionally, the variables i, w, s are set to one, i&equals;w&equals;s&equals;1. In step <highlight><bold>1444</bold></highlight>, F<highlight><subscript>v </subscript></highlight>(s) is set to F<highlight><subscript>v </subscript></highlight>(w), i.e., a component of the short visual feature vector is set to a component of the longer visual feature vector, which results in sub sampling of the key intervals of F<highlight><subscript>v</subscript></highlight>. Step <highlight><bold>1446</bold></highlight> increments the variable w by m<highlight><subscript>c</subscript></highlight>. In test <highlight><bold>1448</bold></highlight>, it is checked whether w&gt;i W, i.e., if a complete key interval has been sub sampled. If not <highlight><bold>1450</bold></highlight>, the variable s is incremented in by 1 in step <highlight><bold>1452</bold></highlight> and step <highlight><bold>1444</bold></highlight> is repeated. If yes <highlight><bold>1454</bold></highlight>, in step <highlight><bold>1456</bold></highlight> i is incremented by 1 and w is set to i m&plus;1. This w is the next component of F<highlight><subscript>v </subscript></highlight>to sub sample. A further test <highlight><bold>1458</bold></highlight>, determines if s&gt;M. If no <highlight><bold>1460</bold></highlight>, the vector F<highlight><subscript>v </subscript></highlight>(of length M) is not completely filled yet and s is set to s&plus;1 in step <highlight><bold>1452</bold></highlight> and the process is repeated starting with step <highlight><bold>1444</bold></highlight>. If test <highlight><bold>1458</bold></highlight> is true, on the other hand, the vector F<highlight><subscript>v </subscript></highlight>of fixed length M is output in step <highlight><bold>1464</bold></highlight> and the sub sampling process stops in <highlight><bold>1466</bold></highlight>. </paragraph>
<paragraph id="P-0170" lvl="0"><number>&lsqb;0170&rsqb;</number> The process <highlight><bold>1470</bold></highlight> starts at step <highlight><bold>1471</bold></highlight>. In step <highlight><bold>1472</bold></highlight>, the variables W (the number of regions per key interval), N (length of F<highlight><subscript>v</subscript></highlight>), M (length of F<highlight><subscript>v</subscript></highlight>), m<highlight><subscript>f</subscript></highlight>&equals;&boxur;N/M&boxul; (the averaging rate), m&equals;M/W, and the vector F<highlight><subscript>v </subscript></highlight>are input to the system. Additionally, the variables i, w, s are set to one, i&equals;w&equals;s&equals;1. In step <highlight><bold>1444</bold></highlight>, F<highlight><subscript>v </subscript></highlight>(s) is set to the average of the m<highlight><subscript>f </subscript></highlight>component of F<highlight><subscript>v</subscript></highlight>, F<highlight><subscript>v</subscript></highlight>(w), F<highlight><subscript>v</subscript></highlight>(w&plus;W), F<highlight><subscript>v</subscript></highlight>(w &plus;2W), . . . , F<highlight><subscript>v</subscript></highlight>(w&plus;m<highlight><subscript>f</subscript></highlight>&minus;1)W), i.e., a component of the short visual feature vector is set to the average of corresponding m<highlight><subscript>f </subscript></highlight>component of the longer visual feature vector F<highlight><subscript>v</subscript></highlight>. That is, </paragraph>
<paragraph lvl="0"><in-line-formula><highlight><italic>F</italic></highlight><highlight><subscript>v</subscript></highlight>(<highlight><italic>s</italic></highlight>)&equals;&lsqb;<highlight><italic>F</italic></highlight><highlight><subscript>v</subscript></highlight>(<highlight><italic>w</italic></highlight>)&plus;<highlight><italic>F</italic></highlight><highlight><subscript>v</subscript></highlight>(<highlight><italic>w&plus;W</italic></highlight>)&plus;<highlight><italic>F</italic></highlight><highlight><subscript>v</subscript></highlight>(<highlight><italic>w&plus;</italic></highlight>2<highlight><italic>W</italic></highlight>)&plus;. . . &plus;<highlight><italic>F</italic></highlight><highlight><subscript>v</subscript></highlight>(<highlight><italic>w&plus;</italic></highlight>(<highlight><italic>m</italic></highlight><highlight><subscript>f</subscript></highlight>&minus;1)<highlight><italic>W</italic></highlight>)&rsqb;/<highlight><italic>m</italic></highlight><highlight><subscript>f</subscript></highlight>. </in-line-formula></paragraph>
<paragraph id="P-0171" lvl="0"><number>&lsqb;0171&rsqb;</number> Step <highlight><bold>1476</bold></highlight> increments the variable w by m<highlight><subscript>f</subscript></highlight>. In test <highlight><bold>1478</bold></highlight>, it is checked whether w&gt;i W, i.e., if a complete key interval has been handled. If not <highlight><bold>1480</bold></highlight>, the variable s is incremented in by 1 in <highlight><bold>1482</bold></highlight> and the averaging step <highlight><bold>1474</bold></highlight> is repeated. If yes <highlight><bold>1484</bold></highlight>, in step <highlight><bold>1486</bold></highlight> i is incremented by 1 and w is set to i m&plus;1. This w is the right component of F<highlight><subscript>v </subscript></highlight>to proceed with further filling of F<highlight><subscript>v </subscript></highlight>by averaging. A further test <highlight><bold>1488</bold></highlight>, determines if s&gt;M. If no <highlight><bold>1460</bold></highlight>, the vector F<highlight><subscript>v </subscript></highlight>(of length M) is not completely filled yet and s is set to s&plus;1 in step <highlight><bold>1482</bold></highlight> and the process is repeated starting with step <highlight><bold>1474</bold></highlight>. If test <highlight><bold>1488</bold></highlight> is true, on the other hand, the vector F<highlight><subscript>v </subscript></highlight>of fixed length M is output in step <highlight><bold>1494</bold></highlight> and the averaging process stops in <highlight><bold>1496</bold></highlight>. </paragraph>
<paragraph id="P-0172" lvl="0"><number>&lsqb;0172&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 15</cross-reference> shows a flowchart of the process <highlight><bold>1500</bold></highlight> of combining the visual feature vector F<highlight><subscript>v </subscript></highlight><highlight><bold>1510</bold></highlight> and the textual feature vector F<highlight><subscript>t </subscript></highlight><highlight><bold>1520</bold></highlight>. The output is a combined feature vector F <highlight><bold>1560</bold></highlight> that represents such disparate sources of information as textual information (e.g., speech transcript) and visual information (i.e., images or sequences of images). These vectors are used in process <highlight><bold>400</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 4</cross-reference> to induce classifiers. The vectors may be labeled or unlabeled and the learning is supervised and unsupervised, respectively. Process <highlight><bold>1500</bold></highlight> (in <cross-reference target="DRAWINGS">FIG. 15</cross-reference>) are optionally transformed to assure compatibility in various ways. There is an optional transformation <highlight><bold>1530</bold></highlight> of the visual feature vector F<highlight><subscript>v </subscript></highlight>and an optional transformation <highlight><bold>1540</bold></highlight> of the textual feature vector F<highlight><subscript>t</subscript></highlight>. These transformations can be with respect to range of the feature values appearing in the two kinds of vectors F<highlight><subscript>v </subscript></highlight>and F<highlight><subscript>t</subscript></highlight>, this is achieved by scaling (normalization) or re-quantization. The transformations may also have to be performed with respect to the competitive lengths of the two kinds of vectors with respect to some norm or measure. In step <highlight><bold>1550</bold></highlight> the textual feature vector F<highlight><subscript>t </subscript></highlight><highlight><bold>1520</bold></highlight> and the visual feature vector F<highlight><subscript>v </subscript></highlight><highlight><bold>1510</bold></highlight> are combined by concatenation to produce a unified representation in a single vector of disparate kinds of features. Having created the unified representation, the combined feature vector F <highlight><bold>1560</bold></highlight>, of the training data, standard methods of classifier induction are used. </paragraph>
<paragraph id="P-0173" lvl="0"><number>&lsqb;0173&rsqb;</number> Turning our attention now to <cross-reference target="DRAWINGS">FIG. 16</cross-reference>, here is shown a system diagram of the present invention. The media item <highlight><bold>1601</bold></highlight> could be in any format like NTSC/PAL <highlight><bold>1603</bold></highlight>, MPEG1/MPEG2 file or stream <highlight><bold>1609</bold></highlight>, AVI file or stream <highlight><bold>1613</bold></highlight>, some arbitrary format xxx <highlight><bold>1619</bold></highlight> or Real Video <highlight><bold>1623</bold></highlight>. Depending on the format of the media, it is processed through the corresponding decoder, NTSC/PAL <highlight><bold>1603</bold></highlight> through the frame grabber hardware <highlight><bold>1607</bold></highlight>, MPEG1/MPEG2 through the MPEG Decoder <highlight><bold>1611</bold></highlight>, AVI <highlight><bold>1613</bold></highlight> through the AVI Decoder <highlight><bold>1617</bold></highlight>, some unknown format xxx <highlight><bold>1619</bold></highlight> through the corresponding decoder <highlight><bold>1621</bold></highlight>, and Real Video <highlight><bold>1623</bold></highlight> through the Real Video Decoder <highlight><bold>1627</bold></highlight>. </paragraph>
<paragraph id="P-0174" lvl="0"><number>&lsqb;0174&rsqb;</number> The output of each of the decoders in <cross-reference target="DRAWINGS">FIG. 16</cross-reference> will be some generic video frame format like RGB or YIQ <highlight><bold>1629</bold></highlight> and the data will be in machine readable form. The media item categorization algorithm operates on this <highlight><bold>1629</bold></highlight> data. The categorization engine <highlight><bold>1633</bold></highlight>, first computed visual and textual vector representations, F<highlight><subscript>v </subscript></highlight>and F<highlight><subscript>t</subscript></highlight>, respectively from the decoded media item <highlight><bold>1629</bold></highlight> and uses category representations <highlight><bold>1631</bold></highlight> to generate the category <highlight><bold>1637</bold></highlight>. </paragraph>
<paragraph id="P-0175" lvl="0"><number>&lsqb;0175&rsqb;</number> In <cross-reference target="DRAWINGS">FIG. 17</cross-reference> the idea of associating a feature vector F with a blocks of the media item as opposed to associating the feature vector with the entire media item is introduced. The feature vector F is then a function of time of frame number F(t) or F(n), as introduced in <cross-reference target="DRAWINGS">FIG. 17B</cross-reference>. To emphasize the fact that the media item or stream has time associated with it, we refer to such a media item as M(t) <highlight><bold>1750</bold></highlight>. But first consider <cross-reference target="DRAWINGS">FIG. 17A</cross-reference> where is show the feature vector F <highlight><bold>1755</bold></highlight> computation of a media item <highlight><bold>1700</bold></highlight>. Let the media item <highlight><bold>1700</bold></highlight> be a visual media stream. The stream contains textual information <highlight><bold>1710</bold></highlight>, either in the form of captioning or in the form of an an audio track (or both), and visual information <highlight><bold>1720</bold></highlight> in the form of a sequence of images (frames). An individual frame in the media stream is associated with a frame number n <highlight><bold>1725</bold></highlight>, which corresponds to unique time t <highlight><bold>1715</bold></highlight> in the stream, given that the stream starts at n&equals;0 and t&equals;0. The media item is of length T <highlight><bold>1718</bold></highlight> in terms of time and of length N <highlight><bold>1728</bold></highlight> in terms of frames. If there are 30 frames per second and T is expressed in seconds, then N&equals;30 T. The textual feature vector F<highlight><subscript>t </subscript></highlight>(refer to <cross-reference target="DRAWINGS">FIG. 14</cross-reference>) is computed from the textual information <highlight><bold>1710</bold></highlight>. The visual feature vector F<highlight><subscript>v </subscript></highlight>(refer to FIG. <highlight><bold>14</bold></highlight>) is computed from a subset of the frames, as described in <cross-reference target="DRAWINGS">FIGS. 7A and 7B</cross-reference>. In the case of <cross-reference target="DRAWINGS">FIG. 17</cross-reference>A, these are the key frames (or key intervals) n<highlight><subscript>1 </subscript></highlight>(<highlight><bold>1730</bold></highlight>), n<highlight><subscript>2 </subscript></highlight>(<highlight><bold>1735</bold></highlight>), n<highlight><subscript>3 </subscript></highlight>(<highlight><bold>1740</bold></highlight>), and n<highlight><subscript>4 </subscript></highlight>(<highlight><bold>1745</bold></highlight>). </paragraph>
<paragraph id="P-0176" lvl="0"><number>&lsqb;0176&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 17B</cross-reference> is focussed on generating feature vectors F(t) or F(n) with the ultimate aim of dividing up a media steam <highlight><bold>1750</bold></highlight> into contiguous segments of one or more categories. In <cross-reference target="DRAWINGS">FIG. 17B, a</cross-reference> media stream <highlight><bold>1750</bold></highlight> of length T <highlight><bold>1754</bold></highlight> seconds and N <highlight><bold>1758</bold></highlight> frames is shown. As opposed to the media item <highlight><bold>1700</bold></highlight> in <cross-reference target="DRAWINGS">FIG. 17</cross-reference>A, where it is assumed that the item is about one subject or category, for media item <highlight><bold>1750</bold></highlight>, the topic, subject or category of the media item may change as a function of time t <highlight><bold>1715</bold></highlight>. Similar to <cross-reference target="DRAWINGS">FIG. 17</cross-reference>A, key frames (or key intervals) n<highlight><subscript>1 </subscript></highlight>(<highlight><bold>1761</bold></highlight>), n<highlight><subscript>2 </subscript></highlight>(<highlight><bold>1762</bold></highlight>), n<highlight><subscript>3 </subscript></highlight>(<highlight><bold>1763</bold></highlight>), n<highlight><subscript>4 </subscript></highlight>(<highlight><bold>1764</bold></highlight>) through n<highlight><subscript>9 </subscript></highlight>(<highlight><bold>1769</bold></highlight>) are selected based on some visual criterion. However, in this case, the textual F<highlight><subscript>t </subscript></highlight>and visual feature vector F<highlight><subscript>v </subscript></highlight>are determined from an continuous subset of the media stream, subset <highlight><bold>1780</bold></highlight> of the textual information and subset <highlight><bold>1790</bold></highlight> of the visual information. This continuous subset of textual and visual information is called a &ldquo;block.&rdquo; Here the continuous subset is of length T<highlight><subscript>0 </subscript></highlight><highlight><bold>1785</bold></highlight> seconds or N<highlight><subscript>0 </subscript></highlight><highlight><bold>1795</bold></highlight> frames (N<highlight><subscript>0</subscript></highlight>&equals;30&times;T<highlight><subscript>0</subscript></highlight>). The textual feature vector F<highlight><subscript>t </subscript></highlight>is computed from all the textual information in block <highlight><bold>1780</bold></highlight>. The visual feature vector F<highlight><subscript>v</subscript></highlight>, on the other hand, is computed from the key intervals that are contained in block <highlight><bold>1790</bold></highlight>. For the block, the key intervals are n<highlight><subscript>2 </subscript></highlight>(<highlight><bold>1762</bold></highlight>) and n<highlight><subscript>3 </subscript></highlight>(<highlight><bold>1763</bold></highlight>). A visual feature vector F<highlight><subscript>v </subscript></highlight>is computed from these key intervals by one of the methods described in <cross-reference target="DRAWINGS">FIG. 13</cross-reference>. The textual F<highlight><subscript>t </subscript></highlight>and visual feature vector F<highlight><subscript>v </subscript></highlight>are then combined into a overall feature vector F(n<highlight><subscript>0</subscript></highlight>) or F(t<highlight><subscript>0</subscript></highlight>) <highlight><bold>1775</bold></highlight> by the process described in <cross-reference target="DRAWINGS">FIG. 15</cross-reference>. By using such a moving block of both visual and textual information, a category and/or topic C(n) or C(t) <highlight><bold>1780</bold></highlight> is obtained as a function of time t <highlight><bold>1715</bold></highlight> or frame number n <highlight><bold>1725</bold></highlight>. It is this function that this invention uses to divide the media item into segments that correspond to different categories. </paragraph>
<paragraph id="P-0177" lvl="0"><number>&lsqb;0177&rsqb;</number> In <cross-reference target="DRAWINGS">FIG. 17</cross-reference>C, the media stream <highlight><bold>1750</bold></highlight> of length T <highlight><bold>1754</bold></highlight> seconds and N <highlight><bold>1758</bold></highlight> frames is indexed in a third way, by word count w <highlight><bold>1797</bold></highlight>. Here there are W <highlight><bold>1799</bold></highlight> words in media item w, 1&lE;w &lE;W. Category and or topic can then also expressed a function of word count, w, i.e., C(w) <highlight><bold>1785</bold></highlight>. </paragraph>
<paragraph id="P-0178" lvl="0"><number>&lsqb;0178&rsqb;</number> Continuing to <cross-reference target="DRAWINGS">FIG. 18</cross-reference>, here is shown the process <highlight><bold>1800</bold></highlight> for dividing up the media item M(t) <highlight><bold>1750</bold></highlight> into contiguous time segments where each segment is associated with one or more classes and the segments are optionally aggregated into larger more homogeneous segments according to certain rules. Here the rules can be heuristic rules, learned rules from training data, or both. </paragraph>
<paragraph id="P-0179" lvl="0"><number>&lsqb;0179&rsqb;</number> Storage device <highlight><bold>1810</bold></highlight> contains one or streaming multimedia items M(t) <highlight><bold>1750</bold></highlight>. Output of this device <highlight><bold>1810</bold></highlight> is the temporal multimedia item <highlight><bold>1750</bold></highlight>, which, importantly, in this figure is not of one single category but can be a concatenation of one or more multimedia segments of different category (subject/topic). Device <highlight><bold>1810</bold></highlight> streams a multimedia item M(t) <highlight><bold>1750</bold></highlight>, which in block process <highlight><bold>1820</bold></highlight> is divided up into blocks. This is the process described in <cross-reference target="DRAWINGS">FIG. 17</cross-reference>. The output <highlight><bold>1820</bold></highlight> is a block of multimedia data B(t) <highlight><bold>1830</bold></highlight>, which is a function of time t. Each block contains a portion of textual data <highlight><bold>1780</bold></highlight> (see <cross-reference target="DRAWINGS">FIG. 17</cross-reference>) plus a portion of visual data <highlight><bold>1790</bold></highlight> (<cross-reference target="DRAWINGS">FIG. 17</cross-reference>). The length of these blocks depends on the frequency of change in category in the multimedia item M(t) <highlight><bold>1750</bold></highlight>. Each block B(t) <highlight><bold>1830</bold></highlight> is in itself a multimedia item. Using the prior art described in <cross-reference target="DRAWINGS">FIG. 1, a</cross-reference> sparse textual feature vector F<highlight><subscript>t</subscript></highlight>(t) is extracted, further, using the novel techniques described in <cross-reference target="DRAWINGS">FIG. 13, a</cross-reference> sparse visual feature vector F<highlight><subscript>v</subscript></highlight>(t) is extracted in step <highlight><bold>1840</bold></highlight> from block B(t) <highlight><bold>1830</bold></highlight>. Using the vector combining process described in <cross-reference target="DRAWINGS">FIG. 15</cross-reference>, these two vectors are combined in process <highlight><bold>1840</bold></highlight> into vector F(t) <highlight><bold>1850</bold></highlight>, which is the output of process <highlight><bold>1840</bold></highlight>. The application process <highlight><bold>1860</bold></highlight>, uses the classification phase <highlight><bold>615</bold></highlight> (described in <cross-reference target="DRAWINGS">FIG. 6</cross-reference> with the target media stream M <highlight><bold>660</bold></highlight> equal to B(t) <highlight><bold>1830</bold></highlight>). Output <highlight><bold>1870</bold></highlight>, C(t), of this application process <highlight><bold>1860</bold></highlight> is a categorization/classification of the media stream M(t) <highlight><bold>1750</bold></highlight> as a function of time t. At each time t, one or more categories are associated with media stream <highlight><bold>1750</bold></highlight>, denoted as C(t). </paragraph>
<paragraph id="P-0180" lvl="0"><number>&lsqb;0180&rsqb;</number> There are a number of problems associated with this output <highlight><bold>1870</bold></highlight> C(t). Even if the classification error for each block is small, say 5%, for example, there is a possible random error in every 20 key frames, key intervals, or blocks of key intervals. These problems are described in the next figure, <cross-reference target="DRAWINGS">FIG. 19</cross-reference>. An optional aggregation process <highlight><bold>1880</bold></highlight> develops the output <highlight><bold>1870</bold></highlight>, C(t), into output <highlight><bold>1890</bold></highlight>, C(t), according to one or more aggregation rules. These rules can be heuristic and/or can be learned. A heuristic approach is given in <cross-reference target="DRAWINGS">FIG. 20</cross-reference>. An approach, based on machine learning, is described in <cross-reference target="DRAWINGS">FIG. 21</cross-reference>-<highlight><bold>22</bold></highlight>. </paragraph>
<paragraph id="P-0181" lvl="0"><number>&lsqb;0181&rsqb;</number> But first, in <cross-reference target="DRAWINGS">FIG. 19</cross-reference>, the problems with output <highlight><bold>1870</bold></highlight> C(t) are described. <cross-reference target="DRAWINGS">FIG. 19A</cross-reference> shows a graph <highlight><bold>1900</bold></highlight> of the discrete changes in the true category C<highlight><subscript>t </subscript></highlight><highlight><bold>1905</bold></highlight> of a streaming media item M(t) <highlight><bold>1750</bold></highlight>, as a function of time t, frame number n, or word count w <highlight><bold>1906</bold></highlight>. The length of the media item is T seconds, N frames, W words <highlight><bold>1903</bold></highlight>. This media item M(t) is, for example, a portion of a news program. The media item starts with a segment <highlight><bold>1910</bold></highlight> about &ldquo;US President,&rdquo; category c<highlight><subscript>1</subscript></highlight>, this is followed by a segment <highlight><bold>1915</bold></highlight> in the category &ldquo;European Union,&rdquo; category c<highlight><subscript>2</subscript></highlight>. Segment <highlight><bold>1915</bold></highlight> is followed by a segment <highlight><bold>1920</bold></highlight> about the weather <highlight><bold>1920</bold></highlight> (category c<highlight><subscript>3</subscript></highlight>), which is also the category of the sixth segment <highlight><bold>1935</bold></highlight>. Between the two weather segments <highlight><bold>1920</bold></highlight> and <highlight><bold>1935</bold></highlight>, there is a segment <highlight><bold>1925</bold></highlight> about &ldquo;Free trade,&rdquo; category c<highlight><subscript>4</subscript></highlight>, followed by a segment about &ldquo;Crime in the cities,&rdquo; category c<highlight><subscript>5</subscript></highlight>. The media item ends with segment <highlight><bold>1940</bold></highlight> about, &ldquo;Baseball league,&rdquo; category c<highlight><subscript>6</subscript></highlight>. Hence, the true category C<highlight><subscript>t </subscript></highlight><highlight><bold>1905</bold></highlight> of the media item is a function of time, for example, for interval <highlight><bold>1910</bold></highlight> C<highlight><subscript>t</subscript></highlight>(t)&equals;c<highlight><subscript>1</subscript></highlight>, for interval <highlight><bold>1915</bold></highlight> C<highlight><subscript>t</subscript></highlight>(t)&equals;c<highlight><subscript>2</subscript></highlight>, for intervals <highlight><bold>1920</bold></highlight> and <highlight><bold>1935</bold></highlight> C<highlight><subscript>t</subscript></highlight>(t)&equals;c<highlight><subscript>3 </subscript></highlight>(the weather), and so on. The function C<highlight><subscript>t</subscript></highlight>(t) is a discrete function, the function can take on one or more values of a finite number of categories. (A media item does not have to be classified into one distinct class, it may be classified into a multitude of classes.) </paragraph>
<paragraph id="P-0182" lvl="0"><number>&lsqb;0182&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 19B</cross-reference> shows a graph <highlight><bold>1950</bold></highlight> of the estimated categorization function (classifier output) C(t) <highlight><bold>1955</bold></highlight> (<highlight><bold>1870</bold></highlight> in <cross-reference target="DRAWINGS">FIG. 18</cross-reference>) as a function of time t <highlight><bold>1906</bold></highlight> as obtained by application process <highlight><bold>1860</bold></highlight> in <cross-reference target="DRAWINGS">FIG. 18</cross-reference>. This function is an illustrative example and not an actual depiction of what the graph would look like. In reality the function is discreet and misclassifications do not necessarily lie close to the true class. The function is also a function of frame number n <highlight><bold>1906</bold></highlight> and word count w <highlight><bold>1906</bold></highlight>, C(n) and C(w), respectively. Category C(t) <highlight><bold>1955</bold></highlight> is shown as a one-dimensional function <highlight><bold>1955</bold></highlight>. If the category C(t) could be computed perfectly, this function would be equal to the discrete function C<highlight><subscript>t</subscript></highlight>(t) of <cross-reference target="DRAWINGS">FIG. 19A</cross-reference>. However, due to noise and other problems, the function C(t) <highlight><bold>1955</bold></highlight> is only approximately equal to the true function C<highlight><subscript>t</subscript></highlight>(t). That is, C(t)&ap;c<highlight><subscript>1 </subscript></highlight><highlight><bold>1960</bold></highlight> in segment <highlight><bold>1910</bold></highlight>, C(t)&ap;c<highlight><subscript>2 </subscript></highlight><highlight><bold>1965</bold></highlight> in segment <highlight><bold>1915</bold></highlight>, C(t)&ap;c<highlight><subscript>3 </subscript></highlight><highlight><bold>1970</bold></highlight> in segment <highlight><bold>1920</bold></highlight>, . . . , C(t)&ap;c<highlight><subscript>6 </subscript></highlight><highlight><bold>1990</bold></highlight> in segment <highlight><bold>1940</bold></highlight>. Here &ldquo;C(t)&ap;c<highlight><subscript>1</subscript></highlight>&rdquo; means that C(t)&equals;c<highlight><subscript>1 </subscript></highlight>at many of the sample points t where C(t) is computed. </paragraph>
<paragraph id="P-0183" lvl="0"><number>&lsqb;0183&rsqb;</number> A first problem is that the calculated categorization of the blocks within the true segments, <highlight><bold>1910</bold></highlight>, <highlight><bold>1915</bold></highlight>, <highlight><bold>1920</bold></highlight>, . . . , <highlight><bold>1940</bold></highlight>, varies and is subject to outlying categorizations within the true segments from the true category C<highlight><subscript>t </subscript></highlight><highlight><bold>1905</bold></highlight>, that is, C(t) not always equals C<highlight><subscript>t</subscript></highlight>. Furthermore, because of the block process (described in <cross-reference target="DRAWINGS">FIG. 17</cross-reference>), there will always be blocks that span different segments of different categories. This will be the case at the true segment boundaries like <highlight><bold>1991</bold></highlight>, <highlight><bold>1994</bold></highlight>, <highlight><bold>1996</bold></highlight>, where the category changes. Consider, for example, the boundaries between true category segments <highlight><bold>1910</bold></highlight> and <highlight><bold>1915</bold></highlight>. Here a block of data <highlight><bold>1961</bold></highlight> is used denoted by B(t) <highlight><bold>1962</bold></highlight>, from this block of data, a feature vector F(t) <highlight><bold>1963</bold></highlight> is computed. This feature vector is classified into a class C(t) <highlight><bold>1964</bold></highlight> according to media item categorization described in, e.g., <cross-reference target="DRAWINGS">FIG. 16</cross-reference>. However, because the block <highlight><bold>1961</bold></highlight> spans multiple true category segments, the output of the classifier in these type of regions is unreliable. </paragraph>
<paragraph id="P-0184" lvl="0"><number>&lsqb;0184&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 20A</cross-reference> shows a temporal display of a portion of the media stream of <cross-reference target="DRAWINGS">FIG. 19</cross-reference>. This partial media item starts with a segment <highlight><bold>1910</bold></highlight> of category c<highlight><subscript>1 </subscript></highlight>&ldquo;US President,&rdquo; this is followed by a segment <highlight><bold>1915</bold></highlight> of category c<highlight><subscript>2</subscript></highlight>, &ldquo;European Union .&rdquo; Segment <highlight><bold>1915</bold></highlight> is followed by a segment <highlight><bold>1920</bold></highlight> about the weather <highlight><bold>1920</bold></highlight> (category c<highlight><subscript>3</subscript></highlight>). The true C<highlight><subscript>t</subscript></highlight>(t) <highlight><bold>1905</bold></highlight>, as a function of time t, frame number n, or word count w <highlight><bold>1906</bold></highlight> is shown in the upper region. Also shown is the classification of the media item as a function of time C(t) <highlight><bold>1955</bold></highlight> in the lower region. In reality this classification is a sequence of categories, c<highlight><subscript>1</subscript></highlight>c<highlight><subscript>8</subscript></highlight>c<highlight><subscript>1 </subscript></highlight>. . . c<highlight><subscript>2</subscript></highlight>c<highlight><subscript>1</subscript></highlight>, each classification is obtained from a block of multimedia B(t) <highlight><bold>1962</bold></highlight> where the blocks are determined as in <cross-reference target="DRAWINGS">FIG. 17</cross-reference> and the category for each block is determined as in described in <cross-reference target="DRAWINGS">FIG. 18</cross-reference>. Within a media segment of a certain category, there will be random misclassifications. This is shown, for example, as the C(t) <highlight><bold>1955</bold></highlight> for segment <highlight><bold>1910</bold></highlight>, which is c<highlight><subscript>1</subscript></highlight>c<highlight><subscript>8</subscript></highlight>c<highlight><subscript>1</subscript></highlight>c<highlight><subscript>1</subscript></highlight>c<highlight><subscript>2</subscript></highlight>c<highlight><subscript>1</subscript></highlight>c<highlight><subscript>1</subscript></highlight>c<highlight><subscript>1</subscript></highlight>c<highlight><subscript>7</subscript></highlight>c<highlight><subscript>1</subscript></highlight>c<highlight><subscript>1</subscript></highlight>. A number of heuristic rules <highlight><bold>2050</bold></highlight> on how the aggregate the local classifications are shown in <cross-reference target="DRAWINGS">FIG. 20A</cross-reference>. A first rule is that a media item segment of a certain category should be at least s seconds <highlight><bold>2055</bold></highlight>, where s is dependent on the type of programming. An immediate consequence of this rule is that there are few changes in category over time, that is, C(t) <highlight><bold>1955</bold></highlight> is a discrete function and each discrete value is taken on for longer time intervals (&gt;s seconds <highlight><bold>2055</bold></highlight>). An aggregation process <highlight><bold>1880</bold></highlight> (<cross-reference target="DRAWINGS">FIG. 18</cross-reference>) process groups the local classifications c into larger segments of the same category. Examples of such rules are  
<table-cwu id="TABLE-US-00002">
<number>2</number>
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="3">
<colspec colname="OFFSET" colwidth="35PT" align="left"/>
<colspec colname="1" colwidth="105PT" align="left"/>
<colspec colname="2" colwidth="77PT" align="center"/>
<thead>
<row>
<entry></entry>
<entry></entry>
</row>
<row>
<entry></entry>
<entry namest="OFFSET" nameend="2" align="center" rowsep="1"></entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry></entry>
<entry>c<highlight><subscript>x</subscript></highlight>c<highlight><subscript>y</subscript></highlight>c<highlight><subscript>x </subscript></highlight>&rarr; c<highlight><subscript>x</subscript></highlight>c<highlight><subscript>x</subscript></highlight>c<highlight><subscript>x</subscript></highlight></entry>
<entry>2060</entry>
</row>
<row>
<entry></entry>
<entry>c<highlight><subscript>x</subscript></highlight>c<highlight><subscript>x</subscript></highlight>c<highlight><subscript>y</subscript></highlight>c<highlight><subscript>y</subscript></highlight>c<highlight><subscript>x </subscript></highlight>&rarr; c<highlight><subscript>x</subscript></highlight>c<highlight><subscript>x</subscript></highlight>c<highlight><subscript>x</subscript></highlight>c<highlight><subscript>x</subscript></highlight>c<highlight><subscript>x</subscript></highlight></entry>
<entry>2065</entry>
</row>
<row>
<entry></entry>
<entry>c<highlight><subscript>x</subscript></highlight>c<highlight><subscript>y</subscript></highlight>c<highlight><subscript>y</subscript></highlight>c<highlight><subscript>x</subscript></highlight>c<highlight><subscript>x </subscript></highlight>&rarr; c<highlight><subscript>x</subscript></highlight>c<highlight><subscript>x</subscript></highlight>c<highlight><subscript>x</subscript></highlight>c<highlight><subscript>x</subscript></highlight>c<highlight><subscript>x</subscript></highlight></entry>
<entry>2070</entry>
</row>
<row>
<entry></entry>
<entry>c<highlight><subscript>x</subscript></highlight>c<highlight><subscript>x</subscript></highlight>c<highlight><subscript>y</subscript></highlight>c<highlight><subscript>y</subscript></highlight>c<highlight><subscript>y</subscript></highlight>c<highlight><subscript>x</subscript></highlight>c<highlight><subscript>x </subscript></highlight>&rarr; c<highlight><subscript>x</subscript></highlight>c<highlight><subscript>x</subscript></highlight>c<highlight><subscript>x</subscript></highlight>c<highlight><subscript>x</subscript></highlight>c<highlight><subscript>x</subscript></highlight>c<highlight><subscript>x</subscript></highlight>c<highlight><subscript>x</subscript></highlight></entry>
<entry>2075</entry>
</row>
<row>
<entry></entry>
<entry namest="OFFSET" nameend="2" align="center" rowsep="1"></entry>
</row>
</tbody>
</tgroup>
</table>
</table-cwu>
</paragraph>
<paragraph id="P-0185" lvl="0"><number>&lsqb;0185&rsqb;</number> Repeated application of these rules changes the local classifications which may initially be fragmented, and will change the local classifications into more contiguous segments of constant category. <cross-reference target="DRAWINGS">FIG. 20A</cross-reference> concentrates on rules of sequences of block classifications, or block classifications within segments of a single category. That is, these are region-based rules. </paragraph>
<paragraph id="P-0186" lvl="0"><number>&lsqb;0186&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 20</cross-reference>B, on the other hand, concentrates on rules for finding the boundaries between segments of different categories. Classification C(t) <highlight><bold>1955</bold></highlight> of the blocks around the boundaries are unreliable because these classifications are based on blocks B(t) <highlight><bold>1830</bold></highlight> that span multiple categories of the underlying media M(t) <highlight><bold>1750</bold></highlight>. A number of cues <highlight><bold>2000</bold></highlight> from the multimedia can be used to more accurately find category boundaries. The exact location of the categories boundaries can be pinpointed by using the various modalities that are present in a media item. The cues in these various modalities are audio silences <highlight><bold>2005</bold></highlight>, speaker changes <highlight><bold>2010</bold></highlight>, end-of-sentence indications in the speech transcript <highlight><bold>2015</bold></highlight>, a shot break in the visual track <highlight><bold>2020</bold></highlight>, and the presence of&ldquo;&gt;&gt;&rdquo; in the closed-captioning (&ldquo;&gt;&gt;&rdquo; is intended to indicate change of subject) <highlight><bold>2025</bold></highlight>. </paragraph>
<paragraph id="P-0187" lvl="0"><number>&lsqb;0187&rsqb;</number> Given the combined feature vectors F(t), i.e., the vector representing the visual information F<highlight><subscript>v</subscript></highlight>(t) combined with the vector representing the textual information F<highlight><subscript>t</subscript></highlight>(t), each block can be classified into a category. One way to achieve this is to use a classifier to categorize every block independently using the combined feature vector of the block. A series of heuristic rules such as described in <cross-reference target="DRAWINGS">FIGS. 20A and 20B</cross-reference> can then be used to aggregate the categorization and more accurately determine the category boundaries. </paragraph>
<paragraph id="P-0188" lvl="0"><number>&lsqb;0188&rsqb;</number> Another way to achieve this is to build a model for predicting the categories of the blocks, consisting of states, based on states of other blocks. Each block can be associated with one state. The categorizer then predicts a cost of each state associated with every block based on the combined feature vector. The optimal sequence of states is selected by minimizing the cost. The category can be equal to the state or the category can be a function of the state. This approach is described in <cross-reference target="DRAWINGS">FIGS. 21 and 22</cross-reference>. </paragraph>
<paragraph id="P-0189" lvl="0"><number>&lsqb;0189&rsqb;</number> Consider the simplified flowchart of <cross-reference target="DRAWINGS">FIG. 21A</cross-reference> that takes media stream M(t) <highlight><bold>2105</bold></highlight> as input, or, equivalently, that takes a stream of visual and textual feature vectors F(t) <highlight><bold>2110</bold></highlight> as input. Here is shown system <highlight><bold>2100</bold></highlight>, a categorization process (also called, application process) <highlight><bold>1860</bold></highlight> followed by aggregation process <highlight><bold>1880</bold></highlight>, exactly as in <cross-reference target="DRAWINGS">FIG. 18</cross-reference>. Assume we only have two categories, i.e., &ldquo;sport&rdquo; and &ldquo;disaster&rdquo; denoted by C<highlight><subscript>1 </subscript></highlight>and C<highlight><subscript>2</subscript></highlight>, respectively. The features F<highlight><subscript>i</subscript></highlight>&equals;F (t<highlight><subscript>i</subscript></highlight>) <highlight><bold>2110</bold></highlight> for each block can be one of three values: F<highlight><subscript>s </subscript></highlight>sport scene, F<highlight><subscript>t </subscript></highlight>talking head, and F<highlight><subscript>d </subscript></highlight>disaster scene. The input <highlight><bold>2110</bold></highlight> to the system is a sequence of feature vector F<highlight><subscript>i</subscript></highlight>&equals;F (t<highlight><subscript>i</subscript></highlight>) <highlight><bold>2110</bold></highlight>, which takes on values F<highlight><subscript>s</subscript></highlight>, F<highlight><subscript>t </subscript></highlight>and F<highlight><subscript>d </subscript></highlight>derived from (a block of the) media stream M(t) <highlight><bold>2105</bold></highlight>. </paragraph>
<paragraph id="P-0190" lvl="0"><number>&lsqb;0190&rsqb;</number> In a simple model, we let the state for each block be the category associated with the block, i.e., the category or state can only be &ldquo;sport&rdquo; or &ldquo;disaster.&rdquo; The classifier then is C (F<highlight><subscript>s</subscript></highlight>)&equals;C<highlight><subscript>1</subscript></highlight>, C (F<highlight><subscript>d</subscript></highlight>)&equals;C<highlight><subscript>2</subscript></highlight>, C (F<highlight><subscript>t</subscript></highlight>)&equals;C<highlight><subscript>1 </subscript></highlight>or C<highlight><subscript>2</subscript></highlight>. The output of the application process <highlight><bold>1840</bold></highlight> is C<highlight><subscript>i</subscript></highlight>&equals;C(t<highlight><subscript>i</subscript></highlight>)&equals;S(t<highlight><subscript>i</subscript></highlight>) <highlight><bold>2115</bold></highlight>. The idea then is to transform the sequence of ambiguous categories or states C<highlight><subscript>i</subscript></highlight>&equals;C(t<highlight><subscript>i</subscript></highlight>)&equals;S(t<highlight><subscript>i</subscript></highlight>) into a sequence of unambiguous states S(t<highlight><subscript>i</subscript></highlight>) <highlight><bold>2120</bold></highlight> (or, equivalently, a sequence of categories C<highlight><subscript>i</subscript></highlight>&equals;C(t<highlight><subscript>i</subscript></highlight>)), which is of minimal cost, as computed in <highlight><bold>1860</bold></highlight>. That is, input media stream <highlight><bold>2105</bold></highlight> is categorized into a smoothly labeled (categorized) stream <highlight><bold>2120</bold></highlight>. </paragraph>
<paragraph id="P-0191" lvl="0"><number>&lsqb;0191&rsqb;</number> To achieve this, in the process <highlight><bold>2150</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 21</cross-reference>B, using training data <highlight><bold>2130</bold></highlight> and an optional set of manual rules <highlight><bold>2135</bold></highlight>, a set of multi modality state transition costs L (C<highlight><subscript>i</subscript></highlight>, C<highlight><subscript>j</subscript></highlight>); i&equals;1, 2,j&equals;1, 2 (<highlight><bold>2140</bold></highlight>) is determined. </paragraph>
<paragraph id="P-0192" lvl="0"><number>&lsqb;0192&rsqb;</number> A possibility for the process of determining state transition costs in process <highlight><bold>2150</bold></highlight> and the aggregation process <highlight><bold>1880</bold></highlight> is, e.g., a Markov model. In a Markov probability model, the probability of a sequence of states s<highlight><subscript>1</subscript></highlight>, . . . , s<highlight><subscript>T </subscript></highlight>(each state takes on the value &ldquo;sport&rdquo;&equals;C<highlight><subscript>1 </subscript></highlight>or &ldquo;disaster&rdquo;&equals;C<highlight><subscript>2</subscript></highlight>) is decomposed as </paragraph>
<paragraph lvl="0"><in-line-formula><highlight><italic>p</italic></highlight>(<highlight><italic>s</italic></highlight><highlight><subscript>1</subscript></highlight><highlight><italic>, . . . , s</italic></highlight><highlight><subscript>T</subscript></highlight>)&equals;<highlight><italic>p</italic></highlight>(<highlight><italic>s</italic></highlight><highlight><subscript>1</subscript></highlight>) <highlight><italic>p</italic></highlight>(<highlight><italic>s</italic></highlight><highlight><subscript>2</subscript></highlight><highlight><italic>&verbar;s</italic></highlight><highlight><subscript>1</subscript></highlight>) . . . <highlight><italic>p</italic></highlight>(<highlight><italic>s</italic></highlight><highlight><subscript>T</subscript></highlight><highlight><italic>&verbar;s</italic></highlight><highlight><subscript>T&minus;1</subscript></highlight>). </in-line-formula></paragraph>
<paragraph id="P-0193" lvl="0"><number>&lsqb;0193&rsqb;</number> To estimate the conditional probability p(C<highlight><subscript>2</subscript></highlight>&verbar;C<highlight><subscript>1</subscript></highlight>) (where C<highlight><subscript>1 </subscript></highlight>or C<highlight><subscript>2 </subscript></highlight>indicates &ldquo;sport&rdquo; or &ldquo;disaster&rdquo; and, e.g., p(C<highlight><subscript>2</subscript></highlight>&verbar;C<highlight><subscript>1</subscript></highlight>)&equals;L(C<highlight><subscript>1</subscript></highlight>, C<highlight><subscript>2</subscript></highlight>) in <highlight><bold>2140</bold></highlight>), we count the number occurrences &num; (C<highlight><subscript>1</subscript></highlight>, C<highlight><subscript>2</subscript></highlight>) of the sequence segments s<highlight><subscript>i&minus;1</subscript></highlight>, s<highlight><subscript>i </subscript></highlight>in the training set <highlight><bold>2130</bold></highlight> such that s<highlight><subscript>i&minus;1</subscript></highlight>&equals;C<highlight><subscript>1 </subscript></highlight>and s<highlight><subscript>i</subscript></highlight>&equals;C<highlight><subscript>2</subscript></highlight>. The conditional probability can then be estimated as </paragraph>
<paragraph lvl="0"><in-line-formula><highlight><italic>p</italic></highlight>(<highlight><italic>C</italic></highlight><highlight><subscript>2</subscript></highlight><highlight><italic>&verbar;C</italic></highlight><highlight><subscript>1</subscript></highlight>)&equals;&num; (<highlight><italic>C</italic></highlight><highlight><subscript>1</subscript></highlight><highlight><italic>, C</italic></highlight><highlight><subscript>2</subscript></highlight>)/&num; &lcub;(<highlight><italic>C</italic></highlight><highlight><subscript>1</subscript></highlight><highlight><italic>, C</italic></highlight>); <highlight><italic>C</italic></highlight>&equals;sport, disaster&rcub;. </in-line-formula></paragraph>
<paragraph id="P-0194" lvl="0"><number>&lsqb;0194&rsqb;</number> That is, in system <highlight><bold>2150</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 20</cross-reference>B, p(C<highlight><subscript>2</subscript></highlight>&verbar;C<highlight><subscript>1</subscript></highlight>) is the number of times that category C<highlight><subscript>1 </subscript></highlight>is followed by category C<highlight><subscript>2 </subscript></highlight>divided by the number of times that category C<highlight><subscript>1 </subscript></highlight>is followed by any category. In one model, the cost function L(C<highlight><subscript>1</subscript></highlight>, C<highlight><subscript>2</subscript></highlight>) <highlight><bold>2140</bold></highlight> an be selected to be &minus;log p(C<highlight><subscript>2</subscript></highlight>&verbar;C<highlight><subscript>1</subscript></highlight>). Hence, when the probability of the transition of one state to another is low, the corresponding cost is high. </paragraph>
<paragraph id="P-0195" lvl="0"><number>&lsqb;0195&rsqb;</number> In aggregation process <highlight><bold>1880</bold></highlight> (of <cross-reference target="DRAWINGS">FIGS. 18 and 21</cross-reference>A) the most likely sequence of states s<highlight><subscript>1</subscript></highlight>, . . . , s<highlight><subscript>T </subscript></highlight>is computed. The more likely a sequence of states s<highlight><subscript>1</subscript></highlight>, . . . , s<highlight><subscript>T </subscript></highlight>is, the less the total cost </paragraph>
<paragraph lvl="0"><in-line-formula>Total cost&equals;<highlight><italic>L</italic></highlight>(<highlight><italic>s</italic></highlight><highlight><subscript>2</subscript></highlight><highlight><italic>, s</italic></highlight><highlight><subscript>1</subscript></highlight>)&plus;<highlight><italic>L</italic></highlight>(<highlight><italic>s</italic></highlight><highlight><subscript>3</subscript></highlight><highlight><italic>, s</italic></highlight><highlight><subscript>2</subscript></highlight>)&plus;. . . &plus;<highlight><italic>L</italic></highlight>(<highlight><italic>s</italic></highlight><highlight><subscript>T</subscript></highlight><highlight><italic>, s</italic></highlight><highlight><subscript>T&minus;1</subscript></highlight>) &equals;<highlight><italic>L</italic></highlight>(<highlight><italic>C</italic></highlight><highlight><subscript>2</subscript></highlight><highlight><italic>, C</italic></highlight><highlight><subscript>1</subscript></highlight>)&plus;<highlight><italic>L</italic></highlight>(<highlight><italic>C</italic></highlight><highlight><subscript>3</subscript></highlight><highlight><italic>, C</italic></highlight><highlight><subscript>2</subscript></highlight>)&plus;. . . &plus;<highlight><italic>L</italic></highlight>(C<highlight><subscript>T</subscript></highlight><highlight><italic>, C</italic></highlight><highlight><subscript>T&minus;1</subscript></highlight>). </in-line-formula></paragraph>
<paragraph id="P-0196" lvl="0"><number>&lsqb;0196&rsqb;</number> This cost is minimized in <highlight><bold>1880</bold></highlight> over all possible sequences of states s<highlight><subscript>1</subscript></highlight>, . . . , s<highlight><subscript>T</subscript></highlight>, or, equivalently, over all possible sequences of categories C<highlight><subscript>1</subscript></highlight>, . . . , C<highlight><subscript>T</subscript></highlight>, resulting in C<highlight><subscript>i</subscript></highlight>&equals;C(t<highlight><subscript>i</subscript></highlight>)&equals;S(t<highlight><subscript>i</subscript></highlight>) <highlight><bold>2120</bold></highlight> </paragraph>
<paragraph id="P-0197" lvl="0"><number>&lsqb;0197&rsqb;</number> For some example rules and costs now see <cross-reference target="DRAWINGS">FIG. 22</cross-reference>. Assume that the block based classifier output indicates that </paragraph>
<paragraph id="P-0198" lvl="2"><number>&lsqb;0198&rsqb;</number> F<highlight><subscript>s</subscript></highlight>&rarr;sport </paragraph>
<paragraph id="P-0199" lvl="2"><number>&lsqb;0199&rsqb;</number> F<highlight><subscript>t</subscript></highlight>&rarr;sport or disaster </paragraph>
<paragraph id="P-0200" lvl="2"><number>&lsqb;0200&rsqb;</number> F<highlight><subscript>d</subscript></highlight>&rarr;disaster </paragraph>
<paragraph id="P-0201" lvl="7"><number>&lsqb;0201&rsqb;</number> where F<highlight><subscript>s</subscript></highlight>, F<highlight><subscript>t</subscript></highlight>, and F<highlight><subscript>d </subscript></highlight>are combined textual and visual feature vectors as shown in <highlight><bold>2205</bold></highlight>. Further assume that from the Markov state model <highlight><bold>2150</bold></highlight> described in <cross-reference target="DRAWINGS">FIG. 21</cross-reference>B, it is observed that <highlight><bold>2210</bold></highlight>:  
<table-cwu id="TABLE-US-00003">
<number>3</number>
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="4">
<colspec colname="OFFSET" colwidth="35PT" align="left"/>
<colspec colname="1" colwidth="91PT" align="left"/>
<colspec colname="2" colwidth="14PT" align="center"/>
<colspec colname="3" colwidth="77PT" align="center"/>
<thead>
<row>
<entry></entry>
<entry></entry>
</row>
<row>
<entry></entry>
<entry namest="OFFSET" nameend="3" align="center" rowsep="1"></entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry></entry>
<entry>L(sport, sport)</entry>
<entry>is</entry>
<entry>small</entry>
</row>
<row>
<entry></entry>
<entry>L(sport, disaster)</entry>
<entry>is</entry>
<entry>large</entry>
</row>
<row>
<entry></entry>
<entry>L(disaster, sport)</entry>
<entry>is</entry>
<entry>large</entry>
</row>
<row>
<entry></entry>
<entry>L(disaster, disaster)</entry>
<entry>is</entry>
<entry>small</entry>
</row>
<row>
<entry></entry>
<entry namest="OFFSET" nameend="3" align="center" rowsep="1"></entry>
</row>
</tbody>
</tgroup>
</table>
</table-cwu>
</paragraph>
<paragraph id="P-0202" lvl="0"><number>&lsqb;0202&rsqb;</number> Imagine we have the sequence <highlight><bold>2215</bold></highlight>. Based upon the visual and auditory information, frame <highlight><bold>2220</bold></highlight> has feature vector F<highlight><subscript>s </subscript></highlight>(<highlight><bold>2225</bold></highlight>), frame <highlight><bold>2230</bold></highlight> has feature vector F<highlight><subscript>t </subscript></highlight>(<highlight><bold>2235</bold></highlight>), and frame <highlight><bold>2240</bold></highlight> has feature vector F<highlight><subscript>s </subscript></highlight>(<highlight><bold>2245</bold></highlight>). That is we have the following the following sequence of feature vectors </paragraph>
<paragraph id="P-0203" lvl="2"><number>&lsqb;0203&rsqb;</number> F<highlight><subscript>s</subscript></highlight>&rarr;F<highlight><subscript>t</subscript></highlight>&rarr;F<highlight><subscript>s </subscript></highlight></paragraph>
<paragraph id="P-0204" lvl="0"><number>&lsqb;0204&rsqb;</number> Without state modeling, there are two interpretations from the classifier using <highlight><bold>2205</bold></highlight>: </paragraph>
<paragraph id="P-0205" lvl="2"><number>&lsqb;0205&rsqb;</number> sport&rarr;sport&rarr;sport&rarr;sport </paragraph>
<paragraph id="P-0206" lvl="2"><number>&lsqb;0206&rsqb;</number> sport&rarr;disaster&rarr;sport&rarr;sport </paragraph>
<paragraph id="P-0207" lvl="0"><number>&lsqb;0207&rsqb;</number> The first interpretation is more likely <highlight><bold>2250</bold></highlight> by using our state model <highlight><bold>2210</bold></highlight>. </paragraph>
<paragraph id="P-0208" lvl="0"><number>&lsqb;0208&rsqb;</number> For the second sequence <highlight><bold>2255</bold></highlight>, we have frame <highlight><bold>2260</bold></highlight> has feature vector F<highlight><subscript>s</subscript></highlight>(<highlight><bold>2265</bold></highlight>), frame <highlight><bold>2270</bold></highlight> has feature vector F<highlight><subscript>t </subscript></highlight>(<highlight><bold>2275</bold></highlight>), and frame <highlight><bold>2280</bold></highlight> has feature vector F<highlight><subscript>s </subscript></highlight>(<highlight><bold>2285</bold></highlight>). And we have the sequence of feature vectors </paragraph>
<paragraph id="P-0209" lvl="2"><number>&lsqb;0209&rsqb;</number> F<highlight><subscript>s</subscript></highlight>&rarr;F<highlight><subscript>d</subscript></highlight>&rarr;F<highlight><subscript>t </subscript></highlight></paragraph>
<paragraph id="P-0210" lvl="0"><number>&lsqb;0210&rsqb;</number> Again, there are two interpretations using the classifier (without state modeling, using <highlight><bold>2205</bold></highlight>): </paragraph>
<paragraph id="P-0211" lvl="2"><number>&lsqb;0211&rsqb;</number> sport&rarr;disaster&rarr;sport </paragraph>
<paragraph id="P-0212" lvl="2"><number>&lsqb;0212&rsqb;</number> sport&rarr;disaster&rarr;disaster </paragraph>
<paragraph id="P-0213" lvl="0"><number>&lsqb;0213&rsqb;</number> Consequently, using <highlight><bold>2210</bold></highlight>, the second interpretation <highlight><bold>2290</bold></highlight> is the more likely one. </paragraph>
<paragraph id="P-0214" lvl="0"><number>&lsqb;0214&rsqb;</number> <cross-reference target="DRAWINGS">FIGS. 21 and 22</cross-reference> are just an example of the use of state transition learning for smoothing the categorization of video over time and thereby arriving and a better segmentation. Other methods are obvious to those skilled in the art. </paragraph>
<paragraph id="P-0215" lvl="0"><number>&lsqb;0215&rsqb;</number> We continue by listing a number of business applications that are hard or impossible to achieve with prior art technology. After describing these applications, we pictorially give a further explanation in FIGS. <highlight><bold>23</bold></highlight>-<highlight><bold>27</bold></highlight>. A first number of uses are: </paragraph>
<paragraph id="P-0216" lvl="1"><number>&lsqb;0216&rsqb;</number> i. Categorizing media elements into a number of categories. For example, let the media element be a news item and let us predefine categories like, sports, weather, politics. The addition of visual features, beyond textual features, in the feature vector that represents the media elements allows for more precise (in the sense of a lower error rate) categorization of the media element. </paragraph>
<paragraph id="P-0217" lvl="1"><number>&lsqb;0217&rsqb;</number> ii. Automatically determining a number of natural categories that a set of media elements can be partitioned into. That is, discovering a set of categories that describes the set of media element compactly. Here the addition of visual features, beyond textual features, in the feature vector that represent the media elements allows for more refined partitioning of the media elements. </paragraph>
<paragraph id="P-0218" lvl="1"><number>&lsqb;0218&rsqb;</number> iii. Given that a media element has been classified into a category that is either predefined as in 1) or automatically learned as in 2) the current invention allows to more precisely assign a topic to the classified media element. Say that a media element has been classified as a sports video segment, the addition of visual features allows for better, (i.e., more accurately) assigning a topic (basketball, soccer, golf, etc.) to the given media element. </paragraph>
<paragraph id="P-0219" lvl="1"><number>&lsqb;0219&rsqb;</number> iv. The combination of textual features and visual features into one feature vector that represents the media element enables better detection or identification of a media element in a certain category with a certain topic, in a collection of media elements. Here, better detection means lower false positive and lower false negative rates. </paragraph>
<paragraph id="P-0220" lvl="1"><number>&lsqb;0220&rsqb;</number> v. The combination of textual features and visual features into one feature vector that represents the media element enables better detection or identification of a particular media element, in a collection of media elements. Here, better detection means lower false positive and lower false negative rates. </paragraph>
<paragraph id="P-0221" lvl="1"><number>&lsqb;0221&rsqb;</number> vi. The automatic generation of MPEG-7 descriptors, as defined by the International Organisation for Standardisation/Organisation Internationale de Normalisation, ISO/IEC JTC1/SC29/WG11 specification &ldquo;Coding of Moving Pictures and Audio.&rdquo; These descriptors are metadata items (digitally encoded annotations) which would be embedded in the bitstreams of videos (television; movies), sometime between the time of content creation (&ldquo;filming&rdquo; or &ldquo;capture&rdquo;) and the time of broadcast/release. These metadata items are then available to all downstream processes (post-production/editing stages of preparation of the complete video product, distribution channels for movie releases, or by receivers/viewers of the broadcast), for various purposes, in particular, retrieval from video archives by content-based querying (in other words, facilitating the finding of video clips of interest, or a specific video clip, from within large collections of video). The descriptors can be used to explicitly label events of interest in a video when they happen, such as the scoring of goals in soccer matches. Manually-controlled processes for creation of such annotations are available now, but the work is tedious and expensive. </paragraph>
<paragraph id="P-0222" lvl="0"><number>&lsqb;0222&rsqb;</number> The business process patent covers the use of the invention in each of the following specific business applications. </paragraph>
<paragraph id="P-0223" lvl="0"><number>&lsqb;0223&rsqb;</number> The invention has many direct applications: </paragraph>
<paragraph id="P-0224" lvl="0"><number>&lsqb;0224&rsqb;</number> A first application is locating (illegal) copies of media items on the Internet or other (public) databases. This application involves searching for digital copies of media elements on the Internet or other (public) databases. With the wide spread use of digital media (audio and video), the illegal copying and distribution of media are becoming a significant problem for the media industry. For example, there are a number of web sites that post illegal copies of video on the Internet. The media is encoded in one of the popular formats (AVI, MPEG1 or MPEG2 for video). Typically, the filenames under which the media is posted, are not indicative of the content of the media files. To identify a posted media item as a known media item, a comparison of the media content of the file (video and audio) is necessary. </paragraph>
<paragraph id="P-0225" lvl="0"><number>&lsqb;0225&rsqb;</number> The classification method described in this invention can be used to perform this comparison. In this case, the media items of interest (say several movies) are used as the reference media items to generate a representation in terms of textual/visual feature vectors. The classification engine is now deployed with these vectors. The target media items are transferred from web sites at the Internet to the computing system that houses the classification engine described in this invention. That is, the media element needs to be downloaded to the machine on which the classification engine is running. The downloading operation can be achieved in multiple ways, an operator could feed URL&apos;s to down loader software, which would download the files to the local machine or alternatively, a web crawler robot could be designed to locate URL&apos;s that hold media files. This can be done by looking at the filename extensions (.mpeg, etc). The URL&apos;s located by the crawler robot or human operator can be filtered based on various criteria, like size of the media items, to generate a list of URL&apos;s for downloaded software. </paragraph>
<paragraph id="P-0226" lvl="0"><number>&lsqb;0226&rsqb;</number> Once a target media item has been downloaded to the local machine, the classification engine is deployed to generate an report about similarities to the media items of interest. </paragraph>
<paragraph id="P-0227" lvl="0"><number>&lsqb;0227&rsqb;</number> This application provides functionality similar to video water marking in that the search engine detects the intrinsic properties (features) of the media item instead of the embedded water marks. </paragraph>
<paragraph id="P-0228" lvl="0"><number>&lsqb;0228&rsqb;</number> The present invention can be employed in the management of large video databases. Such collections of video clips (media items) need to be managed and searched in several environments like TV news, documentary, movie and sitcom productions. In these production environments, media items in the database will be used to produce program material, often the same media item in different productions. It is important to keep track of the usage of a media item from the perspective of rights management and royalty payments. The media item classification technologies discussed in this invention can be used in this process. </paragraph>
<paragraph id="P-0229" lvl="0"><number>&lsqb;0229&rsqb;</number> Every media item (s) which is entered into the database is first used as target media item and searched against a data structure of feature vectors that represent the reference media items in the database. This operation generates an index report of similar media items in the database. The media item to be entered into the database is stored along with similar media items, items of the same category. </paragraph>
<paragraph id="P-0230" lvl="0"><number>&lsqb;0230&rsqb;</number> The feature vector the data structure is stored along with the database and used retrieve content. As per the above procedure, the data structure of feature vectors will continually grow as more and more media items are added to the database. Several tasks, like removing redundant copies of the media items, selecting all media items in certain categories and with certain topics, etc., are straightforwardly accomplished. </paragraph>
<paragraph id="P-0231" lvl="0"><number>&lsqb;0231&rsqb;</number> This invention can be used to segment a video stream into a series of time-continuous media items. Given that a large number of (categories, topic) pairs are defined, a target video stream can be categorized, with a topic associated, as a function of t, time, or n, frame number. This is achieved by determining a combined textual/visual feature vector as a function of time or frame number. A certain window of text and visual information is used to compute the feature vector. Typically, this window is continuous time for audio data and, thus, continuous time for textual data. The window is discrete time for visual information, with the finest time resolution each frame. The window can be causal, i.e., &lsqb;t, t&plus;T&rsqb; and &lsqb;n, n&plus;N&rsqb;, or non-causal, i.e., &lsqb;t&minus;T/2, t&plus;T/2&rsqb; and &lsqb;n&minus;N, n&plus;M&rsqb;. Here, (t, n), (t&plus;T, n&plus;N), (t&minus;T/2, n&minus;N), (t&plus;T/2, n&plus;M) are corresponding (time, frame number) pairs. The sequences of frames n&minus;N, . . . , n and n&minus;N, . . . , n &plus;M, do not have to be subsequent frames or evenly spaced frames. The frames, n, n&minus;N, n&plus;M can be selected to be key frames, or frames with other special characteristics, e.g., maximal/minimal apparent motion or optical flow. </paragraph>
<paragraph id="P-0232" lvl="0"><number>&lsqb;0232&rsqb;</number> Classifying the media stream within the window, with the means described in the current invention, results in a discrete function C(t), i.e., the function can take on a potentially very large number of discrete values. This function will be roughly constant when the category, topic does not change and change to another constant value when the category, topic changes at some time t. This change will, in general, be gradual within the interval of the window. </paragraph>
<paragraph id="P-0233" lvl="0"><number>&lsqb;0233&rsqb;</number> Segmenting the media stream into time-continuous media items with distinct category, topic is now an issue of detecting changes in the value of C(t), as described in this patent application. There are a multitude of applications of this invention when the target media stream is segmented into separate media item. An application, for instance, is monitoring a given television for the occurrence of instances of a pre-specified set of media items. Such broadcast monitoring can be used to detect any type of pre-produced media material. The more typical use is for verifying the broadcasting of TV commercial messages (advertisements). Advertisers (companies whose products are being advertised) require an independent verification of the actual broadcasting of the commercial in order to make payments to the broadcaster. This process currently relies on a human viewer sampling the channel to verify the airing of a commercial. Hence, it is a labor intensive and error prone process. </paragraph>
<paragraph id="P-0234" lvl="0"><number>&lsqb;0234&rsqb;</number> The media element similarity measurement process described in this invention that combines textual and visual features can be used to serve the function of the human viewer. The commercial messages to be monitored is a set reference media items S. As described in this invention, these reference media element are used to generate a feature vector. To monitor a given channel, Channel X, (a target media stream) for commercials, a computing system that houses the classifier described in this invention is used. Depending on the type of broadcast (National Television System Committee (NTSC), Phase Alternating Line (PAL), digital, analog/digital audio), the media element (tuned to Channel X), visual, speech and captioning, is decoded and input to the computing system. </paragraph>
<paragraph id="P-0235" lvl="0"><number>&lsqb;0235&rsqb;</number> The media element classifier operates on the target media stream and produces a report. This report, in the case of commercial monitoring, will include the title of the commercial detected (reference media item identifier), the date and approximate time at which the commercial started, the date and approximate time at which the commercial ended and some type of classification quality, e.g., some similarity measure between reference media item and a segment of the target media stream, hence, a similarity measure between the combined feature vector of the target media segment and the combined feature vector of the reference media item. </paragraph>
<paragraph id="P-0236" lvl="0"><number>&lsqb;0236&rsqb;</number> An application of the present invention targeted towards the task of video indexing is video event detection. Video indexing can be defined as the operation of designating video items (media items) with certain predefined labels. There exists a significant body of prior art on the subject of video indexing. For example, consider a video of a soccer game, indexing this video will result in annotation table that looks as follows:  
<table-cwu id="TABLE-US-00004">
<number>4</number>
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="4">
<colspec colname="1" colwidth="56PT" align="center"/>
<colspec colname="2" colwidth="56PT" align="left"/>
<colspec colname="3" colwidth="49PT" align="left"/>
<colspec colname="4" colwidth="56PT" align="left"/>
<thead>
<row>
<entry></entry>
</row>
<row><entry namest="1" nameend="4" align="center" rowsep="1"></entry>
</row>
<row>
<entry>Event Number</entry>
<entry>&tilde; Begin Time</entry>
<entry>&tilde; End Time</entry>
<entry>Label</entry>
</row>
<row><entry namest="1" nameend="4" align="center" rowsep="1"></entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry>1</entry>
<entry>00:00:10:12</entry>
<entry>00:00:12:10</entry>
<entry>Penalty Kick</entry>
</row>
<row>
<entry>2</entry>
<entry>00:20:12:10</entry>
<entry>00:20:13:10</entry>
<entry>Field Goal</entry>
</row>
<row>
<entry>3</entry>
<entry>00:33:12:09</entry>
<entry>00:35:12:10</entry>
<entry>Penalty Corner</entry>
</row>
<row>
<entry>4</entry>
<entry>. . .</entry>
<entry>. . . </entry>
<entry>. . .</entry>
</row>
<row><entry namest="1" nameend="4" align="center" rowsep="1"></entry>
</row>
</tbody>
</tgroup>
</table>
</table-cwu>
</paragraph>
<paragraph id="P-0237" lvl="0"><number>&lsqb;0237&rsqb;</number> There are several approaches to generating such reports, using software algorithms, described in the prior art. One of the approaches to event detection has been disclosed in R. Mohan. This approach uses reference video segments (examples of how a typical event would look like) and compares the target stream to the reference video segment based on generating codes for both the reference segment and the target segment. The discussion provided by Mohan however does not address the problem of performing such similarity measurements between a target stream and a multiplicity (large number) of reference streams, nor does it address including the textual information. Essentially, the target stream is simultaneously compared to the reference segments in a sequential fashion, one reference segment at a time, only using visual information. This inherently limits the number of reference segments that can be used in the comparisons. </paragraph>
<paragraph id="P-0238" lvl="0"><number>&lsqb;0238&rsqb;</number> The classification methods discussed in this invention can be applied to the video event detection problem as follows. The multiple example videos (media items) for the events to be detected are selected. These videos form the reference media streams S. The reference media streams are used to compute reference feature vectors. </paragraph>
<paragraph id="P-0239" lvl="0"><number>&lsqb;0239&rsqb;</number> The search engine described in this invention is deployed using these reference feature vectors. The target media stream (the video to be annotated) is fed to the appropriate decoder and the classification engine operates on the target media stream to generate the report. This report is a tabulation of the events in the target stream as shown in the table above. </paragraph>
<paragraph id="P-0240" lvl="0"><number>&lsqb;0240&rsqb;</number> This event detection is not limited to off-line video annotation, but also can be performed in real-time. Applications are in the arena of monitoring and human machine interaction. Events, such as, dangerous situations, human gestures combined with spoken command, etc. Can be detected in real time by employing the classification engine described in this invention with an appropriate feature vectors. </paragraph>
<paragraph id="P-0241" lvl="0"><number>&lsqb;0241&rsqb;</number> Another use of this invention is the categorization of multimedia email. Today, the content of email consists of text, possibly with attachments, or it consists of an html&mdash;hypertext markup language&mdash;file, which is itself text, possibly with references to other files or data objects that may provide non-textual data to be used by a browser when the html file is displayed. In the future, we envision email whose content is primarily a video message, possibly embedded in or accompanying a text file (e.g., an html file) used to control the display of the video message. </paragraph>
<paragraph id="P-0242" lvl="0"><number>&lsqb;0242&rsqb;</number> Such video email may well be created in a scenario such as the following, in which we assume the computer belonging to the user&mdash;here being Sam Sender&mdash;is equipped with speakers, a video camera trained on the user, and voice recognition software. Sam Sender wishes to send a message to Richard Receiver, a customer service representative at the Message Receiver Corp. Assuming that Sam Sender has named his computer Sybil, Sam says or signals, &ldquo;Sybil, send a message to Richard Receiver at MessageReceiverCorp.com, with caption: &lsquo;Complaint about billing error.&rsquo;&rdquo; The computer identifies the email address of the recipient, detects that the intended text caption for the message is &ldquo;Complaint about billing error,&rdquo; prepares to store the video and sound components of the message in a file, turns on the camera, and then says to Sam &ldquo;Ready to record message.&rdquo; The Sam recites his message using colorful language, while gesticulating and making faces. All of this is recorded by the computer. Sam finishes by saying or signaling &ldquo;Sybil, send message.&rdquo; The computer would then create a file containing the caption to be displayed, the video (including sound) that was recorded, the address of the sender, and any other information needed to enable the ultimate display of the captioned video by Richard Receiver&apos;s computer. </paragraph>
<paragraph id="P-0243" lvl="0"><number>&lsqb;0243&rsqb;</number> The classifiers induced by the method of this invention, if trained with appropriate training data, could be used to classify such video email. </paragraph>
<paragraph id="P-0244" lvl="0"><number>&lsqb;0244&rsqb;</number> For instance, in the scenario above, the mail server at the Message Receiver Corp. might apply a classifier to categorize the message as one that should be handled by Bill Bungler in the billing department, consequently sending a copy of the message directly to Bill Bungler in order to expedite resolution of the problem, while also sending a note to Richard Receiver informing him of this. </paragraph>
<paragraph id="P-0245" lvl="0"><number>&lsqb;0245&rsqb;</number> The following figures give a further depiction and description of the above described uses of the invention. They are provided as explanation of the business applications of the invention. </paragraph>
<paragraph id="P-0246" lvl="0"><number>&lsqb;0246&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 23</cross-reference> is a business method for determining profiles of those multimedia item categories of particular interest to various different users retrieving multimedia content from private and/or public networks. Here a private and/or public network <highlight><bold>2300</bold></highlight>, contains multimedia items in multiple ways. The multimedia items URL 1 (<highlight><bold>2303</bold></highlight>), . . . , URL 2 (<highlight><bold>2306</bold></highlight>), . . . , URL N (<highlight><bold>2309</bold></highlight>) each could a be a Web page that is indexed by a Universal Resource Locator (URL). The multimedia items can also be databases containing multimedia content, i.e., Database 1 (<highlight><bold>2304</bold></highlight>), . . . , Database M (<highlight><bold>2308</bold></highlight>). From Web pages and these databases, the multimedia may be retrieved and viewed by the users through any means (e.g., FTP&mdash;file transfer protocol). The users, <highlight><bold>2341</bold></highlight>, <highlight><bold>2343</bold></highlight>, <highlight><bold>2345</bold></highlight>, . . . , i.e., , User i, i&equals;1, . . . , m, retrieving multimedia documents <highlight><bold>2303</bold></highlight>, <highlight><bold>2306</bold></highlight>, . . . and <highlight><bold>2304</bold></highlight>, <highlight><bold>2308</bold></highlight> . . . , do so through the clients <highlight><bold>2342</bold></highlight>, <highlight><bold>2344</bold></highlight>, <highlight><bold>2346</bold></highlight>, . . . . These clients retrieve the multimedia items from one or more servers (not shown) at which the multimedia items reside. The clients can be thin with little compute power and storage available, but, in general, can be a computing device of any size. The clients <highlight><bold>2342</bold></highlight>, <highlight><bold>2344</bold></highlight>, . . . , and <highlight><bold>2346</bold></highlight> are connected to the servers through communication links <highlight><bold>2353</bold></highlight>, <highlight><bold>2356</bold></highlight>, . . . , and <highlight><bold>2359</bold></highlight>, respectively. These links can be wireless (e.g., wireless Ethernet, BlueTooth) or hardwired. </paragraph>
<paragraph id="P-0247" lvl="0"><number>&lsqb;0247&rsqb;</number> The communication links <highlight><bold>2353</bold></highlight>, <highlight><bold>2356</bold></highlight>, . . . , and <highlight><bold>2359</bold></highlight> can be either overtly or covertly monitored, by processes which are indicated with the circular shapes, <highlight><bold>2363</bold></highlight>, <highlight><bold>2366</bold></highlight> through <highlight><bold>2369</bold></highlight>. As part of these processes, the intercepted multimedia items can be categorized in terms of category, subject, topic, object, etc. For each individual user, a profile, profiles <highlight><bold>2372</bold></highlight>, <highlight><bold>2376</bold></highlight>, . . . , <highlight><bold>2379</bold></highlight>, i.e., Profiles j, j&equals;1, . . . , m, are generated by building an textual-visual feature vector F from the intercepted multimedia. The profiles can be simply a set of such vectors, F<highlight><subscript>i</subscript></highlight>, i&equals;0, . . . , n. These vectors can also be used to learn the preferences and dislikes of a user, users <highlight><bold>2341</bold></highlight>, <highlight><bold>2343</bold></highlight>, <highlight><bold>2345</bold></highlight>. Such a description of the user is stored in profiles <highlight><bold>2373</bold></highlight>, <highlight><bold>2376</bold></highlight>, <highlight><bold>2379</bold></highlight>, . . . . Applications of such profiles are direct marketing, user-profile-defined push news, news alerts. For instance, a cable company could learn the user profiles and recommend programs based on the user profile to individual viewers. Similar function can be built into set top boxes. </paragraph>
<paragraph id="P-0248" lvl="0"><number>&lsqb;0248&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 24A</cross-reference> depicts a business application of the multimedia item categorizer subject of this invention for building index tables to organize a multimedia item database in terms of categories or indices. The database <highlight><bold>2400</bold></highlight> contains multimedia items, m&equals;1, . . . , M, <highlight><bold>2401</bold></highlight>, <highlight><bold>2402</bold></highlight>, <highlight><bold>2403</bold></highlight>, . . . , <highlight><bold>2406</bold></highlight>, . . . , <highlight><bold>2409</bold></highlight>. Each multimedia item is of unknown category, subject, object or topic. There may also be global parametric data associated with these multimedia items. Process <highlight><bold>2415</bold></highlight>, is the series of computational step to determine a visual and textual feature vector, F&equals;F<highlight><subscript>v</subscript></highlight>&plus;F<highlight><subscript>t </subscript></highlight><highlight><bold>2416</bold></highlight> for all multimedia items. These feature vectors are input to categorizer <highlight><bold>2420</bold></highlight> (a categorizer as described in <cross-reference target="DRAWINGS">FIGS. 6A and 6B</cross-reference>). </paragraph>
<paragraph id="P-0249" lvl="0"><number>&lsqb;0249&rsqb;</number> For each multimedia item m (<highlight><bold>2406</bold></highlight>) in database <highlight><bold>2400</bold></highlight> a pair (item i, &lcub;cat j, cat k, cat l, . . . &rcub;) as <highlight><bold>2425</bold></highlight> is determined. These pairs <highlight><bold>2422</bold></highlight> are indicated by arrows <highlight><bold>2427</bold></highlight>, where the examples are (item n, &lcub;cat 2&rcub;), (item i, &lcub;cat j&rcub;) <highlight><bold>2426</bold></highlight>, (item m, &lcub;cat 3, 6&rcub;) <highlight><bold>2429</bold></highlight> and (item 3, &lcub;cat N&rcub;) <highlight><bold>2428</bold></highlight>. Each of these pairs contains an item, indicated by item i in <highlight><bold>2425</bold></highlight>, and a set of categories indicated by the set &lcub;cat j, cat k, cat l, . . . &rcub;. Hence, every item in the multimedia database is annotated by a series, or set, of categories that the particular item belongs to. These pairs (item i, &lcub;cat j, cat k, cat l, . . . &rcub;) are used to populate the hash table <highlight><bold>2410</bold></highlight>. </paragraph>
<paragraph id="P-0250" lvl="0"><number>&lsqb;0250&rsqb;</number> This table <highlight><bold>2410</bold></highlight>, has a number of N indices <highlight><bold>2412</bold></highlight> and an associated number of N entries <highlight><bold>2414</bold></highlight>. The indices <highlight><bold>2412</bold></highlight>, cat 1, . . . , cat i, . . . , cat N are associated with the N possible categories that are (expected to be) found in the database. Here these categories can be learned or can be a predetermined set of categories. The entries <highlight><bold>2414</bold></highlight>, e.g., <highlight><bold>2415</bold></highlight>, <highlight><bold>2416</bold></highlight>, <highlight><bold>2417</bold></highlight>, <highlight><bold>2418</bold></highlight>, <highlight><bold>2419</bold></highlight>, contain the item numbers that are associated with the corresponding categories in the indices <highlight><bold>2412</bold></highlight> as is determined by categorizer <highlight><bold>2420</bold></highlight>. Hence, <highlight><bold>2427</bold></highlight> (item n, &lcub;cat 2&rcub;) is placed in index Cat 2 with corresponding entry <highlight><bold>2416</bold></highlight> containing the item number n. Note that each entry can contain multiple item numbers, indicated by entry <highlight><bold>2416</bold></highlight> &lcub;j . . . &rcub; This simply means that all these items &lcub;j . . . &rcub; are categorized as the corresponding category number in the index <highlight><bold>2412</bold></highlight> associated with entry <highlight><bold>2416</bold></highlight>. Of course, an item can also be classified into multiple categories, indicated by arrows <highlight><bold>2428</bold></highlight> &lcub;item m, &lcub;cat x, y&rcub;) and <highlight><bold>2429</bold></highlight> &lcub;item m, &lcub;cat x, y&rcub;). Table <highlight><bold>2410</bold></highlight> can now be used to retrieve multimedia items of a certain category. For example, to find items of category i the corresponding item numbers is given in list or set &lcub;j . . . &rcub; as in <highlight><bold>2417</bold></highlight>. That is, the set of multimedia items contains item j. The next figure further explains this process. </paragraph>
<paragraph id="P-0251" lvl="0"><number>&lsqb;0251&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 24B</cross-reference> is a business application for retrieving of multimedia items from database <highlight><bold>2400</bold></highlight> based on the database organization and database indexing described in <cross-reference target="DRAWINGS">FIG. 24A</cross-reference>. Here is again shown the multimedia database <highlight><bold>2400</bold></highlight> containing multimedia items, m&equals;1, . . . , M, <highlight><bold>2401</bold></highlight>, <highlight><bold>1402</bold></highlight>, <highlight><bold>2403</bold></highlight>, . . . , <highlight><bold>2406</bold></highlight>, . . . , <highlight><bold>2409</bold></highlight>. The populated hash table <highlight><bold>2410</bold></highlight> from <cross-reference target="DRAWINGS">FIG. 24</cross-reference>B, contains (index, entry) pairs such as <highlight><bold>2429</bold></highlight> &lcub;item m, &lcub;cat x, y&rcub;)., indicating that item m and 6 belongs to categories x and y. Hence, the search for multimedia items of (say) category Cat i, simply is performed by indexing into index entry pair, (Cat i, &lcub;j . . . &rcub;) <highlight><bold>2417</bold></highlight> of table <highlight><bold>2410</bold></highlight>. The list &lcub;j . . . &rcub; or set &lcub;j . . . &rcub; in the corresponding entry of index <highlight><bold>2417</bold></highlight> are the multimedia items that are of category Cat i. The entries <highlight><bold>2414</bold></highlight> contain pointers <highlight><bold>2460</bold></highlight>, <highlight><bold>2462</bold></highlight>, <highlight><bold>2464</bold></highlight>, <highlight><bold>2466</bold></highlight>, . . . , <highlight><bold>2468</bold></highlight> to the multimedia items <highlight><bold>2401</bold></highlight>, . . . , <highlight><bold>2409</bold></highlight> in database <highlight><bold>2400</bold></highlight>. </paragraph>
<paragraph id="P-0252" lvl="0"><number>&lsqb;0252&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 25</cross-reference> is a business application and system <highlight><bold>2500</bold></highlight> where one or more of multimedia items are received and the items are routed according to the multimedia items category. The systems takes as input any multimedia item <highlight><bold>2510</bold></highlight>. This item may contain streaming multimedia and this could be a streaming multimedia item of different categories in different time segments of the stream. This multimedia item is input <highlight><bold>2515</bold></highlight> to the categorizer <highlight><bold>2520</bold></highlight>. An incarnation of categorizer <highlight><bold>2520</bold></highlight> is, for example, shown in <cross-reference target="DRAWINGS">FIG. 18</cross-reference>. Switch <highlight><bold>2530</bold></highlight> is a router that can direct the multimedia item <highlight><bold>2510</bold></highlight> to various destinations, Destination 1 (<highlight><bold>2550</bold></highlight>), Destination 2 (<highlight><bold>2560</bold></highlight>), through <highlight><bold>2570</bold></highlight>, Destination N (<highlight><bold>2590</bold></highlight>). Switch <highlight><bold>2530</bold></highlight> is different from an ordinary switch in that the multimedia item <highlight><bold>2510</bold></highlight> can be routed to one or more destinations <highlight><bold>2540</bold></highlight> at the same time. An example in <cross-reference target="DRAWINGS">FIG. 25</cross-reference> is given by extra switch position <highlight><bold>2538</bold></highlight> in addition to <highlight><bold>2535</bold></highlight>. Herewith, multimedia item <highlight><bold>2510</bold></highlight> is sent to Destination 2 (<highlight><bold>2560</bold></highlight>) but also to some other destination Destination n, with n determined by <highlight><bold>2538</bold></highlight>. </paragraph>
<paragraph id="P-0253" lvl="0"><number>&lsqb;0253&rsqb;</number> A use of system <highlight><bold>2500</bold></highlight> is, for example, the categorization of incoming multimedia e-mail and the determination what the correct procedure for handling each particular incoming e-mail is. The incoming e-mail can be routed to a sender, a folder, a person, a personal folder, a corporate folder, and a corporate department. The multimedia e-mail may also be duplicated before being routed. </paragraph>
<paragraph id="P-0254" lvl="0"><number>&lsqb;0254&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 26</cross-reference> is a business application <highlight><bold>2600</bold></highlight> where one or more multimedia items <highlight><bold>2610</bold></highlight> are received and for each item some decision <highlight><bold>2690</bold></highlight> based on the category of the multimedia item is made. This system is broader than the system described in <cross-reference target="DRAWINGS">FIG. 25</cross-reference>, in that the business decision <highlight><bold>2690</bold></highlight> does not need to be a decision to route the multimedia item to certain physical or logical places. The systems takes as input one or more multimedia items <highlight><bold>2610</bold></highlight>. These items may contain streaming multimedia and could be a streaming multimedia item of different categories in different time segments of the stream. This multimedia item is input <highlight><bold>2615</bold></highlight> to the categorizer <highlight><bold>2560</bold></highlight>. The categorizer may, for instance, be as the one shown in <cross-reference target="DRAWINGS">FIG. 18</cross-reference>. The output <highlight><bold>2625</bold></highlight> of categorizer <highlight><bold>2620</bold></highlight> is input to the decision process <highlight><bold>2690</bold></highlight>. The multimedia items <highlight><bold>2610</bold></highlight> could be digital multimedia copies and the feature vectors computed by categorizer <highlight><bold>2560</bold></highlight> could represent restricted multimedia items and the comparison determines if the digital multimedia copies are similar to restricted multimedia items and the decision is that the digital multimedia copies are subject to a second restriction. This second restriction could include any one or more of the following: a copyright restriction, a trademark restriction, intellectual property restriction, parental guidance restriction, common decency restrictions, user-defined restrictions. </paragraph>
<paragraph id="P-0255" lvl="0"><number>&lsqb;0255&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 27</cross-reference> is a business application <highlight><bold>2700</bold></highlight> where one or more multimedia items on a private and/or public network <highlight><bold>2710</bold></highlight> are examined and for each found multimedia item a decision based on the category of the multimedia item is made. The private and/or public network <highlight><bold>2710</bold></highlight> contains an, in principle, very large number multimedia items. Examples of these items in <cross-reference target="DRAWINGS">FIG. 27</cross-reference> are the items labeled <highlight><bold>2702</bold></highlight>, <highlight><bold>2704</bold></highlight> and <highlight><bold>2706</bold></highlight>. These items can be accessed and downloaded, if necessary, by process <highlight><bold>2720</bold></highlight>. Process <highlight><bold>2720</bold></highlight> is the combination of a crawler process <highlight><bold>2730</bold></highlight> and a down loader process <highlight><bold>2740</bold></highlight>. The crawler process <highlight><bold>2730</bold></highlight> is a process that finds the multimedia items <highlight><bold>2702</bold></highlight>, <highlight><bold>2704</bold></highlight>, . . . , <highlight><bold>2706</bold></highlight> on the private and/or public network <highlight><bold>2710</bold></highlight> The down loader process <highlight><bold>2740</bold></highlight>, on the other hand, examines the multimedia items <highlight><bold>2702</bold></highlight>, <highlight><bold>2704</bold></highlight>, . . . , <highlight><bold>2706</bold></highlight>, and if multimedia content is found that is suspicious according to a set of predefined rules which can contain a categorizer as <highlight><bold>2780</bold></highlight>. If the content is suspicious, the multimedia item is downloaded and is input <highlight><bold>2775</bold></highlight> to the categorizer <highlight><bold>2780</bold></highlight>. The categorizer may, for instance, be as the one shown in <cross-reference target="DRAWINGS">FIG. 18</cross-reference>. The output <highlight><bold>2785</bold></highlight> of categorizer <highlight><bold>2780</bold></highlight> is input to the decision process <highlight><bold>2790</bold></highlight>. The multimedia items <highlight><bold>2702</bold></highlight>, <highlight><bold>2704</bold></highlight>, <highlight><bold>2706</bold></highlight> could be digital multimedia copies and the feature vectors computed by categorizer <highlight><bold>2780</bold></highlight> could represent restricted multimedia items and the comparison determines if the digital multimedia copies are similar to the restricted multimedia items and the decision is that the digital multimedia copies are subject to a second restriction. This second restriction includes any one or more of the following: a copyright restriction, a trademark restriction, intellectual property restriction, parental guidance restriction, common decency restrictions, user-defined restrictions. </paragraph>
</section>
</detailed-description>
</subdoc-description>
<subdoc-claims>
<heading lvl="1">We claim: </heading>
<claim id="CLM-00001">
<claim-text><highlight><bold>1</bold></highlight>. A business method comprising the steps of: 
<claim-text>monitoring one or more multimedia items accessed by a user, each multimedia item having two or more disparate modalities, the disparate modalities being at least one or more visual modalities and one or more textual modalities; </claim-text>
<claim-text>creating a visual feature vector for each of the visual modalities and a textual feature vector for each of the textual modalities; </claim-text>
<claim-text>concatenating the visual feature vectors and the textual feature vectors into a unified feature vector; </claim-text>
<claim-text>categorizing each of the multimedia items by categorizing the respective unified feature vector; and </claim-text>
<claim-text>assembling a user profile based on the categorization. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00002">
<claim-text><highlight><bold>2</bold></highlight>. A business method, as in <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, further comprising the step of: 
<claim-text>using the user profile to match one or more multimedia items stored in one or more databases. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00003">
<claim-text><highlight><bold>3</bold></highlight>. A business method, as in <dependent-claim-reference depends_on="CLM-00002">claim 2</dependent-claim-reference>, where one or more of the databases are part of a computer that is connected to one or more networks. </claim-text>
</claim>
<claim id="CLM-00004">
<claim-text><highlight><bold>4</bold></highlight>. A business method, as in <dependent-claim-reference depends_on="CLM-00003">claim 3</dependent-claim-reference>, where the networks include any one or more of the following: an Internet, an intranet, an extranet, a corporate network, a government network, a infrared network, and a radio frequency network. </claim-text>
</claim>
<claim id="CLM-00005">
<claim-text><highlight><bold>5</bold></highlight>. A business method, as in <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, where the categories include any one or more of the following: a product, a service, an interest, a retail item, a hobby, a food item, an item of clothing, a travel package, a vacation destination, a financial product, a business partner, a business interest, a medical product, a commercial, and a social interest. </claim-text>
</claim>
<claim id="CLM-00006">
<claim-text><highlight><bold>6</bold></highlight>. A business method, as in <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, where the category includes any one or more of the following services: consulting, legal, real estate, medical, technical, physical training, diet, cosmetic, fashion, governmental, automotive, design, architecture, personal assistants, games, on-line games of chance, dating services, and landscaping. </claim-text>
</claim>
<claim id="CLM-00007">
<claim-text><highlight><bold>7</bold></highlight>. A business method comprising the steps of: 
<claim-text>scanning one or more multimedia items in a database, each multimedia item having two or more disparate modalities, the disparate modalities being at least one or more visual modalities and one or more textual modalities; </claim-text>
<claim-text>creating a visual feature vector for each of the visual modalities and a textual feature vector for each of the textual modalities; </claim-text>
<claim-text>concatenating the visual feature vectors and the textual feature vectors into a unified feature vector; </claim-text>
<claim-text>categorizing each of the multimedia items by categorizing the respective unified feature vector; and </claim-text>
<claim-text>creating one or more indices of the database based on the categorization. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00008">
<claim-text><highlight><bold>8</bold></highlight>. A business method, as in <dependent-claim-reference depends_on="CLM-00007">claim 7</dependent-claim-reference>, where the database resides on any one or more of the following: a network server, a web site, a personal computer, a server farm, and a network disk array. </claim-text>
</claim>
<claim id="CLM-00009">
<claim-text><highlight><bold>9</bold></highlight>. A business method, as in <dependent-claim-reference depends_on="CLM-00007">claim 7</dependent-claim-reference>, further comprising the step of: 
<claim-text>making a business decision based on the classifications. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00010">
<claim-text><highlight><bold>10</bold></highlight>. A business method, as in <dependent-claim-reference depends_on="CLM-00009">claim 9</dependent-claim-reference>, where the categorization is used to organize a collection of the multimedia items into a database where the multimedia items are retrievable based on an annotation of the classification. </claim-text>
</claim>
<claim id="CLM-00011">
<claim-text><highlight><bold>11</bold></highlight>. A business method, as in <dependent-claim-reference depends_on="CLM-00011">claim 10</dependent-claim-reference>, where the multimedia items that are retrieved are used as a response to a query of a search engine. </claim-text>
</claim>
<claim id="CLM-00012">
<claim-text><highlight><bold>12</bold></highlight>. A business method, as in <dependent-claim-reference depends_on="CLM-00009">claim 9</dependent-claim-reference>, where the multimedia item is a multimedia e-mail and the multimedia e-mail is routed based on one or more of the categories assigned to the multimedia e-mail. </claim-text>
</claim>
<claim id="CLM-00013">
<claim-text><highlight><bold>13</bold></highlight>. A business method, as in <dependent-claim-reference depends_on="CLM-00011">claim 12</dependent-claim-reference>, where the multimedia e-mail is routed to any one or more of the following: a sender, a folder, a person, a personal folder, a corporate folder, and a corporate department. </claim-text>
</claim>
<claim id="CLM-00014">
<claim-text><highlight><bold>14</bold></highlight>. A business method, as in <dependent-claim-reference depends_on="CLM-00011">claim 13</dependent-claim-reference>, where the multimedia e-mail is multiplied before being routed. </claim-text>
</claim>
<claim id="CLM-00015">
<claim-text><highlight><bold>15</bold></highlight>. A business method comprising the steps of: 
<claim-text>scanning one or more multimedia items in a database, each multimedia item having two or more disparate modalities, the disparate modalities being at least one or more visual modalities and one or more textual modalities; </claim-text>
<claim-text>creating a visual feature vector for each of the visual modalities and a textual feature vector for each of the textual modalities; </claim-text>
<claim-text>concatenating the visual feature vectors and the textual feature vectors into a unified feature vector; </claim-text>
<claim-text>categorizing each of the multimedia items by categorizing the respective unified feature vector; </claim-text>
<claim-text>comparing one or more of the unified feature vectors to one or more other feature vectors; and </claim-text>
<claim-text>making a decision based on the comparison. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00016">
<claim-text><highlight><bold>16</bold></highlight>. A business method, as in <dependent-claim-reference depends_on="CLM-00011">claim 15</dependent-claim-reference>, where the multimedia items are digital multimedia copies and the feature vectors represent restricted multimedia items and the comparison determines if the digital multimedia copies are similar to the restricted multimedia items and the decision is that the digital multimedia copies are subject to a second restriction. </claim-text>
</claim>
<claim id="CLM-00017">
<claim-text><highlight><bold>17</bold></highlight>. A business method, as in <dependent-claim-reference depends_on="CLM-00011">claim 16</dependent-claim-reference>, where the second restriction includes any one or more of the following: a copyright restriction, a trademark restriction, intellectual property restriction, parental guidance restriction, common decency restrictions, user-defined restrictions. </claim-text>
</claim>
<claim id="CLM-00018">
<claim-text><highlight><bold>18</bold></highlight>. A business method, as in <dependent-claim-reference depends_on="CLM-00011">claim 16</dependent-claim-reference>, where the digital multimedia copies are accessible over the Internet. </claim-text>
</claim>
<claim id="CLM-00019">
<claim-text><highlight><bold>19</bold></highlight>. A business method, as in <dependent-claim-reference depends_on="CLM-00011">claim 15</dependent-claim-reference>, where the comparison indicates a degree of similarity and the decision is that the multimedia item contains a known content.</claim-text>
</claim>
</subdoc-claims>
<subdoc-drawings id="DRAWINGS">
<heading lvl="0" align="CENTER">Drawings</heading>
<representative-figure>16</representative-figure>
<figure id="figure-D00000">
<image id="EMI-D00000" file="US20030004966A1-20030102-D00000.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00001">
<image id="EMI-D00001" file="US20030004966A1-20030102-D00001.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00002">
<image id="EMI-D00002" file="US20030004966A1-20030102-D00002.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00003">
<image id="EMI-D00003" file="US20030004966A1-20030102-D00003.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00004">
<image id="EMI-D00004" file="US20030004966A1-20030102-D00004.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00005">
<image id="EMI-D00005" file="US20030004966A1-20030102-D00005.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00006">
<image id="EMI-D00006" file="US20030004966A1-20030102-D00006.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00007">
<image id="EMI-D00007" file="US20030004966A1-20030102-D00007.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00008">
<image id="EMI-D00008" file="US20030004966A1-20030102-D00008.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00009">
<image id="EMI-D00009" file="US20030004966A1-20030102-D00009.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00010">
<image id="EMI-D00010" file="US20030004966A1-20030102-D00010.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00011">
<image id="EMI-D00011" file="US20030004966A1-20030102-D00011.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00012">
<image id="EMI-D00012" file="US20030004966A1-20030102-D00012.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00013">
<image id="EMI-D00013" file="US20030004966A1-20030102-D00013.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00014">
<image id="EMI-D00014" file="US20030004966A1-20030102-D00014.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00015">
<image id="EMI-D00015" file="US20030004966A1-20030102-D00015.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00016">
<image id="EMI-D00016" file="US20030004966A1-20030102-D00016.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00017">
<image id="EMI-D00017" file="US20030004966A1-20030102-D00017.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00018">
<image id="EMI-D00018" file="US20030004966A1-20030102-D00018.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00019">
<image id="EMI-D00019" file="US20030004966A1-20030102-D00019.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00020">
<image id="EMI-D00020" file="US20030004966A1-20030102-D00020.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00021">
<image id="EMI-D00021" file="US20030004966A1-20030102-D00021.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00022">
<image id="EMI-D00022" file="US20030004966A1-20030102-D00022.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00023">
<image id="EMI-D00023" file="US20030004966A1-20030102-D00023.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00024">
<image id="EMI-D00024" file="US20030004966A1-20030102-D00024.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00025">
<image id="EMI-D00025" file="US20030004966A1-20030102-D00025.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00026">
<image id="EMI-D00026" file="US20030004966A1-20030102-D00026.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00027">
<image id="EMI-D00027" file="US20030004966A1-20030102-D00027.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00028">
<image id="EMI-D00028" file="US20030004966A1-20030102-D00028.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00029">
<image id="EMI-D00029" file="US20030004966A1-20030102-D00029.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00030">
<image id="EMI-D00030" file="US20030004966A1-20030102-D00030.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00031">
<image id="EMI-D00031" file="US20030004966A1-20030102-D00031.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00032">
<image id="EMI-D00032" file="US20030004966A1-20030102-D00032.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00033">
<image id="EMI-D00033" file="US20030004966A1-20030102-D00033.TIF" imf="TIFF" ti="DR"/>
</figure>
</subdoc-drawings>
</patent-application-publication>
