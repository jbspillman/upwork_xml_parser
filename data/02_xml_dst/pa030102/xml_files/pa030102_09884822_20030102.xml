<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE patent-application-publication SYSTEM "pap-v16-2002-01-01.dtd" [
<!ENTITY US20030005248A1-20030102-D00000.TIF SYSTEM "US20030005248A1-20030102-D00000.TIF" NDATA TIF>
<!ENTITY US20030005248A1-20030102-D00001.TIF SYSTEM "US20030005248A1-20030102-D00001.TIF" NDATA TIF>
<!ENTITY US20030005248A1-20030102-D00002.TIF SYSTEM "US20030005248A1-20030102-D00002.TIF" NDATA TIF>
<!ENTITY US20030005248A1-20030102-D00003.TIF SYSTEM "US20030005248A1-20030102-D00003.TIF" NDATA TIF>
<!ENTITY US20030005248A1-20030102-D00004.TIF SYSTEM "US20030005248A1-20030102-D00004.TIF" NDATA TIF>
<!ENTITY US20030005248A1-20030102-D00005.TIF SYSTEM "US20030005248A1-20030102-D00005.TIF" NDATA TIF>
<!ENTITY US20030005248A1-20030102-D00006.TIF SYSTEM "US20030005248A1-20030102-D00006.TIF" NDATA TIF>
<!ENTITY US20030005248A1-20030102-D00007.TIF SYSTEM "US20030005248A1-20030102-D00007.TIF" NDATA TIF>
<!ENTITY US20030005248A1-20030102-D00008.TIF SYSTEM "US20030005248A1-20030102-D00008.TIF" NDATA TIF>
<!ENTITY US20030005248A1-20030102-D00009.TIF SYSTEM "US20030005248A1-20030102-D00009.TIF" NDATA TIF>
<!ENTITY US20030005248A1-20030102-D00010.TIF SYSTEM "US20030005248A1-20030102-D00010.TIF" NDATA TIF>
<!ENTITY US20030005248A1-20030102-D00011.TIF SYSTEM "US20030005248A1-20030102-D00011.TIF" NDATA TIF>
<!ENTITY US20030005248A1-20030102-D00012.TIF SYSTEM "US20030005248A1-20030102-D00012.TIF" NDATA TIF>
<!ENTITY US20030005248A1-20030102-D00013.TIF SYSTEM "US20030005248A1-20030102-D00013.TIF" NDATA TIF>
<!ENTITY US20030005248A1-20030102-D00014.TIF SYSTEM "US20030005248A1-20030102-D00014.TIF" NDATA TIF>
<!ENTITY US20030005248A1-20030102-D00015.TIF SYSTEM "US20030005248A1-20030102-D00015.TIF" NDATA TIF>
<!ENTITY US20030005248A1-20030102-D00016.TIF SYSTEM "US20030005248A1-20030102-D00016.TIF" NDATA TIF>
<!ENTITY US20030005248A1-20030102-D00017.TIF SYSTEM "US20030005248A1-20030102-D00017.TIF" NDATA TIF>
<!ENTITY US20030005248A1-20030102-D00018.TIF SYSTEM "US20030005248A1-20030102-D00018.TIF" NDATA TIF>
<!ENTITY US20030005248A1-20030102-D00019.TIF SYSTEM "US20030005248A1-20030102-D00019.TIF" NDATA TIF>
<!ENTITY US20030005248A1-20030102-D00020.TIF SYSTEM "US20030005248A1-20030102-D00020.TIF" NDATA TIF>
<!ENTITY US20030005248A1-20030102-D00021.TIF SYSTEM "US20030005248A1-20030102-D00021.TIF" NDATA TIF>
<!ENTITY US20030005248A1-20030102-D00022.TIF SYSTEM "US20030005248A1-20030102-D00022.TIF" NDATA TIF>
<!ENTITY US20030005248A1-20030102-D00023.TIF SYSTEM "US20030005248A1-20030102-D00023.TIF" NDATA TIF>
<!ENTITY US20030005248A1-20030102-D00024.TIF SYSTEM "US20030005248A1-20030102-D00024.TIF" NDATA TIF>
<!ENTITY US20030005248A1-20030102-D00025.TIF SYSTEM "US20030005248A1-20030102-D00025.TIF" NDATA TIF>
<!ENTITY US20030005248A1-20030102-D00026.TIF SYSTEM "US20030005248A1-20030102-D00026.TIF" NDATA TIF>
<!ENTITY US20030005248A1-20030102-D00027.TIF SYSTEM "US20030005248A1-20030102-D00027.TIF" NDATA TIF>
<!ENTITY US20030005248A1-20030102-D00028.TIF SYSTEM "US20030005248A1-20030102-D00028.TIF" NDATA TIF>
<!ENTITY US20030005248A1-20030102-D00029.TIF SYSTEM "US20030005248A1-20030102-D00029.TIF" NDATA TIF>
<!ENTITY US20030005248A1-20030102-D00030.TIF SYSTEM "US20030005248A1-20030102-D00030.TIF" NDATA TIF>
]>
<patent-application-publication>
<subdoc-bibliographic-information>
<document-id>
<doc-number>20030005248</doc-number>
<kind-code>A1</kind-code>
<document-date>20030102</document-date>
</document-id>
<publication-filing-type>new</publication-filing-type>
<domestic-filing-data>
<application-number>
<doc-number>09884822</doc-number>
</application-number>
<application-number-series-code>09</application-number-series-code>
<filing-date>20010619</filing-date>
</domestic-filing-data>
<technical-information>
<classification-ipc>
<classification-ipc-primary>
<ipc>G06F012/00</ipc>
</classification-ipc-primary>
<classification-ipc-edition>07</classification-ipc-edition>
</classification-ipc>
<classification-us>
<classification-us-primary>
<uspc>
<class>711</class>
<subclass>165000</subclass>
</uspc>
</classification-us-primary>
<classification-us-secondary>
<uspc>
<class>711</class>
<subclass>114000</subclass>
</uspc>
</classification-us-secondary>
</classification-us>
<title-of-invention>Apparatus and method for instant copy of data</title-of-invention>
</technical-information>
<inventors>
<first-named-inventor>
<name>
<given-name>Stephen</given-name>
<middle-name>S.</middle-name>
<family-name>Selkirk</family-name>
</name>
<residence>
<residence-us>
<city>Broomfield</city>
<state>CO</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</first-named-inventor>
<inventor>
<name>
<given-name>Charles</given-name>
<middle-name>A.</middle-name>
<family-name>Milligan</family-name>
</name>
<residence>
<residence-us>
<city>Golden</city>
<state>CO</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
</inventors>
<correspondence-address>
<name-1>Wayne P. Bailey</name-1>
<name-2>Storage Technology Corporation</name-2>
<address>
<address-1>One StorageTek Drive</address-1>
<city>Louisville</city>
<state>CO</state>
<postalcode>80028-4309</postalcode>
<country>
<country-code>US</country-code>
</country>
</address>
</correspondence-address>
</subdoc-bibliographic-information>
<subdoc-abstract>
<paragraph id="A-0001" lvl="0">A system is provided to support dynamically changeable virtual mapping schemes in a data processing system. The present invention separates processing of data unit requirements from the selection of which storage subsystems to use for storage by using a storage methodologies inventory. A stored data management subsystem contains one or more hosts. A plurality of data storage elements is functionally coupled to the one or more hosts. The plurality of data storage elements is organized using a plurality of layers of mapping tables. The plurality of layers of mapping tables provides unique identification of location of the data such that individual data entries in a mapping table is variable and self-defining with respect to the amount of data managed. In addition, the present system provides various instant copy mechanisms for copying data upon receiving a write operation to either a source or copy data. The instant copy mechanisms may be selected based on the type of mapping originally used to store the data that is to be copied. </paragraph>
</subdoc-abstract>
<subdoc-description>
<cross-reference-to-related-applications>
<heading lvl="1">CROSS REFERENCE TO RELATED APPLICATIONS </heading>
<paragraph id="P-0001" lvl="0"><number>&lsqb;0001&rsqb;</number> This application claims the benefit of the filing date of corresponding U.S. Provisional Patent Application No. 60/212,260, entitled MULTI-LAYER MAPPING TABLES, filed Jun. 20, 2000. In addition, the present invention is related to applications entitled A SYSTEM TO SUPPORT DYNAMICALLY FLEXIBLE DATA DEFINITIONS AND STORAGE REQUIREMENTS, Ser. No. 09/751,635, Attorney Docket Number 00-059-DSK, filed on Dec. 29, 2000; USING CURRENT RECOVERY MECHANISMS TO IMPLEMENT DYNAMIC MAPPING OPERATIONS, Ser. No. 09/800,714, Attorney Docket Number 00-061-DSK, filed on Mar. 8, 2001; RECOVERY OF DYNAMIC MAPS AND DATA MANAGED THEREBY, Ser. No. 09/752,253, Attorney Docket Number 00-063-DSK, filed on Dec. 30, 2000; FLOATING VIRTUALIZATION LAYERS, Ser. No. 09/752,071, Attorney Docket Number 00-116-DSK, filed on Dec. 29, 2000; SELF DEFINING DATA UNITS, Ser. No. 09/751,641, Attorney Docket Number 00-117-DSK, filed on Dec. 29, 2000; DYNAMICALLY CHANGEABLE VIRTUAL MAPPING SCHEME, Ser. No. 09/751,772, Attorney Docket Number 00-062-DSK, filed on Dec. 29, 2000; APPARATUS AND METHOD FOR DYNAMICALLY CHANGEABLE VIRTUAL MAPPING SCHEME, Ser. No. ______, Attorney Docket Number 00-060-DSK and APPARATUS AND METHOD FOR INSTANT COPY OF DATA IN A DYNAMICALLY CHANGEABLE VIRTUAL MAPPING ENVIRONMENT, Ser. No. ______ , Attorney Docket Number 2001-006-DSK, both of which are filed on even date hereof. All of the above related applications are assigned to the same assignee, and are incorporated herein by referenced.</paragraph>
</cross-reference-to-related-applications>
<summary-of-invention>
<section>
<heading lvl="1">BACKGROUND OF THE INVENTION </heading>
<paragraph id="P-0002" lvl="0"><number>&lsqb;0002&rsqb;</number> 1. Technical Field </paragraph>
<paragraph id="P-0003" lvl="0"><number>&lsqb;0003&rsqb;</number> The present invention relates generally to an improved data processing system and in particular to a data storage subsystem for use with a data processing system. Still more particularly, the present invention provides an apparatus and method for dynamically changeable virtual mapping scheme in a data processing system. </paragraph>
<paragraph id="P-0004" lvl="0"><number>&lsqb;0004&rsqb;</number> 2. Description of Related Art </paragraph>
<paragraph id="P-0005" lvl="0"><number>&lsqb;0005&rsqb;</number> In computer systems and data storage subsystems, one problem is performing a data file copy operation in a manner that minimizes the use of processing resources and data storage memory. Previously, data files were copied in their entirety by the processor, such that two exact copies of the selected data file were resident in the data storage memory. This operation consumed twice the amount of memory for the storage of two identical copies of the data file. Additionally, this operation required the intervention of the processor to effect the copy of the original data file. </paragraph>
<paragraph id="P-0006" lvl="0"><number>&lsqb;0006&rsqb;</number> A data file snapshot copy is an improvement over this type of copy process. This snapshot copy process includes a dynamically mapped virtual data storage subsystem. This subsystem stores data files received from a processor in back-end data storage devices by mapping the processor assigned data file identifier to a logical address that identifies the physical storage location of the data. This dynamically mapped virtual data storage subsystem performs a copy of a data file by creating a duplicate data file pointer to a data file identifier in a mapping table to reference the original data file. In this dynamically mapped virtual data storage subsystem, the data files are referred to as a collection of &ldquo;virtual tracks&rdquo; and each data file is identified by unique virtual track addresses (VTAs). The use of a mapping table provides the opportunity to replace the process of copying the entirety of a data file in the data storage devices with a process that manipulates the contents of the mapping table. A data file appears to have been copied if the name used to identify the original data file and the name used to identify the copy data file are both mapped to the same physical data storage location. </paragraph>
<paragraph id="P-0007" lvl="0"><number>&lsqb;0007&rsqb;</number> This mechanism enables the processor to access the data file via two virtual track addresses while only a single physical copy of the data file resides on the back-end data storage devices in the data storage subsystem. This process minimizes the time required to execute the copy operation and the amount of memory used since the copy operation is carried out by creating a new pointer to the original data file and does not require any copying of the data file itself. </paragraph>
<paragraph id="P-0008" lvl="0"><number>&lsqb;0008&rsqb;</number> One implementation of the snapshot copy process provides a two-table approach. One table has table entries for each virtual device track pointing to another table containing the physical track location for the entry. Each physical track table entry identifies the number of virtual track entries that point to this entry by use of a reference count mechanism. Each virtual track entry that points to the physical track is called a &ldquo;reference.&rdquo; The reference count increments when a new virtual track table entry pointer points to this physical entry (e.g. snap) and the reference count decrements when a virtual track table entry pointer is removed (e.g. update source after a snap). When a reference count is zero, then that physical track can be deleted from the back-end since it is known that there are no references to the physical track. </paragraph>
<paragraph id="P-0009" lvl="0"><number>&lsqb;0009&rsqb;</number> System administrators are beginning to realize that &ldquo;point in time&rdquo; or &ldquo;instant&rdquo; copies of data are extremely useful. However, the system administrator has to specifically plan for and request execution of these copies at the host level, such as setting up mirrored volumes or using the snapshot commands available in virtual mapping subsystems. </paragraph>
</section>
<section>
<heading lvl="1">SUMMARY OF THE INVENTION </heading>
<paragraph id="P-0010" lvl="0"><number>&lsqb;0010&rsqb;</number> The present invention provides a system to support dynamically changeable virtual mapping schemes in a data processing system. The present invention separates processing of data unit requirements from the selection of which storage subsystems to use for storage by using a storage methodologies inventory. A stored data management subsystem contains one or more hosts. A plurality of data storage elements is functionally coupled to the one or more hosts. The plurality of data storage elements is organized using a plurality of layers of mapping tables. The plurality of layers of mapping tables provides unique identification of location of the data such that individual data entries in a mapping table is variable and self-defining with respect to the amount of data managed. </paragraph>
<paragraph id="P-0011" lvl="0"><number>&lsqb;0011&rsqb;</number> In addition, the present invention provides various instant copy mechanisms for copying data upon receiving a write operation to either original or to copy data. The instant copy mechanisms may be selected based on the type of mapping originally used to store the data that is to be copied. Other features and advantages of the present invention will be described in, or will become apparent to those of ordinary skill in the art in view of, the following detailed description of the preferred embodiments. </paragraph>
</section>
</summary-of-invention>
<brief-description-of-drawings>
<section>
<heading lvl="1">BRIEF DESCRIPTION OF THE DRAWINGS </heading>
<paragraph id="P-0012" lvl="0"><number>&lsqb;0012&rsqb;</number> The novel features believed characteristic of the invention are set forth in the appended claims. The invention itself, however, as well as a preferred mode of use, further objectives and advantages thereof, will best be understood by reference to the following detailed description of an illustrative embodiment when read in conjunction with the accompanying drawings, wherein: </paragraph>
<paragraph id="P-0013" lvl="0"><number>&lsqb;0013&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 1</cross-reference> is a pictorial representation of a distributed data processing system in which the present invention may be implemented; </paragraph>
<paragraph id="P-0014" lvl="0"><number>&lsqb;0014&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 2</cross-reference> is a block diagram of a storage subsystem in accordance with a preferred embodiment of the present invention; </paragraph>
<paragraph id="P-0015" lvl="0"><number>&lsqb;0015&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 3</cross-reference> is an exemplary block diagram of the conceptual relationship between the virtual device structures and the logical device structures in accordance with a preferred embodiment of the present invention; </paragraph>
<paragraph id="P-0016" lvl="0"><number>&lsqb;0016&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 4</cross-reference> is an illustration of the use of multiple RAID groups for providing potential for future higher performance requests in accordance with a preferred embodiment of the present invention; </paragraph>
<paragraph id="P-0017" lvl="0"><number>&lsqb;0017&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 5</cross-reference> is a flowchart illustrating a data unit/virtual device structure data processing methodology in accordance with a preferred embodiment of the present invention; </paragraph>
<paragraph id="P-0018" lvl="0"><number>&lsqb;0018&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 6</cross-reference> is an exemplary block diagram of the management API branch illustrated in <cross-reference target="DRAWINGS">FIG. 5</cross-reference> in accordance with a preferred embodiment of the present invention; </paragraph>
<paragraph id="P-0019" lvl="0"><number>&lsqb;0019&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 7</cross-reference> is an exemplary diagram of a virtual track table and a track number table; </paragraph>
<paragraph id="P-0020" lvl="0"><number>&lsqb;0020&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 8</cross-reference> is an exemplary illustration of a hierarchical relationship of a mapping table in accordance with a preferred embodiment of the present invention; </paragraph>
<paragraph id="P-0021" lvl="0"><number>&lsqb;0021&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 9</cross-reference> is an exemplary diagram of a portion of a mapping table describing an address range with four distinct sections in accordance with a preferred embodiment of the present invention; </paragraph>
<paragraph id="P-0022" lvl="0"><number>&lsqb;0022&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 10</cross-reference> is an exemplary block diagram of a multi-layer mapping table in accordance with a preferred embodiment of the present invention; </paragraph>
<paragraph id="P-0023" lvl="0"><number>&lsqb;0023&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 11</cross-reference> is an exemplary illustration of FlexRAID in accordance with the preferred embodiment of the present invention; </paragraph>
<paragraph id="P-0024" lvl="0"><number>&lsqb;0024&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 12</cross-reference> is an exemplary illustration of a control block entry format and modifications to the control block entry in accordance with a preferred embodiment of the present invention; </paragraph>
<paragraph id="P-0025" lvl="0"><number>&lsqb;0025&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 13</cross-reference> which is an exemplary meta-data block in accordance with a preferred embodiment of the present invention; </paragraph>
<paragraph id="P-0026" lvl="0"><number>&lsqb;0026&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 14</cross-reference> is an exemplary example of a default variable within boundary information in accordance with a preferred embodiment of the present invention; </paragraph>
<paragraph id="P-0027" lvl="0"><number>&lsqb;0027&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 15</cross-reference> is an exemplary diagram of a variable dynamically changeable mapping scheme virtual storage volume in accordance with the present invention; </paragraph>
<paragraph id="P-0028" lvl="0"><number>&lsqb;0028&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 16</cross-reference> is a high level flowchart outlining the primary steps in performing an instant copy of a portion of data in a variable dynamically changeable mapping scheme virtual volume storage sub-system; </paragraph>
<paragraph id="P-0029" lvl="0"><number>&lsqb;0029&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 17</cross-reference> illustrates the various copy control methods and their association with data mapping mechanisms; </paragraph>
<paragraph id="P-0030" lvl="0"><number>&lsqb;0030&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 18</cross-reference> is an exemplary diagram illustrating an instant copy of old data when a write operation is performed on the original data; </paragraph>
<paragraph id="P-0031" lvl="0"><number>&lsqb;0031&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 19</cross-reference> is an exemplary diagram illustrating an instant copy of old data when a write operation is performed to the copy data area; </paragraph>
<paragraph id="P-0032" lvl="0"><number>&lsqb;0032&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 20</cross-reference> is an exemplary diagram illustrating how the copy operations described with regard to <cross-reference target="DRAWINGS">FIGS. 18 and 19</cross-reference> may be used with variable size mapping units; </paragraph>
<paragraph id="P-0033" lvl="0"><number>&lsqb;0033&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 21</cross-reference> is an exemplary diagram illustrating an instant copy method, i.e. method A<highlight><bold>2</bold></highlight>, in which all writes are made to the copy data area; </paragraph>
<paragraph id="P-0034" lvl="0"><number>&lsqb;0034&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 22</cross-reference> illustrates a separation of two copies of data in accordance with the present invention; </paragraph>
<paragraph id="P-0035" lvl="0"><number>&lsqb;0035&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 23A</cross-reference> illustrates how the present invention may be extended to multiple copy areas; </paragraph>
<paragraph id="P-0036" lvl="0"><number>&lsqb;0036&rsqb;</number> <cross-reference target="DRAWINGS">FIGS. 23B and C</cross-reference> show how the same operation as in <cross-reference target="DRAWINGS">FIG. 23A</cross-reference> may be performed for various amounts of new data in each of the two copy areas; and </paragraph>
<paragraph id="P-0037" lvl="0"><number>&lsqb;0037&rsqb;</number> <cross-reference target="DRAWINGS">FIGS. 24A and 24B</cross-reference> illustrate an instant copy method, method C, that may be used to copy data which was originally mapped using a full pointer system, such as log-structured file mapping. </paragraph>
</section>
</brief-description-of-drawings>
<detailed-description>
<section>
<heading lvl="1">DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENT </heading>
<paragraph id="P-0038" lvl="0"><number>&lsqb;0038&rsqb;</number> With reference now to the figures, <cross-reference target="DRAWINGS">FIG. 1</cross-reference> depicts a pictorial representation of a distributed data processing system in which the present invention may be implemented. Distributed data processing system <highlight><bold>100</bold></highlight> is a network of computers in which the present invention may be implemented. Distributed data processing system <highlight><bold>100</bold></highlight> contains a network <highlight><bold>102</bold></highlight>, which is the medium used to provide communications links between various devices and computers connected together within distributed data processing system <highlight><bold>100</bold></highlight>. Network <highlight><bold>102</bold></highlight> may include permanent connections, such as wire or fiber optic cables, or temporary connections made through telephone connections. </paragraph>
<paragraph id="P-0039" lvl="0"><number>&lsqb;0039&rsqb;</number> In the depicted example, a server <highlight><bold>104</bold></highlight> is connected to network <highlight><bold>102</bold></highlight> along with storage subsystem <highlight><bold>106</bold></highlight>. In addition, clients <highlight><bold>108</bold></highlight>, <highlight><bold>110</bold></highlight>, and <highlight><bold>112</bold></highlight> also are connected to network <highlight><bold>102</bold></highlight>. These clients <highlight><bold>108</bold></highlight>, <highlight><bold>110</bold></highlight>, and <highlight><bold>112</bold></highlight> may be, for example, personal computers or network computers. For purposes of this application, a network computer is any computer, coupled to a network, which receives a program or other application from another computer coupled to the network. In the depicted example, server <highlight><bold>104</bold></highlight> provides data, such as boot files, operating system images, and applications to clients <highlight><bold>108</bold></highlight>-<highlight><bold>112</bold></highlight>. Clients <highlight><bold>108</bold></highlight>, <highlight><bold>110</bold></highlight>, and <highlight><bold>112</bold></highlight> are clients to server <highlight><bold>104</bold></highlight>. Distributed data processing system <highlight><bold>100</bold></highlight> may include additional servers, clients, and other devices not shown. Distributed data processing system <highlight><bold>100</bold></highlight> may be implemented as one or more of a number of different types of networks, such as, for example, an intranet, a local area network (LAN), or a wide area network (WAN). Network <highlight><bold>102</bold></highlight> contains various links, such as, for example, fiber optic links, packet switched communication links, enterprise systems connection (ESCON) fibers, small computer system interface (SCSI) cable, wireless communication links. In these examples, storage subsystem <highlight><bold>106</bold></highlight> may be connected to server <highlight><bold>104</bold></highlight> using ESCON fibers. <cross-reference target="DRAWINGS">FIG. 1</cross-reference> is intended as an example and not as an architectural limitation for the present invention. </paragraph>
<paragraph id="P-0040" lvl="0"><number>&lsqb;0040&rsqb;</number> Turning next to <cross-reference target="DRAWINGS">FIG. 2, a</cross-reference> block diagram of a storage subsystem is depicted in accordance with a preferred embodiment of the present invention. Storage subsystem <highlight><bold>200</bold></highlight> may be used to implement storage subsystem <highlight><bold>106</bold></highlight> in <cross-reference target="DRAWINGS">FIG. 1</cross-reference>. As illustrated in <cross-reference target="DRAWINGS">FIG. 2</cross-reference>, storage subsystem <highlight><bold>200</bold></highlight> includes storage devices <highlight><bold>202</bold></highlight>, interface <highlight><bold>204</bold></highlight>, interface <highlight><bold>206</bold></highlight>, cache memory <highlight><bold>208</bold></highlight>, processors <highlight><bold>210</bold></highlight>-<highlight><bold>224</bold></highlight>, and shared memory <highlight><bold>226</bold></highlight>. </paragraph>
<paragraph id="P-0041" lvl="0"><number>&lsqb;0041&rsqb;</number> Interfaces <highlight><bold>204</bold></highlight> and <highlight><bold>206</bold></highlight> in storage subsystem <highlight><bold>200</bold></highlight> provide a communication gateway through which communication between a data processing system and storage subsystem <highlight><bold>200</bold></highlight> may occur. In this example, interfaces <highlight><bold>204</bold></highlight> and <highlight><bold>206</bold></highlight> may be implemented using a number of different mechanisms, such as ESCON cards, SCSI cards, fiber channel interfaces, modems, network interfaces, or a network hub. Although the depicted example illustrates the use of two interface units, any number of interface cards may be used depending on the implementation. </paragraph>
<paragraph id="P-0042" lvl="0"><number>&lsqb;0042&rsqb;</number> In this example, storage subsystem <highlight><bold>200</bold></highlight> is a shared virtual array. Storage subsystem <highlight><bold>200</bold></highlight> is a virtual storage system in that each physical storage device in storage subsystem <highlight><bold>200</bold></highlight> may be represented to a data processing system, such as client <highlight><bold>108</bold></highlight> in <cross-reference target="DRAWINGS">FIG. 1</cross-reference>, as a number of virtual devices. In this example, storage devices <highlight><bold>202</bold></highlight> are a set of disk drives set up as a redundant array of independent disks (RAID) system. Of course, other storage devices may be used other than disk drives. For example, optical drives may be used within storage devices <highlight><bold>202</bold></highlight>. Further, a mixture of different device types may be used, such as, disk drives and tape drives. </paragraph>
<paragraph id="P-0043" lvl="0"><number>&lsqb;0043&rsqb;</number> Data being transferred between interfaces <highlight><bold>204</bold></highlight> and <highlight><bold>206</bold></highlight> and storage devices <highlight><bold>202</bold></highlight> are temporarily placed into cache memory <highlight><bold>208</bold></highlight>. Additionally, cache memory <highlight><bold>208</bold></highlight> may be accessed by processors <highlight><bold>210</bold></highlight>-<highlight><bold>224</bold></highlight>, which are used to handle reading and writing data for storage devices <highlight><bold>202</bold></highlight>. Shared memory <highlight><bold>226</bold></highlight> is used by processors <highlight><bold>210</bold></highlight>-<highlight><bold>224</bold></highlight> to handle and track the reading and writing of data to storage devices <highlight><bold>202</bold></highlight>. </paragraph>
<paragraph id="P-0044" lvl="0"><number>&lsqb;0044&rsqb;</number> The present invention manages virtual storage facilities comprising an organization of computer equipment, for example, a host network, data transfer means, storage controller means, permanent storage means and attachment means connecting these devices together. The data storage facilities also may include management information associated with data units such that the management information provides an inventory of capabilities with upper and lower boundaries that may limit the options available to store the data and still meet a user&apos;s criteria. For purposes of this application, a data unit is a logical entity known to a owning entity that is composed of a number of data elements and meta-data and a data element is a grouping of data bits or bytes that the subsystem chooses to manage as a consistent set. Such management information may be independent of attributes of or characteristics of the devices in the physical storage subsystem actually used to store the data elements, but may consist of imputed associations with those attributes through, for example, changeable rule sets, processes or algorithms. These rule sets, processes or algorithms may be changed by user demand or via processes, that may monitor data unit usage and manipulation. The storage of data elements may be adjusted to comply with modifications in, for example, the rules sets, processes or algorithms. </paragraph>
<paragraph id="P-0045" lvl="0"><number>&lsqb;0045&rsqb;</number> In addition, the present invention may include such management information processing with respect to storage device attributes which may include, for example, empirically derived relationships that may infer boundaries, explicitly stated relationships that may stipulate boundaries, relationships that may exist only on demand and combinations of standard storage subsystem relationships such as, for example, RAID in all its forms and hierarchical storage management (HSM) in all its forms. Also, relation of the management information and the subsystem device attributes may be modified resulting in the storage of the data units having to be adjusted in which such a change of relations between the management information and the subsystem attributes include encapsulated logic. </paragraph>
<paragraph id="P-0046" lvl="0"><number>&lsqb;0046&rsqb;</number> The relation between the management information and the subsystem device attributes may also include attributes of implied storage devices not present in the physical subsystem. The relation between the management information and the subsystem device attributes may also include apparently mutual exclusive sets of criteria, for example, criteria satisfied by multiple instances of data storage and criteria satisfied by storage of data at multiple layers on the storage subsystem. The relation between the management information and the subsystem device attributes may also be conditionally applied, such as, for example, between a specified criteria and a default criteria and between a plurality of specified criteria. </paragraph>
<paragraph id="P-0047" lvl="0"><number>&lsqb;0047&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 3</cross-reference> is an exemplary block diagram of the conceptual relationship between the virtual device structures and the logical device structures in accordance with a preferred embodiment of the present invention. The present invention provides a subsystem level application program interface (API) <highlight><bold>312</bold></highlight> from host system <highlight><bold>302</bold></highlight> which allows a user to construct data unit definitions or virtual devices. These data unit definitions or virtual devices, such as, for example, virtual device structures <highlight><bold>304</bold></highlight>, <highlight><bold>306</bold></highlight> and <highlight><bold>308</bold></highlight>, may be called &ldquo;Virtual Device Structures&rdquo; (VDS). A subsystem in turn will implement logical device structures with mapping functions <highlight><bold>310</bold></highlight> and mapping VDSs into the physical world managed by the subsystem. The data may be mapped into networked storage subsystem <highlight><bold>318</bold></highlight> which may consist of logical definitions <highlight><bold>312</bold></highlight>, <highlight><bold>314</bold></highlight> and <highlight><bold>316</bold></highlight>. Networked storage subsystem <highlight><bold>318</bold></highlight> may also consist of storage units <highlight><bold>324</bold></highlight> and <highlight><bold>326</bold></highlight> in which the data is stored. Also, data may be stored in technology storage subsystem <highlight><bold>320</bold></highlight> which may be a RAID and in physical storage devices <highlight><bold>322</bold></highlight>. </paragraph>
<paragraph id="P-0048" lvl="0"><number>&lsqb;0048&rsqb;</number> VDSs may be defined by requesting the use of performance structures like striping, redundancy structures like mirroring and demand copies, and location structures like remote location of copies or archive copies, either alone or in combination. These VDSs also may have scheduling and synchronizing information that allow complete policies to be defined within the structure. Multiple technology selections may also be used, for example, disk and tape in the same virtual device structure. The ability to modify structure rules and the ability to adjust already stored data to the new rules is also provided. </paragraph>
<paragraph id="P-0049" lvl="0"><number>&lsqb;0049&rsqb;</number> A VDS may include a subsystem virtual device definition table which may consist of, for example, the following: </paragraph>
<paragraph id="P-0050" lvl="7"><number>&lsqb;0050&rsqb;</number> Virtual Definition 1: </paragraph>
<paragraph id="P-0051" lvl="2"><number>&lsqb;0051&rsqb;</number> Performance requirements: </paragraph>
<paragraph id="P-0052" lvl="3"><number>&lsqb;0052&rsqb;</number> a) sustainable data transfer rate </paragraph>
<paragraph id="P-0053" lvl="3"><number>&lsqb;0053&rsqb;</number> b) sustainable start input output (SIO) commands per second </paragraph>
<paragraph id="P-0054" lvl="3"><number>&lsqb;0054&rsqb;</number> c) parallel SIO </paragraph>
<paragraph id="P-0055" lvl="2"><number>&lsqb;0055&rsqb;</number> Availability requirements </paragraph>
<paragraph id="P-0056" lvl="3"><number>&lsqb;0056&rsqb;</number> a) time to first accessibility of data </paragraph>
<paragraph id="P-0057" lvl="3"><number>&lsqb;0057&rsqb;</number> b) time to hold off new users for consistency checks </paragraph>
<paragraph id="P-0058" lvl="2"><number>&lsqb;0058&rsqb;</number> Reliability requirements </paragraph>
<paragraph id="P-0059" lvl="3"><number>&lsqb;0059&rsqb;</number> a) allowed probability of data block loss </paragraph>
<paragraph id="P-0060" lvl="3"><number>&lsqb;0060&rsqb;</number> b) allowed probability of data file loss </paragraph>
<paragraph id="P-0061" lvl="2"><number>&lsqb;0061&rsqb;</number> Capacity Management requirements </paragraph>
<paragraph id="P-0062" lvl="3"><number>&lsqb;0062&rsqb;</number> a) maximum size of data unit </paragraph>
<paragraph id="P-0063" lvl="0"><number>&lsqb;0063&rsqb;</number> The definition of Performance requirements, Availability requirements, Reliability requirements and Capacity Management requirements (PARC) for each data unit is available to the owning entity to interpret and to modify the entity. The owning entity may:  
<table-cwu id="TABLE-US-00001">
<number>1</number>
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="1" colwidth="56PT" align="center"/>
<colspec colname="2" colwidth="161PT" align="left"/>
<thead>
<row>
<entry></entry>
</row>
<row><entry namest="1" nameend="2" align="center" rowsep="1"></entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry>1)</entry>
<entry>share access to the data definition with or</entry>
</row>
<row>
<entry></entry>
<entry>without the data;</entry>
</row>
<row>
<entry>2)</entry>
<entry>allow the data definition to be associated with</entry>
</row>
<row>
<entry></entry>
<entry>the data;</entry>
</row>
<row>
<entry>3)</entry>
<entry>allow the data definition to be distributed</entry>
</row>
<row>
<entry></entry>
<entry>with the data; and</entry>
</row>
<row>
<entry>4)</entry>
<entry>make a copy of the definition and have more</entry>
</row>
<row>
<entry></entry>
<entry>than one definition for the same data unit,</entry>
</row>
<row>
<entry></entry>
<entry>wherein</entry>
</row>
<row>
<entry></entry>
<entry>a) the copy process may modify one or more of</entry>
</row>
<row>
<entry></entry>
<entry>the extant definitions and expect the subsystem</entry>
</row>
<row>
<entry></entry>
<entry>to make the necessary changes so that the data</entry>
</row>
<row>
<entry></entry>
<entry>unit will comply with all definitions;</entry>
</row>
<row>
<entry></entry>
<entry>b) the copy process may distribute data units</entry>
</row>
<row>
<entry></entry>
<entry>or portions thereof with selected definitions;</entry>
</row>
<row>
<entry></entry>
<entry>and</entry>
</row>
<row>
<entry></entry>
<entry>c) the copy process may distribute data units</entry>
</row>
<row>
<entry></entry>
<entry>or portions thereof with selected subsets of</entry>
</row>
<row>
<entry></entry>
<entry>the full definition.</entry>
</row>
<row><entry namest="1" nameend="2" align="center" rowsep="1"></entry>
</row>
</tbody>
</tgroup>
</table>
</table-cwu>
</paragraph>
<paragraph id="P-0064" lvl="0"><number>&lsqb;0064&rsqb;</number> Storage performance, availability, reliability and capacity systems (PARCs) are dynamic subsystems that support flexible definitions of data storage requirements at the data level. The present invention is based on providing a subsystem level application program interface (API) that allows a user to request or imply a demand for the use of data storage capabilities. Such data storage capabilities may be defined by requesting capabilities associated with data units that may invoke the use of performance structures like, for example, striping, redundancy structures like mirroring and demand copies, and location or availability structures like, for example, remote location of copies or tape archives copies. These capabilities may also have scheduling and synchronizing information that may allow complete policies to be defined and associated with individual data units or sets of data units. </paragraph>
<paragraph id="P-0065" lvl="0"><number>&lsqb;0065&rsqb;</number> Therefore, the present invention anticipates future requirements by matching the definition associated with a data unit to a logical device definition with expanded capabilities, for example, using multiple sets of stripe groups to effect the availability of providing at a later date the performance of wider stripes than originally implied by the performance requested, using more layers or copies of redundancy data to later provide the ability to improve the reliability when specifications change and become higher than originally required, and actually making additional copies of the data on devices that employ different technologies possibly even in remote locations. </paragraph>
<paragraph id="P-0066" lvl="0"><number>&lsqb;0066&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 4</cross-reference> is an illustration of the use of multiple RAID groups for providing potential for future higher performance requests in accordance with a preferred embodiment of the present invention. <cross-reference target="DRAWINGS">FIG. 4</cross-reference> shows the use of multiple RAID groups where data may be read in parallel for records n<highlight><subscript>j </subscript></highlight>through n<highlight><subscript>k </subscript></highlight>from RAID stripe group A <highlight><bold>402</bold></highlight> and then records n<highlight><subscript>k&plus;1 </subscript></highlight>through n<highlight><subscript>x </subscript></highlight>may be read in parallel from RAID stripe group B <highlight><bold>404</bold></highlight> and then records n<highlight><subscript>x&plus;1 </subscript></highlight>through n<highlight><subscript>y </subscript></highlight>may be read in parallel from RAID stripe group C <highlight><bold>406</bold></highlight> and then cycling back to RAID stripe group A <highlight><bold>402</bold></highlight> for the next set of records n<highlight><subscript>y&plus;1 </subscript></highlight>through n<highlight><subscript>z</subscript></highlight>. Later if performance requirements demand higher throughput records n<highlight><subscript>j </subscript></highlight>through n<highlight><subscript>x </subscript></highlight>may be read in parallel from RAID stripe group A <highlight><bold>402</bold></highlight> and RAID stripe group B <highlight><bold>404</bold></highlight> simultaneously or records n<highlight><subscript>j </subscript></highlight>through n<highlight><subscript>y </subscript></highlight>from RAID stripe group A <highlight><bold>402</bold></highlight>, RAID stripe group B <highlight><bold>404</bold></highlight>, and RAID stripe group C <highlight><bold>406</bold></highlight> simultaneously. </paragraph>
<paragraph id="P-0067" lvl="0"><number>&lsqb;0067&rsqb;</number> All RAID stripe groups may be read at once up to the point of anticipated performance requirements. If all RAID stripe groups are read at once, but the system does not meet a newly imposed performance requirement, then the data may be rewritten to a higher performance capability. The present invention also provides a facility for reviewing and modifying or adjusting the interpretation of &ldquo;appropriate&rdquo; data storage characteristics after the data element has already been stored. The specific way in which the host systems use the data will imply additional requirements initially not specified. These new requirements may be added to the overall specification and the implementation changed to accommodate the changes. </paragraph>
<paragraph id="P-0068" lvl="0"><number>&lsqb;0068&rsqb;</number> For example, the characteristics for a data unit may be historically maintained in a meta-data record associated with that data unit and may be updated as the use of the data is monitored. Updates may then trigger subsystem activity to modify the stored characteristics for the data unit. For example, the subsystem may note that a specific portion of the data is referenced in concert with another portion and as a consequence will set staging control meta-data that will fetch the anticipated data when the companion data is accessed. In addition, a facility for accepting new specifications for data storage characteristics after the data unit has been stored is provided. The ability for modifying where and/or how an already stored data unit is managed is provided, including, but not limited to the subsystem actually changing where and/or how the associated data elements are stored. The modification of data element storage may be required to meet newly interpreted or specified data unit storage characteristics. When new requirements are imposed on a set of data units and the system has not anticipated the requirements, the present invention builds a new logical device definition from the specified or interpreted data storage characteristics. </paragraph>
<paragraph id="P-0069" lvl="0"><number>&lsqb;0069&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 5</cross-reference> is a flowchart illustrating a data unit/virtual device structure data processing methodology in accordance with a preferred embodiment of the present invention. A top down approach may be used by building towards characteristics of known physical device types. For a collection of data elements with a virtual data unit address understood by host system (step <highlight><bold>502</bold></highlight>) the data unit virtual device structure address is processed (step <highlight><bold>504</bold></highlight>). The assigned virtual address communicated to the subsystem may be the same as or different from the virtual data unit address that is known to the host system. The data unit/VDS requirements interpretation is processed (step <highlight><bold>506</bold></highlight>), then the data units/VDS requirements are processed to map methodologies for implementation (step <highlight><bold>508</bold></highlight>). Then the storage subsystem selection processing for the data unit/VDS identifies which storage implementation methodologies are mapped to which potential subsystems and selections for subsystem use are made (step <highlight><bold>510</bold></highlight>). Virtual data units are then communicated to the storage subsystem or subsystems (step <highlight><bold>512</bold></highlight>). Each storage subsystem creates a logical device structure to map the virtual data unit (step <highlight><bold>514</bold></highlight>). </paragraph>
<paragraph id="P-0070" lvl="0"><number>&lsqb;0070&rsqb;</number> Management interface <highlight><bold>516</bold></highlight> may manage data unit requirements inventory <highlight><bold>522</bold></highlight>, storage methodologies inventory <highlight><bold>518</bold></highlight> and receives and provides input from/to storage subsystem capabilities inventory <highlight><bold>520</bold></highlight>. Data unit requirements inventory receives input from data unit virtual device structure address processing (step <highlight><bold>504</bold></highlight>) and storage subsystem selection in processing data unit/VDS storage implementation methodologies when such methodologies are mapped to potential subsystems (step <highlight><bold>510</bold></highlight>). Storage methodologies inventory <highlight><bold>518</bold></highlight> receives input from data and provides input to data units/VDS requirements to implement methodologies processing (step <highlight><bold>508</bold></highlight>). </paragraph>
<paragraph id="P-0071" lvl="0"><number>&lsqb;0071&rsqb;</number> With storage virtualization, a host server is freed from the restrictions of actual storage mechanisms. Furthermore, the actual storage mechanism is freed from the restrictions of the presentation to the host server. Data storage is presented to the host server as an emulation of some device or media type or model. The data may actually be stored on one or more different types of devices and/or media. While storage management is concerned with physical characteristics of storage systems, devices and media, storage virtualization is concerned with masking the physical characteristics of storage systems and taking the control of these physical characteristics from the user or system administrator. </paragraph>
<paragraph id="P-0072" lvl="0"><number>&lsqb;0072&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 6</cross-reference> is an exemplary block diagram of the management API branch illustrated in <cross-reference target="DRAWINGS">FIG. 5</cross-reference> in accordance with a preferred embodiment of the present invention. In this example, Management API <highlight><bold>610</bold></highlight> may receive input from operations <highlight><bold>602</bold></highlight>, host <highlight><bold>604</bold></highlight> or through vendors updating existent capabilities <highlight><bold>608</bold></highlight>, which may be via a distributed data processing system, such as, for example, internet <highlight><bold>606</bold></highlight>. PARC system management <highlight><bold>612</bold></highlight> provides input and provides output to/from management API <highlight><bold>610</bold></highlight>. PARC system management <highlight><bold>612</bold></highlight> receives input from inventory of data unit requirements <highlight><bold>614</bold></highlight> along with inventory of storage methodologies <highlight><bold>616</bold></highlight> and inventory of storage subsystem capabilities <highlight><bold>618</bold></highlight>. Inventory of storage subsystem capabilities may be made up of existent storage subsystem capabilities <highlight><bold>620</bold></highlight> and installed storage subsystem capabilities <highlight><bold>622</bold></highlight>. If a data unit requirement or a storage methodology requires a particular storage subsystem capability, it needs to be determined as to whether the storage subsystem capability actually exists and, if so, whether the capability is actually installed on an available subsystem. If the storage subsystem capability is actually installed on an available subsystem, the required capability may be provided to satisfy data unit requirements <highlight><bold>614</bold></highlight> and/or implement a storage methodology <highlight><bold>616</bold></highlight>. However, if the data unit requirement or the storage methodology finds no capability existent within the inventory of storage subsystem capabilities, the data unit requirement and/or the storage methodology may request updates to subsystem capabilities <highlight><bold>618</bold></highlight> by way of vendor update existent capabilities <highlight><bold>608</bold></highlight>. </paragraph>
<paragraph id="P-0073" lvl="0"><number>&lsqb;0073&rsqb;</number> Furthermore, operations may be advised when existent capabilities provide a superior solution over that provided by the installed capabilities. Operations may also be informed when no solution is available utilizing the installed capabilities but may be made available via existent but not installed capabilities. Then operations may be advised when no solution may be found for the stated requirements. </paragraph>
<paragraph id="P-0074" lvl="0"><number>&lsqb;0074&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 7</cross-reference> is an exemplary diagram of a virtual track table and a track number table. Virtual track table <highlight><bold>702</bold></highlight> and track number table <highlight><bold>704</bold></highlight> are tables that may be used in a storage subsystem by a processor, such as processors <highlight><bold>210</bold></highlight>-<highlight><bold>224</bold></highlight> in <cross-reference target="DRAWINGS">FIG. 2</cross-reference>. Some of the mapping schemes force the manipulation of many pointers in order to perform operations on large sets of mapped data. Some mapping schemes also force the allocation of mapping tables for all possible virtual addresses whether or not those addresses are actually used. <cross-reference target="DRAWINGS">FIG. 7</cross-reference> depicts prior art which is extended by the present invention. </paragraph>
<paragraph id="P-0075" lvl="0"><number>&lsqb;0075&rsqb;</number> The present invention also provides a system using a multi-layer virtual mapping tree method that provides very fast access to the mapped data locations and still minimizes the storage required for the mapping tables themselves. The multiple-layer tables allow fast lookup algorithms, but may allow only the mapping table units that point to allocated storage units to be instantiated, therefore, saving mapping table space. These multi-layer mapping tables, in the case of data copies, also allow only the mapping table units for changed data to be instantiated, again saving mapping table space. </paragraph>
<paragraph id="P-0076" lvl="0"><number>&lsqb;0076&rsqb;</number> In addition, the present invention provides for using a hidden copy/snap copy scheme where the mapped units that are involved in the snap copy are tracked in multiple ways, for example using bit maps, pointer tables, and multilayer bit maps, thereby reducing the table entries to be manipulated to perform snapshot-like operations. An improvement for some workloads and types of data is to use dynamically assigned pointer ranges to track the snap copy data, thereby possibly using less storage than bit maps. Furthermore, the present invention provides using manipulation of entire sections (i.e. subtrees) of the multi-layer virtual mapping tree to speed up operation on large sets of data and to allow additional functions that may be time consuming using other methods. </paragraph>
<paragraph id="P-0077" lvl="0"><number>&lsqb;0077&rsqb;</number> To achieve the above objectives of the present invention, the original multi-layer map tree may be modified to add map table meta-data. After modification of the original multi-layer map tree a map table section separation may be performed both in the horizontal and vertical directions. Also, the operations of promoting and demoting sections of the map table (subtrees) may be added. Therefore, this gives the present invention the added flexibility of operation and increased access to data locations while saving on map table space. </paragraph>
<paragraph id="P-0078" lvl="0"><number>&lsqb;0078&rsqb;</number> The present invention manages virtual storage facilities comprising an organization of computer equipment, for example, a host network, data transfer means, storage controller means and permanent storage means and attachment means connecting these devices together. The computer storage subsystem may be organized using multiple layers of mapping tables which may provide unique identification of the storage location of the data such that individual entries in the mapping tables are variable and may be made self-defining with respect to the amount of data managed. The layers of the tables are variable and may be made self-defining as to existence and may be implemented on a piecemeal basis. The existence of individual tables or parts of individual tables is variable and such that actual presence of any of the table information is by demand. </paragraph>
<paragraph id="P-0079" lvl="0"><number>&lsqb;0079&rsqb;</number> The present invention may also further include a range of data wherein the range of data management is correlated to the layer of the mapping tables addressed. The coordination may be, for example, an algorithm, via a pointer system, via a pointer to correlation logic or via a tree structure. The range of data managed may also be independent of the layer of the tables accessed. Mapping consistency is managed algorithmically or mapping consistency is managed via pointers to boundary information. The boundary information may include, for example, description of size of data units mapped, a description for a like set of entries, a unique description for each entry, a specified default size for a set of entries, including exception flags for modified entries and a bit map. The description of the size of the data units mapped may be by way of a pointer with an address range or a pointer with a unit size. Mapping consistency may be managed via a combination of algorithms, boundary information, and pointers to boundary information. Multiple layers may include a first level of management directing to one or more intermediate levels of management, thereby directing to a final level of management, which may provide the necessary controls to directly access the data. </paragraph>
<paragraph id="P-0080" lvl="0"><number>&lsqb;0080&rsqb;</number> A means to indicate that the virtual address space mapped by an entry is not known, not used or not allocated may also be included. Therefore, individual tables in the mapping may then be able to be paged to secondary storage and brought into primary storage when needed. Tables for unallocated space may not be instantiated at all until the unallocated space is used or allocated. Boundary information may consist of, for example, fixed mapping wherein every entry in the table has the same extent, i.e. range of virtual space, and location which may be computed, variable mapping in which every entry in the table is unique and default variable in which there is a default extent size and a map of which entries are exceptions. </paragraph>
<paragraph id="P-0081" lvl="0"><number>&lsqb;0081&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 8</cross-reference> is an exemplary illustration of a hierarchical relationship of a mapping table in accordance with a preferred embodiment of the present invention. A map may be made up of several layers. These layers create a hierarchy used to facilitate the location of an entry that may describe the actual location of the data. Each layer points to a finer granularity than the layer below it. For example, the level 1 vector table <highlight><bold>804</bold></highlight> entries each describe, for example eight gigabytes of a Logical Unit Number (LUN) address, each level 1 entry points to level 2 vector table <highlight><bold>806</bold></highlight> whose entries each describe, for example, eight megabytes of a LUN address. The amount of space required to store layers <highlight><bold>802</bold></highlight> and <highlight><bold>804</bold></highlight> is small enough in this example so that dedicated memory may be set aside to hold them, thereby ensuring that any access requiring data stored in either level <highlight><bold>802</bold></highlight> or <highlight><bold>804</bold></highlight> will be found. Therefore, hits at these levels (<highlight><bold>802</bold></highlight> &amp; <highlight><bold>804</bold></highlight>) will speed the processing of the mapping tables. </paragraph>
<paragraph id="P-0082" lvl="0"><number>&lsqb;0082&rsqb;</number> In this example, LUN table <highlight><bold>802</bold></highlight> is a vector table with, for example, 256 entries, in which LUN table <highlight><bold>802</bold></highlight> is indexed by combining target and LUN addresses. There is one entry for each target and LUN combination. The entry contains a vector to the next table in level 1 vector table <highlight><bold>804</bold></highlight> layer or contains a null value if no target address of LUN address has been created. LUN table <highlight><bold>802</bold></highlight>, in this example, requires 1024 bytes of memory. LUN table <highlight><bold>802</bold></highlight> may be pinned in memory. </paragraph>
<paragraph id="P-0083" lvl="0"><number>&lsqb;0083&rsqb;</number> Level 1 vector table <highlight><bold>804</bold></highlight> contains, in this example, 256 entries that represent the LUN eight gigabyte segments. Level 1 vector table <highlight><bold>804</bold></highlight> is indexed by using the most significant byte or bits <highlight><bold>31</bold></highlight>-<highlight><bold>24</bold></highlight> of the logical block address. Each entry either contains a vector to level 2 table <highlight><bold>806</bold></highlight> or contains a null value. While the space to store all the level 1 <highlight><bold>804</bold></highlight> pointers is reserved, in this example, for 256 entries, level 2 table <highlight><bold>806</bold></highlight> is only populated with enough entries to represent the size of the host LUN. That is, if the host LUN has a capacity of, for example, 50 gigabytes, there may be seven entries in level 1 vector table <highlight><bold>804</bold></highlight>. Level 1 vector table <highlight><bold>804</bold></highlight> requires, for example, up to 256 K of memory. Level 1 vector table <highlight><bold>804</bold></highlight> is also pinned in memory. </paragraph>
<paragraph id="P-0084" lvl="0"><number>&lsqb;0084&rsqb;</number> Level 2 vector table <highlight><bold>806</bold></highlight> contains, in this example, 1024 entries and is indexed by bits <highlight><bold>23</bold></highlight>-<highlight><bold>14</bold></highlight> of the logical block address. The entries in level 2 vector table <highlight><bold>806</bold></highlight> may contain either a pointer to a control block table or a null value. As each level 2 vector table <highlight><bold>806</bold></highlight> represents, for example, eight gigabytes of LUN address, a null value may be present for addressed that exceed the capacity of the LUN up to, for example, eight gigabytes of boundary. Each entry in level 2 vector table <highlight><bold>806</bold></highlight> represents, for example, eight megabytes of LUN address. Level 2 vector table <highlight><bold>806</bold></highlight> may require, for example, 4096 bytes of memory and is pageable. Level 2 vector table <highlight><bold>806</bold></highlight> may have a higher priority than control block table <highlight><bold>808</bold></highlight> and may only be swapped out of the table memory space to make room for more table information when necessary (i.e., when no lower level table information is available to be swapped out). </paragraph>
<paragraph id="P-0085" lvl="0"><number>&lsqb;0085&rsqb;</number> The lowest layer, in this example, in the map is control block table <highlight><bold>808</bold></highlight>. Control block table <highlight><bold>808</bold></highlight> is made up of, for example, 256 control block entries. Bits <highlight><bold>13</bold></highlight>-<highlight><bold>6</bold></highlight> of the logical block address are used as an index into control block table <highlight><bold>808</bold></highlight>. Control block table, in this example, represents eight megabytes of the LUN address. Each control block table <highlight><bold>808</bold></highlight> requires, for example, 4096 bytes of memory. Control block table <highlight><bold>808</bold></highlight> is pageable and may have a lower priority than level 2 vector table <highlight><bold>806</bold></highlight> and may be swapped out of the mapping table memory space to make room for more entries (e.g., other level <highlight><bold>808</bold></highlight> entries) before level 2 vector table <highlight><bold>806</bold></highlight> is swapped. Control block table <highlight><bold>808</bold></highlight> may be swapped on a LRU basis. <cross-reference target="DRAWINGS">FIG. 9</cross-reference> is an exemplary diagram of a portion of a mapping table describing an address range with four distinct sections in accordance with a preferred embodiment of the present invention. Hashing algorithms are a well known mechanism for storage subsystems to manage space and resolve an input address to a physical storage location. Hash algorithm 1 <highlight><bold>902</bold></highlight> and hash algorithm 2 <highlight><bold>904</bold></highlight> are serially implemented algorithms that may be used in a storage subsystem by a processor, such as processors <highlight><bold>210</bold></highlight>-<highlight><bold>224</bold></highlight> in <cross-reference target="DRAWINGS">FIG. 2</cross-reference>. Hash algorithm 2 <highlight><bold>904</bold></highlight> may resolve to several sections. Each section may in turn be mapped using different mapping techniques, such as, for example, load point and offset section <highlight><bold>906</bold></highlight>, not allocated section <highlight><bold>908</bold></highlight> log-structured file (LSF) section <highlight><bold>910</bold></highlight> and RAID section <highlight><bold>912</bold></highlight>. These mapping techniques, load point and offset, log-structured files, using a pointer and length for unallocated space, RAID, and the like, are well known mapping techniques in the computer industry. The present invention is directed to a mechanism for combining mapping techniques and dynamically modifying the specific mapping technique used for a particular subset of data. Such dynamic modification may include changing the mapping technique from one technique to another, then to possibly to another, or perhaps back to a previously used technique. </paragraph>
<paragraph id="P-0086" lvl="0"><number>&lsqb;0086&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 10</cross-reference> is an exemplary block diagram of a multi-layer mapping table in accordance with a preferred embodiment of the present invention. Hash algorithm 1 <highlight><bold>1002</bold></highlight> produces hash values as output. Logical address in <highlight><bold>1006</bold></highlight> is input into hash table <highlight><bold>1002</bold></highlight>. Hash algorithm 1 <highlight><bold>1002</bold></highlight> may not require modification due to dynamic changes to the mapping. Hash algorithm 1 <highlight><bold>1002</bold></highlight> may only need modification to reflect logical device definition changes. Hash value out <highlight><bold>1008</bold></highlight> is input into hash algorithm 2 <highlight><bold>1004</bold></highlight>. Hash algorithm 2 <highlight><bold>1004</bold></highlight> may be modified as the mapping changes. Hash value output <highlight><bold>1010</bold></highlight> from hash algorithm 2 <highlight><bold>1004</bold></highlight> is input into pointer table <highlight><bold>1011</bold></highlight>. Pointer table <highlight><bold>1011</bold></highlight> consists of pointer entries <highlight><bold>1012</bold></highlight>-N. The pointer changes as the mapping changes. Hash algorithm 2 <highlight><bold>1004</bold></highlight> and pointer table <highlight><bold>1011</bold></highlight> are held consistent with respect to range. Also included are mapping table endpoints <highlight><bold>1028</bold></highlight>, <highlight><bold>1030</bold></highlight>, <highlight><bold>1032</bold></highlight>, <highlight><bold>1034</bold></highlight>, <highlight><bold>1036</bold></highlight> and N&prime;. Mapping table endpoints <highlight><bold>1028</bold></highlight>-N&prime; can be omitted, for example, in the case of unallocated space. Mapping table endpoints <highlight><bold>1028</bold></highlight>-N&prime; may be a single entry or a complex set of entries and further logic, for example, such as hash algorithm 3 <highlight><bold>1034</bold></highlight> which produces hash output <highlight><bold>1040</bold></highlight> which is an input to hash table <highlight><bold>1044</bold></highlight>. The total number of mapping table entry points may vary dynamically during use as the table entries are modified. </paragraph>
<paragraph id="P-0087" lvl="0"><number>&lsqb;0087&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 11</cross-reference> is an exemplary illustration of FlexRAID in accordance with the preferred embodiment of the present invention. FlexRAID allows a controller software to place RAID stripes or mirrors across any of the drives attached to the controller software. Conventional RAID systems bind sets of drives together to form a RAID set. These Raid sets may determine the RAID level for the LUN they represent. For example, a set of five drives may be bound together in a RAID set. This RAID set of five drives may be presented to a host as a LUN structure and a self-describing RAID stripe or mirror. </paragraph>
<paragraph id="P-0088" lvl="0"><number>&lsqb;0088&rsqb;</number> In an architecture with a paging LSF map structure, data locations referenced by a computing system may not be updated in a fixed location. Data is located by a translation table identifying the current residency address of the data. This permits the data to be relocated anywhere in the subsystem while maintaining a logically consistent address to the host computer system. </paragraph>
<paragraph id="P-0089" lvl="0"><number>&lsqb;0089&rsqb;</number> By expanding on this concept, a RAID group may be thought of as a single stripe with a RAID type attribute. For example, each RAID stripe written may be thought of as a RAID stripe group. Therefore, since static binding of disk drives may not be required, a collection of disk drives may be scattered on a &ldquo;best fit algorithm&rdquo; across any or all members of the storage pool. Thus, disk drives may be added to the storage pool without requiring conformance to disk geometry or topology attributes. </paragraph>
<paragraph id="P-0090" lvl="0"><number>&lsqb;0090&rsqb;</number> The present invention obtains a unit of allocation for the RAID stripe from a set of disk drives. The set may be the minimal number of disks necessary for the required stripe size. However, unlike conventional RAID disk subsystems, the set may include a grouping of disks, none of which is the same type and model as the others. The stripe may be spread across the subset of the set of drives where the subset consists of the required number of units for the RAID stripe. The next stripe written may include some of the drives for the previous stripe, but also some drives in the domain which may better satisfy a space and load balance fit. </paragraph>
<paragraph id="P-0091" lvl="0"><number>&lsqb;0091&rsqb;</number> In this example, a set of disk drives, such as, disk drives <highlight><bold>1110</bold></highlight>, <highlight><bold>1120</bold></highlight>, <highlight><bold>1130</bold></highlight>, <highlight><bold>1140</bold></highlight>, <highlight><bold>1150</bold></highlight> and <highlight><bold>1160</bold></highlight> are shown. In this example, disk drive <highlight><bold>1110</bold></highlight> has a capacity of <highlight><bold>500</bold></highlight> allocation units. The other disk drives <highlight><bold>1120</bold></highlight>-<highlight><bold>1160</bold></highlight> have a capacity of 1000 allocation units of space. A RAID stripe, for example, RAID stripe <highlight><bold>1</bold></highlight> <highlight><bold>1111</bold></highlight>, is a 4&plus;1 RAID 4 group. Four allocation units for data, for example, stripe locations <highlight><bold>1111</bold></highlight>, <highlight><bold>1121</bold></highlight>, <highlight><bold>1131</bold></highlight> and <highlight><bold>1141</bold></highlight>, each on a different disk drive, are combined with 1 allocation unit for parity data at location <highlight><bold>1151</bold></highlight> to be written as one stripe. The data units are not required to be symmetrical in location, such as, occupying the same physical addresses on the disk. Each unit is maintained as a discrete unit and not tied geometrically to the other units. </paragraph>
<paragraph id="P-0092" lvl="0"><number>&lsqb;0092&rsqb;</number> Following the allocation and RAID stripe <highlight><bold>1</bold></highlight> write, four disk drives, for example, disk drives <highlight><bold>1120</bold></highlight>, <highlight><bold>1130</bold></highlight>, <highlight><bold>1140</bold></highlight> and <highlight><bold>1150</bold></highlight>, have 999 data allocation units free and available for allocation, while disk drive <highlight><bold>1110</bold></highlight> has 499 allocation units free and disk drive <highlight><bold>1160</bold></highlight> still has 1000 allocation units free. Disk drives <highlight><bold>1110</bold></highlight>, <highlight><bold>1120</bold></highlight>, <highlight><bold>1130</bold></highlight>, <highlight><bold>1140</bold></highlight> and <highlight><bold>1150</bold></highlight> each have 1 data allocation unit not available (i.e., allocated and written). </paragraph>
<paragraph id="P-0093" lvl="0"><number>&lsqb;0093&rsqb;</number> Then the next data write operation is initiated for stripe <highlight><bold>2</bold></highlight> also a 4&plus;1 RAID level four group. The space management mechanism acquires five more data allocation units, such as, for example stripe locations <highlight><bold>1122</bold></highlight>, <highlight><bold>1132</bold></highlight>, <highlight><bold>1142</bold></highlight> and <highlight><bold>1152</bold></highlight>, four for data and one for parity data at stripe location <highlight><bold>1161</bold></highlight>. As a result, disk drive <highlight><bold>1160</bold></highlight> with 1000 units available is selected to provide one allocation unit. Following this allocation and RAID stripe write, disk drive <highlight><bold>1110</bold></highlight> has 499 units free for allocation, the next four disk drives, for example, disk drives <highlight><bold>1120</bold></highlight>, <highlight><bold>1130</bold></highlight>, <highlight><bold>1140</bold></highlight> and <highlight><bold>1150</bold></highlight> have 998 units free for allocation and drive <highlight><bold>1160</bold></highlight> has 999 units free for allocation. </paragraph>
<paragraph id="P-0094" lvl="0"><number>&lsqb;0094&rsqb;</number> In a similar fashion, stripe <highlight><bold>3</bold></highlight> may be allocated and written to stripe locations <highlight><bold>1123</bold></highlight>, <highlight><bold>1133</bold></highlight>, <highlight><bold>1143</bold></highlight>, <highlight><bold>1153</bold></highlight> and <highlight><bold>1162</bold></highlight>. In addition, stripe <highlight><bold>4</bold></highlight> may be written as a different RAID group size, for example as a 3&plus;1 RAID group and may be allocated and written to locations <highlight><bold>1124</bold></highlight>, <highlight><bold>1134</bold></highlight>, <highlight><bold>1144</bold></highlight> and <highlight><bold>1163</bold></highlight>. Furthermore, a different RAID level may be used, for example, stripe <highlight><bold>5</bold></highlight> may be written as a mirror (RAID level 1) using locations <highlight><bold>1154</bold></highlight> and <highlight><bold>1164</bold></highlight>. </paragraph>
<paragraph id="P-0095" lvl="0"><number>&lsqb;0095&rsqb;</number> Various algorithms may be used to determine which disk drives and which allocation units on the disk drives are to be allocated for a new stripe. This may include, but is not limited to, the following:  
<table-cwu id="TABLE-US-00002">
<number>2</number>
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="1" colwidth="56PT" align="center"/>
<colspec colname="2" colwidth="161PT" align="left"/>
<thead>
<row>
<entry></entry>
</row>
<row><entry namest="1" nameend="2" align="center" rowsep="1"></entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry>a)</entry>
<entry>the allocation priority may be set by which</entry>
</row>
<row>
<entry></entry>
<entry>disk drives have the most number of free</entry>
</row>
<row>
<entry></entry>
<entry>allocation units;</entry>
</row>
<row>
<entry>b)</entry>
<entry>the highest percentage of free allocation</entry>
</row>
<row>
<entry></entry>
<entry>units;</entry>
</row>
<row>
<entry>c)</entry>
<entry>the allocation units closest to the most recent</entry>
</row>
<row>
<entry></entry>
<entry>previous stripe write;</entry>
</row>
<row>
<entry>d)</entry>
<entry>the allocation may be based on selecting the</entry>
</row>
<row>
<entry></entry>
<entry>highest or lowest performance disk drive first;</entry>
</row>
<row>
<entry>e)</entry>
<entry>prorated allocation based on trying to fill a</entry>
</row>
<row>
<entry></entry>
<entry>certain number of drives such that they all</entry>
</row>
<row>
<entry></entry>
<entry>reach maximum capacity simultaneously</entry>
</row>
<row>
<entry></entry>
<entry>independent of device capacity; or</entry>
</row>
<row>
<entry>e)</entry>
<entry>any combination of the above.</entry>
</row>
<row><entry namest="1" nameend="2" align="center" rowsep="1"></entry>
</row>
</tbody>
</tgroup>
</table>
</table-cwu>
</paragraph>
<paragraph id="P-0096" lvl="0"><number>&lsqb;0096&rsqb;</number> In addition to the allocation of stripes freely across the disk drives in a pool of disk drives, the present invention allows for the stripes to use different RAID configurations. For example, one RAID stripe may be stored as a 4&plus;1 RAID 5 configuration, while the next stripe may be stored as a simple 1&plus;1 mirror, and further, the next stripe may be stored as a 3&plus;1 RAID 4 configuration. </paragraph>
<paragraph id="P-0097" lvl="0"><number>&lsqb;0097&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 12</cross-reference> is an exemplary illustration of a control block entry format and modifications to the control block entry in accordance with a preferred embodiment of the present invention. A control block entry which, for example, may be contained within control block entry table <highlight><bold>808</bold></highlight> in <cross-reference target="DRAWINGS">FIG. 8</cross-reference> may contain the following format: </paragraph>
<paragraph id="P-0098" lvl="0"><number>&lsqb;0098&rsqb;</number> Bytes <highlight><bold>0</bold></highlight>-<highlight><bold>3</bold></highlight> may contain: </paragraph>
<paragraph id="P-0099" lvl="2"><number>&lsqb;0099&rsqb;</number> LUN <highlight><bold>1202</bold></highlight> is the LUN address of a back-end storage device that contains data; </paragraph>
<paragraph id="P-0100" lvl="2"><number>&lsqb;0100&rsqb;</number> Flags <highlight><bold>1204</bold></highlight> is made of up to four one-bit flags. In this example, only two flags are identified: </paragraph>
<paragraph id="P-0101" lvl="3"><number>&lsqb;0101&rsqb;</number> Update flag which indicates that data in a cache has been updated and the data on the LUN is no longer current </paragraph>
<paragraph id="P-0102" lvl="3"><number>&lsqb;0102&rsqb;</number> Busy flag which indicates the data is in transition and should not be altered at this time; Unused byte <highlight><bold>1206</bold></highlight>; and </paragraph>
<paragraph id="P-0103" lvl="2"><number>&lsqb;0103&rsqb;</number> Number of blocks <highlight><bold>1208</bold></highlight> which is the number of blocks used to contain a 32 K byte entry. Number of blocks <highlight><bold>1208</bold></highlight> is required for compression as it allows variable length entries to be written to a disk. </paragraph>
<paragraph id="P-0104" lvl="0"><number>&lsqb;0104&rsqb;</number> Bytes <highlight><bold>4</bold></highlight>-<highlight><bold>7</bold></highlight> may contain: </paragraph>
<paragraph id="P-0105" lvl="2"><number>&lsqb;0105&rsqb;</number> Logical Block Address <highlight><bold>1210</bold></highlight> which is the Logical Block Address on the data on the LUN. </paragraph>
<paragraph id="P-0106" lvl="0"><number>&lsqb;0106&rsqb;</number> Bytes <highlight><bold>8</bold></highlight>-<highlight><bold>12</bold></highlight> may contain: </paragraph>
<paragraph id="P-0107" lvl="2"><number>&lsqb;0107&rsqb;</number> Cache pointer <highlight><bold>1212</bold></highlight> which is used in conjunction but not in place of the LUN/LBA address. If an entry is in cache, cache pointer <highlight><bold>1212</bold></highlight> contains a pointer to the cache entry. </paragraph>
<paragraph id="P-0108" lvl="0"><number>&lsqb;0108&rsqb;</number> Bytes <highlight><bold>13</bold></highlight>-<highlight><bold>16</bold></highlight> may contain: </paragraph>
<paragraph id="P-0109" lvl="2"><number>&lsqb;0109&rsqb;</number> Unused bytes <highlight><bold>1214</bold></highlight>-<highlight><bold>1220</bold></highlight>. </paragraph>
<paragraph id="P-0110" lvl="0"><number>&lsqb;0110&rsqb;</number> However, in order to provide RAID recovery, the mapping structure may be modified to identify either the parity drive associated with the RAID stripe or another drive in the mirrored set. The modifications to the control block entry may be: </paragraph>
<paragraph id="P-0111" lvl="2"><number>&lsqb;0111&rsqb;</number> Bytes <highlight><bold>0</bold></highlight>-<highlight><bold>3</bold></highlight> may contain in addition to LUN <highlight><bold>1202</bold></highlight>, Flags <highlight><bold>1204</bold></highlight> and Number of Blocks <highlight><bold>1208</bold></highlight> as described above, Parity LUN <highlight><bold>1222</bold></highlight>. In addition, Bytes <highlight><bold>13</bold></highlight>-<highlight><bold>16</bold></highlight> may contain Parity LUN Logical Block Address <highlight><bold>1224</bold></highlight> instead of unused bytes <highlight><bold>1214</bold></highlight>-<highlight><bold>1220</bold></highlight>. Parity LUN <highlight><bold>1222</bold></highlight> is the LUN address of the drive containing either the parity information for the RAID stripe or another drive n the mirrored set containing host information. Parity LUN Logical Block Address <highlight><bold>1224</bold></highlight> is the logical block on PLUN <highlight><bold>1222</bold></highlight> that the parity is written to. </paragraph>
<paragraph id="P-0112" lvl="0"><number>&lsqb;0112&rsqb;</number> Another important feature of FlexRAID is the self-describing nature of data. Each stripe in the stripe contains information that describes the contents of the stripe and the stripe&apos;s relation to other member of the stripe. </paragraph>
<paragraph id="P-0113" lvl="0"><number>&lsqb;0113&rsqb;</number> A difference between FlexRAid and a conventional RAID system is that meta-data is associated with each stripe. As each data stripe is written, the meta-data is prefaced to the stripe. The meta-data written to the parity disk may be actually an XOR&apos;ed parity of other pieces of meta-data. The meta-data may be, for example, one 512 byte block long and may contain the information as shown in <cross-reference target="DRAWINGS">FIG. 13</cross-reference> which is an exemplary meta-data block in accordance with a preferred embodiment of the present invention. </paragraph>
<paragraph id="P-0114" lvl="0"><number>&lsqb;0114&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 14</cross-reference> is an exemplary illustration of a default variable within boundary information in accordance with a preferred embodiment of the present invention. There may be a default extent size and a map of which entries are exceptions <highlight><bold>1401</bold></highlight>. In this example, assume a layer n with table na and entry x. Also, assume a location xx in which a first extent begins. An extent is EE bytes in size, however, in this example, each extent is 100 sectors. Except as noted in an exception map <highlight><bold>1401</bold></highlight>, the location of the extent may be calculated up to an exception point. A map of storage such as map <highlight><bold>1402</bold></highlight>, is implied by the algorithm and may be calculated so map <highlight><bold>1402</bold></highlight> may not even be stored until exceptions exist. </paragraph>
<paragraph id="P-0115" lvl="0"><number>&lsqb;0115&rsqb;</number> Entry <highlight><bold>1406</bold></highlight>, which in this example is 500 bytes in size, is stored in exception size table <highlight><bold>1412</bold></highlight>. In addition, entry <highlight><bold>1408</bold></highlight>, which is 25 bytes in size, is stored in exception size table <highlight><bold>1412</bold></highlight>. Additional entries may be stored in exception size table <highlight><bold>1412</bold></highlight> up to nth entry <highlight><bold>1410</bold></highlight> which is 200 bytes in size. Implied map <highlight><bold>1402</bold></highlight> is implied by a regular mapping every 100 sections up to n&times;100 sections. </paragraph>
<paragraph id="P-0116" lvl="0"><number>&lsqb;0116&rsqb;</number> As shown in <cross-reference target="DRAWINGS">FIG. 14</cross-reference>, actual storage allocation map <highlight><bold>1404</bold></highlight> will require the entries from maps <highlight><bold>1401</bold></highlight> and <highlight><bold>1412</bold></highlight> to interpret addresses. Since the first three entries in map <highlight><bold>1401</bold></highlight> are empty, the first data entry stored in map <highlight><bold>1412</bold></highlight> is entry <highlight><bold>1406</bold></highlight> which, in this example, is 500 bytes in size. Therefore, entry <highlight><bold>1406</bold></highlight> will be stored in map <highlight><bold>1412</bold></highlight> and redirects address calculations beginning at the third 100 sector entry. The data associated with entry <highlight><bold>1406</bold></highlight> will be stored from 300 to 800 in actual storage map <highlight><bold>1404</bold></highlight>. Since the next three entries in map <highlight><bold>1401</bold></highlight> are empty, entry <highlight><bold>1408</bold></highlight> will be stored in map <highlight><bold>1412</bold></highlight> and control addressing beginning at 1100. This takes into account the end of entry <highlight><bold>1406</bold></highlight> which is at 800 in storage allocation map <highlight><bold>1404</bold></highlight> plus the spaces for the next three empty entries in map <highlight><bold>1401</bold></highlight>. </paragraph>
<paragraph id="P-0117" lvl="0"><number>&lsqb;0117&rsqb;</number> Entry <highlight><bold>1408</bold></highlight> is 25 bytes in size. Therefore, data for entry <highlight><bold>1408</bold></highlight> will be stored beginning at 1100 in storage allocation map <highlight><bold>1404</bold></highlight> and ending at 1125 in storage allocation map <highlight><bold>1404</bold></highlight>. Then entry <highlight><bold>1410</bold></highlight> will be stored in map <highlight><bold>1412</bold></highlight>. Since the entries after entry <highlight><bold>1408</bold></highlight> are empty in map <highlight><bold>1401</bold></highlight>, data for nth entry <highlight><bold>1410</bold></highlight> will be stored in storage allocation <highlight><bold>1404</bold></highlight> at a location x times 100 from 1125 which is the end of entry <highlight><bold>1408</bold></highlight> in storage allocation <highlight><bold>1404</bold></highlight>. Nth entry <highlight><bold>1410</bold></highlight> is 200 bytes in size. Therefore, since nth entry <highlight><bold>1410</bold></highlight> is the last entry in map <highlight><bold>1412</bold></highlight>, storage allocation <highlight><bold>1404</bold></highlight> ends at a value of 1125 plus x times 100 plus the 200 bytes represented by nth entry <highlight><bold>1410</bold></highlight>. </paragraph>
<paragraph id="P-0118" lvl="0"><number>&lsqb;0118&rsqb;</number> Therefore, the present invention provides a system using a multi-layer virtual mapping scheme where the mapped units that are involved in managing storage allocation are tracked using a bit map and an exception table, thus reducing the table entries to be manipulated to perform storage management operations. </paragraph>
<paragraph id="P-0119" lvl="0"><number>&lsqb;0119&rsqb;</number> In addition, the present invention provides a mechanism by which instant copies of stored data in a variable dynamically changeable mapping scheme storage device may be made. The mechanism of the present invention may be embodied as hardware, software, or a combination of hardware and software. In a preferred embodiment, the instant copy mechanism of the present invention is implemented as software instructions executed by a processor, such as any one or more of processors <highlight><bold>210</bold></highlight>-<highlight><bold>224</bold></highlight>. Although the principle embodiment is software instructions executed by a processor, it should be clear to those of ordinary skill in the art that the functions of the present invention may be &ldquo;hard coded&rdquo; into hardware modules and components without departing from the spirit and scope of the present invention. </paragraph>
<paragraph id="P-0120" lvl="0"><number>&lsqb;0120&rsqb;</number> In order to clarify the operations and processes of an instant copy using the present invention described hereafter, it is first beneficial to define some terminology that will be used in the description. First, it should be noted that there are three views of the instant copy operation that will need to be considered: (1) the host/server/using system view, (2) the storage system view, and (3) the physical placement/physical addressing view. </paragraph>
<paragraph id="P-0121" lvl="0"><number>&lsqb;0121&rsqb;</number> From the viewpoint of the using system (e.g., a server or application processor) the instant copy is seen as a command to do an operation such as, for example, copy FROM: address A through address B in a data source (this is what is known as the original data) TO: address A&prime; through B&prime; in a data destination (this is what is known as the copy data). However, from the viewpoint of the storage system, these address ranges are inputs to a mapping. </paragraph>
<paragraph id="P-0122" lvl="0"><number>&lsqb;0122&rsqb;</number> A mapping translates or converts from the using system&apos;s &lsquo;virtual&rsquo; address to a management system&apos;s logical structures and then to the storage system &lsquo;physical&rsquo; location(s) (or address(es)) where the data is stored. Note that in a virtual storage system the mapping of the virtual addresses to physical locations may change over time. </paragraph>
<paragraph id="P-0123" lvl="0"><number>&lsqb;0123&rsqb;</number> The physical placement of the original data is described by a mapping mechanism known as the original data map. This identifies the physical storage location used to store the original data. This is not part of the using system view, but is in the storage system view. </paragraph>
<paragraph id="P-0124" lvl="0"><number>&lsqb;0124&rsqb;</number> The physical placement of the copy data is described by a mapping mechanism known as the copy data map. This identifies the physical storage location used to store the copy data. This is not part of the using system view, but is in the storage system view. </paragraph>
<paragraph id="P-0125" lvl="0"><number>&lsqb;0125&rsqb;</number> In addition to the above, the following are some additional definitions to be aware of: </paragraph>
<paragraph id="P-0126" lvl="2"><number>&lsqb;0126&rsqb;</number> The Initial physical storage area is the set of physical storage locations of the original data at the moment of the instant copy operation. It is also the location of the copy data upon the initial execution of the instant copy command. After the execution of the instant copy command/operation, write activity by the using system will cause/require the use of additional physical storage area(s). (Because a write to either the original data or to the copy data causes a difference between the two, and they can no longer be stored as the same physical data, therefore physical storage is required to store, at a minimum, the difference). </paragraph>
<paragraph id="P-0127" lvl="2"><number>&lsqb;0127&rsqb;</number> Additional Physical Storage Areas are the locations of storage space used to accommodate the additional data resulting from the using system (server) writing to either the original data or to the copy data. </paragraph>
<paragraph id="P-0128" lvl="2"><number>&lsqb;0128&rsqb;</number> Old Data is that data that existed and was known as the original data at the moment of the instant copy operation, and is also now associated with the copy data. (i.e., the specific data address, such as a record, has not been the target of a write operation since the instant copy). </paragraph>
<paragraph id="P-0129" lvl="2"><number>&lsqb;0129&rsqb;</number> New data is that data that was written, by the using system, to either the original data or the copy data, subsequent to the instant copy command execution. </paragraph>
<paragraph id="P-0130" lvl="2"><number>&lsqb;0130&rsqb;</number> New original data is that new data that was written, by the using system, specifically to the original data. </paragraph>
<paragraph id="P-0131" lvl="0"><number>&lsqb;0131&rsqb;</number> New copy data is that new data that was written, by the using system, specifically to the copy data. </paragraph>
<paragraph id="P-0132" lvl="2"><number>&lsqb;0132&rsqb;</number> Old original data is data associated with the original data that has not been overwritten (not been changed) by the using system (server) since the instant copy command execution. </paragraph>
<paragraph id="P-0133" lvl="2"><number>&lsqb;0133&rsqb;</number> Old copy data is data associated with the copy data that has not been overwritten (not been changed) by the using system (server) since the instant copy command execution. </paragraph>
<paragraph id="P-0134" lvl="0"><number>&lsqb;0134&rsqb;</number> Having set forth the terminology that will be used in the description of the instant copy operations and processes of the present invention, the following is a description of exemplary embodiments of the instant copy features of the present invention. It should be noted that the following description sets forth exemplary embodiments which are not intended to be limiting but rather illustrative of the present invention. </paragraph>
<paragraph id="P-0135" lvl="0"><number>&lsqb;0135&rsqb;</number> The known mechanisms for making instant copy operations create a copy data map in the following manner. First, a copy of the original data map is made and used for the copy data map. A local mechanism is then used to track the changes to the original map. For example, a bit map may be used that identifies which blocks have been written to. </paragraph>
<paragraph id="P-0136" lvl="0"><number>&lsqb;0136&rsqb;</number> The present invention differs from these known instant copy mechanisms in that at first only a pointer to the original data map is created. Thereafter, as needed, the copy data map, starting as a single pointer, is modified with additional information (meta-data) relating only to those sections which change as write activity occurs. Thus, the present invention reduces the amount of storage space required to make an instant copy since a complete duplicate of the original data map is not required. </paragraph>
<paragraph id="P-0137" lvl="0"><number>&lsqb;0137&rsqb;</number> As mentioned above, mapping of data may take many forms, including load point and offset, log-structured file, and the like. Moreover, with the present invention as described above, different types of mapping may be used within the same virtual storage volume. In other words, the storage device may be a variable dynamically changeable mapping scheme storage device. </paragraph>
<paragraph id="P-0138" lvl="0"><number>&lsqb;0138&rsqb;</number> In order to make an instant copy of data stored in a variable dynamically changeable mapping scheme storage device, the type of mapping used to store the data must be identified and an appropriate copy technique used to copy the data. The present invention provides a mechanism for identifying the mapping used and selecting an appropriate copy technique to copy the data based on the identified mapping. </paragraph>
<paragraph id="P-0139" lvl="0"><number>&lsqb;0139&rsqb;</number> Moreover, the particular copy techniques themselves, are aspects of the present invention that aid in minimizing the amount of meta-data associated with a virtual storage volume. Using these various copy techniques allows multiple instant copy procedures to be performed in parallel, multiple mirror processes to be performed in parallel and allows parallel instant copy operations to be performed on various physical devices. </paragraph>
<paragraph id="P-0140" lvl="0"><number>&lsqb;0140&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 15</cross-reference> is an exemplary diagram of a variable dynamically changeable mapping scheme virtual storage volume in accordance with the present invention. As shown in <cross-reference target="DRAWINGS">FIG. 15, a</cross-reference> plurality of files, i.e. groups of related extents, are stored on virtual volume <highlight><bold>1510</bold></highlight>. Each file is viewed by the host system in a different manner with associated policy constraints. These policy constraints are requirements, placed on the storage system by the using system, that force specific storage outcomes, such as specifically demanding a type or classification of performance and/or redundancy (e.g. mirroring, as in the case of file 2) or specifically demanding certain levels of performance or redundancy, (as with file 3). The using system may also not supply specific policy constraints, leaving the storage system to use appropriate defaults. </paragraph>
<paragraph id="P-0141" lvl="0"><number>&lsqb;0141&rsqb;</number> For example, files 1, 2 and 3 are viewed by the host system as load point and offset files, i.e. the files are accessible using load point and offset address mapping. In addition, file 1 has no policy constraints, file 2 is mirrored, and file 3 has the policy constraint 6-9&apos;s Reliability High Performance (The notion of a specific number &ldquo;N&rdquo; of 9&apos;s of reliability (e.g. 6-9&apos;s) (Six nine&apos;s) means that the probability of not losing a piece of data is 0.999 . . . where &ldquo;N&rdquo; is the number of 9&apos;s following the decimal point. Therefore, a 6-9&apos;s reliability means a 0.999999 probability of no data loss). File 4 is viewed by the host system as a cylinder-head-record type of addressing file with no policy constraints. </paragraph>
<paragraph id="P-0142" lvl="0"><number>&lsqb;0142&rsqb;</number> The storage sub-system, e.g. the RAID or other type of storage device sub-system, uses a type of mapping for each of the file data based on the host system&apos;s view of the data and policy constraints. For example, file 1 is mapped using a load point and offset mapping mechanism, file 2 is mapped using a load point and offset mapping mechanism with mirroring, file 3 is mapped using a log-structured file mapping mechanism, file 4 is mapped using a log-structured file mapping mechanism, and the unallocated space is mapped using pointer bounds of voids. These various mapping mechanisms have been described previously with regard to the description of <cross-reference target="DRAWINGS">FIG. 9</cross-reference> above. </paragraph>
<paragraph id="P-0143" lvl="0"><number>&lsqb;0143&rsqb;</number> As can be seen from <cross-reference target="DRAWINGS">FIG. 15, a</cross-reference> single virtual volume may contain many different files having data recorded in different areas of the virtual volume and may have varying data section sizes, i.e. extent sizes. Furthermore, each file may have a different type of mapping within the virtual volume. Because these data sections are variable and may have dynamically changeable mapping schemes, it is difficult to perform a simple copy operation from the virtual volume to a new storage area. </paragraph>
<paragraph id="P-0144" lvl="0"><number>&lsqb;0144&rsqb;</number> For example, assume that a portion of data ranging from A<highlight><bold>1</bold></highlight> to An is to be the subject of an instant copy operation. This portion of data contains data from files 2 and 3. File 2 is mapped using load point and offset with mirroring. File 3 is mapped using log-structured file mapping. Thus, the data for both file 2 and file 3 in the selected portion cannot be copied using the same copy mechanism due to the difference in mapping. </paragraph>
<paragraph id="P-0145" lvl="0"><number>&lsqb;0145&rsqb;</number> Thus, in order to perform an instant copy operation, it is important to identifying the sections of virtual memory containing the data to be copied and the type of mapping mechanisms used for mapping the data to the virtual volume. The present invention provides a mechanism for performing these functions as well as selecting an appropriate copying technique for performing the instant copy operation. </paragraph>
<paragraph id="P-0146" lvl="0"><number>&lsqb;0146&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 16</cross-reference> is a high level flowchart outlining the primary steps in performing an instant copy of a portion of data in a variable dynamically changeable mapping scheme virtual volume storage sub-system. As shown in <cross-reference target="DRAWINGS">FIG. 16</cross-reference>, the operation starts with receiving an instant copy command (step <highlight><bold>1610</bold></highlight>). The extents of the data which is to be copied are parsed into sets based on type of mapping used to locate data elements (step <highlight><bold>1620</bold></highlight>). </paragraph>
<paragraph id="P-0147" lvl="0"><number>&lsqb;0147&rsqb;</number> For example, assume that a portion of virtual volume space ranging from positions 0 to 400 is to be copied using an instant copy procedure. Further, assume that this portion of the virtual volume space is comprised of three files, one ranging from 0 to 100 and having load point and offset type mapping, a second ranging from 101 to 305 having load point and offset with mirroring type mapping, and a third ranging from 306 to 400 having a log-structured file type mapping. In order to perform an instant copy of this section of the virtual volume space, each of these files must be copied using a different instant copy mechanism in order to provide an optimum copy procedure. </paragraph>
<paragraph id="P-0148" lvl="0"><number>&lsqb;0148&rsqb;</number> Returning to <cross-reference target="DRAWINGS">FIG. 16</cross-reference>, once the extents of the data which is to be copied are parsed, the copy mechanism(s) for each data mapping type in the portion of data to be copied are then set up (step <highlight><bold>1630</bold></highlight>). In a preferred embodiment, for example, data mapped using load point offset mapping may be copied using one of the copy control methods A<highlight><bold>1</bold></highlight>, A<highlight><bold>2</bold></highlight> . . . An as will be described in greater detail hereafter. Data mapped using mirroring may use copy control method B, as will be described in greater detail hereafter, and data mapped using pointer mapped mapping mechanisms, such as log-structured files, may make use of copy control method C, as will be described hereafter. </paragraph>
<paragraph id="P-0149" lvl="0"><number>&lsqb;0149&rsqb;</number> The instant copy procedure is then performed using the identified copy control methods (step <highlight><bold>1640</bold></highlight>) and the operation terminates. This operation may be repeated for each portion of data to be copied using the instant copy procedure. </paragraph>
<paragraph id="P-0150" lvl="0"><number>&lsqb;0150&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 17</cross-reference> illustrates the various copy control methods and their association with data mapping mechanisms. As shown in <cross-reference target="DRAWINGS">FIG. 17</cross-reference>, if the original data mapping mechanism used was a full pointer scheme, such as a log-structured file, the instant copy control method will use a full pointer table (C). If however, the original mapping mechanism used was a load point and offset type mapping, one or more of the following may be used, depending on the circumstances: </paragraph>
<paragraph id="P-0151" lvl="2"><number>&lsqb;0151&rsqb;</number> 1) When the meta-data type provides pointers for changed blocks only, two different copy mechanisms may be used. The first copy mechanism is copying unchanged blocks of data (that is the old data) from a source storage area to a new copy storage area. The second copy mechanism is copying changed blocks of data (that is the new version of the data) from a source storage area to a new copy storage area. These mechanisms are explained more fully hereafter. </paragraph>
<paragraph id="P-0152" lvl="2"><number>&lsqb;0152&rsqb;</number> 2) The meta-data type may provide a full bit map for the copied area. Again the copy mechanism may be either copying unchanged blocks of data (old data) from a source storage area to a new copy storage area or copying changed blocks of data from a source storage area to a new copy storage area; </paragraph>
<paragraph id="P-0153" lvl="2"><number>&lsqb;0153&rsqb;</number> 3) When the meta-data provides multilevel bit maps, the levels of the bit maps must first be resolved to the lowest order bit map covering each subsection or area of the data that is so mapped. Then the same mechanisms described in 2) above may be applied for each individual subsection or area of the data. </paragraph>
<paragraph id="P-0154" lvl="2"><number>&lsqb;0154&rsqb;</number> 4) Partial Area Bit Maps are a combination of extent mapping (which may use any known technique for mapping extents, such as load point and offset) within a large area to be changed, and of bit maps for each extent that has been changed within the area. </paragraph>
<paragraph id="P-0155" lvl="0"><number>&lsqb;0155&rsqb;</number> <cross-reference target="DRAWINGS">FIGS. 18A, 18B</cross-reference>, and <highlight><bold>18</bold></highlight>C illustrate an initialization of an instant copy procedure in accordance with a first copy method, copy method A<highlight><bold>1</bold></highlight>, of the present invention. As shown in <cross-reference target="DRAWINGS">FIG. 18</cross-reference>A, the original virtualization structure has a presentation to the using system called the original data <highlight><bold>1820</bold></highlight> which is mapped into a storage system presentation via the original data map <highlight><bold>1860</bold></highlight> to the initial physical storage area <highlight><bold>1840</bold></highlight> that stores the data. </paragraph>
<paragraph id="P-0156" lvl="0"><number>&lsqb;0156&rsqb;</number> The current state of the art, shown in <cross-reference target="DRAWINGS">FIG. 18</cross-reference>B, is exemplified by the StorageTek SVA <highlight><bold>9</bold></highlight>500. As shown in <cross-reference target="DRAWINGS">FIG. 18</cross-reference>B, just after the instant copy command is initiated, there exist two instances of the data map, the original data map <highlight><bold>1860</bold></highlight> and the copy data map <highlight><bold>1870</bold></highlight>. Both maps point at the exact same physical space and will do so until changes are made to the original data or to the copy data. </paragraph>
<paragraph id="P-0157" lvl="0"><number>&lsqb;0157&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 18C</cross-reference> is an exemplary diagram illustrating the initialization of an instant copy in accordance with the present invention. As shown in <cross-reference target="DRAWINGS">FIG. 18</cross-reference>C, the copy meta-data <highlight><bold>1810</bold></highlight> indicates the data to be copied as well as various parameters about the data to be copied including, for example, the load point and offset of the data. The copy meta-data <highlight><bold>1810</bold></highlight> is generated based on the parsing of data types described above. That is, when an instant copy procedure is initiated on a portion of data in the virtual volume, the data types of the data in the portion of data are identified and grouped into sets based on the type of mapping used to map the data to the virtual volume. The meta-data for these various sets of data to be copied are the copy meta-data <highlight><bold>1810</bold></highlight>. </paragraph>
<paragraph id="P-0158" lvl="0"><number>&lsqb;0158&rsqb;</number> The example shown in <cross-reference target="DRAWINGS">FIG. 18C</cross-reference> makes use of a source, identified as original data <highlight><bold>1820</bold></highlight>, and a copy known as the copy data <highlight><bold>1830</bold></highlight>. The original data <highlight><bold>1820</bold></highlight> is mapped to an initial physical storage area <highlight><bold>1840</bold></highlight> used for original data. The initial physical storage area <highlight><bold>1840</bold></highlight> may be, for example, the location for a virtual volume vol<highlight><subscript>v</subscript></highlight>x. The copy data <highlight><bold>1830</bold></highlight> may be the designation for a virtual volume vol<highlight><subscript>v</subscript></highlight>y. The instant copy procedure is used to logically copy portions of the original data <highlight><bold>1820</bold></highlight> from the virtual volume vol<highlight><subscript>v</subscript></highlight>x to the virtual volume vol<highlight><subscript>v</subscript></highlight>y. </paragraph>
<paragraph id="P-0159" lvl="0"><number>&lsqb;0159&rsqb;</number> When the instant copy operation is initiated, there is no initial copying performed. This is because both the original data <highlight><bold>1820</bold></highlight> and the copy data <highlight><bold>1830</bold></highlight> are identical and thus, only one set of data need be stored. Two separate items of data are required only when a write occurs to overwrite an item of the original data <highlight><bold>1820</bold></highlight>, or the copy data <highlight><bold>1830</bold></highlight>. Thus, the actual copying is not performed until a write occurs to the either the original data <highlight><bold>1820</bold></highlight> or the copy data <highlight><bold>1830</bold></highlight>. One of the items of data will be stored in the initial physical storage area <highlight><bold>1840</bold></highlight>, and the other in the additional physical storage area <highlight><bold>1850</bold></highlight>. </paragraph>
<paragraph id="P-0160" lvl="0"><number>&lsqb;0160&rsqb;</number> At some time later after initialization of the instant copy operation, a write operation may occur. The write operation may be made to either one of the original data <highlight><bold>1820</bold></highlight> or the copy data <highlight><bold>1830</bold></highlight>. Whether the write operation occurs to the original data <highlight><bold>1820</bold></highlight> or the copy data <highlight><bold>1830</bold></highlight> depends on the particular use to which the original data <highlight><bold>1820</bold></highlight> and the copy data <highlight><bold>1830</bold></highlight> are put. For example, if the copy data <highlight><bold>1830</bold></highlight> is used as a backup storage for the originally stored data in the original data <highlight><bold>1820</bold></highlight>, writes will be made to the original data <highlight><bold>1820</bold></highlight>. If the copy data <highlight><bold>1830</bold></highlight> is used as a test mechanism, the writes may be made to the copy data <highlight><bold>1830</bold></highlight>. </paragraph>
<paragraph id="P-0161" lvl="0"><number>&lsqb;0161&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 19</cross-reference> is an exemplary diagram illustrating an instant copy of data when a write operation is performed on the original data <highlight><bold>1920</bold></highlight>. As shown in <cross-reference target="DRAWINGS">FIG. 19</cross-reference>, when new data <highlight><bold>1910</bold></highlight> is to be written to a location in the initial physical storage area <highlight><bold>1940</bold></highlight>, the original data in this location is copied to the additional storage area <highlight><bold>1950</bold></highlight> as old copy data <highlight><bold>1915</bold></highlight>. A new pointer P<highlight><bold>1</bold></highlight> is generated and stored in a pointer table of the copy data map <highlight><bold>1870</bold></highlight> to point to the old copy data <highlight><bold>1915</bold></highlight> in the additional physical storage area <highlight><bold>1950</bold></highlight>. Once the old copy data is copied from the initial physical storage area <highlight><bold>1940</bold></highlight> to the additional storage area <highlight><bold>1950</bold></highlight>, the new original data <highlight><bold>1910</bold></highlight> may be written to the physical location that was occupied by the old copy data. Thus, the original pointer in the pointer table of the original data map <highlight><bold>1960</bold></highlight> pointing to the location of the original data <highlight><bold>1920</bold></highlight> now may be used to reference the new data <highlight><bold>1910</bold></highlight>, and the new pointer P<highlight><bold>1</bold></highlight> may be used to reference the old copy data <highlight><bold>1915</bold></highlight> copied to the additional physical storage area <highlight><bold>1950</bold></highlight>. </paragraph>
<paragraph id="P-0162" lvl="0"><number>&lsqb;0162&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 20A</cross-reference> is an exemplary diagram illustrating an instant copy of data when a write operation is performed to the copy data <highlight><bold>2030</bold></highlight>. As shown in <cross-reference target="DRAWINGS">FIG. 20</cross-reference>A, new data <highlight><bold>2011</bold></highlight> is to be written to the copy data <highlight><bold>2030</bold></highlight>. In this case, the new data may be written to the copy data <highlight><bold>2030</bold></highlight> without having to copy data from the initial physical storage area <highlight><bold>2040</bold></highlight> to the additional physical storage area <highlight><bold>2050</bold></highlight> because any original data in the initial physical storage area <highlight><bold>2040</bold></highlight> and in the original data <highlight><bold>2020</bold></highlight> are the same. Since this data is the same, there is no need to copy it from one location to another. In this case a new pointer P<highlight><bold>2</bold></highlight> is added to the pointer table of the copy data map <highlight><bold>2070</bold></highlight> that points to the new data written to the copy data <highlight><bold>2030</bold></highlight> in the additional physical storage area <highlight><bold>2050</bold></highlight>. </paragraph>
<paragraph id="P-0163" lvl="0"><number>&lsqb;0163&rsqb;</number> As is shown in <cross-reference target="DRAWINGS">FIG. 20B, a</cross-reference> subsequent write <highlight><bold>2011</bold></highlight>A to the same copy data <highlight><bold>2030</bold></highlight> location (or to the original data <highlight><bold>1910</bold></highlight> in <cross-reference target="DRAWINGS">FIG. 19</cross-reference>) will result in no additional copy activity. The new data (copy or original) will be replaced by the new data and the pointer P<highlight><bold>1</bold></highlight> updated to pointer P<highlight><bold>2</bold></highlight> that points to the new new data. </paragraph>
<paragraph id="P-0164" lvl="0"><number>&lsqb;0164&rsqb;</number> The above write operations may occur in combination with regard to the same original data <highlight><bold>1920</bold></highlight> and copy data <highlight><bold>1930</bold></highlight>. That is, a first write operation may occur to the original data area <highlight><bold>1920</bold></highlight>, thereby causing the old data in the original data <highlight><bold>1920</bold></highlight> to be copied to the additional physical storage area <highlight><bold>1950</bold></highlight>. The original pointer now references the new original data in the initial physical storage area <highlight><bold>1940</bold></highlight> and a new pointer P<highlight><bold>1</bold></highlight> is generated to point to the old data in the additional physical storage area <highlight><bold>1950</bold></highlight>. A second write operation <highlight><bold>2011</bold></highlight>A may occur to the copy data <highlight><bold>1930</bold></highlight>, which does not require that the old copy data <highlight><bold>1915</bold></highlight> in the additional physical storage area <highlight><bold>1950</bold></highlight> be copied. A new pointer P<highlight><bold>2</bold></highlight> need only be generated in the meta-data to point to the new copy data <highlight><bold>2011</bold></highlight>A in the additional physical storage area <highlight><bold>1950</bold></highlight>. </paragraph>
<paragraph id="P-0165" lvl="0"><number>&lsqb;0165&rsqb;</number> As mentioned above, each data map which constitutes the meta-data contains a pointer table having pointers for data in each of the original data and the copy data. The meta-data further includes information in correlation with the pointers in the pointer table indicating whether the pointers point to new or old data. The correlation information can be explicit as in setting new and old flags or can be implicit as in putting the pointers into a structure that identifies old from new. </paragraph>
<paragraph id="P-0166" lvl="0"><number>&lsqb;0166&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 21</cross-reference> is an exemplary diagram illustrating how the copy operations described above with regard to <cross-reference target="DRAWINGS">FIGS. 19 and 20</cross-reference> may be used with variable size mapping units. As shown in <cross-reference target="DRAWINGS">FIG. 21</cross-reference>, the original data <highlight><bold>2120</bold></highlight> has had two sections of data overwritten by new data <highlight><bold>2110</bold></highlight> and <highlight><bold>2111</bold></highlight>. As a consequence, the old data <highlight><bold>2115</bold></highlight> and <highlight><bold>2116</bold></highlight> that was in these two sections has been copied to the additional physical storage area <highlight><bold>2150</bold></highlight>. Furthermore, new pointers P<highlight><bold>1</bold></highlight> and P<highlight><bold>2</bold></highlight> were generated in the pointer table of the metadata to reference the old copy data <highlight><bold>2115</bold></highlight> and <highlight><bold>2116</bold></highlight> that was copied to the additional physical storage area <highlight><bold>2150</bold></highlight>. </paragraph>
<paragraph id="P-0167" lvl="0"><number>&lsqb;0167&rsqb;</number> In addition, one section of data in the copy data <highlight><bold>2130</bold></highlight> has been overwritten by new data <highlight><bold>2117</bold></highlight>. As with the copy procedure described above in <cross-reference target="DRAWINGS">FIG. 19</cross-reference>, the original data in the section overwritten need not be copied to the additional physical storage area <highlight><bold>2150</bold></highlight>. Thus, only a new pointer P<highlight><bold>3</bold></highlight> <highlight><bold>2127</bold></highlight> is generated in the pointer table to point to the new data in the additional physical storage area <highlight><bold>2150</bold></highlight>. </paragraph>
<paragraph id="P-0168" lvl="0"><number>&lsqb;0168&rsqb;</number> The sections of old and new data may have variable size. For example, old data <highlight><bold>2116</bold></highlight> may be of a different size than old data <highlight><bold>2117</bold></highlight>. Thus, it is important to keep track in the meta-data of the size of each section of data pointed to by the pointers in the pointer table. Accordingly, the meta-data stores a data section size in association with the pointers in the pointer table. In this way, and processor that makes use of the data stored in the original physical storage area <highlight><bold>2140</bold></highlight> and/or additional physical storage area <highlight><bold>2150</bold></highlight> will be aware of the extent of the data sections. </paragraph>
<paragraph id="P-0169" lvl="0"><number>&lsqb;0169&rsqb;</number> While the above copy methods provide a very efficient mechanism for copying data mapped using load point and offset, the amount of copying may be such that the above method is not the most efficient method for performing the instant copy operation. That is, even with the above method, it would be beneficial to reduce the number of actual copy operations required to generate the original data and copy data copies of the data. Therefore, it would be beneficial to have an instant copy method by which copying of data may be performed with minimal actual copying of the data. </paragraph>
<paragraph id="P-0170" lvl="0"><number>&lsqb;0170&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 22</cross-reference> is an exemplary diagram illustrating an instant copy method, i.e. method A<highlight><bold>2</bold></highlight>, in which all writes are made to the additional physical storage area <highlight><bold>2250</bold></highlight>. With this method, whenever a write operation is to be made to either the original data <highlight><bold>2220</bold></highlight> or the copy data <highlight><bold>2230</bold></highlight>, the write operation is only made to the additional physical storage area <highlight><bold>2250</bold></highlight>. Thus, as shown in <cross-reference target="DRAWINGS">FIG. 22</cross-reference> new data <highlight><bold>2211</bold></highlight>-<highlight><bold>2212</bold></highlight> to be written to the original data <highlight><bold>2220</bold></highlight> is actually written to the additional physical storage area <highlight><bold>2250</bold></highlight>. Similarly, new data <highlight><bold>2217</bold></highlight> that is to be written to the copy data <highlight><bold>2230</bold></highlight> is also written to the additional physical storage area <highlight><bold>2250</bold></highlight>. Thus, the additional physical storage area is a combination of new data for the original data <highlight><bold>2220</bold></highlight> and new data for the copy data <highlight><bold>2230</bold></highlight>. </paragraph>
<paragraph id="P-0171" lvl="0"><number>&lsqb;0171&rsqb;</number> As with the copy methods described in FIGS. <highlight><bold>18</bold></highlight>-<highlight><bold>21</bold></highlight>, new pointers p<highlight><bold>1</bold></highlight>-p<highlight><bold>5</bold></highlight> are generated in the data maps to point to the new data for the original data <highlight><bold>2220</bold></highlight> and the new data for the copy data <highlight><bold>2230</bold></highlight>. With this copy method, however, pointers p<highlight><bold>1</bold></highlight>-p<highlight><bold>2</bold></highlight> that point to the new data for the original data <highlight><bold>2220</bold></highlight> are linked to one another and pointers to the new and old data for the copy data <highlight><bold>2230</bold></highlight> may also be linked to one another. In this way, as will be described hereafter, the two copies of data, i.e. the original data and the copy data may be separated to generate the two separate copies. </paragraph>
<paragraph id="P-0172" lvl="0"><number>&lsqb;0172&rsqb;</number> As shown in <cross-reference target="DRAWINGS">FIG. 23</cross-reference>, in order to separate the two copies of data, i.e. the original data and the copy data, a determination is made as to how much new original data is present in the additional physical storage area and how much copy data is present in the additional physical storage area. This determination may be made based on the information stored in the meta-data that keeps track of which pointers point to new data for the original data, which pointers point to new data for the copy data, and which pointers point to old data. The copy that has the most new data in the additional physical storage area <highlight><bold>2250</bold></highlight> will remain in the additional physical storage area <highlight><bold>2250</bold></highlight> while the other copy will be moved to the initial physical storage area <highlight><bold>2240</bold></highlight>. The old data that is in the initial physical storage area <highlight><bold>2240</bold></highlight> that is related to the new data that is to remain in the additional physical storage area <highlight><bold>2250</bold></highlight> will also be moved to the additional physical storage area <highlight><bold>2250</bold></highlight>. </paragraph>
<paragraph id="P-0173" lvl="0"><number>&lsqb;0173&rsqb;</number> For example, assume that the additional physical storage area contains 80% new data for the original data and 20% new data for the copy data. Since moving the new data for the copy data results in less actual copying of data, the new data for the copy data will be copied, i.e. moved, to the initial physical storage area and the remaining old data in the original data is copied to the additional physical storage area. Thus, the initial physical storage area contains 20% new data for the copy data and 80% old data. The additional physical storage area contains 80% new data for the original data and 20% old data. As a result, the copy data and the original data are in the opposite locations to what is normally expected. </paragraph>
<paragraph id="P-0174" lvl="0"><number>&lsqb;0174&rsqb;</number> This instant copy method can be extended to multiple copy areas, as shown in <cross-reference target="DRAWINGS">FIG. 24A</cross-reference>. The instant copy method, method A<highlight><bold>3</bold></highlight>, shown in <cross-reference target="DRAWINGS">FIG. 24A</cross-reference> illustrates how three virtual volume areas may be used to perform an instant copy. In this example, all new data to be written to the original data is written to a first additional physical storage area <highlight><bold>2451</bold></highlight> rather than the initial physical storage area <highlight><bold>2440</bold></highlight>. All new data to be written to the copy data is written to the second additional physical storage area <highlight><bold>2450</bold></highlight>. In the particular example shown in <cross-reference target="DRAWINGS">FIG. 24</cross-reference>A, as a result of the above write methodology, the original data <highlight><bold>2420</bold></highlight> contains 80% new data and the copy data <highlight><bold>2430</bold></highlight> contains 10% new data, the first additional physical storage area <highlight><bold>2451</bold></highlight> contains the 80% new original data, and the second additional physical storage area <highlight><bold>2450</bold></highlight> contains the 10% new copy datadata. </paragraph>
<paragraph id="P-0175" lvl="0"><number>&lsqb;0175&rsqb;</number> When it becomes necessary to separate the various copies of data, the same movement methodology described above is used with the three data areas. That is, since the first additional physical storage area <highlight><bold>2451</bold></highlight> contains 80% new data for the original data, the old data is moved to the first additional physical storage area <highlight><bold>2451</bold></highlight>. Since the second additional physical storage area <highlight><bold>2450</bold></highlight> contains 10% new data, this new data <highlight><bold>2417</bold></highlight> is copied to the initial physical storage area <highlight><bold>2440</bold></highlight>. As a result, the initial physical storage area <highlight><bold>2440</bold></highlight> contains copy data and the first additional physical storage area <highlight><bold>2451</bold></highlight> contains the original data. <cross-reference target="DRAWINGS">FIGS. 24B and 24C</cross-reference> show how this same operation is performed for various amounts of new data in each of the two copy areas. This operation may further be expanded to embodiments using a copy method, method An, having more than two copy areas if so desired. </paragraph>
<paragraph id="P-0176" lvl="0"><number>&lsqb;0176&rsqb;</number> The mirror copy method that exists in the art today requires that a user of the storage system first establish mirror sets. When an instant copy is required, one of the mirrors in the set is removed from the set and no changes are allowed to that instance of data until it has been copied to another location. While the copy operation is proceeding, any writes to the rest of the original mirror set are noted (e.g., via a log or via bitmaps). After the copy operation is completed, the mirror is reestablished by restoring the copied data to the mirror and the noted writes are performed on the new addition to the mirror. </paragraph>
<paragraph id="P-0177" lvl="0"><number>&lsqb;0177&rsqb;</number> <cross-reference target="DRAWINGS">FIGS. 25A and 25B</cross-reference> illustrate an instant copy method, method C, that may be used to copy data which was originally mapped using a full pointer system, such as log-structured file mapping. With this instant copy method, pointers to old data, i.e. data that has not been changed by a write operation, are referenced in the meta-data by a range of pointers, as will be described hereafter. By using such ranges of pointers, the amount of pointer information stored in the meta-data of the instant copy can be reduced. </paragraph>
<paragraph id="P-0178" lvl="0"><number>&lsqb;0178&rsqb;</number> Instant copy meta-data is that information needed to identify the original data extents that are to be copied and the copy data nomenclature that will identify the copy data and any controls related to the operation. Controls might be the timing of the operation such as &ldquo;do this at midnight local time&rdquo; or could be frequency &ldquo;do this after every 1000 writes to the original data&rdquo;. Once a copy is initiated, pointers are created as necessary to map all of the original and the copy data either directly or indirectly. There is also storage meta-data that identifies individual instances of stored data and might indicate age, size, generation, expiration date, frequency of use, date of last use, etc. This meta-data is associated with the pointers that allow one to locate the data so it can be referenced with the data and updated as necessary. Data is copied only when there are requirements that two or more versions of an instance of data be maintained. </paragraph>
<paragraph id="P-0179" lvl="0"><number>&lsqb;0179&rsqb;</number> As shown in <cross-reference target="DRAWINGS">FIG. 25</cross-reference>A, an initial physical storage area <highlight><bold>2510</bold></highlight> contains data that has not been changed by write operations. Original pointers <highlight><bold>2520</bold></highlight> to this original data are stored in the meta-data for the virtual volume stored in this initial physical storage area <highlight><bold>2510</bold></highlight>. If an instant copy operation is performed, the original pointers <highlight><bold>2520</bold></highlight> to the original data in the old data space <highlight><bold>2510</bold></highlight> will be used in the meta-data of the instant copy. Thus, only two pointers are needed to identify an instant copy of data, i.e. a begin pointer and end pointer to the original data pointers to be used. Therefore, rather than having to copy all of the original pointers to the instant copy meta-data, these original pointers <highlight><bold>2520</bold></highlight> may be referenced by the instant copy meta-data as a range of pointers. </paragraph>
<paragraph id="P-0180" lvl="0"><number>&lsqb;0180&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 25B</cross-reference> shows an example of how the above pointer ranges may be used to reduce the size of the instant copy meta-data. As shown in <cross-reference target="DRAWINGS">FIG. 25</cross-reference>B, pointers A, B, C, D and E all initially point to original data in the initial physical storage area <highlight><bold>2510</bold></highlight>. At some time later, an instant copy operation is initiated and some of the data in the initial physical storage area has corresponding new data written to the additional physical storage area <highlight><bold>2550</bold></highlight>. In particular, the data associated with pointers B, D and E have corresponding new data in the additional physical storage area <highlight><bold>2550</bold></highlight>. As a consequence, the pointers in the instant copy meta-data comprise pointers A and C which point to original data in the initial physical storage area <highlight><bold>2510</bold></highlight> and pointers B, D and E point to new data in the additional physical storage area <highlight><bold>2550</bold></highlight>. In addition, there are ranges of pointers with which no new data has been written. These ranges of pointer can be referenced in the instant copy meta-data by referencing simply the first and last pointers of the pointers to the original data. </paragraph>
<paragraph id="P-0181" lvl="0"><number>&lsqb;0181&rsqb;</number> Thus, for example, the range <highlight><bold>2530</bold></highlight> is identified in the instant copy meta-data as pointers P<highlight><bold>1</bold></highlight> and P<highlight><bold>2</bold></highlight> with an indication that these pointers refer to a range of pointers inclusively. Similarly, the range <highlight><bold>2540</bold></highlight> may be designated by the begin and end pointers P<highlight><bold>3</bold></highlight> and P<highlight><bold>4</bold></highlight>. Thus, the size of the pointer table in the instant copy meta-data is reduced. </paragraph>
<paragraph id="P-0182" lvl="0"><number>&lsqb;0182&rsqb;</number> As shown in FIGS. <highlight><bold>18</bold></highlight>-<highlight><bold>25</bold></highlight>B, the present invention provides various instant copy methods that may be used to perform an instant copy operation on data based on the mapping method originally used to store the data. The various instant copy methods provide varying levels of actual copying performed and amounts of meta-data that needs to be maintained. Each method has its own advantages and disadvantages and thus, the use of each may be selected based on the particular application to which the present invention is placed. With all these instant copy methods, however, an efficient instant copy may be performed in view of the original mapping of the data to be copied. </paragraph>
<paragraph id="P-0183" lvl="0"><number>&lsqb;0183&rsqb;</number> It is important to note that while the present invention has been described in the context of a fully functioning data processing system, those of ordinary skill in the art will appreciate that the processes of the present invention are capable of being distributed in the form of a computer readable medium of instructions and a variety of forms and that the present invention applies equally regardless of the particular type of signal bearing media actually used to carry out the distribution. Examples of computer readable media include recordable-type media such as a floppy disc, a hard disk drive, a RAM, CD-ROMs, and transmission-type media such as digital and analog communications links. </paragraph>
<paragraph id="P-0184" lvl="0"><number>&lsqb;0184&rsqb;</number> The description of the present invention has been presented for purposes of illustration and description, and is not intended to be exhaustive or limited to the invention in the form disclosed. Many modifications and variations will be apparent to those of ordinary skill in the art. The embodiment was chosen and described in order to best explain the principles of the invention, the practical application, and to enable others of ordinary skill in the art to understand the invention for various embodiments with various modifications as are suited to the particular use contemplated. </paragraph>
</section>
</detailed-description>
</subdoc-description>
<subdoc-claims>
<heading lvl="1">What is claimed is: </heading>
<claim id="CLM-00001">
<claim-text><highlight><bold>1</bold></highlight>. A method of performing an instant copy of data, comprising: 
<claim-text>receiving a write operation to a data location in one of an initial physical storage area and an additional physical storage area; </claim-text>
<claim-text>performing an instant copy operation on the data location; and </claim-text>
<claim-text>writing new data to the data location in accordance with the write operation, wherein the instant copy operation includes generating a pointer to one of the new data and original data in the data location. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00002">
<claim-text><highlight><bold>2</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein the data location is in the initial physical storage area, and wherein the instant copy operation includes copying a portion of original data from the data location in the initial physical storage area to a second data location in the additional physical storage area in response to receiving the write operation. </claim-text>
</claim>
<claim id="CLM-00003">
<claim-text><highlight><bold>3</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00002">claim 2</dependent-claim-reference>, wherein the pointer points to the portion of original data copied to the additional physical storage area. </claim-text>
</claim>
<claim id="CLM-00004">
<claim-text><highlight><bold>4</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein the data location is in the additional physical storage area. </claim-text>
</claim>
<claim id="CLM-00005">
<claim-text><highlight><bold>5</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00004">claim 4</dependent-claim-reference>, wherein the instant copy operation includes not copying a portion of original data from the initial physical storage area to the additional physical storage area. </claim-text>
</claim>
<claim id="CLM-00006">
<claim-text><highlight><bold>6</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00004">claim 4</dependent-claim-reference>, wherein the pointer points to new data written to the data location in the additional physical storage area. </claim-text>
</claim>
<claim id="CLM-00007">
<claim-text><highlight><bold>7</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein the initial physical storage area is a variable dynamically changeable mapping scheme storage area. </claim-text>
</claim>
<claim id="CLM-00008">
<claim-text><highlight><bold>8</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00002">claim 2</dependent-claim-reference>, wherein the portion of original data has a size that is different with respect to other portions of data in the initial physical storage area. </claim-text>
</claim>
<claim id="CLM-00009">
<claim-text><highlight><bold>9</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein the pointer is generated in a pointer table of meta-data associated with the additional physical storage area. </claim-text>
</claim>
<claim id="CLM-00010">
<claim-text><highlight><bold>10</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00008">claim 8</dependent-claim-reference>, further comprising storing the pointer and an associated size of the portion of original data in a meta-data data structure. </claim-text>
</claim>
<claim id="CLM-00011">
<claim-text><highlight><bold>11</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein the pointer is stored in a pointer table of meta-data having a plurality of pointers, and wherein the plurality of pointers include a pair of pointers representing a range of pointers that point to portions of original data that have not been changed by a write operation. </claim-text>
</claim>
<claim id="CLM-00012">
<claim-text><highlight><bold>12</bold></highlight>. A computer program product in a computer readable medium for performing an instant copy of data, comprising: 
<claim-text>first instructions for receiving a write operation to a data location in one of an initial physical storage area and an additional physical storage area; </claim-text>
<claim-text>second instructions for performing an instant copy operation on the data location; and </claim-text>
<claim-text>third instructions for writing new data to the data location in accordance with the write operation, wherein the instant copy operation includes generating a pointer to one of the new data and original data in the data location. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00013">
<claim-text><highlight><bold>13</bold></highlight>. The computer program product of <dependent-claim-reference depends_on="CLM-00011">claim 12</dependent-claim-reference>, wherein the data location is in the initial physical storage area, and wherein the second instructions for performing an instant copy operation include instructions for copying a portion of original data from the data location in the initial physical storage area to a second data location in the additional physical storage area in response to receiving the write operation. </claim-text>
</claim>
<claim id="CLM-00014">
<claim-text><highlight><bold>14</bold></highlight>. The computer program product of <dependent-claim-reference depends_on="CLM-00011">claim 13</dependent-claim-reference>, wherein the pointer points to the portion of original data copied to the additional physical storage area. </claim-text>
</claim>
<claim id="CLM-00015">
<claim-text><highlight><bold>15</bold></highlight>. The computer program product of <dependent-claim-reference depends_on="CLM-00011">claim 12</dependent-claim-reference>, wherein the data location is in the additional physical storage area. </claim-text>
</claim>
<claim id="CLM-00016">
<claim-text><highlight><bold>16</bold></highlight>. The computer program product of <dependent-claim-reference depends_on="CLM-00011">claim 15</dependent-claim-reference>, wherein the second instructions for performing the instant copy operation include instructions for not copying a portion of original data from the initial physical storage area to the additional physical storage area. </claim-text>
</claim>
<claim id="CLM-00017">
<claim-text><highlight><bold>17</bold></highlight>. The computer program product of <dependent-claim-reference depends_on="CLM-00011">claim 15</dependent-claim-reference>, wherein the pointer points to new data written to the data location in the additional physical storage area. </claim-text>
</claim>
<claim id="CLM-00018">
<claim-text><highlight><bold>18</bold></highlight>. The computer program product of <dependent-claim-reference depends_on="CLM-00011">claim 12</dependent-claim-reference>, wherein the initial physical storage area is a variable dynamically changeable mapping scheme storage area. </claim-text>
</claim>
<claim id="CLM-00019">
<claim-text><highlight><bold>19</bold></highlight>. The computer program product of <dependent-claim-reference depends_on="CLM-00011">claim 13</dependent-claim-reference>, wherein the portion of original data has a size that is different with respect to other portions of data in the initial physical storage area. </claim-text>
</claim>
<claim id="CLM-00020">
<claim-text><highlight><bold>20</bold></highlight>. The computer program product of <dependent-claim-reference depends_on="CLM-00011">claim 12</dependent-claim-reference>, wherein the second instructions for performing the instant copy operation include instructions for generating the pointer in a pointer table of meta-data associated with the additional physical storage area. </claim-text>
</claim>
<claim id="CLM-00021">
<claim-text><highlight><bold>21</bold></highlight>. The computer program product of <dependent-claim-reference depends_on="CLM-00011">claim 19</dependent-claim-reference>, wherein the second instructions for performing the instant copy operation include instructions for storing the pointer and an associated size of the portion of original data in a meta-data data structure. </claim-text>
</claim>
<claim id="CLM-00022">
<claim-text><highlight><bold>22</bold></highlight>. The computer program product of <dependent-claim-reference depends_on="CLM-00011">claim 12</dependent-claim-reference>, wherein the second instructions for performing the instant copy operation include instructions for storing the pointer in a pointer table of meta-data having a plurality of pointers, and wherein the plurality of pointers include a pair of pointers representing a range of pointers that point to portions of original data that have not been changed by a write operation. </claim-text>
</claim>
<claim id="CLM-00023">
<claim-text><highlight><bold>23</bold></highlight>. An apparatus for performing an instant copy of data, comprising: 
<claim-text>means for receiving a write operation to a data location in one of an initial physical storage area and an additional physical storage area; </claim-text>
<claim-text>means for performing an instant copy operation on the data location; and </claim-text>
<claim-text>means for writing new data to the data location in accordance with the write operation, wherein the instant copy operation includes generating a pointer to one of the new data and original data in the data location.</claim-text>
</claim-text>
</claim>
</subdoc-claims>
<subdoc-drawings id="DRAWINGS">
<heading lvl="0" align="CENTER">Drawings</heading>
<representative-figure>3</representative-figure>
<figure id="figure-D00000">
<image id="EMI-D00000" file="US20030005248A1-20030102-D00000.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00001">
<image id="EMI-D00001" file="US20030005248A1-20030102-D00001.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00002">
<image id="EMI-D00002" file="US20030005248A1-20030102-D00002.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00003">
<image id="EMI-D00003" file="US20030005248A1-20030102-D00003.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00004">
<image id="EMI-D00004" file="US20030005248A1-20030102-D00004.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00005">
<image id="EMI-D00005" file="US20030005248A1-20030102-D00005.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00006">
<image id="EMI-D00006" file="US20030005248A1-20030102-D00006.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00007">
<image id="EMI-D00007" file="US20030005248A1-20030102-D00007.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00008">
<image id="EMI-D00008" file="US20030005248A1-20030102-D00008.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00009">
<image id="EMI-D00009" file="US20030005248A1-20030102-D00009.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00010">
<image id="EMI-D00010" file="US20030005248A1-20030102-D00010.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00011">
<image id="EMI-D00011" file="US20030005248A1-20030102-D00011.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00012">
<image id="EMI-D00012" file="US20030005248A1-20030102-D00012.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00013">
<image id="EMI-D00013" file="US20030005248A1-20030102-D00013.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00014">
<image id="EMI-D00014" file="US20030005248A1-20030102-D00014.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00015">
<image id="EMI-D00015" file="US20030005248A1-20030102-D00015.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00016">
<image id="EMI-D00016" file="US20030005248A1-20030102-D00016.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00017">
<image id="EMI-D00017" file="US20030005248A1-20030102-D00017.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00018">
<image id="EMI-D00018" file="US20030005248A1-20030102-D00018.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00019">
<image id="EMI-D00019" file="US20030005248A1-20030102-D00019.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00020">
<image id="EMI-D00020" file="US20030005248A1-20030102-D00020.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00021">
<image id="EMI-D00021" file="US20030005248A1-20030102-D00021.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00022">
<image id="EMI-D00022" file="US20030005248A1-20030102-D00022.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00023">
<image id="EMI-D00023" file="US20030005248A1-20030102-D00023.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00024">
<image id="EMI-D00024" file="US20030005248A1-20030102-D00024.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00025">
<image id="EMI-D00025" file="US20030005248A1-20030102-D00025.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00026">
<image id="EMI-D00026" file="US20030005248A1-20030102-D00026.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00027">
<image id="EMI-D00027" file="US20030005248A1-20030102-D00027.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00028">
<image id="EMI-D00028" file="US20030005248A1-20030102-D00028.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00029">
<image id="EMI-D00029" file="US20030005248A1-20030102-D00029.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00030">
<image id="EMI-D00030" file="US20030005248A1-20030102-D00030.TIF" imf="TIFF" ti="DR"/>
</figure>
</subdoc-drawings>
</patent-application-publication>
