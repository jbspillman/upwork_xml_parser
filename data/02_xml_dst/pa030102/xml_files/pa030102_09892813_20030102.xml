<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE patent-application-publication SYSTEM "pap-v16-2002-01-01.dtd" [
<!ENTITY US20030005114A1-20030102-D00000.TIF SYSTEM "US20030005114A1-20030102-D00000.TIF" NDATA TIF>
<!ENTITY US20030005114A1-20030102-D00001.TIF SYSTEM "US20030005114A1-20030102-D00001.TIF" NDATA TIF>
<!ENTITY US20030005114A1-20030102-D00002.TIF SYSTEM "US20030005114A1-20030102-D00002.TIF" NDATA TIF>
<!ENTITY US20030005114A1-20030102-D00003.TIF SYSTEM "US20030005114A1-20030102-D00003.TIF" NDATA TIF>
<!ENTITY US20030005114A1-20030102-D00004.TIF SYSTEM "US20030005114A1-20030102-D00004.TIF" NDATA TIF>
<!ENTITY US20030005114A1-20030102-D00005.TIF SYSTEM "US20030005114A1-20030102-D00005.TIF" NDATA TIF>
<!ENTITY US20030005114A1-20030102-D00006.TIF SYSTEM "US20030005114A1-20030102-D00006.TIF" NDATA TIF>
<!ENTITY US20030005114A1-20030102-D00007.TIF SYSTEM "US20030005114A1-20030102-D00007.TIF" NDATA TIF>
<!ENTITY US20030005114A1-20030102-D00008.TIF SYSTEM "US20030005114A1-20030102-D00008.TIF" NDATA TIF>
<!ENTITY US20030005114A1-20030102-D00009.TIF SYSTEM "US20030005114A1-20030102-D00009.TIF" NDATA TIF>
]>
<patent-application-publication>
<subdoc-bibliographic-information>
<document-id>
<doc-number>20030005114</doc-number>
<kind-code>A1</kind-code>
<document-date>20030102</document-date>
</document-id>
<publication-filing-type>new</publication-filing-type>
<domestic-filing-data>
<application-number>
<doc-number>09892813</doc-number>
</application-number>
<application-number-series-code>09</application-number-series-code>
<filing-date>20010627</filing-date>
</domestic-filing-data>
<technical-information>
<classification-ipc>
<classification-ipc-primary>
<ipc>G06F015/173</ipc>
</classification-ipc-primary>
<classification-ipc-secondary>
<ipc>G06F009/00</ipc>
</classification-ipc-secondary>
<classification-ipc-edition>07</classification-ipc-edition>
</classification-ipc>
<classification-us>
<classification-us-primary>
<uspc>
<class>709</class>
<subclass>225000</subclass>
</uspc>
</classification-us-primary>
<classification-us-secondary>
<uspc>
<class>709</class>
<subclass>105000</subclass>
</uspc>
</classification-us-secondary>
</classification-us>
<title-of-invention>Globally distributed load balancing</title-of-invention>
</technical-information>
<inventors>
<first-named-inventor>
<name>
<given-name>Nir</given-name>
<middle-name>N.</middle-name>
<family-name>Shavit</family-name>
</name>
<residence>
<residence-us>
<city>Cambridge</city>
<state>MA</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</first-named-inventor>
<inventor>
<name>
<given-name>Steven</given-name>
<middle-name>K.</middle-name>
<family-name>Heller</family-name>
</name>
<residence>
<residence-us>
<city>Acton</city>
<state>MA</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
<inventor>
<name>
<given-name>Christine</given-name>
<middle-name>H.</middle-name>
<family-name>Flood</family-name>
</name>
<residence>
<residence-us>
<city>Westford</city>
<state>MA</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
</inventors>
<correspondence-address>
<name-1>CESARI AND MCKENNA, LLP</name-1>
<name-2></name-2>
<address>
<address-1>88 BLACK FALCON AVENUE</address-1>
<city>BOSTON</city>
<state>MA</state>
<postalcode>02210</postalcode>
<country>
<country-code>US</country-code>
</country>
</address>
</correspondence-address>
</subdoc-bibliographic-information>
<subdoc-abstract>
<paragraph id="A-0001" lvl="0">A garbage collector employs a plurality of task queues for a parallel-execution operation in a garbage-collection cycle. Each task queue is associated with a different ordered pair of the threads that perform the parallel-execution operation in parallel. One of the threads, referred to as that task queue&apos;s &ldquo;enqueuer&rdquo; thread, is the only one that can &ldquo;push&rdquo; onto that queue an identifier of a dynamically identified task. The other thread, referred to as that task queue&apos;s &ldquo;dequeuer,&rdquo; is the only one that can &ldquo;pop&rdquo; tasks from that task queue for execution. Since, for each task queue, there is only one thread that can &ldquo;push&rdquo; task identifiers on to it and only one thread that can &ldquo;pop&rdquo; task identifiers from it, the garbage collector can share dynamically identified tasks optimally among its threads without suffering the cost imposed by making combinations of otherwise separate machine instructions atomic. </paragraph>
</subdoc-abstract>
<subdoc-description>
<summary-of-invention>
<section>
<heading lvl="1">CROSS-REFERENCE TO RELATED APPLICATIONS </heading>
<paragraph id="P-0001" lvl="0"><number>&lsqb;0001&rsqb;</number> This application is related to commonly assigned U.S. patent applications of Nir N. Shavit et al. for Termination Detection for Shared-Memory Parallel Programs and Load-Balancing Queues Employing LIFO/FIFO Work Stealing, both of which were filed on the same date as this application and are hereby incorporated by reference. </paragraph>
</section>
<section>
<heading lvl="1">BACKGROUND OF THE INVENTION </heading>
<paragraph id="P-0002" lvl="0"><number>&lsqb;0002&rsqb;</number> 1. Field of the Invention </paragraph>
<paragraph id="P-0003" lvl="0"><number>&lsqb;0003&rsqb;</number> The present invention is directed to multi-threaded operation in computer systems. It particularly concerns how to allocate tasks among different threads. </paragraph>
</section>
<section>
<heading lvl="1">BACKGROUND INFORMATION </heading>
<paragraph id="P-0004" lvl="0"><number>&lsqb;0004&rsqb;</number> Modem computer systems provide for various types of concurrent operation. A user of a typical desktop computer, for instance, may be simultaneously employing a word-processor program and an e-mail program together with a calculator program. The user&apos;s computer could be using several simultaneously operating processors, each of which could be operating on a different program. More typically, the computer employs only a single main processor, and its operating-system software causes that processor to switch from one program to another rapidly enough that the user cannot usually tell that the different programs are not really executing simultaneously. The different running programs are usually referred to as &ldquo;processes&rdquo; in this connection, and the change from one process to another is said to involve a &ldquo;context switch.&rdquo; In a context switch one process is interrupted, and the contents of the program counter, call stacks, and various registers are stored, including those used for memory mapping. Then the corresponding values previously stored for a previously interrupted process are loaded, and execution resumes for that process. Processor hardware and operating-system software typically have special provisions for performing such context switches. </paragraph>
<paragraph id="P-0005" lvl="0"><number>&lsqb;0005&rsqb;</number> A program running as a computer-system process may take advantage of such provisions to provide separate, concurrent &ldquo;threads&rdquo; of its own execution. In such a case, the program counter and various register contents are stored and reloaded with a different thread&apos;s value, as in the case of a process change, but the memory-mapping values are not changed, so the new thread of execution has access to the same process-specific physical memory as the same process&apos;s previous thread. </paragraph>
<paragraph id="P-0006" lvl="0"><number>&lsqb;0006&rsqb;</number> In some cases, the use of multiple execution threads is merely a matter of programming convenience. For example, compilers for various programming languages, such as the Java programming language, readily provide the &ldquo;housekeeping&rdquo; for spawning different threads, so the programmer is not burdened with handling the details of making different threads&apos; execution appear simultaneous. In the case of multiprocessor systems, though, the use of multiple threads has speed advantages. A process can be performed more quickly if the system allocates different threads to different processors when processor capacity is available. </paragraph>
<paragraph id="P-0007" lvl="0"><number>&lsqb;0007&rsqb;</number> To take advantage of this fact, programmers often identify constituent operations with their programs that particularly lend themselves to parallel execution. When program execution reaches a point where the parallel-execution operation can begin, it starts different execution threads to perform different tasks within that operation. </paragraph>
<paragraph id="P-0008" lvl="0"><number>&lsqb;0008&rsqb;</number> Now, in some parallel-execution operations the tasks to be performed can be identified only dynamically; that is, some of the tasks can be identified only by performing others of the tasks, so the tasks cannot be divided among the threads optimally at the beginning of the parallel-execution operation. Such parallel-execution operations can occur, for instance, in what has come to be called &ldquo;garbage collection,&rdquo; which is the automatic reclamation of dynamically allocated memory. Byte code executed by a Java virtual machine, for instance, often calls for memory to be allocated for data &ldquo;objects&rdquo; if certain program branches are taken. Subsequently, a point in the byte-code program&apos;s execution can be reached at which there is no further possibility that the data stored in that dynamically allocated memory will be used. Without requiring the programmer to provide specific instructions to do so, the virtual machine executing the byte code automatically identifies such &ldquo;unreachable&rdquo; objects and reclaims their memory so that objects allocated thereafter can use it. </paragraph>
<paragraph id="P-0009" lvl="0"><number>&lsqb;0009&rsqb;</number> The general approach employed by the virtual machine&apos;s garbage collector is to identify all objects that are reachable and then reclaim memory that no such reachable object occupies. An object is considered reachable if it is referred to by a reference in a &ldquo;root set&rdquo; of locations, such as global variables, registers, or the call stack, that are recognized as being inherently reachable. An object is also reachable if it is referred to by a reference in a reachable object. </paragraph>
<paragraph id="P-0010" lvl="0"><number>&lsqb;0010&rsqb;</number> So reachable-object identification is a recursive process: the identification of a reachable object can lead to identification of further reachable objects. And, if every reachable object so far identified is thought of as representing a further task, namely, that of identifying any further objects to which it refers, it can be seen that parts of the garbage-collection process include tasks that are identifiable only dynamically. If those tasks are properly programmed, they can be performed in an essentially parallel manner. Specifically, the initial, statically identifiable members of the root set can be divided among a plurality of threads (whose execution will typically be divided among many processors), and those threads can identify reachable objects in parallel. </paragraph>
<paragraph id="P-0011" lvl="0"><number>&lsqb;0011&rsqb;</number> Now, each thread could maintain a list of the tasks that it has thus identified dynamically, and it could proceed to perform all tasks that it has thus identified. But much of the advantage of parallel processing may be lost if each thread performs only those tasks that it has itself identified. That is, one thread may encounter a number of objects that have a large number of references, while others may not. This leaves one thread with many more tasks than the others, so there would be a significant amount of time during which the other threads will have finished all of their tasks in the parallel-execution operation, while another thread still has most of its tasks yet to be performed. </paragraph>
<paragraph id="P-0012" lvl="0"><number>&lsqb;0012&rsqb;</number> As a consequence, such parallel-execution operations are usually so arranged that each thread can perform tasks that other threads have identified. Conventionally, though, this has usually meant that access to the queues that contain identifiers of those tasks needs to be made &ldquo;thread safe.&rdquo; Thread safety in most cases can be afforded only by performing atomically sets of machine instructions that could normally be performed separately. Particularly in the multiprocessor systems in which parallel execution is especially advantageous, performing such &ldquo;atomic&rdquo; operations is quite expensive. So the need for thread safety tends to compromise some of a multiprocessor system&apos;s advantages. </paragraph>
</section>
<section>
<heading lvl="1">SUMMARY OF THE INVENTION </heading>
<paragraph id="P-0013" lvl="0"><number>&lsqb;0013&rsqb;</number> We have invented a way of dividing dynamically allocated tasks among separate threads in such a way as virtually to avoid the expensive atomic operations that thread safety normally requires. In accordance with this invention, the computer system provides a plurality of task queues. Each queue is associated with a different ordered pair of the threads that are to perform the parallel-execution operation. We call one thread of the ordered pair the &ldquo;enqueuer&rdquo; of the queue associated with the ordered pair, and we call the other thread of the ordered pair the &ldquo;dequeuer.&rdquo; When a thread identifies a task, it pushes an identifier of that task onto one or more of the queues of which that thread is an enqueuer. When a thread needs a new tasks to perform, it pops a task identifier from a queue of which it is the dequeuer. For each queue, that is, there is only one thread that pushes task identifiers onto it and only one that pops those identifiers from it. As will be shown below, this enables the most-common accesses to those queues to be made thread safe without resorting to atomic operations. It thus eliminates the performance cost that attends such operations.</paragraph>
</section>
</summary-of-invention>
<brief-description-of-drawings>
<section>
<heading lvl="1">BRIEF DESCRIPTION OF THE DRAWINGS </heading>
<paragraph id="P-0014" lvl="0"><number>&lsqb;0014&rsqb;</number> The invention description below refers to the accompanying drawings, of which: </paragraph>
<paragraph id="P-0015" lvl="0"><number>&lsqb;0015&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 1</cross-reference> is a block diagram of a typical uniprocessor computer system; </paragraph>
<paragraph id="P-0016" lvl="0"><number>&lsqb;0016&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 2</cross-reference> is a block diagram of one type of multiprocessor computer system; </paragraph>
<paragraph id="P-0017" lvl="0"><number>&lsqb;0017&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 3</cross-reference> is a block diagram that illustrates a relationship between source code and object code; </paragraph>
<paragraph id="P-0018" lvl="0"><number>&lsqb;0018&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 4</cross-reference> is a block diagram of a more-complicated relationship between source code and object code; </paragraph>
<paragraph id="P-0019" lvl="0"><number>&lsqb;0019&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 5</cross-reference> is a flow chart that illustrates a sequence of parallel-execution operations; </paragraph>
<paragraph id="P-0020" lvl="0"><number>&lsqb;0020&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 6</cross-reference> is a block diagram of a task-queue array employed by the illustrated embodiment of the present invention; </paragraph>
<paragraph id="P-0021" lvl="0"><number>&lsqb;0021&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 7</cross-reference> is a source-code listing of routines employed by a thread in adding a task-identifier entry to one of the task queues of <cross-reference target="DRAWINGS">FIG. 6</cross-reference>; </paragraph>
<paragraph id="P-0022" lvl="0"><number>&lsqb;0022&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 8</cross-reference> is a data-structure diagram depicting data structures employed by respective execution threads in support of their use of <cross-reference target="DRAWINGS">FIG. 6</cross-reference>&apos;s task-queue array; </paragraph>
<paragraph id="P-0023" lvl="0"><number>&lsqb;0023&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 9</cross-reference> is a source-code listing of routines employed by an execution thread in &ldquo;popping&rdquo; tasks from the task-queue array of <cross-reference target="DRAWINGS">FIG. 6</cross-reference>; and </paragraph>
<paragraph id="P-0024" lvl="0"><number>&lsqb;0024&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 10</cross-reference> is a source-code listing of routines employed by the execution threads in support of the termination-protection approach that the illustrated embodiment employs.</paragraph>
</section>
</brief-description-of-drawings>
<detailed-description>
<section>
<heading lvl="1">DETAILED DESCRIPTION OF AN ILLUSTRATIVE EMBODIMENT </heading>
<paragraph id="P-0025" lvl="0"><number>&lsqb;0025&rsqb;</number> The present invention&apos;s teachings concerning task allocation in parallel-execution operations operation can be implemented in a wide variety of systems. Some of the benefits of employing multiple threads can be obtained in uniprocessor systems, of which <cross-reference target="DRAWINGS">FIG. 1</cross-reference> depicts a typical configuration. Its uniprocessor system <highlight><bold>10</bold></highlight> employs a single microprocessor such as microprocessor <highlight><bold>11</bold></highlight>. In <cross-reference target="DRAWINGS">FIG. 1</cross-reference>&apos;s exemplary system, microprocessor <highlight><bold>11</bold></highlight> receives data, and instructions for operating on them, from on-board cache memory or further cache memory <highlight><bold>12</bold></highlight>, possibly through the mediation of a cache controller <highlight><bold>13</bold></highlight>. The cache controller <highlight><bold>13</bold></highlight> can in turn receive such data from system read/write memory (&ldquo;RAM&rdquo;) <highlight><bold>14</bold></highlight> through a RAM controller <highlight><bold>15</bold></highlight>, or from various peripheral devices through a system bus <highlight><bold>16</bold></highlight>. </paragraph>
<paragraph id="P-0026" lvl="0"><number>&lsqb;0026&rsqb;</number> The RAM <highlight><bold>14</bold></highlight>&apos;s data and instruction contents, which can configure the system to implement the teachings to be described below, will ordinarily have been loaded from peripheral devices such as a system disk <highlight><bold>17</bold></highlight>. Other sources of such RAM contents include communications interface <highlight><bold>18</bold></highlight>, which can receive instructions and data from other computer equipment. </paragraph>
<paragraph id="P-0027" lvl="0"><number>&lsqb;0027&rsqb;</number> Although such systems can implement threads generally and therefore the present invention&apos;s teachings in particular, the application in connection with which the present invention is described by way of example below would more frequently be implemented in a multiprocessor system. Such systems come in a wide variety of configurations. Some may be largely the same as that of <cross-reference target="DRAWINGS">FIG. 1</cross-reference> with the exception that they could include more than one microprocessor such as processor <highlight><bold>11</bold></highlight>, possibly together with respective cache memories, sharing common read/write memory by communication over the common bus <highlight><bold>16</bold></highlight>. </paragraph>
<paragraph id="P-0028" lvl="0"><number>&lsqb;0028&rsqb;</number> In other configurations, parts of the shared memory may be more local to one or more processors than to others. In <cross-reference target="DRAWINGS">FIG. 2</cross-reference>, for instance, one or more microprocessors <highlight><bold>20</bold></highlight> at a location <highlight><bold>22</bold></highlight> may have access both to a local memory module <highlight><bold>24</bold></highlight> and to a further, remote memory module <highlight><bold>26</bold></highlight>, which is provided at a remote location <highlight><bold>28</bold></highlight>. Because of the greater distance, though, port circuitry <highlight><bold>28</bold></highlight> and <highlight><bold>30</bold></highlight> may be necessary to communicate at the lower speed to which an intervening channel <highlight><bold>32</bold></highlight> is limited. A processor <highlight><bold>34</bold></highlight> at the remote location may similarly have different-speed access to both memory modules <highlight><bold>24</bold></highlight> and <highlight><bold>26</bold></highlight>. In such a situation, one or the other or both of the processors may need to fetch code or data or both from a remote location, but it will often be true that parts of the code will be replicated in both places. </paragraph>
<paragraph id="P-0029" lvl="0"><number>&lsqb;0029&rsqb;</number> Regardless of the configuration, different processors can operate on the same code, although that code may be replicated in different physical memory, so different processors can be used to execute different threads of the same process. </paragraph>
<paragraph id="P-0030" lvl="0"><number>&lsqb;0030&rsqb;</number> To illustrate the invention, we will describe its use by a garbage collector. To place garbage collection in context, we briefly review the general relationship between programming and computer operation. When a processor executes a computer program, of course, it executes machine instructions. A programmer typically writes the program, but it is a rare programmer who is familiar with the specific machine instructions in which his efforts eventually result. More typically, the programmer writes higher-level-language &ldquo;source code,&rdquo; from which a computer software-configured to do so generates those machine instructions, or &ldquo;object code.&rdquo;</paragraph>
<paragraph id="P-0031" lvl="0"><number>&lsqb;0031&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 3</cross-reference> represents this sequence. <cross-reference target="DRAWINGS">FIG. 3</cross-reference>&apos;s block <highlight><bold>36</bold></highlight> represents a compiler process that a computer performs under the direction of compiler object code. That object code is typically stored on a persistent machine-readable medium, such as <cross-reference target="DRAWINGS">FIG. 1</cross-reference>&apos;s system disk <highlight><bold>17</bold></highlight>, and it is loaded by transmission of electrical signals into RAM <highlight><bold>15</bold></highlight> to configure the computer system to act as a compiler. But the compiler object code&apos;s persistent storage may instead be provided in a server system remote from the machine that performs the compiling. The electrical signals that carry the digital data by which the computer systems exchange the code are exemplary forms of carrier waves transporting the information. </paragraph>
<paragraph id="P-0032" lvl="0"><number>&lsqb;0032&rsqb;</number> In any event, the compiler converts source code into application object code, as <cross-reference target="DRAWINGS">FIG. 3</cross-reference> indicates, and places it in machine-readable storage such as RAM <highlight><bold>15</bold></highlight> or disk <highlight><bold>17</bold></highlight>. A computer will follow that object code&apos;s instructions in performing the thus-defined application <highlight><bold>38</bold></highlight>, which typically generates output from input. The compiler <highlight><bold>36</bold></highlight> can itself be thought of as an application, one in which the input is source code and the output is object code, but the computer that executes the application <highlight><bold>28</bold></highlight> is not necessarily the same as the one that executes the compiler application <highlight><bold>36</bold></highlight>. </paragraph>
<paragraph id="P-0033" lvl="0"><number>&lsqb;0033&rsqb;</number> The source code need not have been written by a human programmer directly. Integrated development environments often automate the source-code-writing process to the extent that for many applications very little of the source code is produced &ldquo;manually.&rdquo; As will be explained below, moreover, the &ldquo;source&rdquo; code being compiled may sometimes be low-level code, such as the byte-code input to the Java&trade; virtual machine, that programmers almost never write directly. (Sun, the Sun Logo, Sun Microsystems, and Java are trademarks or registered trademarks of Sun Microsystems, Inc., in the United States and other countries.) And, although <cross-reference target="DRAWINGS">FIG. 3</cross-reference> may appear to suggest a batch process, in which all of an application&apos;s object code is produced before any of it is executed, the same processor may both compile and execute the code, in which case the processor may execute its compiler application concurrently with-and, indeed, in a way that can depend upon-its execution of the compiler&apos;s output object code. </paragraph>
<paragraph id="P-0034" lvl="0"><number>&lsqb;0034&rsqb;</number> So the sequence of operations by which source code results in machine-language instructions may be considerably more complicated than one may infer from <cross-reference target="DRAWINGS">FIG. 3</cross-reference>. To give a sense of the complexity that can be involved, we discuss by reference to <cross-reference target="DRAWINGS">FIG. 4</cross-reference> an example of one way in which various levels of source code can result in the machine instructions that the processor executes. The human application programmer produces source code <highlight><bold>40</bold></highlight> written in a high-level language such as the Java programming language. In the case of the Java programming language, a compiler <highlight><bold>42</bold></highlight> converts that code into &ldquo;class files.&rdquo; These predominantly include routines written in instructions, called &ldquo;byte code&rdquo; <highlight><bold>44</bold></highlight>, for a &ldquo;virtual machine&rdquo; that various processors can be programmed to emulate. This conversion into byte code is almost always separated in time from that code&apos;s execution, so that aspect of the sequence is depicted as occurring in a &ldquo;compile-time environment&rdquo; <highlight><bold>46</bold></highlight> separate from a &ldquo;run-time environment&rdquo; <highlight><bold>48</bold></highlight>, in which execution occurs. </paragraph>
<paragraph id="P-0035" lvl="0"><number>&lsqb;0035&rsqb;</number> Most typically, a processor runs the class files&apos; instructions under the control of a virtual-machine program <highlight><bold>50</bold></highlight>, whose purpose is to emulate a machine from whose instruction set the byte codes are drawn. Much of the virtual machine&apos;s action in executing the byte code is most like what those skilled in the art refer to as &ldquo;interpreting,&rdquo; and <cross-reference target="DRAWINGS">FIG. 4</cross-reference> shows that the virtual machine includes an &ldquo;interpreter&rdquo; <highlight><bold>52</bold></highlight> for that purpose. The resultant instructions typically involve calls to a run-time system <highlight><bold>54</bold></highlight>, which handles matters such as loading new class files as they are needed. </paragraph>
<paragraph id="P-0036" lvl="0"><number>&lsqb;0036&rsqb;</number> Many virtual-machine implementations also actually compile the byte code concurrently with the resultant object code&apos;s execution, so <cross-reference target="DRAWINGS">FIG. 4</cross-reference> depicts the virtual machine as additionally including a &ldquo;just-in-time&rdquo; compiler <highlight><bold>56</bold></highlight>. It may be that the resultant object code will make low-level calls to the run-time system, as the drawing indicates. In any event, the code&apos;s execution will include calls to the local operating system <highlight><bold>58</bold></highlight>. </paragraph>
<paragraph id="P-0037" lvl="0"><number>&lsqb;0037&rsqb;</number> In addition to class-file loading, one of the functions that the runtime system performs is garbage collection. As was mentioned above, a Java-language programmer will not ordinarily have explicitly written the programming that performs this function; the compiler/interpreter provides it automatically in response to the programming that the user has written. In doing so, the compiler/interpreter may include parallel-execution operations, and it is by reference to such operations that we will illustrate the present invention&apos;s approach to performing parallel-execution operations. To aid that discussion, we digress to a brief review of garbage-collection nomenclature. </paragraph>
<paragraph id="P-0038" lvl="0"><number>&lsqb;0038&rsqb;</number> In the field of computer systems, considerable effort has been expended on the task of allocating memory to data objects. For the purposes of this discussion, the term object refers to a data structure represented in a computer system&apos;s memory. Other terms sometimes used for the same concept are record and structure. An object may be identified by a reference, a relatively small amount of information that can be used to access the object. A reference can be represented as a &ldquo;pointer&rdquo; or a &ldquo;machine address,&rdquo; which may require, for instance, only sixteen, thirty-two, or sixty-four bits of information, although there are other ways to represent a reference. </paragraph>
<paragraph id="P-0039" lvl="0"><number>&lsqb;0039&rsqb;</number> In some systems, which are usually known as &ldquo;object oriented,&rdquo; objects may have associated methods, which are routines that can be invoked by reference to the object. An object may belong to a class, which is an organizational entity that may contain method code or other information shared by all objects belonging to that class. The specific use employed below to exemplify implementing the present invention&apos;s teachings with reclaiming memory allocated to Java-language objects, which belong to such classes. </paragraph>
<paragraph id="P-0040" lvl="0"><number>&lsqb;0040&rsqb;</number> A modem program executing as a computer-system process often dynamically allocates storage for objects within a part of the process&apos;s memory commonly referred to as the &ldquo;heap.&rdquo; As was mentioned above, a garbage collector reclaims such objects when they are no longer reachable. </paragraph>
<paragraph id="P-0041" lvl="0"><number>&lsqb;0041&rsqb;</number> To distinguish the part of the program that does &ldquo;useful&rdquo; work from that which does the garbage collection, the term mutator is sometimes used; from the collector&apos;s point of view, what the mutator does is mutate active data structures&apos; connectivity. Some garbage-collection approaches rely heavily on interleaving garbage-collection steps among mutator steps. In one type of garbage-collection approach, for instance, the mutator operation of writing a reference is followed immediately by garbage-collector steps used to maintain a reference count in that object&apos;s header, and code for subsequent new-object allocation includes steps for finding space occupied by objects whose reference count has fallen to zero. Obviously, such an approach can slow mutator operation significantly. </paragraph>
<paragraph id="P-0042" lvl="0"><number>&lsqb;0042&rsqb;</number> Other, &ldquo;stop-the-world&rdquo; garbage-collection approaches use somewhat less interleaving. The mutator still typically allocates an object some space within the heap by invoking the garbage collector. The garbage collector keeps track of the fact that the thus-allocated space is occupied, and it refrains from allocating that space to other objects until it determines that the mutator no longer needs access to that object. But a stop-the-world collector performs its memory reclamation during garbage-collection cycles separate from the cycles in which the mutator runs. That is, the collector interrupts the mutator process, finds unreachable objects, reclaims their memory space for reuse, and then restarts the mutator. (The mutator may actually be a multi-threaded program running on multiple processors, so stopping it is nontrivial, but we do not deal here with the way in which it is stopped, since we are concerned here only with what happens after the stopping has occurred.) </paragraph>
<paragraph id="P-0043" lvl="0"><number>&lsqb;0043&rsqb;</number> To provide an example of a way in which the present invention&apos;s teachings can be applied, we assume a &ldquo;stop-the-world&rdquo; garbage collector and focus on the garbage-collection cycle. Since most of the specifics of a garbage-collection cycle are not of particular interest in the present context, <cross-reference target="DRAWINGS">FIG. 5</cross-reference> depicts only part of the cycle, and it depicts that part in a highly abstract manner. Its block <highlight><bold>60</bold></highlight> represents the start of the garbage-collection cycle, and its block <highlight><bold>62</bold></highlight> represents one of a number of the initial garbage-collection steps that are performed by a single thread only. </paragraph>
<paragraph id="P-0044" lvl="0"><number>&lsqb;0044&rsqb;</number> Eventually, the garbage collector reaches a part of its routine that can benefit from multi-threaded execution, and the virtual-machine programming calls upon the operating system to start a number of threads, as block <highlight><bold>64</bold></highlight> indicates, that will execute a subsequent code sequence in parallel. For the sake of example, we assume four threads. This would typically mean that the garbage collector is running in a multiprocessor system of at least that many processors, since the advantages of multithreading in an automatic-garbage-collection context are principally that different processors will at least sometimes execute different threads simultaneously. </paragraph>
<paragraph id="P-0045" lvl="0"><number>&lsqb;0045&rsqb;</number> Each of the threads executes an identical code sequence. The drawing depicts the code sequence somewhat arbitrarily as divided into a number of operations A, B, C, D, and E respectively represented by blocks <highlight><bold>66</bold></highlight>, <highlight><bold>68</bold></highlight>, <highlight><bold>70</bold></highlight>, <highlight><bold>72</bold></highlight>, and <highlight><bold>74</bold></highlight>. These operations&apos; specifics are not germane to the present discussion, but commonly assigned U.S. patent application Ser. No. 09/377,349, filed on Aug. 19, 1999, by Alexander T. Garthwaite for Popular-Object Handling in a Train-Algorithm-Based Garbage Collector and hereby incorporated by reference, gives examples of the types of garbage-collection operations that blocks <highlight><bold>66</bold></highlight>, <highlight><bold>68</bold></highlight>, <highlight><bold>70</bold></highlight>, <highlight><bold>72</bold></highlight>, and <highlight><bold>74</bold></highlight> may include. </paragraph>
<paragraph id="P-0046" lvl="0"><number>&lsqb;0046&rsqb;</number> Although all threads execute the same code sequence, some of the code&apos;s routines take the thread&apos;s identity as an argument, and some of the data that an instruction processes may change between that instruction&apos;s executions by different threads. These factors, together with hardware differences and the vagaries of thread scheduling, result in different threads&apos; completing different operations at different times even in the absence of the dynamic task identification. For the sake of example, though, we assume that there is a point in the routine beyond which execution should not proceed until all threads have reached it, so the drawing includes a block <highlight><bold>76</bold></highlight> to represent a &ldquo;join&rdquo; mechanism for imposing this requirement. It is only after all threads reach the join point that further execution of the garbage-collection cycle can proceed. </paragraph>
<paragraph id="P-0047" lvl="0"><number>&lsqb;0047&rsqb;</number> As was mentioned above, the present invention&apos;s advantages are manifest when the parallel-execution operation includes dynamically identified tasks. So we will assume that <cross-reference target="DRAWINGS">FIG. 5</cross-reference>&apos;s operation B involves essentially only statically identifiable tasks, whereas operation C&apos;s involve tasks principally identifiable only dynamically. For example, assume that operation B involves processing the root set to find reachable objects. The root set may be divided into groups, and different threads may claim different groups to process. Since those tasks&apos; identities are known at the outset, the present invention&apos;s teachings are not needed to assign them to threads. </paragraph>
<paragraph id="P-0048" lvl="0"><number>&lsqb;0048&rsqb;</number> By performing those tasks, though, a garbage-collection thread dynamically identifies further tasks to perform: when operation B identifies an object referred to by the root set, it has also identified the task of following the references in the thus-identified object to find further roots. We will assume that operation C involves processing the reachable objects thus identified, so its tasks are identifiable only dynamically: since it is only by performing one of the tasks that further tasks are identified, the tasks are not known at the beginning of the operation. Because of the task-identification process&apos;s dynamic nature, operation C would be particularly vulnerable to a work imbalance among the threads if each thread performed only those tasks that it had originally identified itself. So the computer system instead enables different threads to perform tasks that other threads have identified. </paragraph>
<paragraph id="P-0049" lvl="0"><number>&lsqb;0049&rsqb;</number> To illustrate how this is accomplished in accordance with the present invention, let us assume that the computer system is a multiprocessor system that includes four separate processors P<highlight><bold>0</bold></highlight>, P<highlight><bold>1</bold></highlight>, P<highlight><bold>2</bold></highlight>, and P<highlight><bold>3</bold></highlight>, on each of which a separate thread executes. In the embodiment that <cross-reference target="DRAWINGS">FIG. 6</cross-reference> illustrates, the computer system keeps track of the identified tasks by employing a two-dimensional array <highlight><bold>80</bold></highlight> of task queues. Each task queue is of the producer/consumer, &ldquo;Lamport queue&rdquo; type (after the type of queue described in Lamport, &ldquo;Proving the Correctness of Multiprocess Programs,&rdquo; <highlight><italic>IEEE Transactions on Software Engineering</italic></highlight>, vol. SE-3, no. 2, March 1977). Each is associated with an ordered pair of threads, the first thread in the ordered pair being called the &ldquo;dequeuer,&rdquo; the second being called the &ldquo;enqueuer.&rdquo; The task queues contain identifiers of tasks to be performed. In this case, the tasks are to process Java-language objects in order to, among other things, find any further objects to which those objects contain references, and <cross-reference target="DRAWINGS">FIG. 6</cross-reference> represents the entries for the sake of example as pointers to pointers to such objects. </paragraph>
<paragraph id="P-0050" lvl="0"><number>&lsqb;0050&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 6</cross-reference> labels each of the task queues as an array element pcb&lsqb;m,n&rsqb;, where m is the number of its dequeuer thread and n is the number of its enqueuer thread. The only thread that pushes task identifiers onto any given task queue is that queue&apos;s enqueuer thread, while the only thread that pops task identifiers from it is its dequeuer thread. For example, each task queue in the left column receives task identifiers only from the thread that executes on processor P<highlight><bold>0</bold></highlight>, while only the thread that executes on processor P<highlight><bold>2</bold></highlight> pops task identifiers from the queues in the array&apos;s third row. </paragraph>
<paragraph id="P-0051" lvl="0"><number>&lsqb;0051&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 6</cross-reference> illustrates this with a scenario in which the thread performed by processor P<highlight><bold>0</bold></highlight> pushes the identifiers of objects A<highlight><bold>0</bold></highlight>-A<highlight><bold>9</bold></highlight> onto successive ones of the first-column queues, of which it is the enqueuer. The thread that processor P<highlight><bold>1</bold></highlight> executes pushes the identifiers of objects B<highlight><bold>0</bold></highlight>-B<highlight><bold>3</bold></highlight> in a similar round-robin order onto the second-column queues. The threads executed by processors P<highlight><bold>2</bold></highlight> and P<highlight><bold>3</bold></highlight> similarly push object identifiers onto the third- and fourth-column queues. </paragraph>
<paragraph id="P-0052" lvl="0"><number>&lsqb;0052&rsqb;</number> The thread that processor P<highlight><bold>0</bold></highlight> executes then pops the identifiers of objects A<highlight><bold>0</bold></highlight>, B<highlight><bold>0</bold></highlight>, C<highlight><bold>0</bold></highlight>, D<highlight><bold>0</bold></highlight>, A<highlight><bold>4</bold></highlight>, C<highlight><bold>4</bold></highlight>, D<highlight><bold>4</bold></highlight>, A<highlight><bold>8</bold></highlight>, and C<highlight><bold>8</bold></highlight> in that order. The thread that processor P<highlight><bold>1</bold></highlight> executes pops the identifiers of objects A<highlight><bold>1</bold></highlight> B<highlight><bold>1</bold></highlight>, C<highlight><bold>1</bold></highlight>, D<highlight><bold>1</bold></highlight>, A<highlight><bold>5</bold></highlight>, C<highlight><bold>5</bold></highlight>, D<highlight><bold>5</bold></highlight>, A<highlight><bold>9</bold></highlight>, and C<highlight><bold>9</bold></highlight>. The thread that processor P<highlight><bold>2</bold></highlight> executes pops the identifiers of objects A<highlight><bold>2</bold></highlight>, B<highlight><bold>2</bold></highlight>, C<highlight><bold>2</bold></highlight>, D<highlight><bold>2</bold></highlight>, A<highlight><bold>6</bold></highlight>, C<highlight><bold>6</bold></highlight>, and D<highlight><bold>6</bold></highlight>. And the thread executed by processor P<highlight><bold>3</bold></highlight> pops the identifiers of objects A<highlight><bold>3</bold></highlight>, B<highlight><bold>3</bold></highlight>, C<highlight><bold>3</bold></highlight>, D<highlight><bold>3</bold></highlight>, A<highlight><bold>7</bold></highlight>, C<highlight><bold>7</bold></highlight>, and D<highlight><bold>7</bold></highlight>. </paragraph>
<paragraph id="P-0053" lvl="0"><number>&lsqb;0053&rsqb;</number> As <cross-reference target="DRAWINGS">FIG. 6</cross-reference> illustrates, this approach results in a relatively even distribution of tasks among threads even though there is a wide variation among the threads in the numbers of tasks that they have identified. Moreover, since each task queue has only one enqueuer and only one dequeuer, this approach avoids the expensive use of atomic operations that the need for thread safety could otherwise require. To show this, we start with <cross-reference target="DRAWINGS">FIG. 7</cross-reference>, which is a listing of a type definition and three of the routines employed by a garbage-collection thread in pushing a task identifier onto a queue in <cross-reference target="DRAWINGS">FIG. 6</cross-reference>&apos;s array <highlight><bold>80</bold></highlight>. </paragraph>
<paragraph id="P-0054" lvl="0"><number>&lsqb;0054&rsqb;</number> The first routine, enque( ), receives as one parameter the identifier of a Java-language object identified as being reachable and thus requiring processing in order to determine whether it contains references to further objects, which would thereby also be reachable. That routine also receives as a parameter a pointer to the &ldquo;execution environment&rdquo; of the thread that has identified that object as reachable and thus as requiring processing to determine whether it contains references to other objects. The execution environment is a data structure, of type ParallelThread, whose data members represent various parameters associated with an execution thread. <cross-reference target="DRAWINGS">FIG. 8</cross-reference> depicts four such structures <highlight><bold>82</bold></highlight>, one for each of four garbage-collection threads. </paragraph>
<paragraph id="P-0055" lvl="0"><number>&lsqb;0055&rsqb;</number> As the enque( ) listing&apos;s second line indicates, that routine reads from the executing thread&apos;s execution environment <highlight><bold>82</bold></highlight> (<cross-reference target="DRAWINGS">FIG. 8</cross-reference>) a &ldquo;number&rdquo; parameter, which is an index that identifies the thread associated with that execution environment. As that listing&apos;s third line indicates, it also obtains from the executing thread&apos;s execution environment the value of a pointer to a data structure <highlight><bold>84</bold></highlight> (<cross-reference target="DRAWINGS">FIG. 8</cross-reference>) that contains parameters related to that thread&apos;s use of <cross-reference target="DRAWINGS">FIG. 6</cross-reference>&apos;s task-queue array <highlight><bold>80</bold></highlight>. Among those parameters is &ldquo;nextpush.&rdquo; Of the queues for which the executing thread is the enqueuer, the &ldquo;nextpush&rdquo; value identifies the one that is to be the next in the round-robin sequence of queues on which the executing thread is to push a task identifier. </paragraph>
<paragraph id="P-0056" lvl="0"><number>&lsqb;0056&rsqb;</number> The enque( ) routine&apos;s fourth line represents calling a PCBfull( ) routine to determine whether that next queue is full. The determination of whether that queue is full is relatively simple. As <cross-reference target="DRAWINGS">FIG. 6</cross-reference> shows, each of the queues in <cross-reference target="DRAWINGS">FIG. 6</cross-reference>&apos;s array <highlight><bold>80</bold></highlight> of task queues is the buff&lsqb; &rsqb; member of a structure that also includes &ldquo;in&rdquo; and &ldquo;out&rdquo; members, which respectively specify the locations in that array onto which and from which the next items are respectively to be pushed and popped. As the PCBfull( ) routine&apos;s listing (<cross-reference target="DRAWINGS">FIG. 7</cross-reference>) indicates, that routine determines whether that task queue is full simply by comparing the &ldquo;in&rdquo; and &ldquo;out&rdquo; members to determine whether the former is (circularly) about to overtake the latter. To prevent an empty queue from being confused with a full one, the illustrated embodiment considers a queue to be full if it has (only) one or two empty slots. </paragraph>
<paragraph id="P-0057" lvl="0"><number>&lsqb;0057&rsqb;</number> As the enque( ) routine&apos;s fourth through sixth lines indicate, that routine advances circularly through the queues of which the executing thread is the enqueuer until it finds one that is not full. As the seventh line indicates, it then calls <cross-reference target="DRAWINGS">FIG. 7</cross-reference>&apos;s PCBpush( ) routine. That routine&apos;s listing shows that it places the object&apos;s identifier into the queue location represented by that queue&apos;s &ldquo;in&rdquo; value, which is then advanced circularly. Having thus pushed an entry onto one of the queues of which the executing thread is the enqueuer, the enque( ) routine then circularly advances the executing thread&apos;s &ldquo;nextpush&rdquo; value, as its eighth line indicates, so that its next push attempt will occur at the next one of the task queues of which the executing thread is the enqueuer. </paragraph>
<paragraph id="P-0058" lvl="0"><number>&lsqb;0058&rsqb;</number> This completes the operation of adding a task identifier to the task-queue array. Note that the sample code imposes no requirement for atomically performing sets of machine instructions that ordinarily are performed separately. We discuss why that is after we review <cross-reference target="DRAWINGS">FIG. 9</cross-reference>&apos;s deque( ) routine. </paragraph>
<paragraph id="P-0059" lvl="0"><number>&lsqb;0059&rsqb;</number> The deque( ) routine is the one that a garbage-collection thread employs to pop a task identifier from <cross-reference target="DRAWINGS">FIG. 6</cross-reference>&apos;s task-queue array <highlight><bold>80</bold></highlight>. The third line in that routine&apos;s listing represents copying from a field in the executing thread&apos;s execution environment <highlight><bold>82</bold></highlight> (<cross-reference target="DRAWINGS">FIG. 8</cross-reference>) a pointer to a data structure <highlight><bold>84</bold></highlight> that contains parameters associated with that thread&apos;s use of <cross-reference target="DRAWINGS">FIG. 6</cross-reference>&apos;s task-queue array <highlight><bold>80</bold></highlight>. Its sixth line represents obtaining the value of that structure&apos;s &ldquo;nextpop&rdquo; field, using it to select one of the queues for which the executing thread is the dequeuer, and determining whether that queue is empty. </paragraph>
<paragraph id="P-0060" lvl="0"><number>&lsqb;0060&rsqb;</number> As <cross-reference target="DRAWINGS">FIG. 9</cross-reference>&apos;s PCBempty( ) routine indicates, that determination is made by simply comparing the encompassing PCB structure&apos;s &ldquo;in&rdquo; value with its &ldquo;out&rdquo; value. If the task queue thus selected is not empty, the deque( ) routine skips the block represented by its sixth through twenty-first lines and performs its twenty-second line&apos;s step. In that step it determines whether there is an inactivity-indicating value in the &ldquo;status&rdquo; field of <cross-reference target="DRAWINGS">FIG. 8</cross-reference>&apos;s data structure <highlight><bold>84</bold></highlight> associated with the executing thread. Ordinarily, that field instead indicates an active value. So, as the twenty-sixth through twenty-eighth lines indicate, the deque( ) routine in most cases simply returns the object identifier produced by performing <cross-reference target="DRAWINGS">FIG. 9</cross-reference>&apos;s PCBpop( ) routine and circularly incrementing the &ldquo;nextpop&rdquo; value. The PCBpop( ) routine merely retrieves the object identifier contained in the task-queue location specified by its &ldquo;out&rdquo; field and circularly advances that field&apos;s value. </paragraph>
<paragraph id="P-0061" lvl="0"><number>&lsqb;0061&rsqb;</number> Now, as was stated above, only one dequeuer thread is associated with each task queue&mdash;i.e., no other thread will perform the deque( ) routine on that same task queue&mdash;so no other thread can modify that task queue&apos;s &ldquo;out&rdquo; field. As was also stated above, though, there is one other thread, namely, that task queue&apos;s enqueuer thread, that also has access to that task queue&apos;s &ldquo;out&rdquo; field: by performing the PCBfull( ) routine (<cross-reference target="DRAWINGS">FIG. 7</cross-reference>) on that task queue, the enqueuer thread compares that field&apos;s contents with the contents of that task queue&apos;s &ldquo;in&rdquo; field to determine whether the queue is full and the enqueuer should therefore refrain from adding an entry to it. Thus basing a decision on a comparison of fields to which different threads have access would ordinarily necessitate making the comparison atomic with the loading of the values to be compared. </paragraph>
<paragraph id="P-0062" lvl="0"><number>&lsqb;0062&rsqb;</number> But the present invention makes this unnecessary. In the first place, since the task queue&apos;s enqueuer thread is the only one that can modify that queue&apos;s &ldquo;in&rdquo; field, the thread performing the enque( ) routine&mdash;i.e., the task queue&apos;s sole enqueuer thread&mdash;always &ldquo;knows&rdquo; what the current contents of that task queue&apos;s &ldquo;in&rdquo; field are. This does not mean that the outcome of the PCBfull( ) routine always correctly indicates whether the task queue is full; in popping a task from that task queue, that dequeuer of task queue could have removed a task between the enqueuer&apos;s reading of the &ldquo;out&rdquo; value and the resultant comparison with the &ldquo;in&rdquo; value in the course of performing the PCBfull( ) routine. But the resultant error would be harmless; it would only cause the enqueuer to look for a different queue onto which to push the task identifier. </paragraph>
<paragraph id="P-0063" lvl="0"><number>&lsqb;0063&rsqb;</number> There is a similar lack of harm in the deque( ) routine resulting from the fact that the PCBempty( ) routine&apos;s loading of the &ldquo;in&rdquo; value is not atomic with its use of that value in a comparison. The possible erroneous conclusion would only be that the queue is empty when it is not, and such a conclusion would just cause the dequeuer thread to search for a task in the next task queue of which that thread is a dequeuer. So the present invention affords concurrent task-queue access without requiring an atomic operation. It therefore greatly reduces the cost that sharing work among threads would otherwise impose. </paragraph>
<paragraph id="P-0064" lvl="0"><number>&lsqb;0064&rsqb;</number> Now, there are circumstances in which <cross-reference target="DRAWINGS">FIG. 9</cross-reference>&apos;s deque( ) routine does necessitate atomic operations. As will now be explained, though, the atomic operations occur only because of the particular termination-detection approach that the illustrated embodiment employs. And a dequeuing thread reaches the part of the deque( ) routine in which the atomic-operation use occurs only, after repeated failures to find a queue in which any task identifier remains, it needs to determine whether all of the parallel-execution operation&apos;s tasks have already been completed. </paragraph>
<paragraph id="P-0065" lvl="0"><number>&lsqb;0065&rsqb;</number> The block represented by the deque( ) routine&apos;s sixth through twenty-first lines is repeated until the deque( ) routine either finds a task queue in which task identifiers remain or determines that the parallel-execution operation has been completed. As the sixth and seventh lines indicate, the thread circularly increments its &ldquo;nextpop&rdquo; field&apos;s value if the task queue specified by that field&apos;s current value is empty. As will be apparent shortly, this ordinarily results in the thread&apos;s looping to determine whether the task queue specified by the now-incremented &ldquo;nextpop&rdquo; value is empty, too. </paragraph>
<paragraph id="P-0066" lvl="0"><number>&lsqb;0066&rsqb;</number> But the routine needs to prevent a situation in which it loops endlessly because all the task queues for which the executing thread is the dequeuer are empty. To this end, it performs the eighth-line step of incrementing the value in the &ldquo;empty_count&rdquo; field of <cross-reference target="DRAWINGS">FIG. 8</cross-reference>&apos;s data structure <highlight><bold>84</bold></highlight> associated with the executing thread. (This value will have been initialized to a value of zero at the beginning of the garbage-collection cycle.) If, as tested in the ninth-line step, the thus-incremented &ldquo;empty_count&rdquo; value has not exceeded a constant &ldquo;term_limit&rdquo; value-which was set in the fifth-line step to twice the number of garbage-collection threads-then the routine returns to the sixth-line step, in which it determines whether the next task queue is empty. </paragraph>
<paragraph id="P-0067" lvl="0"><number>&lsqb;0067&rsqb;</number> If the &ldquo;empty_count&rdquo; value reaches the &ldquo;term limit&rdquo; value, on the other hand, then the routine embarks on a determination of whether the entire parallel-execution operation is actually complete. This happens after deque( ) has cycled twice through all of the task queues of which the executing thread is a dequeuer and found those task queues empty in every case. When this happens, the executing thread performs the tenth-line step of setting to an inactivity-indicating value the &ldquo;status&rdquo; field of <cross-reference target="DRAWINGS">FIG. 8</cross-reference>&apos;s data structure <highlight><bold>84</bold></highlight> corresponding to the executing thread. In the block represented by the eleventh through fourteenth lines, though, it returns that field to an activity-indicating value if any queue of which the executing thread is an enqueuer is not empty. </paragraph>
<paragraph id="P-0068" lvl="0"><number>&lsqb;0068&rsqb;</number> As the fifteenth and sixteenth lines indicate, it is only if that &ldquo;status&rdquo; field still has an inactivity-indicating value after that block&apos;s execution that the system resorts to an atomic operation. Specifically, it performs the sixteenth line&apos;s step of calling <cross-reference target="DRAWINGS">FIG. 10</cross-reference>&apos;s mark_self_inactive( ) routine. It passes that routine the executing thread&apos;s identifying index and a pointer to a common status word, &ldquo;statusBitmap,&rdquo; to which all of the threads are accorded access. The &ldquo;statusBitmap&rdquo; variable can be thought of as a bit field in which different bits correspond to the different simultaneously executing garbage-collection threads. Together these bits represent the activity status of all the garbage-collection threads. The purpose of the mark_self_inactive( ) routine is to reset to zero the bit in &ldquo;statusBitmap&rdquo; that corresponds to the executing thread. To do this, that routine initially copies the value of &ldquo;statusBitmap&rdquo; into a local &ldquo;oldValue&rdquo; variable, as its listing&apos;s fourth line indicates. The listing&apos;s fifth line represents the step of setting the contents of a &ldquo;newValue&rdquo; variable to the result of resetting to zero the bit of &ldquo;oldValue&rdquo; that corresponds to the executing thread. </paragraph>
<paragraph id="P-0069" lvl="0"><number>&lsqb;0069&rsqb;</number> Now, simply writing the value of this &ldquo;newValue&rdquo; variable back into the common &ldquo;statusBitmap&rdquo; location can have undesirable consequences. If some other thread has changed its own bit in that location between the executing thread&apos;s reading of that value and its writing a changed value into it, the executing thread&apos;s writing of the changed value into the common status word would not only change the executing thread&apos;s own bit but also, undesirably, return the other thread&apos;s bit back to its previous value. So the mark_self_inactive( ) routine calls a casInt( ) routine, as the mark_self_inactive( ) routine&apos;s sixth line indicates, to avoid such an untoward result. </paragraph>
<paragraph id="P-0070" lvl="0"><number>&lsqb;0070&rsqb;</number> The casInt( ) routine is an atomic compare-and-swap operation. A compare-and swap operation reads a target location&apos;s value and compares that value with an &ldquo;old value&rdquo; parameter. If they are the same, it swaps the contents of the target location (in this case, the &ldquo;statusBitmap&rdquo; location) with those of its &ldquo;new value&rdquo; parameter. Otherwise, it does not perform the swap. In this case, in other words, the casInt( ) routine does not change the common status word unless, when that atomic operation starts, the contents of the status word have not changed since the mark_self_inactive( ) routine initially read them in the process of creating the value of its &ldquo;oldValue&rdquo; parameter. As that routine&apos;s third through seventh lines indicate, the reading of the status word and the attempt to change it in an atomic operation keep repeating until the seventh line&apos;s step determines that the atomic operation has been completed successfully. </paragraph>
<paragraph id="P-0071" lvl="0"><number>&lsqb;0071&rsqb;</number> As the seventeenth and eighteenth lines of <cross-reference target="DRAWINGS">FIG. 9</cross-reference>&apos;s deque( ) routine indicate, that routine returns a NULL value, indicating that the parallel-execution operation has been completed, if all threads have thus set their respective status-word bits to zeros. The deque( ) routine&apos;s caller concludes from a NULL return value that the parallel-execution operation has been completed, and it can therefore move on to further operations. If not all of StatusBitmap&apos;s bits are zero, on the other hand, the deque( ) routine resets to zero the &ldquo;empty_count&rdquo; field in <cross-reference target="DRAWINGS">FIG. 8</cross-reference>&apos;s data structure <highlight><bold>84</bold></highlight> corresponding to the executing thread. The routine then returns to cycling through the task queues of which the executing thread is the dequeuer. The cycling continues until that routine finds a non-empty task queue or again interrupts itself to determine whether the parallel-execution operation has in fact been completed. </paragraph>
<paragraph id="P-0072" lvl="0"><number>&lsqb;0072&rsqb;</number> If, during this process, the routine does find a task queue that is not empty, it escapes the loop of the sixth through twenty-first lines. This results in its executing the block set forth in the twenty-second through twenty-fifth lines. In that block, if the executing thread&apos;s bit in StatusBitmap is zero (as indicated by an inactivity-indicating value of the &ldquo;status&rdquo; field in the executing thread&apos;s data structure <highlight><bold>84</bold></highlight> in <cross-reference target="DRAWINGS">FIG. 8</cross-reference>), it sets that bit to a one value by calling mark_self_active( ) routine, as the twenty-fourth line indicates. </paragraph>
<paragraph id="P-0073" lvl="0"><number>&lsqb;0073&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 10</cross-reference> shows that this routine is essentially the same as the mark_self_inactive( ) routine, with the exception that it sets the corresponding value to one rather than to zero. So that routine, too, requires an atomic operation. Again, though, this atomic operation is encountered only as part of the particular termination-detection approach that the illustrated embodiment employs; it is not necessitated by different threads&apos; access to common task queue. </paragraph>
<paragraph id="P-0074" lvl="0"><number>&lsqb;0074&rsqb;</number> So the present invention enables different threads to share dynamically identified tasks optimally without, in most cases, exacting the performance cost that the use of atomic operations can impose. It thus constitutes a significant advance in the art. </paragraph>
</section>
</detailed-description>
</subdoc-description>
<subdoc-claims>
<claim id="CLM-00001">
<claim-text><highlight><bold>1</bold></highlight>. A computer system that employs a plurality of execution threads to perform tasks that the threads identify dynamically, the computer system being so programmed as to: 
<claim-text>A) provide a plurality of task queues, each of which is associated with a different ordered pair of the threads, one thread of the ordered pair being denominated the enqueuer of that queue and the other being denominated the dequeuer thereof; </claim-text>
<claim-text>B) when one said thread identifies a task, pushes an identifier of that the task thus identified onto a set of at least one of the queues of which that thread is an enqueuer; and </claim-text>
<claim-text>C) when one said thread requires one of the dynamically identified tasks to perform, causes that thread to perform a task identified by a task identifier fetched by that thread from a task queue of which that thread is the dequeuer. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00002">
<claim-text><highlight><bold>2</bold></highlight>. A computer system as defined in <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference> wherein each said dynamically identified task is the garbage-collection task of performing, for a given object associated with that task, processing that includes identifying in the given object references to other objects and thereby identifying the tasks of performing similar processing for those other objects. </claim-text>
</claim>
<claim id="CLM-00003">
<claim-text><highlight><bold>3</bold></highlight>. A computer system as defined in <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference> wherein each said task identifier is an identifier of the object with which the task is associated. </claim-text>
</claim>
<claim id="CLM-00004">
<claim-text><highlight><bold>4</bold></highlight>. A computer system as defined in <dependent-claim-reference depends_on="CLM-00003">claim 3</dependent-claim-reference> wherein each said dynamically identified task is the garbage-collection task of performing, for a given object associated with that task, processing that includes identifying in the given object references to other objects and thereby identifying the tasks of performing similar processing for those other objects. </claim-text>
</claim>
<claim id="CLM-00005">
<claim-text><highlight><bold>5</bold></highlight>. A computer system as defined in <dependent-claim-reference depends_on="CLM-00003">claim 3</dependent-claim-reference> wherein each said task identifier is a pointer to the object with which the task is associated </claim-text>
</claim>
<claim id="CLM-00006">
<claim-text><highlight><bold>6</bold></highlight>. A computer system as defined in <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference> wherein, when one said thread identifies a task, the computer system pushes an identifier of that thread onto only one of the queues of which that thread is an enqueuer. </claim-text>
</claim>
<claim id="CLM-00007">
<claim-text><highlight><bold>7</bold></highlight>. A computer system as defined in <dependent-claim-reference depends_on="CLM-00006">claim 6</dependent-claim-reference> wherein each said dynamically identified task is the garbage-collection task of performing, for a given object associated with that task, processing that includes identifying in the given object references to other objects and thereby identifying the tasks of performing similar processing for those other objects. </claim-text>
</claim>
<claim id="CLM-00008">
<claim-text><highlight><bold>8</bold></highlight>. A computer system as defined in <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference> wherein identifiers of tasks successively identified by a given thread are not in general pushed onto the same queue. </claim-text>
</claim>
<claim id="CLM-00009">
<claim-text><highlight><bold>9</bold></highlight>. A computer system as defined in <dependent-claim-reference depends_on="CLM-00008">claim 8</dependent-claim-reference> wherein each said dynamically identified task is the garbage-collection task of performing, for a given object associated with that task, processing that includes identifying in the given object references to other objects and thereby identifying the tasks of performing similar processing for those other objects. </claim-text>
</claim>
<claim id="CLM-00010">
<claim-text><highlight><bold>10</bold></highlight>. A computer system as defined in <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference> wherein a task queue is provided for each ordered pair of the threads. </claim-text>
</claim>
<claim id="CLM-00011">
<claim-text><highlight><bold>11</bold></highlight>. A computer system as defined in <dependent-claim-reference depends_on="CLM-00011">claim 10</dependent-claim-reference> wherein each said dynamically identified task is the garbage-collection task of performing, for a given object associated with that task, processing that includes identifying in the given object references to other objects and thereby identifying the tasks of performing similar processing for those other objects. </claim-text>
</claim>
<claim id="CLM-00012">
<claim-text><highlight><bold>12</bold></highlight>. For using a computer system to employ a plurality of execution threads to perform tasks that the threads identify dynamically, a method that includes: 
<claim-text>A) providing a plurality of task queues, each of which is associated with a different ordered pair of the threads, one thread of the ordered pair being denominated the enqueuer of that queue and the other being denominated the dequeuer thereof; </claim-text>
<claim-text>B) when one said thread identifies a task, pushing an identifier of that the task thus identified onto a set of at least one of the queues of which that thread is an enqueuer; and </claim-text>
<claim-text>C) when one said thread requires one of the dynamically identified tasks to perform, causing that thread to perform a task identified by a task identifier fetched by that thread from a task queue of which that thread is the dequeuer. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00013">
<claim-text><highlight><bold>13</bold></highlight>. A method as defined in <dependent-claim-reference depends_on="CLM-00011">claim 12</dependent-claim-reference> wherein each said dynamically identified task is the garbage-collection task of performing, for a given object associated with that task, processing that includes identifying in the given object references to other objects and thereby identifying the tasks of performing similar processing for those other objects. </claim-text>
</claim>
<claim id="CLM-00014">
<claim-text><highlight><bold>14</bold></highlight>. A method as defined in <dependent-claim-reference depends_on="CLM-00011">claim 12</dependent-claim-reference> wherein each said task identifier is an identifier of the object with which the task is associated. </claim-text>
</claim>
<claim id="CLM-00015">
<claim-text><highlight><bold>15</bold></highlight>. A method as defined in <dependent-claim-reference depends_on="CLM-00011">claim 14</dependent-claim-reference> wherein each said dynamically identified task is the garbage-collection task of performing, for a given object associated with that task, processing that includes identifying in the given object references to other objects and thereby identifying the tasks of performing similar processing for those other objects. </claim-text>
</claim>
<claim id="CLM-00016">
<claim-text><highlight><bold>16</bold></highlight>. A method as defined in <dependent-claim-reference depends_on="CLM-00011">claim 14</dependent-claim-reference> wherein each said task identifier is a pointer to the object with which the task is associated </claim-text>
</claim>
<claim id="CLM-00017">
<claim-text><highlight><bold>17</bold></highlight>. A method as defined in <dependent-claim-reference depends_on="CLM-00011">claim 12</dependent-claim-reference> wherein, when one said thread identifies a task, the computer system pushes an identifier of that thread onto only one of the queues of which that thread is an enqueuer. </claim-text>
</claim>
<claim id="CLM-00018">
<claim-text><highlight><bold>18</bold></highlight>. A method as defined in <dependent-claim-reference depends_on="CLM-00011">claim 17</dependent-claim-reference> wherein each said dynamically identified task is the garbage-collection task of performing, for a given object associated with that task, processing that includes identifying in the given object references to other objects and thereby identifying the tasks of performing similar processing for those other objects. </claim-text>
</claim>
<claim id="CLM-00019">
<claim-text><highlight><bold>19</bold></highlight>. A method as defined in <dependent-claim-reference depends_on="CLM-00011">claim 12</dependent-claim-reference> wherein identifiers of tasks successively identified by a given thread are not in general pushed onto the same queue. </claim-text>
</claim>
<claim id="CLM-00020">
<claim-text><highlight><bold>20</bold></highlight>. A method as defined in <dependent-claim-reference depends_on="CLM-00011">claim 19</dependent-claim-reference> wherein each said dynamically identified task is the garbage-collection task of performing, for a given object associated with that task, processing that includes identifying in the given object references to other objects and thereby identifying the tasks of performing similar processing for those other objects. </claim-text>
</claim>
<claim id="CLM-00021">
<claim-text><highlight><bold>21</bold></highlight>. A method as defined in <dependent-claim-reference depends_on="CLM-00011">claim 12</dependent-claim-reference> wherein a task queue is provided for each ordered pair of the threads. </claim-text>
</claim>
<claim id="CLM-00022">
<claim-text><highlight><bold>22</bold></highlight>. A method as defined in <dependent-claim-reference depends_on="CLM-00022">claim 21</dependent-claim-reference> wherein each said dynamically identified task is the garbage-collection task of performing, for a given object associated with that task, processing that includes identifying in the given object references to other objects and thereby identifying the tasks of performing similar processing for those other objects. </claim-text>
</claim>
<claim id="CLM-00023">
<claim-text><highlight><bold>23</bold></highlight>. A storage medium containing instructions readable by a computer system to configure the computer system to employ a plurality of execution threads to perform dynamically identified tasks by: 
<claim-text>A) providing a plurality of task queues, each of which is associated with a different ordered pair of the threads, one thread of the ordered pair being denominated the enqueuer of that queue and the other being denominated the dequeuer thereof, </claim-text>
<claim-text>B) when one said thread identifies a task, pushing an identifier of that the task thus identified onto a set of at least one of the queues of which that thread is an enqueuer; and </claim-text>
<claim-text>C) when one said thread requires one of the dynamically identified tasks to perform, causing that thread to perform a task identified by a task identifier fetched by that thread from a task queue of which that thread is the dequeuer. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00024">
<claim-text><highlight><bold>24</bold></highlight>. A storage medium as defined in <dependent-claim-reference depends_on="CLM-00022">claim 23</dependent-claim-reference> wherein each said dynamically identified task is the garbage-collection task of performing, for a given object associated with that task, processing that includes identifying in the given object references to other objects and thereby identifying the tasks of performing similar processing for those other objects. </claim-text>
</claim>
<claim id="CLM-00025">
<claim-text><highlight><bold>25</bold></highlight>. A storage medium as defined in <dependent-claim-reference depends_on="CLM-00022">claim 23</dependent-claim-reference> wherein each said task identifier is an identifier of the object with which the task is associated. </claim-text>
</claim>
<claim id="CLM-00026">
<claim-text><highlight><bold>26</bold></highlight>. A storage medium as defined in <dependent-claim-reference depends_on="CLM-00022">claim 25</dependent-claim-reference> wherein each said dynamically identified task is the garbage-collection task of performing, for a given object associated with that task, processing that includes identifying in the given object references to other objects and thereby identifying the tasks of performing similar processing for those other objects. </claim-text>
</claim>
<claim id="CLM-00027">
<claim-text><highlight><bold>27</bold></highlight>. A storage medium as defined in <dependent-claim-reference depends_on="CLM-00022">claim 25</dependent-claim-reference> wherein each said task identifier is a pointer to the object with which the task is associated </claim-text>
</claim>
<claim id="CLM-00028">
<claim-text><highlight><bold>28</bold></highlight>. A storage medium as defined in <dependent-claim-reference depends_on="CLM-00022">claim 23</dependent-claim-reference> wherein, when one said thread identifies a task, the computer system pushes an identifier of that thread onto only one of the queues of which that thread is an enqueuer. </claim-text>
</claim>
<claim id="CLM-00029">
<claim-text><highlight><bold>29</bold></highlight>. A storage medium as defined in <dependent-claim-reference depends_on="CLM-00022">claim 28</dependent-claim-reference> wherein each said dynamically identified task is the garbage-collection task of performing, for a given object associated with that task, processing that includes identifying in the given object references to other objects and thereby identifying the tasks of performing similar processing for those other objects. </claim-text>
</claim>
<claim id="CLM-00030">
<claim-text><highlight><bold>30</bold></highlight>. A storage medium as defined in <dependent-claim-reference depends_on="CLM-00022">claim 23</dependent-claim-reference> wherein identifiers of tasks successively identified by a given thread are not in general pushed onto the same queue. </claim-text>
</claim>
<claim id="CLM-00031">
<claim-text><highlight><bold>31</bold></highlight>. A storage medium as defined in <dependent-claim-reference depends_on="CLM-00033">claim 30</dependent-claim-reference> wherein each said dynamically identified task is the garbage-collection task of performing, for a given object associated with that task, processing that includes identifying in the given object references to other objects and thereby identifying the tasks of performing similar processing for those other objects. </claim-text>
</claim>
<claim id="CLM-00032">
<claim-text><highlight><bold>32</bold></highlight>. A storage medium as defined in <dependent-claim-reference depends_on="CLM-00022">claim 23</dependent-claim-reference> wherein a task queue is provided for each ordered pair of the threads. </claim-text>
</claim>
<claim id="CLM-00033">
<claim-text><highlight><bold>33</bold></highlight>. A storage medium as defined in <dependent-claim-reference depends_on="CLM-00033">claim 32</dependent-claim-reference> wherein each said dynamically identified task is the garbage-collection task of performing, for a given object associated with that task, processing that includes identifying in the given object references to other objects and thereby identifying the tasks of performing similar processing for those other objects. </claim-text>
</claim>
<claim id="CLM-00034">
<claim-text><highlight><bold>34</bold></highlight>. A computer signal representing a sequence of instructions that, when executed by a computer system, cause the computer system to employ a plurality of execution threads to perform dynamically identified tasks by: 
<claim-text>A) provide a plurality of task queues, each of which is associated with a different ordered pair of the threads, one thread of the ordered pair being denominated the enqueuer of that queue and the other being denominated the dequeuer thereof; </claim-text>
<claim-text>B) when one said thread identifies a task, pushes an identifier of that the task thus identified onto a set of at least one of the queues of which that thread is an enqueuer; and </claim-text>
<claim-text>C) when one said thread requires one of the dynamically identified tasks to perform, causes that thread to perform a task identified by a task identifier fetched by that thread from a task queue of which that thread is the dequeuer. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00035">
<claim-text><highlight><bold>35</bold></highlight>. A computer signal as defined in <dependent-claim-reference depends_on="CLM-00033">claim 34</dependent-claim-reference> wherein each said dynamically identified task is the garbage-collection task of performing, for a given object associated with that task, processing that includes identifying in the given object references to other objects and thereby identifying the tasks of performing similar processing for those other objects. </claim-text>
</claim>
<claim id="CLM-00036">
<claim-text><highlight><bold>36</bold></highlight>. A computer signal as defined in <dependent-claim-reference depends_on="CLM-00033">claim 34</dependent-claim-reference> wherein each said task identifier is an identifier of the object with which the task is associated. </claim-text>
</claim>
<claim id="CLM-00037">
<claim-text><highlight><bold>37</bold></highlight>. A computer signal as defined in <dependent-claim-reference depends_on="CLM-00033">claim 36</dependent-claim-reference> wherein each said dynamically identified task is the garbage-collection task of performing, for a given object associated with that task, processing that includes identifying in the given object references to other objects and thereby identifying the tasks of performing similar processing for those other objects. </claim-text>
</claim>
<claim id="CLM-00038">
<claim-text><highlight><bold>38</bold></highlight>. A computer signal as defined in <dependent-claim-reference depends_on="CLM-00033">claim 36</dependent-claim-reference> wherein each said task identifier is a pointer to the object with which the task is associated </claim-text>
</claim>
<claim id="CLM-00039">
<claim-text><highlight><bold>39</bold></highlight>. A computer signal as defined in <dependent-claim-reference depends_on="CLM-00033">claim 34</dependent-claim-reference> wherein, when one said thread identifies a task, the computer system pushes an identifier of that thread onto only one of the queues of which that thread is an enqueuer. </claim-text>
</claim>
<claim id="CLM-00040">
<claim-text><highlight><bold>40</bold></highlight>. A computer signal as defined in <dependent-claim-reference depends_on="CLM-00033">claim 39</dependent-claim-reference> wherein each said dynamically identified task is the garbage-collection task of performing, for a given object associated with that task, processing that includes identifying in the given object references to other objects and thereby identifying the tasks of performing similar processing for those other objects. </claim-text>
</claim>
<claim id="CLM-00041">
<claim-text><highlight><bold>41</bold></highlight>. A computer signal as defined in <dependent-claim-reference depends_on="CLM-00033">claim 34</dependent-claim-reference> wherein identifiers of tasks successively identified by a given thread are not in general pushed onto the same queue. </claim-text>
</claim>
<claim id="CLM-00042">
<claim-text><highlight><bold>42</bold></highlight>. A computer signal as defined in <dependent-claim-reference depends_on="CLM-00044">claim 41</dependent-claim-reference> wherein each said dynamically identified task is the garbage-collection task of performing, for a given object associated with that task, processing that includes identifying in the given object references to other objects and thereby identifying the tasks of performing similar processing for those other objects. </claim-text>
</claim>
<claim id="CLM-00043">
<claim-text><highlight><bold>43</bold></highlight>. A computer signal as defined in <dependent-claim-reference depends_on="CLM-00033">claim 34</dependent-claim-reference> wherein a task queue is provided for each ordered pair of the threads. </claim-text>
</claim>
<claim id="CLM-00044">
<claim-text><highlight><bold>44</bold></highlight>. A computer signal as defined in <dependent-claim-reference depends_on="CLM-00044">claim 43</dependent-claim-reference> wherein each said dynamically identified task is the garbage-collection task of performing, for a given object associated with that task, processing that includes identifying in the given object references to other objects and thereby identifying the tasks of performing similar processing for those other objects. </claim-text>
</claim>
<claim id="CLM-00045">
<claim-text><highlight><bold>45</bold></highlight>. A computer system that employs a plurality of execution threads to perform tasks that the threads identify dynamically, the computer system including: 
<claim-text>A) means for providing a plurality of task queues, each of which is associated with a different ordered pair of the threads, one thread of the ordered pair being denominated the enqueuer of that queue and the other being denominated the dequeuer thereof; </claim-text>
<claim-text>B) means for, when one said thread identifies a task, pushing an identifier of that the task thus identified onto a set of at least one of the queues of which that thread is an enqueuer; and </claim-text>
<claim-text>C) means for, when one said thread requires one of the dynamically identified tasks to perform, causing that thread to perform a task identified by a task identifier fetched by that thread from a task queue of which that thread is the dequeuer.</claim-text>
</claim-text>
</claim>
</subdoc-claims>
<subdoc-drawings id="DRAWINGS">
<heading lvl="0" align="CENTER">Drawings</heading>
<representative-figure>4</representative-figure>
<figure id="figure-D00000">
<image id="EMI-D00000" file="US20030005114A1-20030102-D00000.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00001">
<image id="EMI-D00001" file="US20030005114A1-20030102-D00001.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00002">
<image id="EMI-D00002" file="US20030005114A1-20030102-D00002.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00003">
<image id="EMI-D00003" file="US20030005114A1-20030102-D00003.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00004">
<image id="EMI-D00004" file="US20030005114A1-20030102-D00004.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00005">
<image id="EMI-D00005" file="US20030005114A1-20030102-D00005.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00006">
<image id="EMI-D00006" file="US20030005114A1-20030102-D00006.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00007">
<image id="EMI-D00007" file="US20030005114A1-20030102-D00007.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00008">
<image id="EMI-D00008" file="US20030005114A1-20030102-D00008.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00009">
<image id="EMI-D00009" file="US20030005114A1-20030102-D00009.TIF" imf="TIFF" ti="DR"/>
</figure>
</subdoc-drawings>
</patent-application-publication>
