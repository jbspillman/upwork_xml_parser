<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE patent-application-publication SYSTEM "pap-v16-2002-01-01.dtd" [
<!ENTITY US20030005029A1-20030102-D00000.TIF SYSTEM "US20030005029A1-20030102-D00000.TIF" NDATA TIF>
<!ENTITY US20030005029A1-20030102-D00001.TIF SYSTEM "US20030005029A1-20030102-D00001.TIF" NDATA TIF>
<!ENTITY US20030005029A1-20030102-D00002.TIF SYSTEM "US20030005029A1-20030102-D00002.TIF" NDATA TIF>
<!ENTITY US20030005029A1-20030102-D00003.TIF SYSTEM "US20030005029A1-20030102-D00003.TIF" NDATA TIF>
<!ENTITY US20030005029A1-20030102-D00004.TIF SYSTEM "US20030005029A1-20030102-D00004.TIF" NDATA TIF>
<!ENTITY US20030005029A1-20030102-D00005.TIF SYSTEM "US20030005029A1-20030102-D00005.TIF" NDATA TIF>
<!ENTITY US20030005029A1-20030102-D00006.TIF SYSTEM "US20030005029A1-20030102-D00006.TIF" NDATA TIF>
<!ENTITY US20030005029A1-20030102-D00007.TIF SYSTEM "US20030005029A1-20030102-D00007.TIF" NDATA TIF>
<!ENTITY US20030005029A1-20030102-D00008.TIF SYSTEM "US20030005029A1-20030102-D00008.TIF" NDATA TIF>
<!ENTITY US20030005029A1-20030102-D00009.TIF SYSTEM "US20030005029A1-20030102-D00009.TIF" NDATA TIF>
]>
<patent-application-publication>
<subdoc-bibliographic-information>
<document-id>
<doc-number>20030005029</doc-number>
<kind-code>A1</kind-code>
<document-date>20030102</document-date>
</document-id>
<publication-filing-type>new</publication-filing-type>
<domestic-filing-data>
<application-number>
<doc-number>09893264</doc-number>
</application-number>
<application-number-series-code>09</application-number-series-code>
<filing-date>20010627</filing-date>
</domestic-filing-data>
<technical-information>
<classification-ipc>
<classification-ipc-primary>
<ipc>G06F009/00</ipc>
</classification-ipc-primary>
<classification-ipc-edition>07</classification-ipc-edition>
</classification-ipc>
<classification-us>
<classification-us-primary>
<uspc>
<class>709</class>
<subclass>107000</subclass>
</uspc>
</classification-us-primary>
</classification-us>
<title-of-invention>Termination detection for shared-memory parallel programs</title-of-invention>
</technical-information>
<inventors>
<first-named-inventor>
<name>
<given-name>Nir</given-name>
<middle-name>N.</middle-name>
<family-name>Shavit</family-name>
</name>
<residence>
<residence-us>
<city>Cambridge</city>
<state>MA</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</first-named-inventor>
<inventor>
<name>
<given-name>Xiaolan</given-name>
<family-name>Zhang</family-name>
</name>
<residence>
<residence-us>
<city>Ossining</city>
<state>NY</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
<inventor>
<name>
<given-name>Christine</given-name>
<middle-name>H.</middle-name>
<family-name>Flood</family-name>
</name>
<residence>
<residence-us>
<city>Westford</city>
<state>MA</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
</inventors>
<correspondence-address>
<name-1>CESARI AND MCKENNA, LLP</name-1>
<name-2></name-2>
<address>
<address-1>88 BLACK FALCON AVENUE</address-1>
<city>BOSTON</city>
<state>MA</state>
<postalcode>02210</postalcode>
<country>
<country-code>US</country-code>
</country>
</address>
</correspondence-address>
</subdoc-bibliographic-information>
<subdoc-abstract>
<paragraph id="A-0001" lvl="0">A &ldquo;garbage collector&rdquo; employed to reclaim memory dynamically allocated to data objects employs multiple execution threads to perform a parallel-execution operation and its garbage-collection cycle. A thread executes tasks that it selects from lists whose entries represent tasks dynamically identified during other tasks&apos; performance. When a thread fails to find a task in one of these lists, it sets to an inactivity-indicating value a field associated with it in a global status word. It also determines whether any field associated with any of the other threads indicates activity. If not, the thread concludes that the parallel-execution operation has been completed. Otherwise, it returns to searching for further tasks to perform. </paragraph>
</subdoc-abstract>
<subdoc-description>
<summary-of-invention>
<section>
<heading lvl="1">CROSS-REFERENCE TO RELATED APPLICATIONS </heading>
<paragraph id="P-0001" lvl="0"><number>&lsqb;0001&rsqb;</number> This application is related to commonly assigned U.S. patent applications of Nir N. Shavit et al. for Globally Distributed Load Balancing and Load-Balancing Queues Employing LIFO/FIFO Work Stealing, both of which were filed on the same date as this application and are hereby incorporated by reference. </paragraph>
</section>
<section>
<heading lvl="1">BACKGROUND OF THE INVENTION </heading>
<paragraph id="P-0002" lvl="0"><number>&lsqb;0002&rsqb;</number> 1. Field of the Invention </paragraph>
<paragraph id="P-0003" lvl="0"><number>&lsqb;0003&rsqb;</number> The present invention is directed to parallel execution in computer processes. It particularly concerns the manner in which different execution threads terminate their work on parallel operations. </paragraph>
<paragraph id="P-0004" lvl="0"><number>&lsqb;0004&rsqb;</number> 2. Background Information </paragraph>
<paragraph id="P-0005" lvl="0"><number>&lsqb;0005&rsqb;</number> Modern computer systems provide for various types of concurrent operation. A user of a typical desktop computer, for instance, may be simultaneously employing a word-processor program and an e-mail program together with a calculator program. The user&apos;s computer could be using several simultaneously operating processors, each of which could be operating on a different program. More typically, the computer employs only a single main processor, and its operating-system software causes that processor to switch from one program to another rapidly enough that the user cannot usually tell that the different programs are not really executing simultaneously. The different running programs are usually referred to as &ldquo;processes&rdquo; in this connection, and the change from one process to another is said to involve a &ldquo;context switch.&rdquo; In a context switch one process is interrupted, and the contents of the program counter, call stacks, and various registers are stored, including those used for memory mapping. Then the corresponding values previously stored for a previously interrupted process are loaded, and execution resumes for that process. Processor hardware and operating-system software typically have special provisions for performing such context switches. </paragraph>
<paragraph id="P-0006" lvl="0"><number>&lsqb;0006&rsqb;</number> A program running as a computer-system process may take advantage of such provisions to provide separate, concurrent &ldquo;threads&rdquo; of its own execution. In such a case, the program counter and various register contents are stored and reloaded with a different thread&apos;s value, as in the case of a process change, but the memory-mapping values are not changed, so the new thread of execution has access to the same process-specific physical memory as the same process&apos;s previous thread. </paragraph>
<paragraph id="P-0007" lvl="0"><number>&lsqb;0007&rsqb;</number> In some cases, the use of multiple execution threads is merely a matter of programming convenience. For example, compilers for various programming languages, such as the Java programming language, readily provide the &ldquo;housekeeping&rdquo; for spawning different threads, so the programmer is not burdened with handling the details of making different threads&apos; execution appear simultaneous. In the case of multiprocessor systems, though, the use of multiple threads has speed advantages. A process can be performed more quickly if the system allocates different threads to different processors when processor capacity is available. </paragraph>
<paragraph id="P-0008" lvl="0"><number>&lsqb;0008&rsqb;</number> To take advantage of this fact, programmers often identify constituent operations with their programs that particularly lend themselves to parallel execution. When program execution reaches a point where the parallel-execution operation can begin, it starts different execution threads to perform different tasks within that operation. </paragraph>
<paragraph id="P-0009" lvl="0"><number>&lsqb;0009&rsqb;</number> Now, in some parallel-execution operations the tasks to be performed can be identified only dynamically; that is, some of the tasks can be identified only by performing others of the tasks, so the tasks cannot be divided among the threads optimally at the beginning of the parallel-execution operation. Such parallel-execution operations can occur, for instance, in what has come to be called &ldquo;garbage collection,&rdquo; which is the automatic reclamation of dynamically allocated memory. Byte code executed by a Java virtual machine, for instance, often calls for memory to be allocated for data &ldquo;objects&rdquo; if certain program branches are taken. Subsequently, a point in the byte-code program&apos;s execution can be reached at which there is no further possibility that the data stored in that dynamically allocated memory will be used. Without requiring the programmer to provide specific instructions to do so, the virtual machine executing the byte code automatically identifies such &ldquo;unreachable&rdquo; objects and reclaims their memory so that objects allocated thereafter can use it. </paragraph>
<paragraph id="P-0010" lvl="0"><number>&lsqb;0010&rsqb;</number> The general approach employed by the virtual machine&apos;s garbage collector is to identify all objects that are reachable and then reclaim memory that no such reachable object occupies. An object is considered reachable if it is referred to by a reference in a &ldquo;root set&rdquo; of locations, such as global variables, registers, or the call stack, that are recognized as being inherently reachable. An object is also reachable if it is referred to by a reference in a reachable object. </paragraph>
<paragraph id="P-0011" lvl="0"><number>&lsqb;0011&rsqb;</number> So reachable-object identification is a recursive process: the identification of a reachable object can lead to identification of further reachable objects. And, if every reachable object so far identified is thought of as representing a further task, namely, that of identifying any further objects to which it refers, it can be seen that parts of the garbage-collection process include tasks that are only dynamically identifiable. If those tasks are properly programmed, they can be performed in an essentially parallel manner. Specifically, the initial, statically identifiable members of the root set can be divided among a plurality of threads (whose execution will typically be divided among many processors), and those threads can identify reachable objects in parallel. </paragraph>
<paragraph id="P-0012" lvl="0"><number>&lsqb;0012&rsqb;</number> Now, a point may be reached at which every remaining identified reachable object has already been claimed by a thread for processing. In such a situation, it may initially appear desirable for other threads to proceed to the next operation: all of the parallel-execution operation&apos;s tasks have been completed. But allowing them to do so could result in highly suboptimal performance. Since reachable-object identification occurs dynamically, the threads still performing tasks of the parallel-execution operation could end up identifying a large number of further tasks, and they would be left to perform those tasks alone. So it is important to be able to detect the fact that a parallel-execution operation may not be completed, even when none of its tasks is currently available for a thread to perform. </paragraph>
</section>
<section>
<heading lvl="1">SUMMARY OF THE INVENTION </heading>
<paragraph id="P-0013" lvl="0"><number>&lsqb;0013&rsqb;</number> We have developed an advantageous way of detecting such possibilities and confirming that all of a parallel-execution operation&apos;s tasks have been completed even though they can be identified only dynamically. Our approach involves the use of what we call a &ldquo;global status word.&rdquo; The global status word includes a field for each the threads involved in performing a parallel-execution operation, and each field indicates whether its associated thread is active or inactive. So long as a thread is finding tasks of the parallel-execution operation and performing them, its field in the global status word contains the activity-representing value. </paragraph>
<paragraph id="P-0014" lvl="0"><number>&lsqb;0014&rsqb;</number> When a thread reaches a point at which it is unsuccessful in finding any further tasks to perform, it resets its field in the global status word to the inactivity-representing value and checks whether any of the other fields has the activity-representing value. If any does, it repeatedly searches for tasks and checks the global status word to determine whether activity-representing values are in any of its fields. So long as the global status word includes any field whose contents represent activity, it concludes that further dynamically identified tasks may need to be performed, and it continues searching for tasks and checking the global status word. If it finds a task, it sets its associated field in the global status word to the activity-representing value and then attempts to claim a task. If it is successful in claiming a task, it performs that task and again searches for further work. If it is unable to claim the task, it resets its global-status-word field to the inactivity-representing value and returns to searching for tasks and checking the global status word. </paragraph>
<paragraph id="P-0015" lvl="0"><number>&lsqb;0015&rsqb;</number> When the point is reached at which none of the global status word&apos;s fields contains an activity-representing value, then the thread concludes that no further tasks remain in the parallel-execution operation, and it can proceed to further program steps. </paragraph>
</section>
</summary-of-invention>
<brief-description-of-drawings>
<section>
<heading lvl="1">BRIEF DESCRIPTION OF THE DRAWINGS </heading>
<paragraph id="P-0016" lvl="0"><number>&lsqb;0016&rsqb;</number> The invention description below refers to the accompanying drawings, of which: </paragraph>
<paragraph id="P-0017" lvl="0"><number>&lsqb;0017&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 1</cross-reference> is a block diagram of a typical uniprocessor computer system; </paragraph>
<paragraph id="P-0018" lvl="0"><number>&lsqb;0018&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 2</cross-reference> is a block diagram of one type of multiprocessor computer system; </paragraph>
<paragraph id="P-0019" lvl="0"><number>&lsqb;0019&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 3</cross-reference> is a block diagram that illustrates a relationship between source code and object code; </paragraph>
<paragraph id="P-0020" lvl="0"><number>&lsqb;0020&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 4</cross-reference> is a block diagram of a more-complicated relationship between source code and object code; </paragraph>
<paragraph id="P-0021" lvl="0"><number>&lsqb;0021&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 5</cross-reference> is a flow chart that illustrates a sequence of parallel-execution operations; </paragraph>
<paragraph id="P-0022" lvl="0"><number>&lsqb;0022&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 6</cross-reference> is a block diagram illustrating work queues that an embodiment of the present invention may employ; </paragraph>
<paragraph id="P-0023" lvl="0"><number>&lsqb;0023&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 7</cross-reference> is a listing of a routine for popping entries from the top of a double-ended queue; </paragraph>
<paragraph id="P-0024" lvl="0"><number>&lsqb;0024&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 8</cross-reference> is a listing of a routine for pushing entries onto the bottom of a double-ending queue; </paragraph>
<paragraph id="P-0025" lvl="0"><number>&lsqb;0025&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 9</cross-reference> is a listing of a routine for popping items from the bottom of a double-ending queue; </paragraph>
<paragraph id="P-0026" lvl="0"><number>&lsqb;0026&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 10</cross-reference> is a block diagram of data structures employed by some embodiments of the present invention to implement overflow lists; </paragraph>
<paragraph id="P-0027" lvl="0"><number>&lsqb;0027&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 11</cross-reference> contains listings of routines employed by the illustrated embodiment to locate tasks when its associated task queue is empty; </paragraph>
<paragraph id="P-0028" lvl="0"><number>&lsqb;0028&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 12</cross-reference> is a listing of a routine that a thread in the illustrated embodiment employs to &ldquo;steal&rdquo; work from other threads&apos; work queues; and </paragraph>
<paragraph id="P-0029" lvl="0"><number>&lsqb;0029&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 13</cross-reference> contains listings for routines that a thread in the illustrated embodiment employs to determine whether tasks are listed in overflow lists or other threads&apos; work queues. </paragraph>
</section>
</brief-description-of-drawings>
<detailed-description>
<section>
<heading lvl="1">DETAILED DESCRIPTION OF AN ILLUSTRATIVE EMBODIMENT </heading>
<paragraph id="P-0030" lvl="0"><number>&lsqb;0030&rsqb;</number> The present invention&apos;s teachings concerning termination of a parallel-execution operation can be implemented in a wide variety of systems. Some of the benefits of employing multiple threads can be obtained in uniprocessor systems, of which <cross-reference target="DRAWINGS">FIG. 1</cross-reference> depicts a typical configuration. Its uniprocessor system <highlight><bold>10</bold></highlight> employs a single microprocessor such as microprocessor <highlight><bold>11</bold></highlight>. In <cross-reference target="DRAWINGS">FIG. 1</cross-reference>&apos;s exemplary system, microprocessor <highlight><bold>11</bold></highlight> receives data, and instructions for operating on them, from on-board cache memory or further cache memory <highlight><bold>12</bold></highlight>, possibly through the mediation of a cache controller <highlight><bold>13</bold></highlight>. The cache controller <highlight><bold>13</bold></highlight> can in turn receive such data from system read/write memory (&ldquo;RAM&rdquo;) <highlight><bold>14</bold></highlight> through a RAM controller <highlight><bold>15</bold></highlight>, or from various peripheral devices through a system bus <highlight><bold>16</bold></highlight>. </paragraph>
<paragraph id="P-0031" lvl="0"><number>&lsqb;0031&rsqb;</number> The RAM <highlight><bold>14</bold></highlight>&apos;s data and instruction contents, which can configure the system to implement the teachings to be described below, will ordinarily have been loaded from peripheral devices such as a system disk <highlight><bold>17</bold></highlight>. Other sources include communications interface <highlight><bold>18</bold></highlight>, which can receive instructions and data from other computer equipment. </paragraph>
<paragraph id="P-0032" lvl="0"><number>&lsqb;0032&rsqb;</number> Although threads generally, and therefore the present invention&apos;s teachings in particular, can be employed in such systems, the application in connection with which the present invention is described by way of example would more frequently be implemented in a multiprocessor system. Such systems come in a wide variety of configurations. Some may be largely the same as that of <cross-reference target="DRAWINGS">FIG. 1</cross-reference>, with the exception that they could include more than one microprocessor such as processor <highlight><bold>11</bold></highlight>, possibly together with respective cache memories, sharing common read/write memory by communication over the common bus <highlight><bold>16</bold></highlight>. </paragraph>
<paragraph id="P-0033" lvl="0"><number>&lsqb;0033&rsqb;</number> In other configurations, parts of the shared memory may be more local to one or more processors than to others. In <cross-reference target="DRAWINGS">FIG. 2</cross-reference>, for instance, one or more microprocessors <highlight><bold>20</bold></highlight> at a location <highlight><bold>22</bold></highlight> may have access both to a local memory module <highlight><bold>24</bold></highlight> and to a further, remote memory module <highlight><bold>26</bold></highlight>, which is provided at a remote location <highlight><bold>28</bold></highlight>. Because of the greater distance, though, port circuitry <highlight><bold>28</bold></highlight> and <highlight><bold>30</bold></highlight> may be necessary to communicate at the lower speed to which an intervening channel <highlight><bold>32</bold></highlight> is limited. A processor <highlight><bold>34</bold></highlight> at the remote location may similarly have different-speed access to both memory modules <highlight><bold>24</bold></highlight> and <highlight><bold>26</bold></highlight>. In such a situation, one or the other or both of the processors may need to fetch code or data or both from a remote location, but it will often be true that parts of the code will be replicated in both places. Regardless of the configuration, different processors can operate on the same code, although that code may be replicated in different physical memory, so different processors can be used to execute different threads of the same process. </paragraph>
<paragraph id="P-0034" lvl="0"><number>&lsqb;0034&rsqb;</number> To illustrate the invention, we will describe its use for properly terminating a parallel-execution operation performed by a garbage collector. To place garbage collection in context, we briefly review the general relationship between programming and computer operation. When a processor executes a computer program, of course, it executes machine instructions. A programmer typically writes the program, but it is a rare programmer who is familiar with the specific machine instructions in which his efforts eventually result. More typically, the programmer writes higher-level-language &ldquo;source code,&rdquo; from which a computer software-configured to do so generates those machine instructions, or &ldquo;object code.&rdquo; </paragraph>
<paragraph id="P-0035" lvl="0"><number>&lsqb;0035&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 3</cross-reference> represents this sequence. <cross-reference target="DRAWINGS">FIG. 3</cross-reference>&apos;s block <highlight><bold>36</bold></highlight> represents a compiler process that a computer performs under the direction of compiler object code. That object code is typically stored on a persistent machine-readable medium, such as <cross-reference target="DRAWINGS">FIG. 1</cross-reference>&apos;s system disk <highlight><bold>17</bold></highlight>, and it is loaded by transmission of electrical signals into RAM <highlight><bold>14</bold></highlight> to configure the computer system to act as a compiler. But the compiler object code&apos;s persistent storage may instead be provided in a server system remote from the machine that performs the compiling. The electrical signals that carry the digital data by which the computer systems exchange the code are exemplary forms of carrier waves transporting the information. </paragraph>
<paragraph id="P-0036" lvl="0"><number>&lsqb;0036&rsqb;</number> In any event, the compiler converts source code into application object code, as <cross-reference target="DRAWINGS">FIG. 3</cross-reference> indicates, and places it in machine-readable storage such as RAM <highlight><bold>14</bold></highlight> or disk <highlight><bold>17</bold></highlight>. A computer will follow that object code&apos;s instructions in performing the thus-defined application <highlight><bold>38</bold></highlight>, which typically generates output from input. The compiler <highlight><bold>36</bold></highlight> can itself be thought of as an application, one in which the input is source code and the output is object code, but the computer that executes the application <highlight><bold>28</bold></highlight> is not necessarily the same as the one that executes the compiler application <highlight><bold>36</bold></highlight>. </paragraph>
<paragraph id="P-0037" lvl="0"><number>&lsqb;0037&rsqb;</number> The source code need not have been written by a human programmer directly. Integrated development environments often automate the source-code-writing process to the extent that for many applications very little of the source code is produced &ldquo;manually.&rdquo;</paragraph>
<paragraph id="P-0038" lvl="0"><number>&lsqb;0038&rsqb;</number> As will be explained below, moreover, the &ldquo;source&rdquo; code being compiled may sometimes be low-level code, such as the byte-code input to the Java&trade; virtual machine, that programmers almost never write directly. (Sun, the Sun Logo, Sun Microsystems, and Java are trademarks or registered trademarks of Sun Microsystems, Inc., in the United States and other countries.) And, although <cross-reference target="DRAWINGS">FIG. 3</cross-reference> may appear to suggest a batch process, in which all of an application&apos;s object code is produced before any of it is executed, the same processor may both compile and execute the code, in which case the processor may execute its compiler application concurrently with&mdash;and, indeed, in a way that can depend upon&mdash;its execution of the compiler&apos;s output object code. </paragraph>
<paragraph id="P-0039" lvl="0"><number>&lsqb;0039&rsqb;</number> So the sequence of operations by which source code results in machine-language instructions may be considerably more complicated than one may infer from <cross-reference target="DRAWINGS">FIG. 3</cross-reference>. To give a sense of the complexity that can be involved, we discuss by reference to <cross-reference target="DRAWINGS">FIG. 4</cross-reference> an example of one way in which various levels of source code can result in the machine instructions that the processor executes. The human application programmer produces source code <highlight><bold>40</bold></highlight> written in a high-level language such as the Java programming language. In the case of the Java programming language, a compiler <highlight><bold>42</bold></highlight> converts that code into &ldquo;class files.&rdquo; These predominantly include routines written in instructions, called &ldquo;byte code&rdquo; <highlight><bold>44</bold></highlight>, for a &ldquo;virtual machine&rdquo; that various processors can be programmed to emulate. This conversion into byte code is almost always separated in time from that code&apos;s execution, so that aspect of the sequence is depicted as occurring in a &ldquo;compile-time environment&rdquo; <highlight><bold>46</bold></highlight> separate from a &ldquo;run-time environment&rdquo; <highlight><bold>48</bold></highlight>, in which execution occurs. </paragraph>
<paragraph id="P-0040" lvl="0"><number>&lsqb;0040&rsqb;</number> Most typically, a processor runs the class files&apos; instructions under the control of a virtual-machine program <highlight><bold>50</bold></highlight>, whose purpose is to emulate a machine from whose instruction set the byte codes are drawn. Much of the virtual machine&apos;s action in executing the byte code is most like what those skilled in the art refer to as &ldquo;interpreting,&rdquo; and <cross-reference target="DRAWINGS">FIG. 4</cross-reference> shows that the virtual machine includes an &ldquo;interpreter&rdquo; <highlight><bold>52</bold></highlight> for that purpose. The resultant instructions typically involve calls to a run-time system <highlight><bold>54</bold></highlight>, which handles matters such as loading new class files as they are needed. </paragraph>
<paragraph id="P-0041" lvl="0"><number>&lsqb;0041&rsqb;</number> Many virtual-machine implementations also actually compile the byte code concurrently with the resultant object code&apos;s execution, so <cross-reference target="DRAWINGS">FIG. 4</cross-reference> depicts the virtual machine as additionally including a &ldquo;just-in-time&rdquo; compiler <highlight><bold>56</bold></highlight>. It may be that the resultant object code will make low-level calls to the run-time system, as the drawing indicates. In any event, the code&apos;s execution will include calls to the local operating system <highlight><bold>58</bold></highlight>. </paragraph>
<paragraph id="P-0042" lvl="0"><number>&lsqb;0042&rsqb;</number> In addition to class-file loading, one of the functions that the runtime system performs is the garbage collection. The programming that performs this function can include parallel-execution operations, and it is by reference to such operations that we will illustrate the present invention&apos;s approach to termination detection. To aid that discussion, we digress to a brief review of garbage-collection nomenclature. </paragraph>
<paragraph id="P-0043" lvl="0"><number>&lsqb;0043&rsqb;</number> In the field of computer systems, considerable effort has been expended on the task of allocating memory to data objects. For the purposes of this discussion, the term object refers to a data structure represented in a computer system&apos;s memory. Other terms sometimes used for the same concept are record and structure. An object may be identified by a reference, a relatively small amount of information that can be used to access the object. A reference can be represented as a &ldquo;pointer&rdquo; or a &ldquo;machine address,&rdquo; which may require, for instance, only sixteen, thirty-two, or sixty-four bits of information, although there are other ways to represent a reference. </paragraph>
<paragraph id="P-0044" lvl="0"><number>&lsqb;0044&rsqb;</number> In some systems, which are usually known as &ldquo;object oriented,&rdquo; objects may have associated methods, which are routines that can be invoked by reference to the object. An object may belong to a class, which is an organizational entity that may contain method code or other information shared by all objects belonging to that class. The specific example below by which we illustrate the present invention&apos;s more-general applicability deals with reclaiming memory allocated to Java-language objects, which belong to such classes. </paragraph>
<paragraph id="P-0045" lvl="0"><number>&lsqb;0045&rsqb;</number> A modem program executing as a computer-system process often dynamically allocates storage for objects within a part of the process&apos;s memory commonly referred to as the &ldquo;heap.&rdquo; As was mentioned above, a garbage collector reclaims such objects when they are no longer reachable. </paragraph>
<paragraph id="P-0046" lvl="0"><number>&lsqb;0046&rsqb;</number> To distinguish the part of the program that does &ldquo;useful&rdquo; work from that which does the garbage collection, the term mutator is sometimes used; from the collector&apos;s point of view, what the mutator does is mutate active data structures&apos; connectivity. Some garbage-collection approaches rely heavily on interleaving garbage-collection steps among mutator steps. In one type of garbage-collection approach, for instance, the mutator operation of writing a reference is followed immediately by garbage-collector steps used to maintain a reference count in that object&apos;s header, and code for subsequent new-object allocation includes steps for finding space occupied by objects whose reference count has fallen to zero. Obviously, such an approach can slow mutator operation significantly. </paragraph>
<paragraph id="P-0047" lvl="0"><number>&lsqb;0047&rsqb;</number> Other, &ldquo;stop-the-world&rdquo; garbage-collection approaches use somewhat less interleaving. The mutator still typically allocates an object space within the heap by invoking the garbage collector, which keeps track of the fact that the thus-allocated region is occupied and refrains from allocating that region to other objects until it determines that the mutator no longer needs access to that object. But a stop-the-world collector performs its memory reclamation during garbage-collection cycles separate from the cycles in which the mutator runs. That is, the collector interrupts the mutator process, finds unreachable objects, reclaims their memory space for reuse, and then restarts the mutator. </paragraph>
<paragraph id="P-0048" lvl="0"><number>&lsqb;0048&rsqb;</number> To provide an example of a way in which the present invention&apos;s teachings can be applied, we assume a &ldquo;stop-the-world&rdquo; garbage collector and focus on the garbage-collection cycle. Since most of the specifics of a garbage-collection cycle are not of particular interest in the present context, <cross-reference target="DRAWINGS">FIG. 5</cross-reference> depicts only part of the cycle, and it depicts that part in a highly abstract manner. Its block <highlight><bold>60</bold></highlight> represents the start of the garbage-collection cycle, and its block <highlight><bold>62</bold></highlight> represents one of a number of the initial garbage-collection steps that are performed by a single thread only. </paragraph>
<paragraph id="P-0049" lvl="0"><number>&lsqb;0049&rsqb;</number> Eventually, the garbage collector reaches a part of its routine that can benefit from multi-threaded execution, and the virtual-machine programming calls upon the operating system to start a number of threads, as block <highlight><bold>64</bold></highlight> indicates, that will execute a subsequent code sequence in parallel. For the sake of example, we assume four threads. This would typically mean that the garbage collector is running in a multiprocessor system of at least that many processors, since the advantages of multithreading in an automatic-garbage-collection context are principally that different processors will at least sometimes execute different threads simultaneously. </paragraph>
<paragraph id="P-0050" lvl="0"><number>&lsqb;0050&rsqb;</number> Each of the threads executes an identical code sequence. The drawing depicts the code sequence somewhat arbitrarily as divided into a number of operations A, B, C, D, and E respectively represented by blocks <highlight><bold>66</bold></highlight>, <highlight><bold>68</bold></highlight>, <highlight><bold>70</bold></highlight>, <highlight><bold>72</bold></highlight>, and <highlight><bold>74</bold></highlight>. These operations&apos; specifics are not germane to the present discussion, but commonly assigned U.S. patent application Ser. No. 09/377,349, filed on Aug. 19, 1999, by Alexander T. Garthwaite for Popular-Object Handling in a Train-Algorithm-Based Garbage Collector and hereby incorporated by reference, gives examples of the types of garbage-collection operations that blocks <highlight><bold>66</bold></highlight>, <highlight><bold>68</bold></highlight>, <highlight><bold>70</bold></highlight>, <highlight><bold>72</bold></highlight>, and <highlight><bold>74</bold></highlight> may include. </paragraph>
<paragraph id="P-0051" lvl="0"><number>&lsqb;0051&rsqb;</number> Although all threads execute the same code sequence, some of the code&apos;s routines take the thread&apos;s identity as an argument, and some of the data that an instruction processes may change between that instruction&apos;s executions by different threads. These factors, together with hardware differences and the vagaries of thread scheduling, result in different threads&apos; completing different operations at different times even in the absence of the dynamic task identification. </paragraph>
<paragraph id="P-0052" lvl="0"><number>&lsqb;0052&rsqb;</number> But there is usually some point in the routine beyond which execution should not proceed until all threads have reached it, so a &ldquo;join&rdquo; mechanism, represented by block <highlight><bold>76</bold></highlight>, imposes this requirement. It is only after all threads reach the join point that further execution of the garbage-collection cycle can proceed. </paragraph>
<paragraph id="P-0053" lvl="0"><number>&lsqb;0053&rsqb;</number> The way in which the join mechanism is implemented is not of particular interest in the present discussion except to help explain the present invention&apos;s purpose by contrast. Whereas the join mechanism insures that execution proceeds no further until all threads have reached the join point, the present invention&apos;s termination mechanism insures that all threads keep working until all of a given operation&apos;s work has been done. </paragraph>
<paragraph id="P-0054" lvl="0"><number>&lsqb;0054&rsqb;</number> Although the present invention&apos;s range of applicability is broader, the need for it tends to be more pronounced in operations that involve dynamically identifiable tasks. To illustrate how the need for such a mechanism can arise, let us assume that <cross-reference target="DRAWINGS">FIG. 5</cross-reference>&apos;s operation B involves essentially only statically identifiable tasks, whereas operation C&apos;s involve tasks principally identifiable only dynamically. For example, assume that operation B involves processing the root set to find reachable objects. The root set may be divided into groups, and different threads may claim different groups to process. Although the present invention&apos;s teachings can be employed to aid in optimal division of this operation&apos;s tasks among threads, the fact that the tasks are statically identifiable makes the job of dividing these tasks equitably among the threads suitable for performance in accordance with one of the approaches described in commonly assigned U.S. patent application Ser. No. 09/697,729, which was filed by Flood et al. on Oct. 26, 2000, for Work-Stealing Queues for Parallel Garbage Collection and is hereby incorporated by reference. </paragraph>
<paragraph id="P-0055" lvl="0"><number>&lsqb;0055&rsqb;</number> By performing those tasks, though, a garbage-collection thread dynamically identifies further tasks to perform. When operation B identifies an object referred to by the root set, that is, it has also identified the task of following the references in the thus-identified object to find further roots. We will assume that operation C involves processing the reachable objects thus identified, so its tasks are identifiable only dynamically: since it is only by performing one of the tasks that further tasks are identified, the tasks are not known at the beginning of the operation. Because of the task-identification process&apos;s dynamic nature, operation C would be particularly vulnerable to a work imbalance among the threads in the absence of features such as those of the present invention, whose purpose in this case is to keep all threads working on operation C so long as any operation-C work is left to do. </paragraph>
<paragraph id="P-0056" lvl="0"><number>&lsqb;0056&rsqb;</number> To help illustrate one application of the present invention&apos;s termination-detection mechanism, we first employ <cross-reference target="DRAWINGS">FIG. 6</cross-reference> to describe operation C, whose termination the present invention can be used to detect. Work queues <highlight><bold>80</bold></highlight><highlight><italic>a, b, c, </italic></highlight>and <highlight><italic>d </italic></highlight>are associated with respective threads. When a thread dynamically identifies a task, it places an identifier of that task in its work queue. In the case of operation C, i.e., reachable-object-identification and processing, a convenient type of task identifier to place in the work queue is an identifier of the reachable object that the garbage-collection thread has found. (In the exemplary program listing to be described below, for example, the entries are pointers to pointers to such objects.) That identifier will represent the task of scanning the further object for references, relocating the object, performing necessary reference updating, etc. </paragraph>
<paragraph id="P-0057" lvl="0"><number>&lsqb;0057&rsqb;</number> Of course, other task granularities are possible. A separate entry could be made for each reference in a newly identified reachable object, for example. </paragraph>
<paragraph id="P-0058" lvl="0"><number>&lsqb;0058&rsqb;</number> As will be discussed further below, a garbage-collection thread performs the tasks in its work queue until that queue is empty, and it then searches other threads&apos; queues for tasks to steal and perform, as will also be explained in more detail. The basic technique of employing dynamic-work-stealing queues is described in a paper by Nimar S. Arora et al., entitled &ldquo;Thread Scheduling for Multiprogrammed Multiprocessors,&rdquo; in the 1998 <highlight><italic>Proceedings of the Tenth Annual ACM Symposium on Parallel Algorithms and Architectures. </italic></highlight>A garbage-collection thread pushes newly found references onto one end of its work queue, which end is arbitrarily referred to as that work queue&apos;s bottom. When the thread is ready to perform a task from its queue, it will pop a reference <highlight><bold>36</bold></highlight> from the bottom of the queue and perform the represented task. When it is out of work, it &ldquo;steals&rdquo; work if possible from another thread&apos;s queue by popping a task from the other, &ldquo;top&rdquo; end of the other thread&apos;s queue. </paragraph>
<paragraph id="P-0059" lvl="0"><number>&lsqb;0059&rsqb;</number> One way of implementing queue control involves use of an index <highlight><bold>82</bold></highlight><highlight><italic>a, b, c, </italic></highlight>or <highlight><italic>d </italic></highlight>(&ldquo;<highlight><bold>82</bold></highlight>&rdquo;) pointing to the next entry to be popped from the top of the queue and an index <highlight><bold>84</bold></highlight><highlight><italic>a, b, c, </italic></highlight>or <highlight><italic>d </italic></highlight>(&ldquo;<highlight><bold>84</bold></highlight>&rdquo;) pointing to the location where the next entry should be added to the bottom of the queue. For reasons to be explained below, the memory word (referred to as &ldquo;age&rdquo; in the code discussed below) that contains the top index also includes a tag <highlight><bold>86</bold></highlight><highlight><italic>a, b, c, </italic></highlight>or <highlight><italic>d </italic></highlight>(&ldquo;<highlight><bold>86</bold></highlight>&rdquo;). The garbage-collection thread associated with the queue increments the bottom index when it pushes a task identifier onto its queue, and it decrements that index when it pops an identifier from it. A stealing thread increments the top index when it pops an identifier from another thread&apos;s queue </paragraph>
<paragraph id="P-0060" lvl="0"><number>&lsqb;0060&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 7</cross-reference> sets forth simplified sample code for a routine, &ldquo;popTop,&rdquo; that a stealing thread could use to pop a task from another thread&apos;s queue. That routine involves the tag field. To explain that field&apos;s purpose, we first consider in detail how the step of popping the queue from the top is performed. </paragraph>
<paragraph id="P-0061" lvl="0"><number>&lsqb;0061&rsqb;</number> To steal from the top of another thread&apos;s work queue, the stealing thread first reads that queue&apos;s top index as part of the &ldquo;age&rdquo; value, as <cross-reference target="DRAWINGS">FIG. 7</cross-reference>&apos;s third line indicates, to find its top entry&apos;s location. The stealing thread then reads the bottom index to make sure that the bottom index is not less than or the same as the top index, i.e., that the queue is not empty. As <cross-reference target="DRAWINGS">FIG. 7</cross-reference>&apos;s fifth and sixth lines indicate, the stealing thread will not pop the top of the queue if the queue is empty. </paragraph>
<paragraph id="P-0062" lvl="0"><number>&lsqb;0062&rsqb;</number> Otherwise, the stealing thread reads the top-index-identified queue entry, as the seventh line indicates. But the stealing thread does not immediately perform the task that the queue entry identifies. This is because, after it has read the top index, a second stealing thread may pop the top entry after the first stealing thread has read the location to which the top entry points but before it increments the top index to indicate that it has claimed the task. If that happens, the first stealing thread could end up attempting to process an object that the second thread had already processed. So, before it actually performs that task, the stealing thread performs an atomic compare-and-swap operation, as the tenth line indicates, in which it effectively pops the top queue entry by incrementing the top index <highlight><bold>82</bold></highlight> if that index&apos;s value is still the same as the one the stealing thread used to read the top queue entry, i.e., if no other second stealing thread popped the queue in the interim. As the tenth line also indicates, the storing operation is actually performed on the entire age word, i.e., on the entire word that contains both the top index <highlight><bold>82</bold></highlight> and the tag <highlight><bold>84</bold></highlight>, rather than only on the top field, for reasons that we will turn to below. </paragraph>
<paragraph id="P-0063" lvl="0"><number>&lsqb;0063&rsqb;</number> If the stealing thread thereby successfully pops the queue, i.e., if the absence of a top-index-value change as determined by the compare-and-swap operation&apos;s comparison has enabled that thread to claim the task by incrementing the top index in that atomic operation&apos;s resultant swap, then the eleventh line&apos;s test will yield a positive result, and the thread proceeds to perform the task whose identifier the popTop routine returns in the routine&apos;s twelfth line. If the top index&apos;s field has changed, on the other hand, then another thread has presumably already popped the queue entry. As the thirteenth line indicates, the routine returns a NULL value in that case, and the first stealing thread concludes from the NULL return value that it has not popped the top entry successfully. In short, an interim change in the top index causes the routine not to increment the top index (as part of the compare-and-swap operation) and, because of the NULL return value, prevents the executing thread from performing the task that the initially read queue entry represents. </paragraph>
<paragraph id="P-0064" lvl="0"><number>&lsqb;0064&rsqb;</number> Thus employing an atomic compare-and-swap operation protects the pop operation&apos;s integrity from interference by other stealing threads. Without the tag field, though, the top-popping operation would still be vulnerable to interference from the (bottom-popping) owner thread. To understand why, first consider how the owner thread pushes queue entries. </paragraph>
<paragraph id="P-0065" lvl="0"><number>&lsqb;0065&rsqb;</number> Unlike stealing threads, the owner thread pushes and pops entries from the bottom of the queue. As the simplified sample code of <cross-reference target="DRAWINGS">FIG. 8</cross-reference> illustrates, pushing a queue entry is simply a matter of reading the bottom index (&ldquo;bot&rdquo;), writing an entry identifier of a work task (&ldquo;task&rdquo;) into the location that the bottom entry identifies, and incrementing the bottom index. Since the owner thread is the only thread that pushes onto the queue, and, as <cross-reference target="DRAWINGS">FIG. 7</cross-reference>&apos;s fifth and sixth lines indicate, a stealing thread will not pop an entry from the queue position identified by the bottom index, there is no need to take special precautions against interference by other, stealing garbage-collection threads. </paragraph>
<paragraph id="P-0066" lvl="0"><number>&lsqb;0066&rsqb;</number> But the owner thread&apos;s popping an entry from the bottom of the queue does require such precautions. Although a stealing thread pops the top only, the top and the bottom entries are the same when there is only one entry left. <cross-reference target="DRAWINGS">FIG. 9</cross-reference> sets forth a simplified example bottom-popping routine, popBottom, that illustrates this. </paragraph>
<paragraph id="P-0067" lvl="0"><number>&lsqb;0067&rsqb;</number> The popBottom routine starts by determining whether to decrement the bottom index. When the queue is in its initial, unpopulated state, the bottom index has an initial value, call it zero, that represents the start of the space allocated to queue entries. If the bottom index&apos;s value is zero, the queue contains no task entries, so no task will be popped, and the bottom index should not be changed. As <cross-reference target="DRAWINGS">FIG. 9</cross-reference>&apos;s third through fifth lines indicate, a zero value of the bottom index therefore causes the routine simply to return a NULL value, and thereby indicate that the queue is empty, without decrementing the bottom index. The owner thread responds to this empty-queue-indicating NULL return value by attempting to find work elsewhere. </paragraph>
<paragraph id="P-0068" lvl="0"><number>&lsqb;0068&rsqb;</number> If the bottom index has any value other than zero, the popBottom routine performs the sixth- and seventh-line steps of decrementing it to indicate that the bottom has moved up, and it then reads the bottom entry, as the eighth line indicates. Unlike the top-index incrementation that a stealing thread performs in the step represented by <cross-reference target="DRAWINGS">FIG. 7</cross-reference>&apos;s tenth line, though, this index change does not mean that the index-changing thread will necessarily perform the task thereby &ldquo;claimed.&rdquo; True, that decrementation of the bottom index does prevent the task whose identifier the owner thread reads in <cross-reference target="DRAWINGS">FIG. 9</cross-reference>&apos;s eighth line from being popped by stealing thread&mdash;if that stealing thread has not reached the step that <cross-reference target="DRAWINGS">FIG. 7</cross-reference>&apos;s fourth line represents. But a stealing thread that has already passed that step may pop that task. </paragraph>
<paragraph id="P-0069" lvl="0"><number>&lsqb;0069&rsqb;</number> The bottom-popping routine therefore checks for this possibility. As <cross-reference target="DRAWINGS">FIG. 9</cross-reference>&apos;s ninth and tenth lines indicate, the first thing the bottom-popping routine does for this purpose is to compare the (now-decremented) bottom index, which identifies the queue location from which it read the entry, with the top, next-steal-location-indicating index as it stood after bottom index was decremented. If the bottom index exceeds the top index, no such interim stealing will occur, so the popBottom returns the task identifier to the owner thread&apos;s calling routine, and the owner thread proceeds with the task thus identified. </paragraph>
<paragraph id="P-0070" lvl="0"><number>&lsqb;0070&rsqb;</number> If the bottom index does not exceed the top index, on the other hand, then the task may or may not have been stolen. One thing is certain, though: its identifier was the last one in the queue, and either the current execution of the bottom popping routine will pop that last task or a stealing thread has done so already. So the bottom-popping routine takes the opportunity to set the bottom index to zero and thereby reverse queue contents&apos; downward progression, as <cross-reference target="DRAWINGS">FIG. 9</cross-reference>&apos;s twelfth line indicates. </paragraph>
<paragraph id="P-0071" lvl="0"><number>&lsqb;0071&rsqb;</number> In the remaining lines of that routine, it also sets the top index to zero. As the thirteenth and fourteenth lines indicate, it prepares to do so by forming a word, newAge, whose top field is zero and, for reasons shortly to be explained, whose tag field is one greater than the tag field read as part of the queue&apos;s age value in the ninth-line step. The precise way in which it completes the act of setting the top index to zero depends on whether a steal occurred. </paragraph>
<paragraph id="P-0072" lvl="0"><number>&lsqb;0072&rsqb;</number> Now, the routine can be sure at this point that a stealing thread has indeed popped that last entry if the originally read top and decremented bottom indexes are not equal, so it tests their equality in the step that the fifteenth line represents. If they are not equal, then the queue location identified as next to be stolen from has advanced beyond the one from which the bottom-popping routine read the task identifier it its eighth line: a stealing thread has already popped that task identified by that task identifier. The routine therefore skips the block represented by the sixteenth through nineteenth lines, it completes the action of zeroing the top index by performing the twentieth line&apos;s step of setting the queue&apos;s age value to newAge, and, as the twenty-first line indicates, it returns a NULL value to its caller. The NULL value tells the owner thread that no work is left in the queue, so it does not attempt to perform the task identified by the entry that popBottom read in its eighth-line step. </paragraph>
<paragraph id="P-0073" lvl="0"><number>&lsqb;0073&rsqb;</number> If the result of the fifteenth-line test is positive, on the other hand, then popBottom can conclude that the identified task had not yet been stolen when it read the task identifier from the queue in its eighth-line step. But that task may have stolen in the interim, and there is as yet nothing that will prevent it from being stolen in the future, because the top index has not yet been reset to zero. To reset it&mdash;and know whether the target task was stolen before the reset occurred&mdash;the routine performs the sixteenth line&apos;s atomic compare-and-swap operation. That operation swaps the contents of queue&apos;s (top-index-containing) age word with those of newAge, which have a top-field value of zero, if and only if a comparison performed before and atomically with the swap indicates that the age word has not changed since it was read. The swap therefore occurs successfully only if, up until the swap occurred, the queue&apos;s top index still indicated that the location from which the routine read the task identifier was yet to be stolen from. </paragraph>
<paragraph id="P-0074" lvl="0"><number>&lsqb;0074&rsqb;</number> As the seventeenth line indicates, the routine then determines whether the (top-index-resetting) swap was successful. If it was, then no steal occurred, and, in view of <cross-reference target="DRAWINGS">FIG. 7</cross-reference>&apos;s tenth line, none will. So it returns the identity of a task, which the owner thread will accordingly perform. If the swap was unsuccessful, then the top index has not been reset, and another thread must have stolen the task. So the routine resets the top index, as the twentieth line indicates, and returns a NULL value to indicate that it found the queue empty. </paragraph>
<paragraph id="P-0075" lvl="0"><number>&lsqb;0075&rsqb;</number> We now turn to the reason for the tag field. Consider a situation in which an owner has pushed only single task onto its queue after having just emptied that queue and therefore set its indexes to zero. Now assume that another thread begins an attempt to steal that task. Further assume that the owner thread does two things after the stealing thread has performed <cross-reference target="DRAWINGS">FIG. 7</cross-reference>&apos;s third-, fourth-, and seventh-line steps of reading the indexes and task identifier but before it reaches the tenth-line step of atomically claiming the task by incrementing the top index if that index has not changed since it was read in the third-line step. Specifically, assume that during that time the owner both (1) pops the task whose identifier the stealing thread read and (2) pushes a new task onto the queue to replace it. </paragraph>
<paragraph id="P-0076" lvl="0"><number>&lsqb;0076&rsqb;</number> The result is that, when the stealing thread&apos;s top-popping routine reaches the comparison part of its tenth-line compare-and-swap operation, the top-index value will have returned to the value it had when the top-popping routine read it in its third-line step. So, if the queue&apos;s age field were simply the top index, without the additional, tag field, the compare-and-swap operation would succeed, and the stealing thread would perform the task whose identifier the top-popping operation read, even though the owner thread had already claimed that task. By advancing the top index, moreover, the stealing thread would prevent the task that it should have claimed from being thereafter claimed by that thread or any other one. And these results would follow not only in that single-entry situation but also in any situation in which the queue gets emptied if other threads additionally steal enough tasks to return the top index to its previous value. </paragraph>
<paragraph id="P-0077" lvl="0"><number>&lsqb;0077&rsqb;</number> As <cross-reference target="DRAWINGS">FIG. 9</cross-reference>&apos;s thirteenth and fourteenth lines indicate, though, the owner thread prevents this by not only resetting the top-index field but also incrementing the tag field. So, when the stealing thread performs the comparison part of the top-popping routine&apos;s compare-and-swap operation represented by <cross-reference target="DRAWINGS">FIG. 7</cross-reference>&apos;s tenth line, it detects the interfering activity because the age value&apos;s tag field has changed, so the stealing thread does not perform the already-claimed task. </paragraph>
<paragraph id="P-0078" lvl="0"><number>&lsqb;0078&rsqb;</number> The foregoing example code was simplified in, among other things, that it assumes an unlimited queue size. In some implementations, the queue size may be limited and subject to overflow. So, before a thread in the illustrated embodiment pushes a task identifier onto its work queue, it determines whether pushing another object identifier onto its queue would cause it to overflow. If so, the thread first employs an appropriate locking mechanism to obtain temporary exclusive access to a common overflow data structure, <cross-reference target="DRAWINGS">FIG. 10</cross-reference>&apos;s structure <highlight><bold>90</bold></highlight>. With the common overflow data structure thus &ldquo;locked,&rdquo; the thread moves identifiers one at a time from the bottom of its queue to an overflow list to which, as will be explained presently, that structure points. Preferably, the thread removes half of the identifiers from its queue and leaves the other half there. </paragraph>
<paragraph id="P-0079" lvl="0"><number>&lsqb;0079&rsqb;</number> The overflow data structure is a table in which each entry includes a class identifier <highlight><bold>92</bold></highlight> and a list pointer <highlight><bold>94</bold></highlight>, which points to corresponding linked list of objects representing tasks in the overflow list. To add a task-representing object to the overflow list, the thread determines the object&apos;s class by reading the class pointer that most object-oriented languages place in object data structures&apos; headers. If the overflow data structure already contains an entry that represents that object&apos;s class, the thread adds the task at the head of the corresponding list. It does so by placing the list-field contents of the class&apos;s overflow-data-structure entry into the object&apos;s erstwhile class-pointer field (labeled &ldquo;next&rdquo; in the drawing to represent its new role as a pointer to the next list element) and placing in that list field a pointer to the added point. (The overflow objects are listed by class so that during retrieval the proper class pointer can be re-installed in each object&apos;s header.) </paragraph>
<paragraph id="P-0080" lvl="0"><number>&lsqb;0080&rsqb;</number> If the overflow data structure <highlight><bold>90</bold></highlight> does not already contain an entry that represents the object&apos;s class, the thread adds such an entry to that structure and places the task-representing object at the head of the associated list by making the added entry&apos;s list pointer point to that object. The thread also NULLs the erstwhile class-pointer field in the object&apos;s header to indicate that it is the last member of its class&apos;s linked list of overflowed tasks. </paragraph>
<paragraph id="P-0081" lvl="0"><number>&lsqb;0081&rsqb;</number> When a thread has exhausted its queue, it determines whether the overflow data structure has any objects in its lists. If not, it attempts to steal from other threads&apos; work queues. Otherwise, the thread obtains a lock on the overflow data structure and retrieves one or more objects from one or more of its lists. In doing so, the thread restores each retrieved object&apos;s class pointer and re-links remaining objects as necessary to maintain the overflow lists. The retrieved objects are pushed onto the bottom of the queue. Although the number of objects retrieved as a result of a single queue exhaustion is not critical, it is advantageous for a thread to retrieve enough to fill half of its queue space. Then, if its queue subsequently overflows and it removes half from its queue as suggested above, the retrieved objects will be in the top half of the queue, so those retrieved objects will not be placed again on an overflow list. </paragraph>
<paragraph id="P-0082" lvl="0"><number>&lsqb;0082&rsqb;</number> Although <cross-reference target="DRAWINGS">FIG. 10</cross-reference> depicts the overflow data structure <highlight><bold>90</bold></highlight> as a compact table, the overflow data structure may be provided efficiently by storing a pointer to a class&apos;s list of overflow objects directly in the class data structure and maintaining the set of classes where overflow lists are non-empty as a linked list of class data structures threaded through a class-data-structure field provided for that purpose. For the sake of convenience, we will assume this organization in the discussion of the present invention&apos;s approach to termination detection, to which we now turn. </paragraph>
<paragraph id="P-0083" lvl="0"><number>&lsqb;0083&rsqb;</number> When an owner thread&apos;s execution of <cross-reference target="DRAWINGS">FIG. 9</cross-reference>&apos;s bottom-popping routine produces a NULL return value, indicating that it has exhausted its own work queue, the thread attempts to find tasks identified by other sources. And it continues doing so until it either (1) thereby finds a further operation-C task to perform or (2) concludes, in accordance with the present invention, that no more such tasks remain. In the example scenario, it can then move on to operation D. </paragraph>
<paragraph id="P-0084" lvl="0"><number>&lsqb;0084&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 11</cross-reference> sets forth in simplified code an example of how a thread can search for further work and determine whether any more exists. As will be explained presently in more detail, an executing thread that has exhausted its own work queue in the illustrated embodiment calls <cross-reference target="DRAWINGS">FIG. 11</cross-reference>&apos;s dequeFindWork routine, passing that routine a pointer to the work queue of which it is the owner. If dequeFindWork returns a NULL value, the executing thread concludes that no further operation-C tasks remain, and it presses on to operation D. If dequeFindWork returns a non-NULL task identifier, on the other hand, the executing thread performs the task thereby identified. The executing thread may push further tasks onto its work queue in the course of performing that task, and, as will be explained below, dequeFindWork may have pushed task identifiers onto the executing thread&apos;s work queue in the course of finding the task identifier that it returns. So the executing thread returns to bottom-popping task identifiers from its own work queue after it has performed the task whose identifier dequeFindWork returns. </paragraph>
<paragraph id="P-0085" lvl="0"><number>&lsqb;0085&rsqb;</number> In <cross-reference target="DRAWINGS">FIG. 11</cross-reference>, dequeFindWork&apos;s second line shows that its first step is to call a helper routine, findWorkHelper, which <cross-reference target="DRAWINGS">FIG. 11</cross-reference> also lists. This routine is what attempts to find tasks identified in locations other than the executing thread&apos;s work queue. Since the illustrated embodiment limits its work-queue memory space and uses overflow lists as a result, the helper routine looks for task identifiers in any overflow lists. The subroutine call in findWorkHelper&apos;s second line represents this search. If the overflow-list-retrieval routine (whose code the drawings omit) is successful in retrieving task identifiers, its return value, as well as those of the helper routine and of dequeFindWork itself, is one of them. </paragraph>
<paragraph id="P-0086" lvl="0"><number>&lsqb;0086&rsqb;</number> If the overflow-list-retrieval routine is unsuccessful, it returns a NULL value. As the helper routine&apos;s third and fourth lines indicate, the helper routine responds to such a NULL return value by trying to steal a task from another thread&apos;s work queue. Now, perusal of <cross-reference target="DRAWINGS">FIG. 7</cross-reference>&apos;s top-popping routine reveals that the illustrated embodiment permits interference from other threads to cause a steal attempt to fail even if the queue contains plenty of task identifiers. To minimize the likelihood that any such interference will occur systematically, the work-stealing routine that Fig. l&apos;s helper routine calls in its fourth line may use the probabilistic approach to stealing that <cross-reference target="DRAWINGS">FIG. 12</cross-reference> sets forth. </paragraph>
<paragraph id="P-0087" lvl="0"><number>&lsqb;0087&rsqb;</number> The stealWork routine that <cross-reference target="DRAWINGS">FIG. 12</cross-reference> lists assumes a common data structure of which <cross-reference target="DRAWINGS">FIG. 6</cross-reference>&apos;s structure <highlight><bold>96</bold></highlight> shows a few fields of interest. The work-queue data structures are assumed to include pointers <highlight><bold>98</bold></highlight> to this common data structure, which <cross-reference target="DRAWINGS">FIG. 12</cross-reference>&apos;s second line refers to as being of the &ldquo;globalDeques&rdquo; data type. Among that structure&apos;s fields is a field <highlight><bold>100</bold></highlight> that tells how many individual-thread work queues there are, and, as the stealWork routine&apos;s third and fourth lines indicate, it sets a loop-iteration limit to, in this example, twice that number. </paragraph>
<paragraph id="P-0088" lvl="0"><number>&lsqb;0088&rsqb;</number> As will now be explained in connection with the stealWork routine&apos;s fifth through thirteenth lines, that routine either succeeds in stealing from another work queue or gives up after making a number of attempts equal to the loop-iteration limit. On each attempt, it makes its seventh-line call to a subroutine (whose code the drawings omit) that randomly chooses a queue other than the one associated with the executing thread, and it tries to steal a task from that queue, as its eighth and ninth lines indicate. If the top-popping routine called in the ninth line fails, the illustrated embodiment also makes the tenth line&apos;s system call to terminate the thread&apos;s current execution time slice in favor of any threads that are waiting for processor time. </paragraph>
<paragraph id="P-0089" lvl="0"><number>&lsqb;0089&rsqb;</number> If the number of repetitions of the loop of <cross-reference target="DRAWINGS">FIG. 12</cross-reference>&apos;s sixth through thirteenth lines reaches the loop-repetition limit without successfully stealing a task, the stealWork routine returns a NULL value, as its fourteenth line indicates, and so does <cross-reference target="DRAWINGS">FIG. 11</cross-reference>&apos;s findWorkHelper routine, as its fourth and sixth lines indicate. </paragraph>
<paragraph id="P-0090" lvl="0"><number>&lsqb;0090&rsqb;</number> Now, a review of, for instance, <cross-reference target="DRAWINGS">FIG. 12</cross-reference>&apos;s stealWork routine reveals that in the illustrated embodiment a thread can &ldquo;give up&rdquo; on seeking work in other threads&apos; work queues&mdash;and its execution of that routine therefore produce a NULL return value&mdash;even in some situations in which one or more of the other queues do contain remaining task identifiers. Allowing the thread to go on to the next operation when it has thus failed to find other work could result in a serious work imbalance. </paragraph>
<paragraph id="P-0091" lvl="0"><number>&lsqb;0091&rsqb;</number> Indeed, such an imbalance could result even if the operation instead used a work-stealing routine that would not give up until all queues are empty. Suppose, for example, that a thread moves on to the next operation because all work queues are empty, but when it does so one or more other threads are still processing respective tasks. Since the operation that it is leaving is one that identifies tasks dynamically, there could actually be a large number of (as yet unidentified) tasks yet to be performed. The thread that failed to find work in the other threads&apos; queues (and, in the illustrated example, in the overflow lists) would then be leaving much of that operation&apos;s tasks to the other threads rather than optimally sharing those tasks. </paragraph>
<paragraph id="P-0092" lvl="0"><number>&lsqb;0092&rsqb;</number> To prevent this, the present invention employs <cross-reference target="DRAWINGS">FIG. 6</cross-reference>&apos;s status word <highlight><bold>102</bold></highlight>. This word includes a separate (typically, single-bit) field corresponding to each of the threads. The steps represented by <cross-reference target="DRAWINGS">FIG. 5</cross-reference>&apos;s block <highlight><bold>62</bold></highlight> include initializing that word&apos;s contents by setting all of those fields to a (say, binary-one) value that represents what we will call an active thread state. When <cross-reference target="DRAWINGS">FIG. 12</cross-reference>&apos;s stealWork routine fails to steal work and as a result causes a NULL return value from the <cross-reference target="DRAWINGS">FIG. 11</cross-reference> dequeFindWork routine&apos;s second-line call of the findWorkHelper routine, dequeFindWork makes its fifth-line call of a routine that changes status word <highlight><bold>102</bold></highlight>&apos;s value. That fifth-line subroutine atomically resets the executing thread&apos;s field in that word to an inactivity-indicating value of, say, a binary zero. (We use the term word in &ldquo;status word&rdquo; because the status word in almost every implementation be of a size that can be accessed in a single machine instruction. This is not an essential part of the invention, but the field resetting does have to be performed in such a way as not to affect other fields, and it has to be possible to read the status &ldquo;word&rdquo; in an atomic fashion.) </paragraph>
<paragraph id="P-0093" lvl="0"><number>&lsqb;0093&rsqb;</number> The dequeFindWork routine then enters a loop that its seventh through nineteenth lines set forth. This loop repeatedly looks for further work, in a way that will be explained shortly. When it finds work, it leaves the loop, as the seventh line indicates, with the result that the dequeFindWork routine returns the thereby-found task&apos;s identifier. So, if the loop finds work, the executing thread performs the task and returns to attempting to pop task identifiers from its own queue until it again exhausts that queue and returns to dequeFindWork to find work again in other locations. In the illustrated scenario, that is, the thread continues to work on <cross-reference target="DRAWINGS">FIG. 5</cross-reference>&apos;s operation C. </paragraph>
<paragraph id="P-0094" lvl="0"><number>&lsqb;0094&rsqb;</number> According to the present invention, the only way in which the thread can leave that operation is for every field of <cross-reference target="DRAWINGS">FIG. 6</cross-reference>&apos;s status word <highlight><bold>102</bold></highlight> to contain the inactivity-indicating value, i.e., for the illustrated embodiment&apos;s status word to contain all zeroes. As was just explained, no thread can set its status field to indicate inactivity while its queue has work. Moreover, a queue places work in the overflow lists only when it has work in its queue, and, when it exhausts its queue, it checks the overflow lists before it marks itself inactive. So no thread can leave the operation unless all of its tasks have been completed. </paragraph>
<paragraph id="P-0095" lvl="0"><number>&lsqb;0095&rsqb;</number> If the status word does not indicate that all work has been completed, the dequeFindWork routine checks for work. Some embodiments may reduce the loop frequency by, as the illustrated embodiment illustrates in its ninth-line step, first allowing any threads waiting for execution time to be accorded some. In any event, it then checks for further work by calling a checkForWork routine, as its tenth line indicates. </paragraph>
<paragraph id="P-0096" lvl="0"><number>&lsqb;0096&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 13</cross-reference> sets forth the checkForWork routine. As that routine&apos;s third line indicates, it determines whether there is a non-NULL value in <cross-reference target="DRAWINGS">FIG. 6</cross-reference>&apos;s common classesWithWork field <highlight><bold>104</bold></highlight>, which is the pointer to the linked list of class structures whose overflow lists contain at least one task identifier each. If there are no task identifiers in overflow lists, the classesWithWork field contains a NULL value. </paragraph>
<paragraph id="P-0097" lvl="0"><number>&lsqb;0097&rsqb;</number> If the classesWithWork field does contain a NULL value, that third-line step also calls a peekDeque routine. As <cross-reference target="DRAWINGS">FIG. 13</cross-reference> shows, the peekDeque routine repeatedly chooses other threads&apos; queues at random, just as <cross-reference target="DRAWINGS">FIG. 12</cross-reference>&apos;s stealWork routine does. Instead of claiming any work thereby found, though, it merely returns a Boolean value that indicates whether it found work in the other queues. So the return value produced by the checkForWork routine&apos;s third-line step indicates whether work was either in the overflow lists or in the other threads&apos; work queues, and this is the value that is returned to <cross-reference target="DRAWINGS">FIG. 11</cross-reference>&apos;s dequeFindWork routine in that routine&apos;s the tenth line. </paragraph>
<paragraph id="P-0098" lvl="0"><number>&lsqb;0098&rsqb;</number> But the dequeFindWork routine does not claim the thereby-discovered task for the executing queue, at least not immediately. If it did claim a task while its field in <cross-reference target="DRAWINGS">FIG. 6</cross-reference>&apos;s common status word <highlight><bold>102</bold></highlight> contained an inactivity-containing value, other threads would neither find that task nor, if they could find no other work, be prevented by an activity-indicating field from leaving operation C. So, as dequeFindWork&apos;s thirteenth and fourteen lines indicate, that routine sets the executing thread&apos;s status-word field to the active state. Only after doing so does it call the findWorkHelper routine and thereby possibly claiming a remaining task for that thread. (Note that any task thereby claimed will not necessarily be the same as the task whose discovery caused the thread to set its status-word field.) As the fifteenth and sixteen lines indicate, it marks itself inactive again if the findWorkHelper nonetheless fails to claim a task, and the loop beginning on the seventh line begins again. Otherwise, the dequeFindWork routine returns the identifier of the claimed task, which the executing thread accordingly performs. </paragraph>
<paragraph id="P-0099" lvl="0"><number>&lsqb;0099&rsqb;</number> In this way, all threads keep working on operation C so long as any of its tasks remain to be done. The present invention thus provides an efficient way of ensuring that a parallel-execution operation&apos;s tasks are well divided among the threads assigned to the operation, even if that operation identifies its tasks dynamically. The present invention thus constitutes a significant advance in the art.</paragraph>
</section>
</detailed-description>
</subdoc-description>
<subdoc-claims>
<heading lvl="1">What is claimed is: </heading>
<claim id="CLM-00001">
<claim-text><highlight><bold>1</bold></highlight>. A computer system that employs a plurality of threads of execution to perform a parallel-execution operation in which the threads identify tasks dynamically and in which the computer system: 
<claim-text>A) provides a global status word that includes a separate status-word field associated with each of the threads; and </claim-text>
<claim-text>B) so operates the threads that each thread: 
<claim-text>i) executes a task-finding routine to find tasks previously identified dynamically and performs tasks thereby found, with the status-word field associated with that thread containing an activity-representing value, until the task-finding routine finds no more tasks; </claim-text>
<claim-text>ii) when the task-finding routine finds no more tasks, sets the contents of the status-word field associated with that thread to an inactivity-indicating value; </claim-text>
<claim-text>iii) while the status-word field associated with any other thread contains an activity-indicating value, searches for a task and, if it finds one, sets the status-word field to the activity-indicating value before attempting to execute a task; and </claim-text>
<claim-text>iv) if none of the status-word fields contains an activity-indicating value, terminates its performance of the parallel-execution operation. </claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002">
<claim-text><highlight><bold>2</bold></highlight>. A computer system as defined in <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference> wherein the parallel-execution operation is a garbage-collection operation. </claim-text>
</claim>
<claim id="CLM-00003">
<claim-text><highlight><bold>3</bold></highlight>. A computer system as defined in <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference> wherein: 
<claim-text>A) each thread has associated with it a respective work queue in which it places task identifiers of tasks that identifies dynamically; </claim-text>
<claim-text>B) the task-finding routine executed by an executing thread includes performing an initial search for a task identifiers in the work queue associated with the executing thread and, if that work queue contains no task identifiers that the executing thread can claim, thereafter performing a further search for a task identifier in at least one other task-storage location. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00004">
<claim-text><highlight><bold>4</bold></highlight>. A computer system as defined in <dependent-claim-reference depends_on="CLM-00003">claim 3</dependent-claim-reference> wherein the parallel-execution operation is a garbage-collection operation. </claim-text>
</claim>
<claim id="CLM-00005">
<claim-text><highlight><bold>5</bold></highlight>. A computer system as defined in <dependent-claim-reference depends_on="CLM-00003">claim 3</dependent-claim-reference> wherein the at least one other task-storage location includes at least one work queue associated with a thread other than the executing thread. </claim-text>
</claim>
<claim id="CLM-00006">
<claim-text><highlight><bold>6</bold></highlight>. A computer system as defined in <dependent-claim-reference depends_on="CLM-00005">claim 5</dependent-claim-reference> wherein: 
<claim-text>A) there is a size limit associated with each work queue; </claim-text>
<claim-text>B) when a given thread dynamically identifies a given task that would cause the number of task entries in the work queue associated with the given thread to exceed the size limit if a task identifier that identifies it were placed in that work queue, the given thread instead places that task identifier in an overflow list instead of in that work queue; and </claim-text>
<claim-text>C) the at least one other task-storage location includes at least one such overflow list. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00007">
<claim-text><highlight><bold>7</bold></highlight>. A computer system as defined in <dependent-claim-reference depends_on="CLM-00005">claim 5</dependent-claim-reference> wherein the task-finding routine includes selecting in a random manner the at least one work queue associated with a thread other than the executing thread. </claim-text>
</claim>
<claim id="CLM-00008">
<claim-text><highlight><bold>8</bold></highlight>. A computer system as defined in <dependent-claim-reference depends_on="CLM-00005">claim 5</dependent-claim-reference> wherein the further search includes repeatedly searching a work queue associated with a thread other than the executing thread until the executing thread thereby finds a task or has performed a number of repetitions equal to a repetition limit greater than one. </claim-text>
</claim>
<claim id="CLM-00009">
<claim-text><highlight><bold>9</bold></highlight>. A computer system as defined in <dependent-claim-reference depends_on="CLM-00008">claim 8</dependent-claim-reference> wherein the task-finding routine includes selecting in a random manner the at least one work queue associated with a thread other than the executing thread. </claim-text>
</claim>
<claim id="CLM-00010">
<claim-text><highlight><bold>10</bold></highlight>. A computer system as defined in <dependent-claim-reference depends_on="CLM-00003">claim 3</dependent-claim-reference> wherein: 
<claim-text>A) there is a size limit associated with each work queue; </claim-text>
<claim-text>B) when a given thread dynamically identifies a given task that would cause the number of task entries in the work queue associated with the given thread to exceed the size limit if a task identifier that identifies it were placed in that work queue, the given thread instead places that task identifier in an overflow list instead of in that work queue; and </claim-text>
<claim-text>C) the at least one other task-storage location includes at least one such overflow list. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00011">
<claim-text><highlight><bold>11</bold></highlight>. A computer system as defined in <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference> wherein the status word fits in a memory location accessible in a single machine instruction. </claim-text>
</claim>
<claim id="CLM-00012">
<claim-text><highlight><bold>12</bold></highlight>. A computer system as defined in <dependent-claim-reference depends_on="CLM-00011">claim 11</dependent-claim-reference> wherein the parallel-execution operation is a garbage-collection operation. </claim-text>
</claim>
<claim id="CLM-00013">
<claim-text><highlight><bold>13</bold></highlight>. A computer system as defined in <dependent-claim-reference depends_on="CLM-00011">claim 11</dependent-claim-reference> wherein each status-word field is a single-bit field. </claim-text>
</claim>
<claim id="CLM-00014">
<claim-text><highlight><bold>14</bold></highlight>. A computer system as defined in <dependent-claim-reference depends_on="CLM-00011">claim 13</dependent-claim-reference> wherein the activity-indicating value is a logic one and the inactivity-indicating value is a logic zero. </claim-text>
</claim>
<claim id="CLM-00015">
<claim-text><highlight><bold>15</bold></highlight>. For employing a plurality of threads of execution to perform a parallel-execution operation in which the threads identify tasks dynamically, a method comprising: 
<claim-text>A) providing a global status word that includes a separate status-word field associated with each of the threads; and </claim-text>
<claim-text>B) so operating the threads that each thread: 
<claim-text>i) executes a task-finding routine to find tasks previously identified dynamically and performs tasks thereby found, with the status-word field associated with that thread containing an activity-representing value, until the task-finding routine finds no more tasks; </claim-text>
<claim-text>ii) when the task-finding routine finds no more tasks, sets the contents of the status-word field associated with that thread to an inactivity-indicating value; </claim-text>
<claim-text>iii) while the status-word field associated with any other thread contains an activity-indicating value, searches for a task and, if it finds one, sets the status-word field to the activity-indicating value before attempting to execute a task; and </claim-text>
<claim-text>iv) if none of the status-word fields contains an activity-indicating value, terminates its performance of the parallel-execution operation. </claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00016">
<claim-text><highlight><bold>16</bold></highlight>. A method as defined in <dependent-claim-reference depends_on="CLM-00011">claim 15</dependent-claim-reference> wherein the parallel-execution operation is a garbage-collection operation. </claim-text>
</claim>
<claim id="CLM-00017">
<claim-text><highlight><bold>17</bold></highlight>. A method as defined in <dependent-claim-reference depends_on="CLM-00011">claim 15</dependent-claim-reference> wherein: 
<claim-text>A) each thread has associated with it a respective work queue in which it places task identifiers of tasks that identifies dynamically; </claim-text>
<claim-text>B) the task-finding routine executed by an executing thread includes performing an initial search for a task identifiers in the work queue associated with the executing thread and, if that work queue contains no task identifiers that the executing thread can claim, thereafter performing a further search for a task identifier in at least one other task-storage location. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00018">
<claim-text><highlight><bold>18</bold></highlight>. A method as defined in <dependent-claim-reference depends_on="CLM-00011">claim 17</dependent-claim-reference> wherein the parallel-execution operation is a garbage-collection operation. </claim-text>
</claim>
<claim id="CLM-00019">
<claim-text><highlight><bold>19</bold></highlight>. A method as defined in <dependent-claim-reference depends_on="CLM-00011">claim 17</dependent-claim-reference> wherein the at least one other task-storage location includes at least one work queue associated with a thread other than the executing thread. </claim-text>
</claim>
<claim id="CLM-00020">
<claim-text><highlight><bold>20</bold></highlight>. A method as defined in <dependent-claim-reference depends_on="CLM-00011">claim 19</dependent-claim-reference> wherein: 
<claim-text>A) there is a size limit associated with each work queue; </claim-text>
<claim-text>B) when a given thread dynamically identifies a given task that would cause the number of task entries in the work queue associated with the given thread to exceed the size limit if a task identifier that identifies it were placed in that work queue, the given thread instead places that task identifier in an overflow list instead of in that work queue; and </claim-text>
<claim-text>C) the at least one other task-storage location includes at least one such overflow list. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00021">
<claim-text><highlight><bold>21</bold></highlight>. A method as defined in <dependent-claim-reference depends_on="CLM-00011">claim 19</dependent-claim-reference> wherein the task-finding routine includes selecting in a random manner the at least one work queue associated with a thread other than the executing thread. </claim-text>
</claim>
<claim id="CLM-00022">
<claim-text><highlight><bold>22</bold></highlight>. A method as defined in <dependent-claim-reference depends_on="CLM-00011">claim 19</dependent-claim-reference> wherein the further search includes repeatedly searching a work queue associated with a thread other than the executing thread until the executing thread thereby finds a task or has performed a number of repetitions equal to a repetition limit greater than one. </claim-text>
</claim>
<claim id="CLM-00023">
<claim-text><highlight><bold>23</bold></highlight>. A method as defined in <dependent-claim-reference depends_on="CLM-00022">claim 22</dependent-claim-reference> wherein the task-finding routine includes selecting in a random manner the at least one work queue associated with a thread other than the executing thread. </claim-text>
</claim>
<claim id="CLM-00024">
<claim-text><highlight><bold>24</bold></highlight>. A method as defined in <dependent-claim-reference depends_on="CLM-00011">claim 17</dependent-claim-reference> wherein: 
<claim-text>A) there is a size limit associated with each work queue; </claim-text>
<claim-text>B) when a given thread dynamically identifies a given task that would cause the number of task entries in the work queue associated with the given thread to exceed the size limit if a task identifier that identifies it were placed in that work queue, the given thread instead places that task identifier in an overflow list instead of in that work queue; and </claim-text>
<claim-text>C) the at least one other task-storage location includes at least one such overflow list. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00025">
<claim-text><highlight><bold>25</bold></highlight>. A method as defined in <dependent-claim-reference depends_on="CLM-00011">claim 15</dependent-claim-reference> wherein the status word fits in a memory location accessible in a single machine instruction. </claim-text>
</claim>
<claim id="CLM-00026">
<claim-text><highlight><bold>26</bold></highlight>. A method as defined in <dependent-claim-reference depends_on="CLM-00022">claim 25</dependent-claim-reference> wherein the parallel-execution operation is a garbage-collection operation. </claim-text>
</claim>
<claim id="CLM-00027">
<claim-text><highlight><bold>27</bold></highlight>. A method as defined in <dependent-claim-reference depends_on="CLM-00022">claim 25</dependent-claim-reference> wherein each status-word field is a single-bit field. </claim-text>
</claim>
<claim id="CLM-00028">
<claim-text><highlight><bold>28</bold></highlight>. A method as defined in <dependent-claim-reference depends_on="CLM-00022">claim 27</dependent-claim-reference> wherein the activity-indicating value is a logic one and the inactivity-indicating value is a logic zero. </claim-text>
</claim>
<claim id="CLM-00029">
<claim-text><highlight><bold>29</bold></highlight>. A storage medium containing instructions readable by a computer system to configure the computer system to employ a plurality of threads of execution to perform a parallel-execution operation in which the threads identify tasks dynamically and in which the computer system: 
<claim-text>A) provides a global status word that includes a separate status-word field associated with each of the threads; and </claim-text>
<claim-text>B) so operates the threads that each thread: 
<claim-text>i) executes a task-finding routine to find tasks previously identified dynamically and performs tasks thereby found, with the status-word field associated with that thread containing an activity-representing value, until the task-finding routine finds no more tasks; </claim-text>
<claim-text>ii) when the task-finding routine finds no more tasks, sets the contents of the status-word field associated with that thread to an inactivity-indicating value; </claim-text>
<claim-text>iii) while the status-word field associated with any other thread contains an activity-indicating value, searches for a task and, if it finds one, sets the status-word field to the activity-indicating value before attempting to execute a task; and </claim-text>
<claim-text>iv) if none of the status-word fields contains an activity-indicating value, terminates its performance of the parallel-execution operation. </claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00030">
<claim-text><highlight><bold>30</bold></highlight>. A storage medium as defined in <dependent-claim-reference depends_on="CLM-00022">claim 29</dependent-claim-reference> wherein the parallel-execution operation is a garbage-collection operation. </claim-text>
</claim>
<claim id="CLM-00031">
<claim-text><highlight><bold>31</bold></highlight>. A storage medium as defined in <dependent-claim-reference depends_on="CLM-00022">claim 29</dependent-claim-reference> wherein: 
<claim-text>A) each thread has associated with it a respective work queue in which it places task identifiers of tasks that identifies dynamically; </claim-text>
<claim-text>B) the task-finding routine executed by an executing thread includes performing an initial search for a task identifiers in the work queue associated with the executing thread and, if that work queue contains no task identifiers that the executing thread can claim, thereafter performing a further search for a task identifier in at least one other task-storage location. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00032">
<claim-text><highlight><bold>32</bold></highlight>. A storage medium as defined in <dependent-claim-reference depends_on="CLM-00033">claim 31</dependent-claim-reference> wherein the parallel-execution operation is a garbage-collection operation. </claim-text>
</claim>
<claim id="CLM-00033">
<claim-text><highlight><bold>33</bold></highlight>. A storage medium as defined in <dependent-claim-reference depends_on="CLM-00033">claim 31</dependent-claim-reference> wherein the at least one other task-storage location includes at least one work queue associated with a thread other than the executing thread. </claim-text>
</claim>
<claim id="CLM-00034">
<claim-text><highlight><bold>34</bold></highlight>. A storage medium as defined in <dependent-claim-reference depends_on="CLM-00033">claim 33</dependent-claim-reference> wherein: 
<claim-text>A) there is a size limit associated with each work queue; </claim-text>
<claim-text>B) when a given thread dynamically identifies a given task that would cause the number of task entries in the work queue associated with the given thread to exceed the size limit if a task identifier that identifies it were placed in that work queue, the given thread instead places that task identifier in an overflow list instead of in that work queue; and </claim-text>
<claim-text>C) the at least one other task-storage location includes at least one such overflow list. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00035">
<claim-text><highlight><bold>35</bold></highlight>. A storage medium as defined in <dependent-claim-reference depends_on="CLM-00033">claim 33</dependent-claim-reference> wherein the task-finding routine includes selecting in a random manner the at least one work queue associated with a thread other than the executing thread. </claim-text>
</claim>
<claim id="CLM-00036">
<claim-text><highlight><bold>36</bold></highlight>. A storage medium as defined in <dependent-claim-reference depends_on="CLM-00033">claim 33</dependent-claim-reference> wherein the further search includes repeatedly searching a work queue associated with a thread other than the executing thread until the executing thread thereby finds a task or has performed a number of repetitions equal to a repetition limit greater than one. </claim-text>
</claim>
<claim id="CLM-00037">
<claim-text><highlight><bold>37</bold></highlight>. A storage medium as defined in <dependent-claim-reference depends_on="CLM-00033">claim 36</dependent-claim-reference> wherein the task-finding routine includes selecting in a random manner the at least one work queue associated with a thread other than the executing thread. </claim-text>
</claim>
<claim id="CLM-00038">
<claim-text><highlight><bold>38</bold></highlight>. A storage medium as defined in <dependent-claim-reference depends_on="CLM-00033">claim 31</dependent-claim-reference> wherein: 
<claim-text>A) there is a size limit associated with each work queue; </claim-text>
<claim-text>B) when a given thread dynamically identifies a given task that would cause the number of task entries in the work queue associated with the given thread to exceed the size limit if a task identifier that identifies it were placed in that work queue, the given thread instead places that task identifier in an overflow list instead of in that work queue; and </claim-text>
<claim-text>C) the at least one other task-storage location includes at least one such overflow list. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00039">
<claim-text><highlight><bold>39</bold></highlight>. A storage medium as defined in <dependent-claim-reference depends_on="CLM-00022">claim 29</dependent-claim-reference> wherein the status word fits in a memory location accessible in a single machine instruction. </claim-text>
</claim>
<claim id="CLM-00040">
<claim-text><highlight><bold>40</bold></highlight>. A storage medium as defined in <dependent-claim-reference depends_on="CLM-00033">claim 39</dependent-claim-reference> wherein the parallel-execution operation is a garbage-collection operation. </claim-text>
</claim>
<claim id="CLM-00041">
<claim-text><highlight><bold>41</bold></highlight>. A storage medium as defined in <dependent-claim-reference depends_on="CLM-00033">claim 39</dependent-claim-reference> wherein each status-word field is a single-bit field. </claim-text>
</claim>
<claim id="CLM-00042">
<claim-text><highlight><bold>42</bold></highlight>. A storage medium as defined in <dependent-claim-reference depends_on="CLM-00044">claim 41</dependent-claim-reference> wherein the activity-indicating value is a logic one and the inactivity-indicating value is a logic zero. </claim-text>
</claim>
<claim id="CLM-00043">
<claim-text><highlight><bold>43</bold></highlight>. A computer signal representing a sequence of instructions that, when executed by a computer system, configures the computer system to employ a plurality of threads of execution to perform a parallel-execution operation in which the threads identify tasks dynamically and in which the computer system: 
<claim-text>A) provides a global status word that includes a separate status-word field associated with each of the threads; and </claim-text>
<claim-text>B) so operates the threads that each thread: 
<claim-text>i) executes a task-finding routine to find tasks previously identified dynamically and performs tasks thereby found, with the status-word field associated with that thread containing an activity-representing value, until the task-finding routine finds no more tasks; </claim-text>
<claim-text>ii) when the task-finding routine finds no more tasks, sets the contents of the status-word field associated with that thread to an inactivity-indicating value; </claim-text>
<claim-text>iii) while the status-word field associated with any other thread contains an activity-indicating value, searches for a task and, if it finds one, sets the status-word field to the activity-indicating value before attempting to execute a task; and </claim-text>
<claim-text>iv) if none of the status-word fields contains an activity-indicating value, terminates its performance of the parallel-execution operation. </claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00044">
<claim-text><highlight><bold>44</bold></highlight>. A computer signal as defined in <dependent-claim-reference depends_on="CLM-00044">claim 43</dependent-claim-reference> wherein the parallel-execution operation is a garbage-collection operation. </claim-text>
</claim>
<claim id="CLM-00045">
<claim-text><highlight><bold>45</bold></highlight>. A computer signal as defined in <dependent-claim-reference depends_on="CLM-00044">claim 43</dependent-claim-reference> wherein: 
<claim-text>A) each thread has associated with it a respective work queue in which it places task identifiers of tasks that identifies dynamically; </claim-text>
<claim-text>B) the task-finding routine executed by an executing thread includes performing an initial search for a task identifiers in the work queue associated with the executing thread and, if that work queue contains no task identifiers that the executing thread can claim, thereafter performing a further search for a task identifier in at least one other task-storage location. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00046">
<claim-text><highlight><bold>46</bold></highlight>. A computer signal as defined in <dependent-claim-reference depends_on="CLM-00044">claim 45</dependent-claim-reference> wherein the parallel-execution operation is a garbage-collection operation. </claim-text>
</claim>
<claim id="CLM-00047">
<claim-text><highlight><bold>47</bold></highlight>. A computer signal as defined in <dependent-claim-reference depends_on="CLM-00044">claim 45</dependent-claim-reference> wherein the at least one other task-storage location includes at least one work queue associated with a thread other than the executing thread. </claim-text>
</claim>
<claim id="CLM-00048">
<claim-text><highlight><bold>48</bold></highlight>. A computer signal as defined in <dependent-claim-reference depends_on="CLM-00044">claim 47</dependent-claim-reference> wherein: 
<claim-text>A) there is a size limit associated with each work queue; </claim-text>
<claim-text>B) when a given thread dynamically identifies a given task that would cause the number of task entries in the work queue associated with the given thread to exceed the size limit if a task identifier that identifies it were placed in that work queue, the given thread instead places that task identifier in an overflow list instead of in that work queue; and </claim-text>
<claim-text>C) the at least one other task-storage location includes at least one such overflow list. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00049">
<claim-text><highlight><bold>49</bold></highlight>. A computer signal as defined in <dependent-claim-reference depends_on="CLM-00044">claim 47</dependent-claim-reference> wherein the task-finding routine includes selecting in a random manner the at least one work queue associated with a thread other than the executing thread. </claim-text>
</claim>
<claim id="CLM-00050">
<claim-text><highlight><bold>50</bold></highlight>. A computer signal as defined in <dependent-claim-reference depends_on="CLM-00044">claim 47</dependent-claim-reference> wherein the further search includes repeatedly searching a work queue associated with a thread other than the executing thread until the executing thread thereby finds a task or has performed a number of repetitions equal to a repetition limit greater than one. </claim-text>
</claim>
<claim id="CLM-00051">
<claim-text><highlight><bold>51</bold></highlight>. A computer signal as defined in <dependent-claim-reference depends_on="CLM-00055">claim 50</dependent-claim-reference> wherein the task-finding routine includes selecting in a random manner the at least one work queue associated with a thread other than the executing thread. </claim-text>
</claim>
<claim id="CLM-00052">
<claim-text><highlight><bold>52</bold></highlight>. A computer signal as defined in <dependent-claim-reference depends_on="CLM-00044">claim 45</dependent-claim-reference> wherein: 
<claim-text>A) there is a size limit associated with each work queue; </claim-text>
<claim-text>B) when a given thread dynamically identifies a given task that would cause the number of task entries in the work queue associated with the given thread to exceed the size limit if a task identifier that identifies it were placed in that work queue, the given thread instead places that task identifier in an overflow list instead of in that work queue; and </claim-text>
<claim-text>C) the at least one other task-storage location includes at least one such overflow list. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00053">
<claim-text><highlight><bold>53</bold></highlight>. A computer signal as defined in <dependent-claim-reference depends_on="CLM-00044">claim 43</dependent-claim-reference> wherein the status word fits in a memory location accessible in a single machine instruction. </claim-text>
</claim>
<claim id="CLM-00054">
<claim-text><highlight><bold>54</bold></highlight>. A computer signal as defined in <dependent-claim-reference depends_on="CLM-00055">claim 53</dependent-claim-reference> wherein the parallel-execution operation is a garbage-collection operation. </claim-text>
</claim>
<claim id="CLM-00055">
<claim-text><highlight><bold>55</bold></highlight>. A computer signal as defined in <dependent-claim-reference depends_on="CLM-00055">claim 53</dependent-claim-reference> wherein each status-word field is a single-bit field. </claim-text>
</claim>
<claim id="CLM-00056">
<claim-text><highlight><bold>56</bold></highlight>. A computer signal as defined in <dependent-claim-reference depends_on="CLM-00055">claim 55</dependent-claim-reference> wherein the activity-indicating value is a logic one and the inactivity-indicating value is a logic zero. </claim-text>
</claim>
<claim id="CLM-00057">
<claim-text><highlight><bold>57</bold></highlight>. A computer system that employs a plurality of threads of execution to perform a parallel-execution operation in which the threads identify tasks dynamically, the computer system including: 
<claim-text>A) means for providing a global status word that includes a separate status-word field associated with each of the threads; and </claim-text>
<claim-text>B) means for so operating the threads that each thread: 
<claim-text>i) executes a task-finding routine to find tasks previously identified dynamically and performs tasks thereby found, with the status-word field associated with that thread containing an activity-representing value, until the task-finding routine finds no more tasks; </claim-text>
<claim-text>ii) when the task-finding routine finds no more tasks, sets the contents of the status-word field associated with that thread to an inactivity-indicating value; </claim-text>
<claim-text>iii) while the status-word field associated with any other thread contains an activity-indicating value, searches for a task and, if it finds one, sets the status-word field to the activity-indicating value before attempting to execute a task; and </claim-text>
<claim-text>iv) if none of the status-word fields contains an activity-indicating value, terminates its performance of the parallel-execution operation.</claim-text>
</claim-text>
</claim-text>
</claim>
</subdoc-claims>
<subdoc-drawings id="DRAWINGS">
<heading lvl="0" align="CENTER">Drawings</heading>
<representative-figure>1</representative-figure>
<figure id="figure-D00000">
<image id="EMI-D00000" file="US20030005029A1-20030102-D00000.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00001">
<image id="EMI-D00001" file="US20030005029A1-20030102-D00001.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00002">
<image id="EMI-D00002" file="US20030005029A1-20030102-D00002.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00003">
<image id="EMI-D00003" file="US20030005029A1-20030102-D00003.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00004">
<image id="EMI-D00004" file="US20030005029A1-20030102-D00004.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00005">
<image id="EMI-D00005" file="US20030005029A1-20030102-D00005.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00006">
<image id="EMI-D00006" file="US20030005029A1-20030102-D00006.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00007">
<image id="EMI-D00007" file="US20030005029A1-20030102-D00007.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00008">
<image id="EMI-D00008" file="US20030005029A1-20030102-D00008.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00009">
<image id="EMI-D00009" file="US20030005029A1-20030102-D00009.TIF" imf="TIFF" ti="DR"/>
</figure>
</subdoc-drawings>
</patent-application-publication>
