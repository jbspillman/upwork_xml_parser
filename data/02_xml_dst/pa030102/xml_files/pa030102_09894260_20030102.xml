<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE patent-application-publication SYSTEM "pap-v16-2002-01-01.dtd" [
<!ENTITY US20030005263A1-20030102-D00000.TIF SYSTEM "US20030005263A1-20030102-D00000.TIF" NDATA TIF>
<!ENTITY US20030005263A1-20030102-D00001.TIF SYSTEM "US20030005263A1-20030102-D00001.TIF" NDATA TIF>
<!ENTITY US20030005263A1-20030102-D00002.TIF SYSTEM "US20030005263A1-20030102-D00002.TIF" NDATA TIF>
<!ENTITY US20030005263A1-20030102-D00003.TIF SYSTEM "US20030005263A1-20030102-D00003.TIF" NDATA TIF>
<!ENTITY US20030005263A1-20030102-D00004.TIF SYSTEM "US20030005263A1-20030102-D00004.TIF" NDATA TIF>
<!ENTITY US20030005263A1-20030102-D00005.TIF SYSTEM "US20030005263A1-20030102-D00005.TIF" NDATA TIF>
<!ENTITY US20030005263A1-20030102-D00006.TIF SYSTEM "US20030005263A1-20030102-D00006.TIF" NDATA TIF>
<!ENTITY US20030005263A1-20030102-D00007.TIF SYSTEM "US20030005263A1-20030102-D00007.TIF" NDATA TIF>
<!ENTITY US20030005263A1-20030102-D00008.TIF SYSTEM "US20030005263A1-20030102-D00008.TIF" NDATA TIF>
<!ENTITY US20030005263A1-20030102-D00009.TIF SYSTEM "US20030005263A1-20030102-D00009.TIF" NDATA TIF>
<!ENTITY US20030005263A1-20030102-D00010.TIF SYSTEM "US20030005263A1-20030102-D00010.TIF" NDATA TIF>
<!ENTITY US20030005263A1-20030102-D00011.TIF SYSTEM "US20030005263A1-20030102-D00011.TIF" NDATA TIF>
<!ENTITY US20030005263A1-20030102-D00012.TIF SYSTEM "US20030005263A1-20030102-D00012.TIF" NDATA TIF>
]>
<patent-application-publication>
<subdoc-bibliographic-information>
<document-id>
<doc-number>20030005263</doc-number>
<kind-code>A1</kind-code>
<document-date>20030102</document-date>
</document-id>
<publication-filing-type>new</publication-filing-type>
<domestic-filing-data>
<application-number>
<doc-number>09894260</doc-number>
</application-number>
<application-number-series-code>09</application-number-series-code>
<filing-date>20010628</filing-date>
</domestic-filing-data>
<technical-information>
<classification-ipc>
<classification-ipc-primary>
<ipc>G06F009/00</ipc>
</classification-ipc-primary>
<classification-ipc-edition>07</classification-ipc-edition>
</classification-ipc>
<classification-us>
<classification-us-primary>
<uspc>
<class>712</class>
<subclass>218000</subclass>
</uspc>
</classification-us-primary>
<classification-us-secondary>
<uspc>
<class>712</class>
<subclass>225000</subclass>
</uspc>
</classification-us-secondary>
</classification-us>
<title-of-invention>Shared resource queue for simultaneous multithreaded processing</title-of-invention>
</technical-information>
<inventors>
<first-named-inventor>
<name>
<given-name>Richard</given-name>
<middle-name>James</middle-name>
<family-name>Eickemeyer</family-name>
</name>
<residence>
<residence-us>
<city>Rochester</city>
<state>MN</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</first-named-inventor>
<inventor>
<name>
<given-name>Steven</given-name>
<middle-name>R.</middle-name>
<family-name>Kunkel</family-name>
</name>
<residence>
<residence-us>
<city>Rochester</city>
<state>MN</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
<inventor>
<name>
<given-name>Hung</given-name>
<middle-name>Q.</middle-name>
<family-name>Le</family-name>
</name>
<residence>
<residence-us>
<city>Austin</city>
<state>TX</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
</inventors>
<assignee>
<organization-name>INTERNATIONAL BUSINESS MACHINES CORPORATION</organization-name>
<address>
<city>ARMONK</city>
<state>NY</state>
</address>
<assignee-type>02</assignee-type>
</assignee>
<correspondence-address>
<name-1>Robert R. Williams</name-1>
<name-2>IBM Corporation, Dept. 917</name-2>
<address>
<address-1>3605 Highway 52 North</address-1>
<city>Rochester</city>
<state>MN</state>
<postalcode>55901-7829</postalcode>
<country>
<country-code>US</country-code>
</country>
</address>
</correspondence-address>
</subdoc-bibliographic-information>
<subdoc-abstract>
<paragraph id="A-0001" lvl="0">A queue, such as a first-in first-out queue, is incorporated into a processing device, such as a multithreaded pipeline processor. The queue may store the resources of more than one thread in the processing device such that the entries of one thread may be interspersed among the entries of another thread. The entries of each thread may be identified by a thread identification, a valid marker to indicate if the resources within the entry are valid, and a bank number. For a particular thread, the bank number tracks the number of times a head pointer pertaining to the first entry has passed a tail pointer. In this fashion, empty entries may be used and the resources may be efficiently allocated. In a preferred embodiment, the shared resource queue may be implemented into an in-order multithreaded pipelined processor as a queue storing resources to be dispatched for execution of instructions. The shared resource queue may also be implemented into a branch information queue or into any queue where more than one thread may require dynamic registers. </paragraph>
</subdoc-abstract>
<subdoc-description>
<summary-of-invention>
<section>
<heading lvl="1">TECHNICAL FIELD </heading>
<paragraph id="P-0001" lvl="0"><number>&lsqb;0001&rsqb;</number> The present invention relates in general to an improved data processor architecture and in particular to a queue within the processor core to support simultaneous hardware multithreading. </paragraph>
</section>
<section>
<heading lvl="1">BACKGROUND OF THE INVENTION </heading>
<paragraph id="P-0002" lvl="0"><number>&lsqb;0002&rsqb;</number> From the standpoint of the computer&apos;s hardware, most systems operate in fundamentally the same manner. Computer processors actually perform very simple operations quickly, such as arithmetic, logical comparisons, and movement of data from one location to another. What is perceived by the user as a new or improved capability of a computer system, however, may actually be the machine performing the same simple operations at very high speeds. Continuing improvements to computer systems require that these processor systems be made ever faster. </paragraph>
<paragraph id="P-0003" lvl="0"><number>&lsqb;0003&rsqb;</number> One measurement of the overall speed of a computer system, also called the throughput, is measured as the number of operations performed per unit of time. Conceptually, the simplest of all possible improvements to system speed is to increase the clock speeds of the various components, particularly the clock speed of the processor. So that if everything runs twice as fast but otherwise works in exactly the same manner, the system will perform a given task in half the time. Computer processors which were constructed from discrete components years ago performed significantly faster reducing the size and number of components; eventually the entire processor was packaged as an integrated circuit on a single chip. The reduced size made it possible to increase the clock speed of the processor, and accordingly increase system speed. </paragraph>
<paragraph id="P-0004" lvl="0"><number>&lsqb;0004&rsqb;</number> Despite the enormous improvement in speed obtained from integrated circuitry, the demand for ever faster computer systems still exists. Hardware designers have been able to obtain still further improvements in speed by greater integration, by further reducing the size of the circuits, and by other techniques. Designers, however, think that physical size reductions cannot continue indefinitely and there are limits to continually increasing processor clock speeds. Attention has therefore been directed to other approaches for further improvements in overall throughput of the computer system. </paragraph>
<paragraph id="P-0005" lvl="0"><number>&lsqb;0005&rsqb;</number> Without changing the clock speed, it is still possible to improve system speed by using multiple processors. The modest cost of individual processors packaged on integrated circuit chips has made this practical. The use of slave processors considerably improves system speed by off-loading work from the central processing unit (CPU) to the slave processor. For instance, slave processors routinely execute repetitive and single special purpose programs, such as input/output device communications and control. It is also possible for multiple CPUs to be placed in a single computer system, typically a host-based system which serves multiple users simultaneously. Each of the different CPUs can separately execute a different task on behalf of a different user, thus increasing the overall speed of the system to execute multiple tasks simultaneously. </paragraph>
<paragraph id="P-0006" lvl="0"><number>&lsqb;0006&rsqb;</number> Coordinating the execution and delivery of results of various functions among multiple CPUs is a tricky business; not so much for slave I/O processors because their functions are pre-defined and limited but it is much more difficult to coordinate functions for multiple CPUs executing general purpose application programs. System designers often do not know the details of the programs in advance. Most application programs follow a single path or flow of steps performed by the processor. While it is sometimes possible to break up this single path into multiple parallel paths, a universal application for doing so is still being researched. Generally, breaking a lengthy task into smaller tasks for parallel processing by multiple processors is done by a software engineer writing code on a case-by-case basis. This ad hoc approach is especially problematic for executing commercial transactions which are not necessarily repetitive or predictable. </paragraph>
<paragraph id="P-0007" lvl="0"><number>&lsqb;0007&rsqb;</number> Thus, while multiple processors improve overall system performance, it is much more difficult to improve the speed at which a single task, such as an application program, executes. If the CPU clock speed is given, it is possible to further increase the speed of the CPU, i.e., the number of operations executed per second, by increasing the average number of operations executed per clock cycle. A common architecture for high performance, single-chip microprocessors is the reduced instruction set computer (RISC) architecture characterized by a small simplified set of frequently used instructions for rapid execution, those simple operations performed quickly as mentioned earlier. As semiconductor technology has advanced, the goal of RISC architecture has been to develop processors capable of executing one or more instructions on each clock cycle of the machine. Another approach to increase the average number of operations executed per clock cycle is to modify the hardware within the CPU. This throughput measure, clock cycles per instruction, is commonly used to characterize architectures for high performance processors. </paragraph>
<paragraph id="P-0008" lvl="0"><number>&lsqb;0008&rsqb;</number> Processor architectural concepts pioneered in high performance vector processors and mainframe computers of the 1970s, such as the CDC-6600 and Cray-1, are appearing in RISC microprocessors. Early RISC machines were very simple single-chip processors. As Very Large Scale Integrated (VLSI) technology improves, additional space becomes available on a semiconductor chip. Rather than increase the complexity of a processor architecture, most designers have decided to use the additional space to implement techniques to improve the execution of a single CPU. Two principal techniques utilized are on-chip caches and instruction pipelines. Cache memories store data that is frequently used near the processor and allow instruction execution to continue, in most cases, without waiting the full access time of a main memory. Some improvement has also been demonstrated with multiple execution units with hardware that speculatively looks ahead to find instructions to execute in parallel. Pipeline instruction execution allows subsequent instructions to begin execution before previously issued instructions have finished. </paragraph>
<paragraph id="P-0009" lvl="0"><number>&lsqb;0009&rsqb;</number> The superscalar processor is an example of a pipeline processor. The performance of a conventional RISC processor can be further increased in the superscalar computer and the Very Long Instruction Word (VLIW) computer, both of which execute more than one instruction in parallel per processor cycle. In these architectures, multiple functional or execution units are connected in parallel to run multiple pipelines. The name implies that these processors are scalar processors capable of executing more than one instruction in each cycle. The elements of superscalar pipelined execution may include an instruction fetch unit to fetch more than one instruction at a time from a cache memory, instruction decoding logic to determine if instructions are independent and can be executed simultaneously, and sufficient execution units to execute several instructions at one time. The execution units may also be pipelined, e.g., floating point adders or multipliers may have a cycle time for each execution stage that matches the cycle times for the fetch and decode stages. </paragraph>
<paragraph id="P-0010" lvl="0"><number>&lsqb;0010&rsqb;</number> In a superscalar architecture, instructions may be completed in-order and/or out-of-order. In-order completion means no instruction can complete before all instructions dispatched ahead of it have been completed. Out-of-order completion means that an instruction is allowed to complete before all instructions ahead of it have been completed, as long as a predefined rules are satisfied. Within a pipelined superscalar processor, instructions are first fetched, decoded and then buffered. Instructions can be dispatched to execution units as resources and operands become available. Additionally, instructions can be fetched and dispatched speculatively based on predictions about branches taken. The result is a pool of instructions in varying stages of execution, none of which have completed by writing final results. These instructions in different stages of interim execution may be stored in a variety of queues used to maintain the in-order appearance of execution. As resources become available and branches are resolved, the instructions are retrieved from their respective queue and &ldquo;retired&rdquo; in program order thus preserving the appearance of a machine that executes the instructions in program order. </paragraph>
<paragraph id="P-0011" lvl="0"><number>&lsqb;0011&rsqb;</number> Another technique called hardware multithreading independently executes smaller sequences of instructions called threads or contexts in a single processor. When a CPU, for any of a number of reasons, stalls and cannot continue processing or executing one of these threads, it switches to and executes another thread. The term &ldquo;multithreading&rdquo; as defined in the computer architecture community is not the same as the software use of the term in which one task is subdivided into multiple related threads. Software multithreading substantially involves the operating system which manipulates and saves data from registers to main memory and maintains the program order of related and dependent instructions before switching tasks. Software multithreading does not require nor is it concerned with hardware multithreading and vice versa. Hardware multithreading manipulates hardware architected registers and execution units and pipelined processors within the processor core to maintain the state of one or more independently executing sets of instructions, called threads, in the processor hardware. Threads could be derived from, for example, different tasks in a multitasking system, different threads compiled from a software multithreading system, or from different I/O processors. What makes hardware multithreading unique and different from all these systems, however, is that more than one thread is independently maintained in a processor&apos;s registers. </paragraph>
<paragraph id="P-0012" lvl="0"><number>&lsqb;0012&rsqb;</number> Hardware multithreading takes on a myriad of forms. Multithreading permits processors having either non-pipelined or pipelined architectures to do useful work on more than one thread in the processor&apos;s registers. One form of multithreading, sometimes referred to as coarse-grained multithreading, is to execute one thread until the executing thread experiences a long latency event, such as retrieving data and/or instructions from memory or a processor interrupt, etc. Fine-grained multithreading, on the other hand, interleaves or switches threads on a cycle-by-cycle basis. Simultaneous hardware multithreading maintains N threads, or N states, in parallel in the processor and simultaneously executes N threads in parallel. Replicating processor registers for each of N threads results in some of the following registers being replicated N times: general purpose registers, floating point registers, condition registers, floating point status and control registers, count registers, link registers, exception registers, save/restore registers, special purpose registers, etc. Special buffers, such as a segment lookaside buffer, may be replicated but if not, each entry can be tagged with the thread number and flushed on every thread switch. Also, some branch prediction mechanisms, e.g., the correlation register and the return stack, may also be replicated. </paragraph>
<paragraph id="P-0013" lvl="0"><number>&lsqb;0013&rsqb;</number> Multithreading may also take on features of one or all of the forms, picking and choosing particular features for particular attributes. Not all of the processor&apos;s features need be replicated for each thread and there may be some shared and some replicated registers. Stages in the pipeline may either be separate or shared. Preferably, there may be no need to replicate some of the larger functions of the processor such as the level one instruction cache, level one data cache, instruction buffer, store queue, instruction dispatcher, functional or execution units, pipelines, translation lookaside buffer (TLB), and branch history table. </paragraph>
<paragraph id="P-0014" lvl="0"><number>&lsqb;0014&rsqb;</number> With respect to the threads there may be private or separate resources, or shared resources. Private resources simplify the management in that pointers from different threads address different data structures and may permit advantageous placement of the queues in different parts of the chip. Some private resources are registers or queues dedicated for the exclusive use of a particular thread. Another example of a private resource may be a split queue or registers having reserved spaces for each thread. An example of a partitioned queue is set forth in U.S. patent application Ser. No. 09/645,08 filed Aug. 24, 2000 entitled <highlight><italic>Method for Implementing a Variable</italic></highlight>-<highlight><italic>Partitioned Queue for Simultaneous Multithreaded Processors</italic></highlight>, which application is owned by the assignee herein and which is hereby incorporated by reference in its entirety. </paragraph>
<paragraph id="P-0015" lvl="0"><number>&lsqb;0015&rsqb;</number> The combination of out-of-order processing with simultaneous multithreading reveal some of the classical dilemmas of processor architecture and design. In order to proceed quickly, each thread may maintain its own out-of order queues of instructions in varying stages of completion, i.e., have its own private resources. Yet, the additional resources required for each thread are expensive and may be cumbersome in terms of space and power. Separate resources, moreover, lack the flexibility required for dynamic simultaneous multithreading. Execution of a thread of high priority, for example, may demand significantly more hardware registers and queues and other resources than has been architected for a single thread. Conversely, a different thread may not utilize all the space it has been allotted and so there are wasted resources. The partitioned queues as above lack the spontaneous flexibility and responsiveness that may be required by simultaneous multithreading. </paragraph>
<paragraph id="P-0016" lvl="0"><number>&lsqb;0016&rsqb;</number> In accordance with a preferred embodiment of the invention, the use of shared resources for the various queue yields performance benefits similar to the split queue structure without increasing the chip area over the split queue case. But, normally shared resources present their own set of problems. If no dispatch flush can be performed on a shared resource, processing is stalled because the shared resource, such as the register renamed pool, is full or otherwise blocked. Under these circumstances all threads are blocked such that no processing of any thread can occur and either a normal flush would occur or the processor would wait until the stalling condition is resolved. To complicate matters even further, there are certain conditions under which a dispatch flush cannot occur, such as when the stalled instruction is of a group of instructions generated from a decoded multiple or more complex instruction but the stalled instruction is not the first of the group. </paragraph>
<paragraph id="P-0017" lvl="0"><number>&lsqb;0017&rsqb;</number> There is thus a need to accommodate dynamic out-of-order processing of multiple threads in a processor architecture. </paragraph>
</section>
<section>
<heading lvl="1">SUMMARY OF THE INVENTION </heading>
<paragraph id="P-0018" lvl="0"><number>&lsqb;0018&rsqb;</number> These needs and others that will become apparent to one skilled in the art are satisfied by a resource queue, comprising: a plurality of entries, each entry having unique resources required for information processing in which the plurality of entries is allocated amongst a plurality of independent hardware threads such that the resources of more than one thread may be within the queue and the entries allocated to one thread may be interspersed among the entries allocated to another thread. </paragraph>
<paragraph id="P-0019" lvl="0"><number>&lsqb;0019&rsqb;</number> The first entry of one thread may wrap around the last entry of the same thread. Each thread may have a head pointer and a tail pointer wherein the head pointer is the first entry of the thread and the tail pointer is the last entry of the thread, and one of the unique resources is a bank number to indicate how many times the head pointer has wrapped around the tail pointer in order to maintain an order of the resources for the thread. There may also be a free pointer for a thread indicating an entry in the queue available for resources of the at least one thread. </paragraph>
<paragraph id="P-0020" lvl="0"><number>&lsqb;0020&rsqb;</number> The information processing may occur within an out-of-order computer processor, and the resource queue may further comprise a load reorder queue and/or a store reorder queue and/or a global completion table and or a branch information queue. </paragraph>
<paragraph id="P-0021" lvl="0"><number>&lsqb;0021&rsqb;</number> The invention may also be considered a resource queue in an out-of-order multithreaded computer processor, comprising: a load reorder queue; a store reorder queue; a global completion table; a branch information queue, wherein at least one of the queues comprises: a plurality of entries, each entry having unique resources required for information processing; the plurality of entries allocated amongst a plurality of independent hardware threads such that the resources of more than one thread may be within the queue; and the entries allocated to one thread may be interspersed among the entries allocated to another thread; and a first entry of one thread being capable of wrapping around the last entry of the same thread; and at least one thread having a head pointer and a tail pointer wherein the head pointer is the first entry of the thread and the tail pointer is the last entry of the thread, and a bank number indicates how many times the head pointer has wrapped around the tail pointer; and at least one free pointer for the at least one thread indicating an entry in the queue is available for resources of the thread. </paragraph>
<paragraph id="P-0022" lvl="0"><number>&lsqb;0022&rsqb;</number> The invention is also a method of allocating a shared resource queue for multithreaded electronic data processing, comprising the steps of determining if the shared resource queue is empty for a particular thread; finding the first entry of a particular thread; determining if the first entry and a free entry of the particular thread are the same; if, not advancing the first entry to the free entry; incrementing a bank number if the first entry passes the last entry before it finds the free entry; and allocating the next free entry by storing resources for the particular thread. </paragraph>
<paragraph id="P-0023" lvl="0"><number>&lsqb;0023&rsqb;</number> The method may further comprise deallocating multithreaded resources in the shared resource queue, comprising the steps of locating the last entry in the shared resource queue pertaining to the particular thread; determining if the last entry is also the first entry for the particular thread; if not, finding the next entry pertaining to the particular thread; determining if the bank number of the next entry is the same as the last entry and if so, deallocating the next entry by marking the resources as invalid; and if not, then skipping over the next entry and decrementing the bank number; and finding the next previous entry pertaining to the particular thread. </paragraph>
<paragraph id="P-0024" lvl="0"><number>&lsqb;0024&rsqb;</number> The shared resource queue may also be flushed by setting a flush point indicative of an oldest entry to be deallocated pertaining to the particular thread, and invalidating all entries between the head pointer and the flush point which have the same and greater bank number than the bank number of the flush point. </paragraph>
<paragraph id="P-0025" lvl="0"><number>&lsqb;0025&rsqb;</number> The invention is also considered a shared resource mechanism in a hardware multithreaded pipeline processor, the pipeline processor simultaneously processing a plurality of threads, the shared resource mechanism comprising: a dispatch stage of the pipeline processor; at least one shared resource queue connected to the dispatch stage; dispatch control logic connected to the dispatch stage and to at least one shared resource queue; and an issue queue of the pipeline processor connected to the dispatch stage and to the at least one shared resource queue, wherein the at least one shared resource queue allocates and deallocates resources for at least two threads passing into the issue queues in response to the dispatch control logic. </paragraph>
<paragraph id="P-0026" lvl="0"><number>&lsqb;0026&rsqb;</number> The invention is also an apparatus to enhance processor efficiency, comprising: means to fetch instructions from a plurality of threads into a hardware multithreaded pipeline processor; means to distinguish the instructions into one of a plurality of threads; means to decode the instructions; means to allocate a plurality of entries in at least one shared resource between at least two of the plurality of threads; means to determine if the instructions have sufficient private resources and at least one shared resource queue for dispatching the instructions; means to dispatch the instructions; means to deallocate the entries in the shared resource when one of the threads are dispatched; means to execute the instructions and the resources for the one of the threads. The apparatus may further comprises a means to flush the shared resource of all of the entries pertaining to a particular thread. </paragraph>
<paragraph id="P-0027" lvl="0"><number>&lsqb;0027&rsqb;</number> The invention is also a computer processing system, comprising: a central processing unit; a semiconductor memory unit attached to the central processing unit; at least one memory drive capable of having removable memory; a keyboard/pointing device controller attached to the central processing unit for attachment to a keyboard and/or a pointing device for a user to interact with the computer processing system; a plurality of adapters connected to the central processing unit to connect to at least one input/output device for purposes of communicating with other computers, networks, peripheral devices, and display devices; a hardware multithreading pipelined processor within the central processing unit to process at least two independent threads of execution, the pipelined processor comprising a fetch stage, a decode stage, and a dispatch stage; and at least one shared resource queue within the central processing unit, the shared resource queue having a plurality of entries pertaining to more than one thread in which entries pertaining to different threads are interspersed among each other. In addition, a first entry of one thread may be located after a last entry of the one thread. Further, the hardware multithreaded pipelined processor in the central processing unit may be an out-of-order processor. </paragraph>
<paragraph id="P-0028" lvl="0"><number>&lsqb;0028&rsqb;</number> The invention is best understood with reference the Drawing and the detailed description of the invention which follows.</paragraph>
</section>
</summary-of-invention>
<brief-description-of-drawings>
<section>
<heading lvl="1">BRIEF DESCRIPTION OF THE DRAWING </heading>
<paragraph id="P-0029" lvl="0"><number>&lsqb;0029&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 1</cross-reference> is a simplified block diagram of a computer that can be used in accordance with an embodiment of the invention. </paragraph>
<paragraph id="P-0030" lvl="0"><number>&lsqb;0030&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 2</cross-reference> is a simplified block diagram of a computer processing unit having various pipelines, registers, and execution units that can take advantage of the shared resource queue feature of the invention. </paragraph>
<paragraph id="P-0031" lvl="0"><number>&lsqb;0031&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 3</cross-reference> is a block diagram of a queue in a multithreaded processor in accordance with an embodiment of the invention shared by two threads. </paragraph>
<paragraph id="P-0032" lvl="0"><number>&lsqb;0032&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 4</cross-reference> is a block diagram of a queue in a multithreaded processor in accordance with another embodiment of the invention in which all entries of the queue are used by resources belonging to two threads. </paragraph>
<paragraph id="P-0033" lvl="0"><number>&lsqb;0033&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 5</cross-reference> is a simplified flow chart of a method by which the processor can determine if a queue has any resources pertaining to a particular thread in accordance with a feature of the invention. </paragraph>
<paragraph id="P-0034" lvl="0"><number>&lsqb;0034&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 6</cross-reference> is a simplified flow chart of a method of how to find a next entry in the shared resource pertaining to a particular thread in accordance with a feature of the invention. </paragraph>
<paragraph id="P-0035" lvl="0"><number>&lsqb;0035&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 7</cross-reference> is a simplified flow chart of method that locates a previous entry in the shared resource pertaining to a particular thread in accordance with a feature of the invention. </paragraph>
<paragraph id="P-0036" lvl="0"><number>&lsqb;0036&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 8</cross-reference> is a simplified flow chart of a method to locate a free entry in the shared resource for a particular thread in accordance with a feature of the invention. </paragraph>
<paragraph id="P-0037" lvl="0"><number>&lsqb;0037&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 9</cross-reference> is a simplified flow chart of a method to allocate an entry in a shared resource to a particular thread in accordance with a feature of the invention. </paragraph>
<paragraph id="P-0038" lvl="0"><number>&lsqb;0038&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 10</cross-reference> is a simplified flow chart of a method to deallocate an entry in a shared resource to a particular thread in accordance with a feature of the invention. </paragraph>
<paragraph id="P-0039" lvl="0"><number>&lsqb;0039&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 11</cross-reference> is a simplified flow chart of a method to flush the resources pertaining to a particular thread in a shared resource queue in accordance with a feature of the invention.</paragraph>
</section>
</brief-description-of-drawings>
<detailed-description>
<section>
<heading lvl="1">DETAILED DESCRIPTION OF THE INVENTION </heading>
<paragraph id="P-0040" lvl="0"><number>&lsqb;0040&rsqb;</number> Referring now to the Drawing wherein like numerals refer to the same or similar elements throughout and in particular with reference to <cross-reference target="DRAWINGS">FIG. 1</cross-reference>, there is depicted a block diagram of the principal components of a processing unit <highlight><bold>112</bold></highlight>, central processing unit (CPU) <highlight><bold>126</bold></highlight> may be connected via system bus <highlight><bold>134</bold></highlight> to RAM <highlight><bold>158</bold></highlight>, diskette drive <highlight><bold>122</bold></highlight>, hard-disk drive <highlight><bold>123</bold></highlight>, CD drive <highlight><bold>124</bold></highlight>, keyboard/pointing-device controller <highlight><bold>184</bold></highlight>, parallel-port adapter <highlight><bold>176</bold></highlight>, network adapter <highlight><bold>185</bold></highlight>, display adapter <highlight><bold>170</bold></highlight> and media communications adapter <highlight><bold>187</bold></highlight>. Internal communications bus <highlight><bold>134</bold></highlight> supports transfer of data, commands and other information between different devices; while shown in simplified form as a single bus, it is typically structured as multiple buses; and may be arranged in a hierarchical form. </paragraph>
<paragraph id="P-0041" lvl="0"><number>&lsqb;0041&rsqb;</number> CPU <highlight><bold>126</bold></highlight> is a general-purpose programmable multithreaded processor, executing instructions stored in memory <highlight><bold>158</bold></highlight>. While a single CPU having multithreaded capabilities is shown in <cross-reference target="DRAWINGS">FIG. 1</cross-reference>, it should be understood that computer systems having multiple CPUs, some of which may not have multithreaded capabilities, are common in servers and can be used in accordance with principles of the invention so long as one CPU has multithreading capabilities. Although the other various components of <cross-reference target="DRAWINGS">FIG. 1</cross-reference> are drawn as single entities, it is also more common that each consist of a plurality of entities and exist at multiple levels. While any appropriate multithreaded processor can be utilized for CPU <highlight><bold>126</bold></highlight>, it is preferably one of the PowerPC&trade; line of microprocessors available from IBM having simultaneous multithreading capabilities. Processing unit <highlight><bold>112</bold></highlight> with CPU <highlight><bold>126</bold></highlight> may be implemented in a computer, such as an IBM pSeries or an IBM iSeries computer running the AIX or other operating system. CPU <highlight><bold>126</bold></highlight> accesses data and instructions from and stores data to volatile random access memory (RAM) <highlight><bold>158</bold></highlight>. CPU <highlight><bold>126</bold></highlight> is suitably programmed to carry out the preferred embodiment as described in more detail in the flowcharts of FIGS. </paragraph>
<paragraph id="P-0042" lvl="0"><number>&lsqb;0042&rsqb;</number> Memory <highlight><bold>158</bold></highlight> is a random-access semiconductor memory (RAM) for storing data and programs; memory is shown conceptually as a single monolithic entity, it being understood that memory is often arranged in a hierarchy of caches and other memory devices. RAM <highlight><bold>158</bold></highlight> typically comprises a number of individual volatile memory modules that store segments of operating system and application software while power is supplied to processing unit <highlight><bold>112</bold></highlight>. The software segments may be partitioned into one or more virtual memory pages that each contain a uniform number of virtual memory addresses. When the execution of software requires more pages of virtual memory than can be stored within RAM <highlight><bold>158</bold></highlight>, pages that are not currently needed are swapped with the required pages, which are stored within non-volatile storage devices <highlight><bold>122</bold></highlight>, <highlight><bold>123</bold></highlight>, or <highlight><bold>124</bold></highlight>. Data storage <highlight><bold>123</bold></highlight> and <highlight><bold>124</bold></highlight> preferably comprise one or more rotating magnetic or optical hard disk drive units, although other types of data storage could be used. </paragraph>
<paragraph id="P-0043" lvl="0"><number>&lsqb;0043&rsqb;</number> Keyboard/pointing-device controller <highlight><bold>184</bold></highlight> interfaces processing unit <highlight><bold>112</bold></highlight> with a keyboard and graphical pointing device. In an alternative embodiment, there may be a separate controller for the keyboard and the graphical pointing device and/or other input devices may be supported, such as microphones, voice response units, etc. Display device adapter <highlight><bold>170</bold></highlight> translates data from CPU <highlight><bold>126</bold></highlight> into video, audio, or other signals utilized to drive a display or other output device. Device adapter <highlight><bold>170</bold></highlight> may support the attachment of a single or multiple terminals, and may be implemented as one or multiple electronic circuit cards or other units. </paragraph>
<paragraph id="P-0044" lvl="0"><number>&lsqb;0044&rsqb;</number> Processing unit <highlight><bold>112</bold></highlight> may include network-adapter <highlight><bold>185</bold></highlight>, media communications interface <highlight><bold>187</bold></highlight>, and parallel-port adapter <highlight><bold>176</bold></highlight>, all of which facilitate communication between processing unit <highlight><bold>112</bold></highlight> and peripheral devices or other data processing system. Parallel port adapter <highlight><bold>176</bold></highlight> may transmit printer-control signals to a printer through a parallel port. Network-adapter <highlight><bold>185</bold></highlight> may connect processing unit <highlight><bold>112</bold></highlight> to a local area network (LAN). A LAN provides a user of processing unit <highlight><bold>112</bold></highlight> with a means of electronically communicating information, including software, with a remote computer or a network logical storage device. In addition, a LAN supports distributed processing which enables processing unit <highlight><bold>112</bold></highlight> to share a task with other data processing systems linked to the LAN. For example, processing unit <highlight><bold>112</bold></highlight> may be connected to a local server computer system via a LAN using an Ethernet, Token Ring, or other protocol, the server in turn being connected to the Internet. Media communications interface <highlight><bold>187</bold></highlight> may comprise a modem connected to a telephone line or other higher bandwidth interfaces through which an Internet access provider or on-line service provider is reached. Media communications interface <highlight><bold>187</bold></highlight> may interface with cable television, wireless communications, or high bandwidth communications lines and other types of connection. An on-line service may provide software that can be downloaded into processing unit <highlight><bold>112</bold></highlight> via media communications interface <highlight><bold>187</bold></highlight>. Furthermore, through the media communications interface <highlight><bold>187</bold></highlight>, processing unit <highlight><bold>112</bold></highlight> can access other sources of software such as a server, electronic mail, or an electronic bulletin board, and the Internet or world wide web. </paragraph>
<paragraph id="P-0045" lvl="0"><number>&lsqb;0045&rsqb;</number> Shown in <cross-reference target="DRAWINGS">FIG. 2</cross-reference> is a multithreaded computer processor architecture <highlight><bold>210</bold></highlight> in accordance with a preferred implementation of the invention. Illustrated is an out-of-order pipelined processor such as that disclosed in <highlight><italic>System and Method for Dispatching Groups of Instructions</italic></highlight>, U.S. Ser. No. 09/108,160 filed Jun. 30, 1998; <highlight><italic>System and Method for Permitting Out</italic></highlight>-<highlight><italic>of</italic></highlight>-<highlight><italic>Order Execution of Load Instructions</italic></highlight>, U.S. Ser. No. 09/213,323 filed Dec. 16, 1998; <highlight><italic>System and Method for Permitting Out</italic></highlight>-<highlight><italic>of</italic></highlight>-<highlight><italic>Order Execution of Load and Store Instructions</italic></highlight>, U.S. Ser. No. 09/213,331 filed Dec. 16, 1998; <highlight><italic>Method and System for Restoring a Processor State Within a Data Processing System in which Instructions are Tracked in Groups</italic></highlight>, U.S. Ser. No. 09/332,413 filed Jul. 14, 1999; <highlight><italic>System and Method for Managing the Execution of Instruction Groups Having Multiple Executable Instructions</italic></highlight>, U.S. Ser. No. 09/434,095 filed Nov. <highlight><bold>5</bold></highlight>, <highlight><bold>1999</bold></highlight>; <highlight><italic>Selective Flush of Shared and Other Pipelined Stages in a Multithreaded Processor</italic></highlight>, U.S. Ser. No. 09/564,930 filed May 4, 2000; and <highlight><italic>Method for Implementing a Variable</italic></highlight>-<highlight><italic>Partitioned Queue for Simultaneous Multithreaded Processors</italic></highlight>, U.S. Ser. No. 09/645,08 filed Aug. 24, 2000, all these patent applications being commonly owned by the assignee herein and which are hereby incorporated by reference in their entireties. </paragraph>
<paragraph id="P-0046" lvl="0"><number>&lsqb;0046&rsqb;</number> The processor as disclosed in the above incorporated applications may be modified as described below to enable simultaneous out-of-order hardware multithreading operations. Multithreading operations in the context of the invention means hardware multithreading in which sequences of instructions, i.e., threads, execute independently from other threads, and in which hardware architected registers, execution units, and pipelined processors maintain the state of one or more independently executing sets of instructions, called threads, in the processor core hardware. With simultaneous multithreading, two or more threads are simultaneously active in the processor&apos;s pipeline. Thus, the processors&apos; pipeline(s) are able to perform useful work on different threads when a processor pipeline stall condition is detected for one thread. Furthermore, while one embodiment of the invention will be described in the context of dual multithreading operations in which only two threads are in the processor&apos;s pipeline, registers, and queues at any one time, the principles and context of the shared queue of the invention pertain to having more than two threads in different stages of the processor&apos;s pipeline and the architected registers and queues. One of skill in the art will appreciate, moreover, that multithreaded pipelined processor architectures not having out-of-order execution or the particular sets of registers and queues as described in the incorporated patent applications can also take advantage of the shared queue feature of the present invention. </paragraph>
<paragraph id="P-0047" lvl="0"><number>&lsqb;0047&rsqb;</number> The block diagram of a multithreaded pipeline processor of <cross-reference target="DRAWINGS">FIG. 2</cross-reference> is greatly simplified; many connections and control lines between the various elements have been omitted for purposes of facilitating understanding of the shared queues in accordance with principles of the invention. Referring now to <cross-reference target="DRAWINGS">FIG. 2</cross-reference> and the instruction cache <highlight><bold>214</bold></highlight> at the top center of the figure, instructions for the threads in the pipeline are fetched into the instruction cache <highlight><bold>214</bold></highlight> from a L2 cache or main memory <highlight><bold>212</bold></highlight>. While the L2 cache and main memory <highlight><bold>212</bold></highlight> have been simplified as a single unit, in reality they are separated from each by a system bus and there may be intermediate caches between the L2 cache and main memory and/or between the L2 cache and the instruction cache <highlight><bold>214</bold></highlight>. The number of cache levels is not important because the utility of the present invention is not limited to the details of a particular memory arrangement. Address tracking and control to the instruction cache <highlight><bold>214</bold></highlight> is provided by the instruction fetch address register <highlight><bold>270</bold></highlight> having at least one address, possibly more, per thread. From the instruction cache <highlight><bold>214</bold></highlight>, the instructions are forwarded to the instruction buffers <highlight><bold>216</bold></highlight> in which evaluation of branch condition may occur in conjunction with the branch prediction logic <highlight><bold>276</bold></highlight>. In a preferred embodiment of the instruction buffers <highlight><bold>216</bold></highlight>, instructions of each thread are distinguishable from instructions of other threads. </paragraph>
<paragraph id="P-0048" lvl="0"><number>&lsqb;0048&rsqb;</number> The decode unit <highlight><bold>218</bold></highlight> may require multiple cycles to complete its function and accordingly, may have multiple pipelines <highlight><bold>218</bold></highlight><highlight><italic>a</italic></highlight>, <highlight><bold>218</bold></highlight><highlight><italic>b</italic></highlight>, etc. Preferably each stage <highlight><bold>218</bold></highlight><highlight><italic>a </italic></highlight>and stage <highlight><bold>218</bold></highlight><highlight><italic>b </italic></highlight>has distinct threads. In the decode unit <highlight><bold>218</bold></highlight>, complex instructions may be simplified or represented in a different form for easier processing by subsequent processor pipeline stages. Other events that may occur in the decode unit <highlight><bold>218</bold></highlight> include the reshuffling or expansion of bits in instruction fields, extraction of information from various fields for, e.g., branch prediction or creating groups of instructions. Some instructions, such as load multiple or store multiple instructions, are very complex and are processed by breaking the instruction into a series of simpler operations or instructions, called microcode, during decode. </paragraph>
<paragraph id="P-0049" lvl="0"><number>&lsqb;0049&rsqb;</number> From the decode unit <highlight><bold>218</bold></highlight>, instructions are forwarded to the dispatch unit <highlight><bold>220</bold></highlight>. The dispatch unit <highlight><bold>220</bold></highlight> may receive control signals from the dispatch control <highlight><bold>240</bold></highlight> in accordance with the referenced applications. At the dispatch unit <highlight><bold>220</bold></highlight> of the processor pipeline, all resources, queues, and renamed pools are checked to determine if they are available for the instructions within the dispatch unit <highlight><bold>220</bold></highlight>. Different instructions have different requirements and all of those requirements must be met before the instruction is dispatched beyond the dispatch unit <highlight><bold>220</bold></highlight>. The dispatch control <highlight><bold>240</bold></highlight> and the dispatch unit <highlight><bold>220</bold></highlight> controls the dispatch of microcoded or other complex instructions that have been decoded into a multitude of simpler instructions, as described above. The processor pipeline, in one embodiment, typically will not dispatch in the middle of a microcoded instruction group; the first instruction of the microcode must be dispatched successfully and the subsequent instructions are dispatched in order. A multithread embodiment of the invention may presume that during any given processor clock cycle an instruction from only one thread is dispatched from the dispatch unit <highlight><bold>220</bold></highlight> to the issue queues <highlight><bold>222</bold></highlight>. </paragraph>
<paragraph id="P-0050" lvl="0"><number>&lsqb;0050&rsqb;</number> From the dispatch unit <highlight><bold>220</bold></highlight>, instructions enter the issue queues <highlight><bold>222</bold></highlight>. There may be multiple issue queues <highlight><bold>222</bold></highlight><highlight><italic>a </italic></highlight>and <highlight><bold>222</bold></highlight><highlight><italic>b </italic></highlight>and more than one thread may be present in each issue queue <highlight><bold>222</bold></highlight><highlight><italic>a</italic></highlight>, <highlight><bold>222</bold></highlight><highlight><italic>b </italic></highlight>in accordance with an embodiment of the shared queues as described herein, or there may be one issue queue per thread depending upon choice of architecture. The issue queues <highlight><bold>222</bold></highlight> may receive control signals from the completion control logic <highlight><bold>236</bold></highlight>, from the dispatch control <highlight><bold>240</bold></highlight>, and from a combination of various queues which may include, but which are not limited to, a non-renamed register tracking mechanism <highlight><bold>242</bold></highlight>, a load reorder queue (LRQ) <highlight><bold>244</bold></highlight>, a store reorder queue (SRQ) <highlight><bold>246</bold></highlight>, a global completion table (GCT) <highlight><bold>248</bold></highlight>, and a rename pools <highlight><bold>250</bold></highlight>. The LRQ <highlight><bold>244</bold></highlight>, the SRQ <highlight><bold>246</bold></highlight>, and/or the GCT <highlight><bold>248</bold></highlight> may be split between threads, may be shared amongst the threads in accordance with an embodiment of the shared queue as described herein; or separate queues may exist for each thread as will be discussed in the context of separate or shared resources below. For tracking purposes, instructions may be tracked singly or in groups in the GCT <highlight><bold>248</bold></highlight> to maintain the order of instructions. The LRQ <highlight><bold>244</bold></highlight> and the SRQ <highlight><bold>246</bold></highlight> may maintain the order of the load and store instructions, respectively, as well as maintaining addresses for the program order. The non-renamed register tracking mechanism <highlight><bold>242</bold></highlight> may track instructions in such registers as special purpose registers, etc. The instructions are dispatched on yet another machine cycle to the designated execution unit which may be one or more condition register units <highlight><bold>224</bold></highlight>, branch units <highlight><bold>226</bold></highlight>, fixed point units <highlight><bold>228</bold></highlight>, floating point units <highlight><bold>230</bold></highlight>, or load/store units <highlight><bold>232</bold></highlight> which load and store data from and to the data cache <highlight><bold>234</bold></highlight>. </paragraph>
<paragraph id="P-0051" lvl="0"><number>&lsqb;0051&rsqb;</number> The successful completion of execution of an instruction or, on the other hand, mispredicted branches or notification of errors which may have occurred in the execution units are forwarded to the completion control logic <highlight><bold>236</bold></highlight>, which may generate and transmit a refetch signal to any of a plurality of queues, the non-renamed register tracking mechanism <highlight><bold>242</bold></highlight>, the LRQ <highlight><bold>244</bold></highlight>, the SRQ <highlight><bold>246</bold></highlight>, the GCT <highlight><bold>248</bold></highlight>, or the renamed pools <highlight><bold>250</bold></highlight>. Alternatively, recovery and/or flush techniques may occur in response to the output of the completion control logic <highlight><bold>236</bold></highlight>. </paragraph>
<paragraph id="P-0052" lvl="0"><number>&lsqb;0052&rsqb;</number> As discussed, though, separate data structures and queues are not as efficient as a structure shared amongst the threads because either the structure is fixed or changes so slowly to be unable to accommodate dynamic and responsive processing. Thus, the preferred embodiment of the invention further contemplates that the data structures and registers be architected as shared resources. Shared resources are those processor registers and queues which can be shared by either thread either separately or at the same time. In some circumstances, the non-renamed tracking register <highlight><bold>242</bold></highlight>, the LRQ <highlight><bold>244</bold></highlight>, the SRQ <highlight><bold>246</bold></highlight>, the GCT <highlight><bold>248</bold></highlight>, the register renamed pools <highlight><bold>250</bold></highlight>, the issue queues <highlight><bold>222</bold></highlight>, the branch information queue (BIQ) (not shown) may be shared amongst threads. Alternatively, some resources may be shared while others may be private, e.g., the GCT <highlight><bold>248</bold></highlight> may be shared while the LRQ <highlight><bold>244</bold></highlight> and the SRQ <highlight><bold>246</bold></highlight> may be private depending upon the particular architecture. The use of shared resources for the various queues yields performance benefits without increasing the chip area over the shared queue case. </paragraph>
<paragraph id="P-0053" lvl="0"><number>&lsqb;0053&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 3</cross-reference> is a diagram of a shared queue <highlight><bold>300</bold></highlight> in accordance with an embodiment of the invention. This queue <highlight><bold>300</bold></highlight> may be any of the queues which are shared by the multiple threads, such as a BIQ (not shown) and other queues shown in <cross-reference target="DRAWINGS">FIG. 2, e</cross-reference>.g., the LRQ <highlight><bold>244</bold></highlight>, the SRQ <highlight><bold>246</bold></highlight>, and the GCT <highlight><bold>248</bold></highlight>. The shared queue <highlight><bold>300</bold></highlight> has a number of entries <highlight><bold>310</bold></highlight>-<highlight><bold>332</bold></highlight>. Each entry has a number of fields, <highlight><bold>340</bold></highlight>-<highlight><bold>346</bold></highlight>; field <highlight><bold>340</bold></highlight> identifies the thread; field <highlight><bold>342</bold></highlight> holds a bank number which represents how many times a head pointer has passed a tail pointer for a particular thread, as will be explained; valid/invalid field <highlight><bold>344</bold></highlight> indicates if the entry contains valid data; and content-specific field <highlight><bold>346</bold></highlight> may hold other content specific to the queue, thread, or entry. For instance, if the queue <highlight><bold>300</bold></highlight> is used as a GCT, then the content <highlight><bold>346</bold></highlight> might be, e.g., pointers to the architected and physical registers used by the instructions, the type of instruction, what execution unit would be used, bits indicating when each instruction is a group is complete, etc. When the shared queue is a LRQ or an SRQ, the queue might contain the virtual addresses of the load/store memory location and/or a pointed to the group in the GCT. The queue <highlight><bold>300</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 3</cross-reference> is illustrated with two independent threads of execution within the pipeline but the concepts apply equally to multiple independent threads greater than two. </paragraph>
<paragraph id="P-0054" lvl="0"><number>&lsqb;0054&rsqb;</number> The queue elements or entries <highlight><bold>310</bold></highlight>-<highlight><bold>332</bold></highlight> are organized into a first-in first-out (FIFO) queue <highlight><bold>300</bold></highlight>. There is a head pointer and a tail pointer for each thread. The head and tail pointers of one thread are independent from those of another thread. An entry can be occupied by only one thread at a particular time. To improve queue allocation time, each thread also has a free pointer which points to the next entry that the thread could use for allocation. Referring specifically to <cross-reference target="DRAWINGS">FIG. 3</cross-reference>, the first thread has a head pointer in entry <highlight><bold>310</bold></highlight> thus representing the newest entry for that thread. The thread identification field <highlight><bold>340</bold></highlight> of entry <highlight><bold>310</bold></highlight> indicates that this entry is associated with thread <highlight><bold>1</bold></highlight>; the bank field indicates that the field is associated with bank <highlight><bold>0</bold></highlight> indicating that the tail pointer has not passed the head pointer for this thread; and the valid field <highlight><bold>344</bold></highlight> indicates the entry is valid. Other entries associated with thread <highlight><bold>1</bold></highlight> are entries <highlight><bold>314</bold></highlight>, <highlight><bold>320</bold></highlight> and the tail pointer <highlight><bold>322</bold></highlight>. The next free entry available for thread <highlight><bold>1</bold></highlight> is entry <highlight><bold>332</bold></highlight>. Similarly, for thread <highlight><bold>2</bold></highlight>, the head pointer is in entry <highlight><bold>316</bold></highlight>, the tail pointer is in entry <highlight><bold>326</bold></highlight>, and its free pointer is in entry <highlight><bold>312</bold></highlight>. </paragraph>
<paragraph id="P-0055" lvl="0"><number>&lsqb;0055&rsqb;</number> In a typical queue the valid entries for one thread are located between the head and tail pointers for that thread when viewed as a circular queue. There are entries between the head and tail pointers of a particular thread but that are outside the range of another thread&apos;s head and tail pointer and hence unusable by the other thread. There may be many unusable entries if there are multiple threads. In a preferred embodiment of the invention, as shown in <cross-reference target="DRAWINGS">FIG. 4</cross-reference>, the previously unusable free entries are made accessible by allowing a thread to have its head pointer pass (wrap around) its tail pointer. A multibit &ldquo;bank&rdquo; number tracks the number of times the head pointer has passed the tail pointer of the same thread. To fully utilize all entries with many threads may require allowing one thread&apos;s head pointer to wrap around its tail pointer multiple times. In an out-of-order processor, the program order of instructions is maintained by comparing bank number and the position in the queue. Thus, as the head pointer passes the tail pointer, the bank number is incremented up to a maximum number determined by the number of bits used to represent the bank number. </paragraph>
<paragraph id="P-0056" lvl="0"><number>&lsqb;0056&rsqb;</number> In order to best understand and take advantage of this wrap-around features, it is useful to establish some basic premises: the tail pointer indicates the oldest entry. As the tail pointer advances, entries are deallocated, i.e., made invalid. The head pointer indicates the newest entry; as the head advances, entries are allocated, i.e., made valid. The free pointer points to the next entry to be allocated, i.e., the next entry for the head. If there is no entry that can be allocated, the free pointer is the same as the head pointer. As the head and tail pointers move in reaction to allocation and deallocation of entries, the head pointer can pass the tail pointer and the tail pointer can pass the head pointer, in either direction, thus preventing unusable entries in the queue. </paragraph>
<paragraph id="P-0057" lvl="0"><number>&lsqb;0057&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 4</cross-reference> is a diagram of the same queue <highlight><bold>300</bold></highlight> in which entry <highlight><bold>322</bold></highlight> has been deallocated by thread <highlight><bold>1</bold></highlight>. The tail pointer for thread <highlight><bold>1</bold></highlight> is now pointing to entry <highlight><bold>320</bold></highlight>. Entries <highlight><bold>312</bold></highlight>, <highlight><bold>332</bold></highlight>, <highlight><bold>334</bold></highlight>, <highlight><bold>328</bold></highlight> and <highlight><bold>324</bold></highlight> have been allocated to thread <highlight><bold>2</bold></highlight>. The head pointer of thread <highlight><bold>2</bold></highlight> in entry <highlight><bold>324</bold></highlight> has passed its tail pointer in entry <highlight><bold>326</bold></highlight>. Note that now the field for bank number <highlight><bold>342</bold></highlight> of the head pointer entry <highlight><bold>324</bold></highlight> indicates that it has passed the tail pointer once by the &ldquo;1&rdquo; in the field. The free pointers of both threads now are in entry <highlight><bold>322</bold></highlight> which could be used by either thread <highlight><bold>1</bold></highlight> or thread <highlight><bold>2</bold></highlight>, but not both at the same time. </paragraph>
<paragraph id="P-0058" lvl="0"><number>&lsqb;0058&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 5</cross-reference> is a simplified flow chart of how the method of invention determines that a queue has or does not have any entries in the queue <highlight><bold>300</bold></highlight> pertaining to a particular thread. This process as well as the processes described in <cross-reference target="DRAWINGS">FIGS. 6 and 7</cross-reference> are performed routinely throughout the allocation, deallocation, and flushing of entries in the queue, as described later. This and the other processes subsequently described herein involve looping which checks every entry in the queue although some of the processes can exit before checking every entry, as will be indicated individually. While the processes are described sequentially, it is important to understand that all the queue entries are preferably checked in parallel in the hardware implementation. </paragraph>
<paragraph id="P-0059" lvl="0"><number>&lsqb;0059&rsqb;</number> Recall that the process in <cross-reference target="DRAWINGS">FIG. 5</cross-reference> determines if a queue has an entry for a particular thread. In <cross-reference target="DRAWINGS">FIG. 5</cross-reference>, the process starts and inquires at step <highlight><bold>512</bold></highlight> if the head pointer is at the same entry as the tail pointer. If the head pointer is at a different entry than the tail pointer then there are entries in the queue pertaining to the thread, or as expressed in the flow chart, the thread is not empty, as in step <highlight><bold>514</bold></highlight> and the process is complete as in step <highlight><bold>525</bold></highlight>. If, however, the head pointer is at the same entry as the tail pointer, as in the inquiry at step <highlight><bold>512</bold></highlight>, then the various fields of that entry are interrogated. The first field interrogated in step <highlight><bold>520</bold></highlight> may be whether the entry having the head pointer is a valid entry. If the entry is not valid, then the queue has no entries for that particular thread, as in step <highlight><bold>524</bold></highlight> and the process completes as in step <highlight><bold>525</bold></highlight>. If, however, the entry is valid, then in step <highlight><bold>522</bold></highlight>, the thread identification field is checked to determine if the entry is associated with the particular thread. If the entry pertains to another thread, then the queue is empty as in step <highlight><bold>524</bold></highlight> for that particular thread. Of course, if the valid entry corresponds to the same thread number as the thread under consideration, then the queue is not empty and the process moves onto other processes as described herein. </paragraph>
<paragraph id="P-0060" lvl="0"><number>&lsqb;0060&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 6</cross-reference> describes a process used in accordance with an embodiment of the invention to find the next entry in the queue starting from a given entry, called the &ldquo;current&rdquo; entry pertaining to a particular thread. At step <highlight><bold>610</bold></highlight>, the process starts and determines in step <highlight><bold>612</bold></highlight> if the queue has any entries belonging to the particular thread in question. If the queue has entries of a certain thread, then in step <highlight><bold>614</bold></highlight>, the process checks if the head pointer and the tail pointer of that thread are in the same entry. If so, then there is only one entry of that thread in the queue and there is no next entry as in step <highlight><bold>616</bold></highlight>. If the head and tail are not in the same entry, then in step <highlight><bold>618</bold></highlight>, the remainder of the process starts from the current entry and the index of the current entry is saved as &ldquo;y&rdquo;. The field for the bank number is set to a bank number of that current entry and saved to &ldquo;z&rdquo;. The process continues to step <highlight><bold>620</bold></highlight> which determines if the current entry contains the head pointer. If the current entry does contain the head pointer pertaining to a particular thread, then there are no other entries for that thread in the queue, as in step <highlight><bold>616</bold></highlight>, and the process completes as in step <highlight><bold>690</bold></highlight>. </paragraph>
<paragraph id="P-0061" lvl="0"><number>&lsqb;0061&rsqb;</number> If the current entry, however, is not the same as the head pointer of a particular thread, in step <highlight><bold>620</bold></highlight>, then the adjacent entry is selected as in step <highlight><bold>622</bold></highlight>. In the queue structure of <cross-reference target="DRAWINGS">FIGS. 3 and 4</cross-reference>, the adjacent entry is the next entry except that if the entry is at the bottom of the queue, then the next entry at the top is checked. The nomenclature &ldquo;&plus;m&rdquo; is used to indicate this behavior. In step <highlight><bold>624</bold></highlight>, the entry is checked to see if it is a valid entry. Then, as in step <highlight><bold>626</bold></highlight>, the thread identification field is interrogated to determine if the thread matches the particular thread seeking its next entry, and then in step <highlight><bold>628</bold></highlight> the bank field is interrogated to determine if it has the same bank number as the original entry. If the response to these inquiries is yes, then in step <highlight><bold>630</bold></highlight>, the next entry has been found and the process of finding the next entry is complete. If the bank number &ldquo;z&rdquo; is not the same as the current entry, then in step <highlight><bold>632</bold></highlight>, the process checks to see if the entry pointer is the same as the tail pointer and if so, then the bank counter is increased by one in step <highlight><bold>634</bold></highlight> to indicate that the current entry is passing the tail pointer. In any event, if the entry is not valid (step <highlight><bold>624</bold></highlight>) and/or the entry does not pertain to the current thread (step <highlight><bold>626</bold></highlight>) and/or the bank number is not the same as the banks of the current entry (step <highlight><bold>628</bold></highlight>) and the entry is not the tail pointer (step <highlight><bold>632</bold></highlight>), then the process advances to the adjacent entry in step <highlight><bold>622</bold></highlight> and the process repeats. Thus, the next entry for a particular thread in a queue having interspersed entries amongst the various threads is determined if the entry is valid (step <highlight><bold>624</bold></highlight>), if the thread field corresponds to the particular thread (step <highlight><bold>626</bold></highlight>), and if the bank number is the same bank as that determined in step <highlight><bold>628</bold></highlight> or if the next higher bank number if the process passes the tail pointer (step <highlight><bold>632</bold></highlight>). </paragraph>
<paragraph id="P-0062" lvl="0"><number>&lsqb;0062&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 7</cross-reference> is a simplified flow chart to find the previous entry in a queue pertaining to a particular thread starting from a given entry called the &ldquo;current&rdquo; entry. If a particular thread has entries in the queue as in step <highlight><bold>712</bold></highlight>, then the inquiry focuses on whether the head pointer and the tail pointer are in the same entry of the queue, as in step <highlight><bold>714</bold></highlight>. If the head and tail pointer of a particular thread are not in the same entry, then in step <highlight><bold>718</bold></highlight>, the index of the current entry is saved as &ldquo;y&rdquo; and the bank number of the current entry is saves to &ldquo;z&rdquo;. The process ascertains whether the entry contains a tail pointer as in step <highlight><bold>720</bold></highlight>, and if so, then there are no previous entries for a particular thread as in step <highlight><bold>716</bold></highlight> and process completes in step <highlight><bold>790</bold></highlight>. </paragraph>
<paragraph id="P-0063" lvl="0"><number>&lsqb;0063&rsqb;</number> If, however, the current entry is not the same as the tail pointer, then in step <highlight><bold>722</bold></highlight> the adjacent entry in an opposing direction to that used in <cross-reference target="DRAWINGS">FIG. 6</cross-reference> selected as the current entry, i.e., in the queue structure of <cross-reference target="DRAWINGS">FIGS. 3 and 4</cross-reference>, the adjacent entry is the previous entry except that if the entry is at the top of the queue, the previous entry at the bottom is checked. That entry is evaluated to determine if the entry is valid (step <highlight><bold>724</bold></highlight>), if the entry pertains to the particular thread (step <highlight><bold>726</bold></highlight>), and if the entry has the same bank number as the current entry (step <highlight><bold>728</bold></highlight>). If the answer to these three inquiries is affirmative, then the previous entry pertaining to that particular thread has been found, as in step <highlight><bold>730</bold></highlight>. If, however, the entry is either not valid (step <highlight><bold>724</bold></highlight>), or the entry does not pertain to the particular thread (step <highlight><bold>726</bold></highlight>), then in step <highlight><bold>722</bold></highlight>, the previous entry is selected as the current entry for evaluation. If the entry is valid (step <highlight><bold>724</bold></highlight>), and if the entry pertains to the particular thread (step <highlight><bold>726</bold></highlight>), but the bank number is incorrect (step <highlight><bold>728</bold></highlight>), then the process determines if the entry contains a tail pointer, as in step <highlight><bold>732</bold></highlight>. If so, then the process decrements the bank number, as in step <highlight><bold>734</bold></highlight> and the process cycles to step <highlight><bold>722</bold></highlight> to check the next previous entry as the current entry. </paragraph>
<paragraph id="P-0064" lvl="0"><number>&lsqb;0064&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 8</cross-reference> is a simplified flow chart of how to locate a free entry in the queue for a particular thread. Recall that a free pointer refers to the next available entry in a queue for a particular thread. A brief overview of the process is that the free pointer is first moved to the next invalid entry starting from the current entry having the head pointer. If there are no more invalid entries, the free pointer is left in the same entry as the head pointer. If the bank number of the entry having the head pointer is the maximum bank number value, and if the free pointer would pass the entry having the tail pointer, the free pointer is left the same as the head pointer because this would require an impossibly large bank number. The value of the free pointer is re-evaluated every cycle or at least each cycle in which an entry is allocated or deallocated. </paragraph>
<paragraph id="P-0065" lvl="0"><number>&lsqb;0065&rsqb;</number> Starting at step <highlight><bold>810</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 8</cross-reference>, if the queue is not empty with respect to a particular thread, the process shifts to step <highlight><bold>822</bold></highlight>. If the queue is empty of a particular thread, as in step <highlight><bold>810</bold></highlight>, and the current entry is a valid entry with a head pointer, as in step <highlight><bold>814</bold></highlight>, the process jumps to step <highlight><bold>820</bold></highlight>. If, however, the entry is not valid (step <highlight><bold>814</bold></highlight>), the process sets the free pointer to be the same as the head pointer and then moves the head pointer to the adjacent entry and also sets the tail pointer to that entry in step <highlight><bold>818</bold></highlight>. The process completes in step <highlight><bold>890</bold></highlight>. </paragraph>
<paragraph id="P-0066" lvl="0"><number>&lsqb;0066&rsqb;</number> Starting at step <highlight><bold>820</bold></highlight>, the counter &ldquo;x&rdquo; for the entry is set to zero and a free pointer is set as the head pointer, in step <highlight><bold>824</bold></highlight>. On the other hand, if at step <highlight><bold>810</bold></highlight>, the queue already contains entries pertaining to the thread under consideration, then in step <highlight><bold>822</bold></highlight>, the counter &ldquo;x&rdquo; is set to the bank number of the entry at the head and again, the free pointer in that entry having the head pointer for the thread is set, as in step <highlight><bold>824</bold></highlight>. In step <highlight><bold>826</bold></highlight> the process advances the free pointer to the adjacent entry and establishes whether that entry is the tail pointer, as in step <highlight><bold>828</bold></highlight>. If so, at step <highlight><bold>830</bold></highlight>, the process checks if the head entry is at the maximum bank number. If so, then the free pointer is set to be a head pointer in step <highlight><bold>832</bold></highlight> and the queue does not have enough banks for the particular thread under consideration. </paragraph>
<paragraph id="P-0067" lvl="0"><number>&lsqb;0067&rsqb;</number> If, in step <highlight><bold>828</bold></highlight>, the entry at the free pointer is not at the tail pointer then it proceeds to check if the entry is the head pointer in step <highlight><bold>834</bold></highlight>. If so, then the queue is full and process is completed as in step <highlight><bold>890</bold></highlight>. If the entry at the free pointer is not at the head pointer, then, in step <highlight><bold>836</bold></highlight>, the entry is checked to see if the entry at the free pointer is valid. If so, that entry is already occupied. If the entry having the free pointer is not valid, it means that the entry is available for the thread under consideration and the process completes in step <highlight><bold>890</bold></highlight>. If the entry is valid, the free pointer moves to the adjacent entry at step <highlight><bold>826</bold></highlight>. </paragraph>
<paragraph id="P-0068" lvl="0"><number>&lsqb;0068&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 9</cross-reference> is a flow chart of allocating entries in a shared resource queue in accordance with an embodiment of the invention. An overview of the process of allocating an entry checks if the entry at the head pointer is also at the free pointer. If not, the head pointer advances to the entry having the free pointer so than an entry can be allocated. If the head pointer passes the tail pointer in this process, the bank number of the new head is made one larger than the bank number of the old head pointer. The entry is marked valid and the thread number is set to this thread. The free pointer then attempts to advance. </paragraph>
<paragraph id="P-0069" lvl="0"><number>&lsqb;0069&rsqb;</number> In more detail now, the process begins at step <highlight><bold>910</bold></highlight> and inquires if the queue contains any entries pertaining to the thread under consideration (step <highlight><bold>902</bold></highlight>). If so, in step <highlight><bold>912</bold></highlight> the counters &ldquo;y&rdquo; and &ldquo;z&rdquo; are set to &ldquo;0&rdquo;, and the counter &ldquo;x&rdquo; is set to bank of the head pointer. If there are no preexisting entries in the queue as determined in step <highlight><bold>902</bold></highlight>, then in step <highlight><bold>914</bold></highlight>, the queue is initialized for that thread by setting the counters &ldquo;x&rdquo; and &ldquo;z&rdquo; to &ldquo;0&rdquo;, and counter &ldquo;y&rdquo; to &ldquo;1&rdquo;. In any event, the process inquires at step <highlight><bold>916</bold></highlight> if the head pointer and the free pointer for a particular thread are at the same entry. If so, this indicates that the queue is full and the process terminates to step <highlight><bold>990</bold></highlight>. If, however, the queue is not full, then at step <highlight><bold>918</bold></highlight>, the process advances the head pointer to the adjacent entry and asks if that entry is also the tail pointer as in step <highlight><bold>920</bold></highlight>. If so, then in step <highlight><bold>922</bold></highlight>, it means that the head pointer has passed the tail pointer and so in step <highlight><bold>922</bold></highlight>, the counter &ldquo;z&rdquo; is set to one and proceeds to step <highlight><bold>924</bold></highlight>. If the head pointer is not the same as the tail pointer (step <highlight><bold>920</bold></highlight>), the process inquires at step <highlight><bold>924</bold></highlight> if the entry is at the free pointer. If not, then the process loops back to step <highlight><bold>918</bold></highlight> to move the head pointer to the adjacent entry. If, however, the entry is at the free pointer in step <highlight><bold>924</bold></highlight>, then in step <highlight><bold>926</bold></highlight> the process sets the bank of the new entry to either the bank of the original head entry or one higher if the tail pointer was passed. </paragraph>
<paragraph id="P-0070" lvl="0"><number>&lsqb;0070&rsqb;</number> In step <highlight><bold>928</bold></highlight>, the process queries the value of &ldquo;y&rdquo; to see if the queue was originally empty at start. If so, the tail pointer is set to the head pointer indicating that the queue has only one entry pertaining to a particular thread as in step <highlight><bold>930</bold></highlight>. Now the new entry is initialized and the entry is set to contain valid information, pertain to the particular thread, and store the information in the entry for successful allocation, as in step <highlight><bold>932</bold></highlight>. The process then terminates at step <highlight><bold>990</bold></highlight>. </paragraph>
<paragraph id="P-0071" lvl="0"><number>&lsqb;0071&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 10</cross-reference> takes the user through the steps to deallocate an entry in a queue according to features of the invention. Briefly, to deallocate an entry, the entry having the tail pointer is made invalid. The tail then advances to the next entry in the same thread with the same bank value, which will be zero. Any entries for this thread with a higher bank number are skipped over by the tail pointer but the bank values of these skipped-over entries are decremented. </paragraph>
<paragraph id="P-0072" lvl="0"><number>&lsqb;0072&rsqb;</number> At step <highlight><bold>1010</bold></highlight>, the process begins and the entry at the tail pointer is made invalid in step <highlight><bold>1012</bold></highlight>. The process then inquires if the entry having the tail pointer also contains the head pointer in step <highlight><bold>1014</bold></highlight>. If so, it means that there was only one entry in the queue pertaining to the thread under consideration and the process completes in step <highlight><bold>1090</bold></highlight> because all the entries have been deallocated by marking the valid field to zero. If, however, there are more entries pertaining to the particular thread, then in step <highlight><bold>1016</bold></highlight>, the entry counter &ldquo;y&rdquo; is set to hold the tail pointer. The process then finds the next entry from that tail pointer pertaining to that thread, as in <cross-reference target="DRAWINGS">FIG. 6</cross-reference>, it sets that entry to contain the tail pointer as in step <highlight><bold>1018</bold></highlight>. In step <highlight><bold>1020</bold></highlight>, the process moves to the adjacent entry and in step <highlight><bold>1022</bold></highlight>, the process checks to see if the entry is at the tail pointer. If so, then all entries between the old tail and the new tail are checked and the process terminates in step <highlight><bold>1090</bold></highlight>. If however, the entry is at the tail pointer, then in steps <highlight><bold>1024</bold></highlight> and <highlight><bold>1026</bold></highlight> the process checks if the entry contains valid information and if the entry pertains to the particular thread. If the answer is no to either or both of the questions the process loops to find the adjacent entry at step <highlight><bold>1020</bold></highlight> pertaining to that thread to be deallocated. If, however, the entry contains valid information (step <highlight><bold>1024</bold></highlight>), and pertains to the thread under consideration (step <highlight><bold>1026</bold></highlight>), then the bank field for that entry is decremented because the tail has moved past the entry. This process moves to the adjacent entry in step <highlight><bold>1020</bold></highlight>. Thus, only those entries for a particular thread have their bank number decremented. </paragraph>
<paragraph id="P-0073" lvl="0"><number>&lsqb;0073&rsqb;</number> <cross-reference target="DRAWINGS">FIGS. 11</cross-reference><highlight><italic>a </italic></highlight>and <highlight><bold>11</bold></highlight><highlight><italic>b </italic></highlight>state how to flush a queue for a particular thread which may occur, for instance, because a branch mispredict or other problem requires the entries to be removed and later either refetched or a different set of instructions are fetched. A flush invalidates all logically contiguous entries starting at the head pointer and going to the flush point by marking the entries as invalid. The process specifies the index of the flush point which is the oldest entry to be flushed and the head pointer will point to the youngest entry just older than the flush point at the end of the process. If the bank number of the entry having the head pointer is the same as the bank number of the flush point, then all entries with that bank number inclusive between the two are flushed. The head pointer then moves to the entry older than the flush point. If, however, the bank numbers are different, note that the head pointer must have a larger bank number, then all entries with a bank number greater than the bank of the flush point are flushed. The head pointer logically points to the youngest entry of the same bank as the flush point. The youngest entry is determined by the relative location of the entries with respect to the entry having the tail pointer. </paragraph>
<paragraph id="P-0074" lvl="0"><number>&lsqb;0074&rsqb;</number> In step <highlight><bold>1110</bold></highlight>, the process starts and in step <highlight><bold>1102</bold></highlight> determines if the queue has any entries for the particular thread. If the thread is empty for the thread, the process terminates at step <highlight><bold>1190</bold></highlight>. If, however, the queue has entries for the thread under consideration, then at step <highlight><bold>1112</bold></highlight>, the counter &ldquo;flush&rdquo; is set to the oldest entry to be flushed. In step <highlight><bold>1114</bold></highlight>, the process determines if the flushed entry is at the tail pointer. If so, the queue for the thread will be empty, so the head pointer is set to the tail pointer. If any entries are not flushed, as determined by step <highlight><bold>1114</bold></highlight>, then in step <highlight><bold>1116</bold></highlight>, the process finds the previous entry as in <cross-reference target="DRAWINGS">FIG. 7</cross-reference> starting from the flushed entry and the head pointer points to this entry. In step <highlight><bold>1120</bold></highlight>, the counter &ldquo;y&rdquo; is set to the tail and the counter &ldquo;z&rdquo; indicates when the &ldquo;flush&rdquo; entry is found and &ldquo;x&rdquo; is set to the bank of the flush entry. </paragraph>
<paragraph id="P-0075" lvl="0"><number>&lsqb;0075&rsqb;</number> In <cross-reference target="DRAWINGS">FIG. 11</cross-reference><highlight><italic>b</italic></highlight>, at steps <highlight><bold>1122</bold></highlight>, <highlight><bold>1124</bold></highlight>, and <highlight><bold>1126</bold></highlight>, the various fields of the entry are interrogated and if the entry is valid (step <highlight><bold>1122</bold></highlight>), pertains to the thread under consideration (step <highlight><bold>1124</bold></highlight>) and has the same bank counter as the bank that is being flushed (step <highlight><bold>1126</bold></highlight>), and if the counter &ldquo;z&rdquo; is not zero (step <highlight><bold>1128</bold></highlight>), then the entry is invalidated and the bank number is set to zero in step <highlight><bold>1134</bold></highlight>. If the entry is invalid (step <highlight><bold>1122</bold></highlight>) and/or the entry does not pertain the thread under consideration (step <highlight><bold>1124</bold></highlight>), then the process moves to the adjacent entry in step <highlight><bold>1140</bold></highlight>. Then the process inquires if that entry is the tail pointer in step <highlight><bold>1142</bold></highlight> and if so, the process completes at step <highlight><bold>1150</bold></highlight>. If not, the process loops back to steps <highlight><bold>1122</bold></highlight>-<highlight><bold>1126</bold></highlight> to check the fields of the particular entry. </paragraph>
<paragraph id="P-0076" lvl="0"><number>&lsqb;0076&rsqb;</number> If, in step <highlight><bold>1126</bold></highlight>, the bank counter is not equal to the bank that is being flushed, the process checks to see if the bank counter is greater than the bank of the entries being flushed, as in step <highlight><bold>1136</bold></highlight>. If the bank counter is greater than the bank counter being flushed, then the entry is flushed in step <highlight><bold>1138</bold></highlight> by setting the entry to be invalid and the bank counter to be zero. </paragraph>
<paragraph id="P-0077" lvl="0"><number>&lsqb;0077&rsqb;</number> If, in step <highlight><bold>1130</bold></highlight>, the entry is the same as the counter &ldquo;flush&rdquo; then the counter &ldquo;z&rdquo; is set to one to indicate that we found the flush entry in step <highlight><bold>1132</bold></highlight>. The entry is made invalid in step <highlight><bold>1134</bold></highlight> and the process advances to step <highlight><bold>1140</bold></highlight>. </paragraph>
<paragraph id="P-0078" lvl="0"><number>&lsqb;0078&rsqb;</number> Note that there are many repetitive steps throughout the figures that easily lend themselves to modular software loops or preferably, if manifested in hardware, to route the digital signals through the same logic. </paragraph>
<paragraph id="P-0079" lvl="0"><number>&lsqb;0079&rsqb;</number> Thus, while the invention has been described with respect to preferred and alternate embodiments, it is to be understood that the invention is not limited to processors which have only out-of-order processing but is particularly useful in such applications. The invention is intended to be manifested in the following claims. </paragraph>
</section>
</detailed-description>
</subdoc-description>
<subdoc-claims>
<heading lvl="1">What is claimed is: </heading>
<claim id="CLM-00001">
<claim-text><highlight><bold>1</bold></highlight>. A resource queue, comprising: 
<claim-text>(a) a plurality of entries, each entry having unique resources required for information processing; </claim-text>
<claim-text>(b) the plurality of entries allocated amongst a plurality of independent hardware threads such that the resources of more than one thread may be within the queue; and </claim-text>
<claim-text>(c) the entries allocated to one thread being capable of being interspersed among the entries allocated to another thread. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00002">
<claim-text><highlight><bold>2</bold></highlight>. The resource queue of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, further comprising: 
<claim-text>(a) a first entry of one thread being capable of wrapping around the last entry of the same thread. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00003">
<claim-text><highlight><bold>3</bold></highlight>. The queue of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, further comprising: 
<claim-text>(a) a head pointer and a tail pointer for at least one thread wherein the head pointer is the first entry of the at least one thread and the tail pointer is the last entry of the at least one thread, and </claim-text>
<claim-text>(b) one of the unique resources is a bank number to indicate how many times the head pointer has wrapped around the tail pointer in order to maintain an order of the resources for the at least one thread. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00004">
<claim-text><highlight><bold>4</bold></highlight>. The resource queue of <dependent-claim-reference depends_on="CLM-00003">claim 3</dependent-claim-reference>, further comprising: 
<claim-text>(a) at least one free pointer for the at least one thread indicating an entry in the queue available for resources of the at least one thread. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00005">
<claim-text><highlight><bold>5</bold></highlight>. The queue of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein the information processing further comprises: 
<claim-text>(a) an out-of-order computer processor, and </claim-text>
<claim-text>(b) the resource queue may further comprise a load reorder queue and/or a store reorder queue and/or a global completion table and or a branch information queue. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00006">
<claim-text><highlight><bold>6</bold></highlight>. A resource queue in an out-of-order multithreaded computer processor, comprising: 
<claim-text>(a) a load reorder queue; </claim-text>
<claim-text>(b) a store reorder queue; </claim-text>
<claim-text>(c) a global completion table; </claim-text>
<claim-text>(d) a branch information queue, </claim-text>
<claim-text>at least one of the queues comprising: 
<claim-text>(i) a plurality of entries, each entry having unique resources required for information processing; </claim-text>
<claim-text>(ii) the plurality of entries allocated amongst a plurality of independent hardware threads such that the resources of more than one thread may be within the queue; and </claim-text>
<claim-text>(iii) the entries allocated to one thread being capable of being interspersed among the entries allocated to another thread; </claim-text>
<claim-text>(iv) a first entry of one thread being capable of wrapping around the last entry of the same thread; </claim-text>
<claim-text>(v) a head pointer and a tail pointer for at least one thread wherein the head pointer is the first entry of the at least one thread and the tail pointer is the last entry of the at least one thread; </claim-text>
<claim-text>(vi) a bank number to indicate how many times the head pointer has wrapped around the tail pointer in order to maintain an order of the resources for the at least one thread; and </claim-text>
<claim-text>(vii) at least one free pointer for the at least one thread indicating an entry in the queue available for resources of the at least one thread. </claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007">
<claim-text><highlight><bold>7</bold></highlight>. A method of allocating a shared resource queue for multithreaded electronic data processing, comprising: 
<claim-text>(a) determining if the shared resource queue is empty for a particular thread; </claim-text>
<claim-text>(b) finding the first entry of a particular thread; </claim-text>
<claim-text>(c) determining if the first entry and a free entry of the particular thread are the same; </claim-text>
<claim-text>(d) if, not advancing the first entry to the free entry; </claim-text>
<claim-text>(e) incrementing a bank number if the first entry passes the last entry before it finds the free entry; </claim-text>
<claim-text>(f) allocating the next free entry by storing resources for the particular thread. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00008">
<claim-text><highlight><bold>8</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00007">claim 7</dependent-claim-reference>, further comprising deallocating multithreaded resources in the shared resource queue, comprising: 
<claim-text>(a) locating the last entry in the shared resource queue pertaining to the particular thread; </claim-text>
<claim-text>(b) determining if the last entry is also the first entry for the particular thread; </claim-text>
<claim-text>(c) if not, finding the next entry pertaining to the particular thread; </claim-text>
<claim-text>(d) determining if the bank number of the next entry is the same as the last entry and if so, deallocating the next entry by marking the resources as invalid; and </claim-text>
<claim-text>(e) if not, then skipping over the next entry and decrementing the bank number; </claim-text>
<claim-text>(f) finding the next previous entry pertaining to the particular thread. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00009">
<claim-text><highlight><bold>9</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00007">claim 7</dependent-claim-reference>, further comprising flushing the shared resource queue, comprising the steps of: 
<claim-text>(a) setting a flush point indicative of an oldest entry to be deallocated pertaining to the particular thread; and </claim-text>
<claim-text>(b) invalidating all entries between the head pointer and the flush point which have the same and greater bank number than the bank number of the flush point. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00010">
<claim-text><highlight><bold>10</bold></highlight>. A shared resource mechanism in a hardware multithreaded pipeline processor, said pipeline processor simultaneously processing a plurality of threads, said shared resource mechanism comprising: 
<claim-text>(a) a dispatch stage of said pipeline processor; </claim-text>
<claim-text>(b) at least one shared resource queue connected to the dispatch stage; </claim-text>
<claim-text>(c) dispatch control logic connected to the dispatch stage and to the at least one shared resource queue; and </claim-text>
<claim-text>(d) an issue queue of said pipeline processor connected to said dispatch stage and to the at least one shared resource queue; </claim-text>
<claim-text>wherein the at least one shared resource queue allocates and deallocates resources for at least two of said plurality of threads passing into said issue queues in response to the dispatch control logic. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00011">
<claim-text><highlight><bold>11</bold></highlight>. An apparatus to enhance processor efficiency, comprising: 
<claim-text>(a) means to fetch instructions from a plurality of threads into a hardware multithreaded pipeline processor; </claim-text>
<claim-text>(b) means to distinguish said instructions into one of a plurality of threads; </claim-text>
<claim-text>(c) means to decode said instructions; </claim-text>
<claim-text>(d) means to allocate a plurality of entries in at least one shared resource between at least two of the plurality of threads; </claim-text>
<claim-text>(e) means to determine if said instructions have sufficient private resources and at least one shared resource queue for dispatching said instructions; </claim-text>
<claim-text>(f) means to dispatch said instructions; </claim-text>
<claim-text>(g) means to deallocate said entries in said at least one shared resource when one of said at least two threads are dispatched; </claim-text>
<claim-text>(h) means to execute said instructions and said resources for the one of said at least two threads. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00012">
<claim-text><highlight><bold>12</bold></highlight>. The apparatus of <dependent-claim-reference depends_on="CLM-00011">claim 11</dependent-claim-reference>, further comprising: 
<claim-text>(a) means to flush the at least one shared resource of all of said entries pertaining to the one of said at least two threads. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00013">
<claim-text><highlight><bold>13</bold></highlight>. A computer processing system, comprising: 
<claim-text>(a) a central processing unit; </claim-text>
<claim-text>(b) a semiconductor memory unit attached to said central processing unit; </claim-text>
<claim-text>(c) at least one memory drive capable of having removable memory; </claim-text>
<claim-text>(d) a keyboard/pointing device controller attached to said central processing unit for attachment to a keyboard and/or a pointing device for a user to interact with said computer processing system; </claim-text>
<claim-text>(e) a plurality of adapters connected to said central processing unit to connect to at least one input/output device for purposes of communicating with other computers, networks, peripheral devices, and display devices; </claim-text>
<claim-text>(f) a hardware multithreading pipelined processor within said central processing unit to process at least two independent threads of execution, said pipelined processor comprising a fetch stage, a decode stage, and a dispatch stage; and </claim-text>
<claim-text>(g) at least one shared resource queue within said central processing unit, said shared resource queue having a plurality of entries pertaining to more than one thread in which entries pertaining to different threads are interspersed among each other. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00014">
<claim-text><highlight><bold>14</bold></highlight>. The computer processor of <dependent-claim-reference depends_on="CLM-00011">claim 13</dependent-claim-reference> wherein a first entry of one thread may be located after a last entry of said one thread. </claim-text>
</claim>
<claim id="CLM-00015">
<claim-text><highlight><bold>15</bold></highlight>. The computer processor of <dependent-claim-reference depends_on="CLM-00011">claim 14</dependent-claim-reference>, wherein the hardware multithreaded pipelined processor in the central processing unit is an out-of-order processor.</claim-text>
</claim>
</subdoc-claims>
<subdoc-drawings id="DRAWINGS">
<heading lvl="0" align="CENTER">Drawings</heading>
<representative-figure>3</representative-figure>
<figure id="figure-D00000">
<image id="EMI-D00000" file="US20030005263A1-20030102-D00000.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00001">
<image id="EMI-D00001" file="US20030005263A1-20030102-D00001.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00002">
<image id="EMI-D00002" file="US20030005263A1-20030102-D00002.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00003">
<image id="EMI-D00003" file="US20030005263A1-20030102-D00003.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00004">
<image id="EMI-D00004" file="US20030005263A1-20030102-D00004.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00005">
<image id="EMI-D00005" file="US20030005263A1-20030102-D00005.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00006">
<image id="EMI-D00006" file="US20030005263A1-20030102-D00006.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00007">
<image id="EMI-D00007" file="US20030005263A1-20030102-D00007.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00008">
<image id="EMI-D00008" file="US20030005263A1-20030102-D00008.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00009">
<image id="EMI-D00009" file="US20030005263A1-20030102-D00009.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00010">
<image id="EMI-D00010" file="US20030005263A1-20030102-D00010.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00011">
<image id="EMI-D00011" file="US20030005263A1-20030102-D00011.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00012">
<image id="EMI-D00012" file="US20030005263A1-20030102-D00012.TIF" imf="TIFF" ti="DR"/>
</figure>
</subdoc-drawings>
</patent-application-publication>
