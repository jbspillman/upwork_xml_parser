<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE patent-application-publication SYSTEM "pap-v16-2002-01-01.dtd" [
<!ENTITY US20030005233A1-20030102-D00000.TIF SYSTEM "US20030005233A1-20030102-D00000.TIF" NDATA TIF>
<!ENTITY US20030005233A1-20030102-D00001.TIF SYSTEM "US20030005233A1-20030102-D00001.TIF" NDATA TIF>
<!ENTITY US20030005233A1-20030102-D00002.TIF SYSTEM "US20030005233A1-20030102-D00002.TIF" NDATA TIF>
<!ENTITY US20030005233A1-20030102-D00003.TIF SYSTEM "US20030005233A1-20030102-D00003.TIF" NDATA TIF>
<!ENTITY US20030005233A1-20030102-D00004.TIF SYSTEM "US20030005233A1-20030102-D00004.TIF" NDATA TIF>
<!ENTITY US20030005233A1-20030102-D00005.TIF SYSTEM "US20030005233A1-20030102-D00005.TIF" NDATA TIF>
<!ENTITY US20030005233A1-20030102-D00006.TIF SYSTEM "US20030005233A1-20030102-D00006.TIF" NDATA TIF>
<!ENTITY US20030005233A1-20030102-D00007.TIF SYSTEM "US20030005233A1-20030102-D00007.TIF" NDATA TIF>
<!ENTITY US20030005233A1-20030102-D00008.TIF SYSTEM "US20030005233A1-20030102-D00008.TIF" NDATA TIF>
]>
<patent-application-publication>
<subdoc-bibliographic-information>
<document-id>
<doc-number>20030005233</doc-number>
<kind-code>A1</kind-code>
<document-date>20030102</document-date>
</document-id>
<publication-filing-type>new</publication-filing-type>
<domestic-filing-data>
<application-number>
<doc-number>09894602</doc-number>
</application-number>
<application-number-series-code>09</application-number-series-code>
<filing-date>20010628</filing-date>
</domestic-filing-data>
<technical-information>
<classification-ipc>
<classification-ipc-primary>
<ipc>G06F013/00</ipc>
</classification-ipc-primary>
<classification-ipc-edition>07</classification-ipc-edition>
</classification-ipc>
<classification-us>
<classification-us-primary>
<uspc>
<class>711</class>
<subclass>136000</subclass>
</uspc>
</classification-us-primary>
</classification-us>
<title-of-invention>Dual organization of cache contents</title-of-invention>
</technical-information>
<inventors>
<first-named-inventor>
<name>
<given-name>J.</given-name>
<middle-name>Peter</middle-name>
<family-name>Stewart</family-name>
</name>
<residence>
<residence-us>
<city>Boca Raton</city>
<state>FL</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</first-named-inventor>
<inventor>
<name>
<given-name>Glreesh</given-name>
<family-name>Sadasivan</family-name>
</name>
<residence>
<residence-us>
<city>Coral Springs</city>
<state>FL</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
</inventors>
<assignee>
<organization-name>Daleen Technologies, Inc.</organization-name>
<address>
<city>Boca Raton</city>
<state>FL</state>
</address>
<assignee-type>02</assignee-type>
</assignee>
<correspondence-address>
<name-1>FLEIT, KAIN, GIBBONS,</name-1>
<name-2>GUTMAN &amp; BONGINI, P.L.</name-2>
<address>
<address-1>ONE BOCA COMMERCE CENTER</address-1>
<address-2>551 NORTHWEST 77TH STREET, SUITE 111</address-2>
<city>BOCA RATON</city>
<state>FL</state>
<postalcode>33487</postalcode>
<country>
<country-code>US</country-code>
</country>
</address>
</correspondence-address>
</subdoc-bibliographic-information>
<subdoc-abstract>
<paragraph id="A-0001" lvl="0">A method, and computer readable medium for control of data in a caching application. An indexed list of a type is used to hold cache elements for ease of lookup while a linked usage list is maintained for the Most Recently Used/Least Recently Used elements. Pointers between the lists are also maintained. This allows the cache to find both a specific entry if it exists and if it does not, and in the latter case the LRU element can be located without the need for a sequential search. Each element in the linked list holds a pointer to a cache element in the, and each cache element record in the indexed list also holds a pointer to its corresponding record in the linked list, in addition to the actual cached data. </paragraph>
</subdoc-abstract>
<subdoc-description>
<summary-of-invention>
<paragraph id="P-0001" lvl="0"><number>&lsqb;0001&rsqb;</number> All of the material in this patent application is subject to copyright protection under the copyright laws of the United States and of other countries. As of the first effective filing date of the present application, this material is protected as unpublished material. However, permission to copy this material is hereby granted to the extent that the copyright owner has no objection to the facsimile reproduction by anyone of the patent documentation or patent disclosure, as it appears in the United States Patent and Trademark Office patent file or records, but otherwise reserves all copyright rights whatsoever. </paragraph>
<section>
<heading lvl="1">CROSS REFERENCE TO RELATED APPLICATIONS </heading>
<paragraph id="P-0002" lvl="0"><number>&lsqb;0002&rsqb;</number> Not Applicable </paragraph>
</section>
<section>
<heading lvl="1">BACKGROUND OF THE INVENTION </heading>
<paragraph id="P-0003" lvl="0"><number>&lsqb;0003&rsqb;</number> 1. Field of the Invention </paragraph>
<paragraph id="P-0004" lvl="0"><number>&lsqb;0004&rsqb;</number> This invention generally relates to the field of computer memory management, and more particularly to computer caching methods and systems, especially as applied to large data files and databases. </paragraph>
<paragraph id="P-0005" lvl="0"><number>&lsqb;0005&rsqb;</number> 2. Description of the Related Art </paragraph>
<paragraph id="P-0006" lvl="0"><number>&lsqb;0006&rsqb;</number> Caching data from slower storage to faster storage is well known. The storage may be faster or slower due to a variety of factors including storage technology. There are many factors, which directly affect the speed of storage. These factors include storage technology e.g., solid state RAM (Random Access Memory) vs. mechanical hard-drives, combined with relative distance and communication throughput between the storage source and the requested destination. Computers use caching methods to enable faster data access the second or subsequent time that accessed data is retrieved. In addition, related data retrieved from memory is also available very quickly. An example of related data access is occurs when accessing a web page on the Internet. Many times only text is what is sought but the entire page from a web site is downloaded with both pictures and text. The initial information is received along with related information. </paragraph>
<paragraph id="P-0007" lvl="0"><number>&lsqb;0007&rsqb;</number> Well-known cache managing techniques enable the location of data elements in a cache quickly. One solution is to organize cache elements in an indexed list of some type. This cache technique although useful does have its shortcomings. One shortcoming is the inability to determine which element is the least-recently-used (LRU) element. Scanning the whole list, to compare time stamps, takes a significant amount of time. Determining the LRU is especially important where the cache is providing only a subset of some larger data set such as a database table. Accordingly, a need exists to provide access to data based on context of the data while being able to also determine what data must be discarded. </paragraph>
<paragraph id="P-0008" lvl="7"><number>&lsqb;0008&rsqb;</number> Exemplary Computer System </paragraph>
<paragraph id="P-0009" lvl="0"><number>&lsqb;0009&rsqb;</number> Referring to <cross-reference target="DRAWINGS">FIG. 1</cross-reference>, there is shown a block diagram <highlight><bold>100</bold></highlight> of the major electronic components of an information processing system <highlight><bold>100</bold></highlight> in accordance with the invention. The electronic components include: a central processing unit (CPU) <highlight><bold>102</bold></highlight>, an Input/Output (I/O) Controller <highlight><bold>104</bold></highlight>, a mouse <highlight><bold>132</bold></highlight> a keyboard <highlight><bold>116</bold></highlight>, a system power and clock source <highlight><bold>106</bold></highlight>; display driver <highlight><bold>108</bold></highlight>; RAM <highlight><bold>110</bold></highlight>, ROM <highlight><bold>112</bold></highlight>, ASIC (application specific integrated circuit) <highlight><bold>114</bold></highlight> and a hard disk drive <highlight><bold>118</bold></highlight>. These are representative components of a computer. The general operation of a computer comprising these elements is well understood. Network interface <highlight><bold>120</bold></highlight> provides connection to a computer network such as Ethernet over TCP/IP or other popular protocol network interfaces. Optional components for interfacing to external peripherals include: a Small Computer Systems Interface (SCSI) port <highlight><bold>122</bold></highlight> for attaching peripherals; a PCMCIA slot <highlight><bold>124</bold></highlight>; and serial port <highlight><bold>126</bold></highlight>. An optional diskette drive <highlight><bold>128</bold></highlight> is shown for loading or saving code to removable diskettes <highlight><bold>130</bold></highlight>. The system <highlight><bold>100</bold></highlight> may be implemented by combination of hardware and software. Moreover, the functionality required for using the invention may be embodied in computer-readable media (such as 3.5 inch diskette <highlight><bold>130</bold></highlight>) to be used in programming an information-processing apparatus (e.g., a personal computer) to perform in accordance with the invention. </paragraph>
<paragraph id="P-0010" lvl="0"><number>&lsqb;0010&rsqb;</number> Given this computer system, the performance is based in part on having the often-used data available to the processor. This is accomplished by moving the actively used data from the hard-drive or I/O ports to the RAM, and even to the Microprocessor platform. Accordingly the need exists for new and improved methods for control and movement of this often used data. </paragraph>
<paragraph id="P-0011" lvl="7"><number>&lsqb;0011&rsqb;</number> Example Software Hierarchy </paragraph>
<paragraph id="P-0012" lvl="0"><number>&lsqb;0012&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 2</cross-reference> is a block diagram <highlight><bold>200</bold></highlight>, illustrating the software hierarchy for the information processing system <highlight><bold>100</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 1</cross-reference> according to the present invention. The BIOS (Basic Input Output System) <highlight><bold>202</bold></highlight> is a set of low level of computer hardware instructions for communications between an operating system <highlight><bold>206</bold></highlight>, device driver <highlight><bold>204</bold></highlight> and hardware <highlight><bold>200</bold></highlight>. Device drivers <highlight><bold>204</bold></highlight> are hardware specific code used to communicate between an operating system <highlight><bold>206</bold></highlight> and hardware peripherals such as a CD ROM drive or printer. Applications <highlight><bold>208</bold></highlight> are software application programs written in C/C&plus;&plus;, assembler or other programming languages. Operating system <highlight><bold>206</bold></highlight> is the master program that loads after BIOS <highlight><bold>202</bold></highlight> initializes, that controls and runs the hardware <highlight><bold>200</bold></highlight>. Examples of operating systems include Windows 3.1/95/98/ME/2000/NT, Unix, Macintosh, OS/2, Sun Solaris and equivalents. One application running on the operating system <highlight><bold>204</bold></highlight> is a relational database product such as the Oracle Database server, IBM DB/2, Microsoft SQL Server or equivalent. </paragraph>
<paragraph id="P-0013" lvl="0"><number>&lsqb;0013&rsqb;</number> The information processing system <highlight><bold>200</bold></highlight> can be configured as a server coupled to one or more clients through a network. (Not shown.) The network can be a private Intranet, Internet or other computer network. In the preferred embodiment, the protocol is HTTP (Hyper Text Transfer Protocol) and the exact hardware/software protocol is not important to this present invention and should not be limited. The clients are capable of running Microsoft Windows 3.1/95/98/NT/2000 or equivalent operating systems. </paragraph>
<paragraph id="P-0014" lvl="0"><number>&lsqb;0014&rsqb;</number> Given this software hierarchy the need exists for additional software that will allow for the just described applications to run in an efficient way by using caching methods that will obviate long and repetitive searching for data and applications. </paragraph>
<paragraph id="P-0015" lvl="7"><number>&lsqb;0015&rsqb;</number> Memory Hierarchy </paragraph>
<paragraph id="P-0016" lvl="0"><number>&lsqb;0016&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 3</cross-reference> illustrates a block diagram <highlight><bold>300</bold></highlight> of four levels of a computer memory. The description to follow is exemplary in nature, and is generally how personal computer memories are designed. The L<highlight><bold>1</bold></highlight> memory <highlight><bold>302</bold></highlight> is contained on the same chip as the Microprocessor such as an INTEL&trade; Pentium&trade; III. The memory operates at the same clock speed as the processor and can be accessed with no latency. The density is about 32 k bytes. This is where the often-used data and instructions are placed. In an example, if a book were being read the L<highlight><bold>1</bold></highlight> would contain not only the first sentence that was requested but also the entire page. This concept is known as drag along. </paragraph>
<paragraph id="P-0017" lvl="0"><number>&lsqb;0017&rsqb;</number> The L<highlight><bold>2</bold></highlight> memory <highlight><bold>304</bold></highlight>, is typically on the same package that contains the microprocessor. It operates with no latency and at the same external clock speed. The density is about 512K bytes and contains more of the information then was stored on the L<highlight><bold>1</bold></highlight>. Using the book example the L<highlight><bold>2</bold></highlight> would contain several pages of the chapter. </paragraph>
<paragraph id="P-0018" lvl="0"><number>&lsqb;0018&rsqb;</number> The L<highlight><bold>3</bold></highlight> memory <highlight><bold>306</bold></highlight>, is typically on the same mother board and is usually DRAM. (Dynamic Random Access Memory) There are several wait states for accessing this memory and the clock speed is a fraction of the microprocessor speed. The density is currently about 64-256 Mbytes. In the example of the book this memory may contain the balance of the chapter of the book. </paragraph>
<paragraph id="P-0019" lvl="0"><number>&lsqb;0019&rsqb;</number> The L<highlight><bold>4</bold></highlight> memory <highlight><bold>308</bold></highlight>, is typically a hard drive. It operates mechanically and therefore the access is quite slow. When data is requested the storage platter must be spun into position and the magnetic read arm must be indexed to the correct track. Finally a sector of data must be read. </paragraph>
<paragraph id="P-0020" lvl="0"><number>&lsqb;0020&rsqb;</number> In general the L<highlight><bold>1</bold></highlight> cache is fast, local, shallow and expensive. At the other extreme is the L<highlight><bold>4</bold></highlight> hard drive, which is slow, distant, dense and inexpensive. </paragraph>
<paragraph id="P-0021" lvl="0"><number>&lsqb;0021&rsqb;</number> Proper cache management dictates that the each cache level should be kept as up to date as possible with the recently requested data. If proper cache management techniques are not used, performance can suffer due to such problems as making frequent data calls from higher lever memory. One undesirable example of this is known as cache thrashing. Thrashing occurs when the cache loads data then stores back this data back to the hard drive only to recall it shortly afterwards. Accordingly the need exists for the communication and control of data between the different cache layers, so that this data can be usually available to the microprocessor by having the often-used data and instructions as close as possible. </paragraph>
<paragraph id="P-0022" lvl="7"><number>&lsqb;0022&rsqb;</number> Dictionary to Encyclopedia Hierarchy </paragraph>
<paragraph id="P-0023" lvl="0"><number>&lsqb;0023&rsqb;</number> Turning now to <cross-reference target="DRAWINGS">FIG. 4</cross-reference>, illustrated is a block diagram <highlight><bold>400</bold></highlight> of a local fast dictionary of recently requested animals (by example). Each entry <highlight><bold>404</bold></highlight>-<highlight><bold>410</bold></highlight> contains some information that has been requested (and optionally a pointer back to the full database or encyclopedia <highlight><bold>412</bold></highlight> of information about the animals <highlight><bold>414</bold></highlight>-<highlight><bold>440</bold></highlight>). </paragraph>
<paragraph id="P-0024" lvl="0"><number>&lsqb;0024&rsqb;</number> Dogs <highlight><bold>410</bold></highlight> in the dictionary may contain descriptions about their life span, as they are being compared to cats <highlight><bold>408</bold></highlight>, birds <highlight><bold>406</bold></highlight> and aardvarks <highlight><bold>404</bold></highlight>. However, if more information is required about these previously selected animals the dictionary may also contain the location in the encyclopedia, which enables faster recall of requested information such as full-grown size. Note that there are two possible relations between the material in the cache (the dictionary of the example) and the larger set of data (the encyclopedia of the example): 1) the cache may hold the complete information of each cached encyclopedia entry, or 2) the cache may just hold the most frequently used parts of each entry, as described above. In both cases, however, the larger data set (the complete &ldquo;encyclopedia&rdquo;) may be too large to fit in cache memory, i.e. the encyclopedia may contain information about 10,000 animals but the dictionary only has room for 500. </paragraph>
<paragraph id="P-0025" lvl="0"><number>&lsqb;0025&rsqb;</number> In order for large databases of information to be useful, they must be sorted according to certain attributes. For our example, each dictionary entry would be added to a search index as it is inserted into the dictionary. The search index (alphabetic in this case) allows for very efficient searching of the dictionary, to find out if it contains a specific entry. The database can be viewed as a full and complete encyclopedia, whereas the alphabetically indexed list can be viewed as an abbreviated dictionary and possibly also a local reference back to the encyclopedia. </paragraph>
<paragraph id="P-0026" lvl="0"><number>&lsqb;0026&rsqb;</number> Working with the dictionary in this way allows for very fast and efficient data access and processing of previously requested information. However once the dictionary is full, a decision must be made as where to put the newly requested data, say a request for information about eagles. Normal caching techniques dictate that the LRU (Least-Recently-Used) entry should be cast off and the new information should be stored. One method to keep track of this would be to add a timestamp, marking the time of the last access, to each entry in the dictionary. However in order to determine which entry is the LRU, every single time stamp must be scanned, which takes time. Accordingly the need exists for a more efficient method to quickly determine which entry in the dictionary is the LRU, while maintaining the indexed dictionary structure. </paragraph>
<paragraph id="P-0027" lvl="7"><number>&lsqb;0027&rsqb;</number> Flow Diagram of Prior Art Caching Methods </paragraph>
<paragraph id="P-0028" lvl="0"><number>&lsqb;0028&rsqb;</number> Turning now to <cross-reference target="DRAWINGS">FIG. 5</cross-reference> flow diagram <highlight><bold>500</bold></highlight> shows the prior art for the determination of the LRU. The flow diagram <highlight><bold>500</bold></highlight> is entered at <highlight><bold>502</bold></highlight> when a request for EAGLES information is made <highlight><bold>504</bold></highlight>. If the EAGLES information is in the cache <highlight><bold>506</bold></highlight>, then the time stamp is updated <highlight><bold>524</bold></highlight> which causes EAGLES to be identified as the MRU, then the information is used <highlight><bold>526</bold></highlight>. If it is not in the cache <highlight><bold>506</bold></highlight>, the information is pulled <highlight><bold>508</bold></highlight> from the next higher level of memory. Once the EAGLES information has been retrieved, the EAGLES information is stored <highlight><bold>512</bold></highlight> in the cache if there is space <highlight><bold>510</bold></highlight>, the time stamp is updated <highlight><bold>524</bold></highlight>, and then the information is used <highlight><bold>526</bold></highlight>. If there is no space <highlight><bold>510</bold></highlight>, then a sequential search of all of the time stamps in the cache is made <highlight><bold>514</bold></highlight>, which determines the LRU. This search must be of the complete contents of the L<highlight><bold>1</bold></highlight> so as to compare the time stamps of all of the entries. The time required for this search can be quite long and will directly impact the response time of the request for the EAGLES information. Once the search is completed, the LRU has been located <highlight><bold>516</bold></highlight>, and the LRU tag is passed to the next Least Recently Used entry. The EAGLES information can now be stored <highlight><bold>520</bold></highlight> in the cache and the time stamp reflects the fact that it is now the MRU, and the search index is updated <highlight><bold>522</bold></highlight>. The time stamp for the EAGLES is updated <highlight><bold>524</bold></highlight>. Finally, the newly stored EAGLES information can be used <highlight><bold>526</bold></highlight> and the flow diagram <highlight><bold>500</bold></highlight> is exited, <highlight><bold>528</bold></highlight>. </paragraph>
<paragraph id="P-0029" lvl="0"><number>&lsqb;0029&rsqb;</number> The sequential search of all of the time stamps required to find the LRU each time a new entry is required in the cache is very time consuming. Accordingly there exists the need for a more efficient method for determining which dictionary entry is the LRU. </paragraph>
</section>
<section>
<heading lvl="1">SUMMARY OF THE INVENTION </heading>
<paragraph id="P-0030" lvl="0"><number>&lsqb;0030&rsqb;</number> Briefly, according to the present invention, disclosed is a method, a system and computer readable medium for simultaneous locating within a cache memory a particular element of information based on a dual indexing scheme consisting of an search index and a second index in the form of a linked list to track usage. </paragraph>
<paragraph id="P-0031" lvl="0"><number>&lsqb;0031&rsqb;</number> The search index is used to quickly locate elements in the cache. The cached elements are organized in an indexed list that permits fast lookup based on the organization of the index. The search index could be organized using any of a number of standard schemes, such as a hash table, a B&plus;Tree index, or any other method that allows quick lookups using a key value. </paragraph>
<paragraph id="P-0032" lvl="0"><number>&lsqb;0032&rsqb;</number> The usage-linked list is used to maintain the chronological order of the elements, by always re-linking to put the most-recently-used (MRU) item first in the linked usage list. This procedure automatically ensures that the least-recently-used element (LRU) is always last in the usage-linked list. By combining the two lists, the indexed dictionary list and the usage-linked list, not only can a data element be located very quickly via the ordering of the index, but also locating the MRU, and LRU element in the usage-linked list is possible. This allows for fast cast off of the LRU data and thus provides for the storage for new data.</paragraph>
</section>
</summary-of-invention>
<brief-description-of-drawings>
<section>
<heading lvl="1">BRIEF DESCRIPTION OF THE DRAWINGS </heading>
<paragraph id="P-0033" lvl="0"><number>&lsqb;0033&rsqb;</number> The subject matter, which is regarded as the invention, is particularly pointed out and distinctly claimed in the claims at the conclusion of the specification. The foregoing and other objects, features, and advantages of the invention will be apparent from the following detailed description taken in conjunction with the accompanying drawings. </paragraph>
<paragraph id="P-0034" lvl="0"><number>&lsqb;0034&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 1</cross-reference> is a block diagram of an exemplary computer system that includes optional components, upon which the present invention can be implemented. </paragraph>
<paragraph id="P-0035" lvl="0"><number>&lsqb;0035&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 2</cross-reference> is a block diagram of an exemplary software hierarchy that executed on the hardware of <cross-reference target="DRAWINGS">FIG. 1</cross-reference> </paragraph>
<paragraph id="P-0036" lvl="0"><number>&lsqb;0036&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 3</cross-reference> is a block diagram of a memory hierarchy that is currently used in commercially available microprocessor systems. </paragraph>
<paragraph id="P-0037" lvl="0"><number>&lsqb;0037&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 4</cross-reference> is a block diagram of a dictionary to encyclopedia hierarchy, as used in the prior art. </paragraph>
<paragraph id="P-0038" lvl="0"><number>&lsqb;0038&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 5</cross-reference> is a flow diagram of prior art caching accessing methods. </paragraph>
<paragraph id="P-0039" lvl="0"><number>&lsqb;0039&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 6</cross-reference> is a block diagram of a dual indexing topology, according to the present invention. </paragraph>
<paragraph id="P-0040" lvl="0"><number>&lsqb;0040&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 7</cross-reference> is a block diagram of a demonstration of dual indexing using a search index and a linked list for tracking usage, according to the present invention. </paragraph>
<paragraph id="P-0041" lvl="0"><number>&lsqb;0041&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 8</cross-reference> is a flow diagram of dual indexing being used to access a cache, according to the present invention.</paragraph>
</section>
</brief-description-of-drawings>
<detailed-description>
<section>
<heading lvl="1">DETAILED DESCRIPTION OF AN EMBODIMENT </heading>
<paragraph id="P-0042" lvl="0"><number>&lsqb;0042&rsqb;</number> It is important to note, that these embodiments are only examples of the many advantageous uses of the innovative teachings herein. In general, statements made in the specification of the present application do not necessarily limit any of the various claimed inventions. Moreover, some statements may apply to some inventive features but not to others. In general, unless otherwise indicated, singular elements may be in the plural and visa versa with no loss of generality. </paragraph>
<paragraph id="P-0043" lvl="0"><number>&lsqb;0043&rsqb;</number> In the drawing like numerals refer to like parts through several views. </paragraph>
<paragraph id="P-0044" lvl="7"><number>&lsqb;0044&rsqb;</number> Discussion of Hardware and Software Implementation Options </paragraph>
<paragraph id="P-0045" lvl="0"><number>&lsqb;0045&rsqb;</number> The present invention as would be known to one of ordinary skill in the art could be produced in hardware or software, or in a combination of hardware and software. However in one embodiment the invention is implemented in software, particularly an application <highlight><bold>206</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 2</cross-reference>. The system, or method, according to the inventive principles as disclosed in connection with the preferred embodiment, may be produced in a single computer system having separate elements or means for performing the individual functions or steps described or claimed or one or more elements or means combining the performance of any of the functions or steps disclosed or claimed, or may be arranged in a distributed computer system, interconnected by any suitable means as would be known by one of ordinary skill in art. </paragraph>
<paragraph id="P-0046" lvl="0"><number>&lsqb;0046&rsqb;</number> According to the inventive principles as disclosed in connection with the preferred embodiment, the invention and the inventive principles are not limited to any particular kind of computer system but may be used with any general purpose computer, as would be known to one of ordinary skill in the art, arranged to perform the functions described and the method steps described. The operations of such a computer, as described above, may be according to a computer program contained on a medium for use in the operation or control of the computer, as would be known to one of ordinary skill in the art. The computer medium, which may be used to hold or contain the computer program product, may be a fixture of the computer such as an embedded memory or may be on a transportable medium such as a disk, as would be known to one of ordinary skill in the art. </paragraph>
<paragraph id="P-0047" lvl="0"><number>&lsqb;0047&rsqb;</number> The invention is not limited to any particular computer program or logic or language, or instruction but may be practiced with any such suitable program, logic or language, or instructions as would be known to one of ordinary skill in the art. Without limiting the principles of the disclosed invention any such computing system can include, inter alia, at least a computer readable medium allowing a computer to read data, instructions, messages or message packets, and other computer readable information from the computer readable medium. The computer readable medium may include non-volatile memory, such as ROM, Flash memory, floppy disk, Disk drive memory, CD-ROM, and other permanent storage. Additionally, a computer readable medium may include, for example, volatile storage such as RAM, buffers, cache memory, and network circuits. </paragraph>
<paragraph id="P-0048" lvl="0"><number>&lsqb;0048&rsqb;</number> Furthermore, the computer readable medium may include computer readable information in a transitory state medium such as a network link and/or a network interface, including a wired network or a wireless network, that allow a computer to read such computer readable information. </paragraph>
<paragraph id="P-0049" lvl="7"><number>&lsqb;0049&rsqb;</number> Dual Indexing Topology </paragraph>
<paragraph id="P-0050" lvl="0"><number>&lsqb;0050&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 6</cross-reference> is a block diagram <highlight><bold>600</bold></highlight> of a dual indexing topology, according to the present invention. Shown is a cache object <highlight><bold>602</bold></highlight> containing two hierarchies: (i) an indexed dictionary <highlight><bold>604</bold></highlight>; and (ii) a linked usage list <highlight><bold>606</bold></highlight>. This cache object <highlight><bold>602</bold></highlight> locates files based on an alphabetical look up of the indexed dictionary <highlight><bold>604</bold></highlight>. In this embodiment, the indexed dictionary <highlight><bold>604</bold></highlight> is shown as an alphabetically indexed dictionary list <highlight><bold>604</bold></highlight>. But other types of search indexes are contemplated and they are within the true scope and spirit of the present invention. The indexed dictionary in this example contains animals, with the index sorted alphabetically <highlight><bold>604</bold></highlight>. Alternatively numeric sorting is complemented or some combination of both. </paragraph>
<paragraph id="P-0051" lvl="0"><number>&lsqb;0051&rsqb;</number> Independent of the method used to order the indexed dictionary <highlight><bold>604</bold></highlight> is the method of searching the indexed dictionary list. Techniques such as an attached sequence number or a hash key may be used and are really independent of the order of the indexed dictionary list. In the example below the order is alphabetical starting with AARDVARKS. Using such a list of animal names a search for a particular animal requires a lookup using alphabetical names of at lease 9 characters each (the size of AARDVARKS) across 1000 entries. Whereas, if a simple index number is associated with each entry, the lookup is accomplished by comparing only three numbers <highlight><bold>999</bold></highlight>. With this search index, the present invention performs a very fast lookup on the indexed dictionary list. </paragraph>
<paragraph id="P-0052" lvl="0"><number>&lsqb;0052&rsqb;</number> A usage-linked list <highlight><bold>606</bold></highlight> contains two entries the MRU and the LRU based on the usage of the elements from a most-recently-used (MRU) element through a least-recently-used (LRU) element. The indexed dictionary <highlight><bold>604</bold></highlight> contains the locations of the dictionary elements in the cache <highlight><bold>608</bold></highlight>. The indexed dictionary list <highlight><bold>604</bold></highlight> contains: AARDVARKS <highlight><bold>612</bold></highlight>, BIRDS <highlight><bold>614</bold></highlight>, CATS <highlight><bold>616</bold></highlight> and DOGS <highlight><bold>618</bold></highlight>. A search is performed for a particular element in the cache <highlight><bold>608</bold></highlight>. It is important to note that using known indexed searching techniques that the entire set of elements in the cache <highlight><bold>608</bold></highlight> does not need to be searched. Stated differently, using known indexed searching techniques, a search for EAGLES might only look at elements in the cache <highlight><bold>608</bold></highlight> located alphabetically between &ldquo;D&rdquo; and &ldquo;F&rdquo; in the dictionary <highlight><bold>604</bold></highlight>. Each element <highlight><bold>612</bold></highlight>-<highlight><bold>618</bold></highlight> in the dictionary index <highlight><bold>604</bold></highlight> contains its specific address in the cache. Each element <highlight><bold>612</bold></highlight>-<highlight><bold>618</bold></highlight> in the dictionary <highlight><bold>604</bold></highlight> also contains a link to an entry in the usage-linked list <highlight><bold>606</bold></highlight>, as further explained below. If information on an animal is requested, the dictionary index <highlight><bold>604</bold></highlight> is accessed to determine if the requested animal is in the cache <highlight><bold>608</bold></highlight>. If the accessed animal is in the dictionary <highlight><bold>604</bold></highlight>, the dictionary index <highlight><bold>604</bold></highlight> contains its particular location in the cache <highlight><bold>608</bold></highlight>. </paragraph>
<paragraph id="P-0053" lvl="0"><number>&lsqb;0053&rsqb;</number> Continuing further, the alphabetically indexed dictionary <highlight><bold>604</bold></highlight>, contains the (optional) links back to the database are listed as <highlight><bold>622</bold></highlight> for AARDVARDS, <highlight><bold>624</bold></highlight> for BIRDS, <highlight><bold>626</bold></highlight> for CATS and <highlight><bold>628</bold></highlight> for DOGS. Finally, each dictionary element <highlight><bold>612</bold></highlight>-<highlight><bold>618</bold></highlight> contains a cross-link to a corresponding entry in the usage-linked list <highlight><bold>606</bold></highlight> in cache <highlight><bold>610</bold></highlight>. In the indexed dictionary <highlight><bold>604</bold></highlight> the cross-links are listed as: <highlight><bold>632</bold></highlight> for the AARDVARKS to linked list element: A, <highlight><bold>634</bold></highlight> for the BIRDS to linked list element: B, <highlight><bold>636</bold></highlight> for the CATS to linked list element: C, and finally <highlight><bold>638</bold></highlight> for the DOGS to linked list element: D. </paragraph>
<paragraph id="P-0054" lvl="0"><number>&lsqb;0054&rsqb;</number> If the requested animal is not in the dictionary index <highlight><bold>604</bold></highlight>, the usage-linked list <highlight><bold>606</bold></highlight> is accessed to determine which entry in the cache is the LRU. The usage-linked list <highlight><bold>606</bold></highlight> contains the address location of the two entries, which are the MRU, and the LRU elements based on the historic usage. These entries are updated <highlight><bold>612</bold></highlight> each time the dictionary is accessed. </paragraph>
<paragraph id="P-0055" lvl="0"><number>&lsqb;0055&rsqb;</number> It is important to note, that this structure of cross referenced dual indexes enables a look up and fast access of information that is in the cache, <highlight><bold>608</bold></highlight>, <highlight><bold>610</bold></highlight>. This dual indexing scheme also allows for fast access of additional information that is in the database but not yet in the cache <highlight><bold>608</bold></highlight>, <highlight><bold>610</bold></highlight>. If the information is not in the cache the linked usage list is used to determine the LRU&apos;s address location in cache <highlight><bold>608</bold></highlight>, <highlight><bold>610</bold></highlight>. The requested information is now stored in this LRU location. The maintenance of the cross pointers and indexed dictionary <highlight><bold>604</bold></highlight> and linked usage list <highlight><bold>606</bold></highlight> takes less time than full sequential searches for the location of the sought for data if it is in the cache <highlight><bold>608</bold></highlight>, <highlight><bold>610</bold></highlight>. </paragraph>
<paragraph id="P-0056" lvl="0"><number>&lsqb;0056&rsqb;</number> In another embodiment, or as an optional feature for improved accessing to a database a pointer or pointers is included. For this embodiment, each of the linked dictionary elements <highlight><bold>612</bold></highlight>-<highlight><bold>618</bold></highlight> includes pointers <highlight><bold>622</bold></highlight>-<highlight><bold>628</bold></highlight> back to the source of the information in the memory or storage such as a database. The use of these pointers <highlight><bold>622</bold></highlight>-<highlight><bold>628</bold></highlight> enables the indexed dictionary list <highlight><bold>604</bold></highlight> to retrieve additional information as necessary. The additional information retrieval is especially important for a smaller indexed dictionary list, which, because of its size cannot hold all the relevant information. Returning to the example of animals, if additional information is required from the database searching for the location of the additional information is not required. This is because each animal&apos;s entry <highlight><bold>612</bold></highlight>-<highlight><bold>618</bold></highlight> in the cache contains links <highlight><bold>622</bold></highlight>-<highlight><bold>628</bold></highlight>. The links greatly increase the speed of lookup. </paragraph>
<paragraph id="P-0057" lvl="0"><number>&lsqb;0057&rsqb;</number> In still, another embodiment, both the number of items in the indexed dictionary <highlight><bold>604</bold></highlight> along with the size of each item is known prior to the loading of the cache. With the number of items and the size of each item known, the exact size of the cache is known. It is also important to note that once an item in the double-linked list for usage tracking has been allocated, it never needs to be de-allocated or moved in memory in any way&mdash;all updates to the ordering of elements is done by changing pointers to reference new elements. Accordingly, the double linked list for tracking usage can be allocated as a single memory block, with all the list items adjacent to each other in memory. In essence, this would be an array of linked list elements. The elements could be addressed just like items in a traditional double-linked list, by pointers to each memory location. However, they could just as easily be addressed by their position in the array of elements, just by replacing the memory pointer by their index. Stated differently, the calculation for the maintenance of the pointers is more efficient using relative addressing comprising a base address, which is the start of the memory block and an offset to the particular entry. Moreover, the allocation of a predetermined or known cache size permits very fast initial loading of the double linked list for tracking usage and may alleviate memory fragmentation and some of the overhead imposed by memory managers. </paragraph>
<paragraph id="P-0058" lvl="0"><number>&lsqb;0058&rsqb;</number> Yet another embodiment would be to merge the double-linked list with the alphabetic index, so that each item in the alphabetic index would contain the embedded previous and next pointers of a double-linked list. These index entries could be maintained in exactly the same manner as a traditional separate linked list. </paragraph>
<paragraph id="P-0059" lvl="7"><number>&lsqb;0059&rsqb;</number> Demonstration of Dual Indexing </paragraph>
<paragraph id="P-0060" lvl="0"><number>&lsqb;0060&rsqb;</number> Turning now to <cross-reference target="DRAWINGS">FIG. 7</cross-reference>, which contains block diagram <highlight><bold>700</bold></highlight> of the demonstration of the dual indexing using the linked dictionary list <highlight><bold>604</bold></highlight> and the linked usage list <highlight><bold>606</bold></highlight>, according to the present invention. Illustrated are three separate times states t<highlight><subscript>0</subscript></highlight>, t<highlight><subscript>1</subscript></highlight>, and t<highlight><subscript>2</subscript></highlight>, During the four steps or states in time, the state of the dictionary <highlight><bold>742</bold></highlight>, the linked usage list <highlight><bold>744</bold></highlight> and the linked list vector <highlight><bold>746</bold></highlight>, with the resultant entry in the linked usage list <highlight><bold>606</bold></highlight> are now described. At time t<highlight><subscript>0</subscript></highlight>, the dictionary <highlight><bold>702</bold></highlight> contains the simple list of AARDVARKS, BIRDS, CATS and DOGS. Each of these entries is pointing <highlight><bold>748</bold></highlight> to a unique entry in the double-linked list for tracking usage <highlight><bold>704</bold></highlight>. The AARDVARKS entry is pointing to the LRU as it was loaded first. Next entries are BIRDS, CATS and finally DOGS, which is the MRU. The resultant linked list vector <highlight><bold>706</bold></highlight> is as simple as A to B to C to D. The linked usage list <highlight><bold>606</bold></highlight> contains A, which is the LRU and D, which is the MRU. Whenever an item becomes the LRU or the MRU, a pointer identifying this item must be updated. </paragraph>
<paragraph id="P-0061" lvl="0"><number>&lsqb;0061&rsqb;</number> At time t<highlight><subscript>1</subscript></highlight>, the entry AARDVARKS is accessed. The data location in the dictionary is not changed <highlight><bold>712</bold></highlight>. However, the usage-linked list <highlight><bold>714</bold></highlight> is adjusted. AARDVARKS is now the MRU and the other entries are re-linked so that BIRDS is the LRU. The linked list vector is <highlight><bold>716</bold></highlight> is now B to C to D to A. Therefore the linked usage list <highlight><bold>606</bold></highlight> contains B which is the LRU and A, which is the MRU. Note that nothing is ever-moved in memory, the only thing that changes is how pointers link the items in the list. This eliminates any overhead involved in memory management and memory fragmentation. Therefore no time is wasted moving data around in memory, only the usage linked list is updated. </paragraph>
<paragraph id="P-0062" lvl="0"><number>&lsqb;0062&rsqb;</number> At time t<highlight><subscript>2</subscript></highlight>, a request is made for EAGLES information. By accessing the indexed dictionary <highlight><bold>604</bold></highlight> in <cross-reference target="DRAWINGS">FIG. 6</cross-reference>, it is quickly determined that the entry EAGLES is not in the cache <highlight><bold>608</bold></highlight>. The location of the LRU to be used in this full cache is easy to determine by accessing the usage-linked list <highlight><bold>606</bold></highlight> in <cross-reference target="DRAWINGS">FIG. 6</cross-reference> and its LRU pointer. The address location of the LRU is currently pointing to the BIRDS entry. The entry BIRDS is removed from the indexed dictionary list and the entry EAGLES is added. The linked usage list <highlight><bold>606</bold></highlight> is updated to point to EAGLES and this node in the list is re-linked to become the MRU. The dictionary list <highlight><bold>722</bold></highlight> now contains AARDVARKS, CATS, DOGS and EAGLES <highlight><bold>722</bold></highlight>. The usage-linked list <highlight><bold>724</bold></highlight> contains the resultant new list with EAGLES being the new MRU, and the other entries, AARDVARKS, CATS and DOGS. The linked list vector is C to D to A to E. The linked usage list <highlight><bold>606</bold></highlight> contains C, which is the LRU, and E, which is the MRU. </paragraph>
<paragraph id="P-0063" lvl="7"><number>&lsqb;0063&rsqb;</number> Flow Diagram of a Dual Indexing According to the Present Invention </paragraph>
<paragraph id="P-0064" lvl="0"><number>&lsqb;0064&rsqb;</number> It is noted that when accessing records in a database, which is typically stored on a hard disk, access time may become significant. This can be exacerbated when several database lookups are involved. Specifically, if the topmost list is stored on one hard drive sector and this list points to other hard drive sectors significant time is used in locating the sought for entry. In one embodiment of the present invention techniques such as B&plus;trees are used to minimize this. This technique is preferred when decision points, called nodes, are on a hard disk rather than in random-access memory. B&plus;trees save time by using nodes with many branches. This allows for fast location of a data item by passing through fewer nodes. Once located, the record is retrieved. Next, the pointers to the record are added to the indexed dictionary list <highlight><bold>604</bold></highlight> and to the linked usage list <highlight><bold>606</bold></highlight>. Finally, the indexed dictionary list <highlight><bold>604</bold></highlight> and the linked usage list are updated. Alternatives to the B&plus;trees technique are techniques that use hash codes, binary trees, numerical indexing, or any of numerous techniques for organizing alphabetical indexes. </paragraph>
<paragraph id="P-0065" lvl="0"><number>&lsqb;0065&rsqb;</number> Turning now to <cross-reference target="DRAWINGS">FIG. 8</cross-reference>, shown is a flow diagram <highlight><bold>800</bold></highlight> of a dual-indexed cache, according to the present invention. The flow diagram <highlight><bold>800</bold></highlight> is entered <highlight><bold>802</bold></highlight> with a request for EAGLES information <highlight><bold>804</bold></highlight>. The dictionary index <highlight><bold>604</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 6</cross-reference> is accessed to determine if EAGLES is in the cache. If the EAGLES information is in the dictionary index <highlight><bold>604</bold></highlight>, the EAGLES element includes the address location in the cache <highlight><bold>806</bold></highlight>. The EAGLES element in the usage-linked list is re-linked to make this item the MRU <highlight><bold>822</bold></highlight>, and the EAGLES information is used <highlight><bold>824</bold></highlight>. </paragraph>
<paragraph id="P-0066" lvl="0"><number>&lsqb;0066&rsqb;</number> If EAGLES is not found <highlight><bold>806</bold></highlight> in the dictionary index <highlight><bold>604</bold></highlight>, then the information must be fetched <highlight><bold>808</bold></highlight> from the next higher memory level, such as a database. A check is performed to determine if there is space available in the cache <highlight><bold>810</bold></highlight>. If there is, the EAGLES information is stored <highlight><bold>812</bold></highlight>, the Eagles linked usage list is re-linked to make this file the MRU <highlight><bold>822</bold></highlight>, and then used <highlight><bold>824</bold></highlight>. </paragraph>
<paragraph id="P-0067" lvl="0"><number>&lsqb;0067&rsqb;</number> If there is no space <highlight><bold>810</bold></highlight> then according to the present invention, the LRU element from the usage-linked list <highlight><bold>606</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 6</cross-reference> is accessed to determine the LRU element location <highlight><bold>814</bold></highlight>. Now, the information in the LRU element location is deleted <highlight><bold>816</bold></highlight> if necessary. The EAGLES information can now be added to the cache <highlight><bold>608</bold></highlight>, and the dictionary index <highlight><bold>604</bold></highlight>. The usage-linked list now points to the desired EAGLES information. The usage-linked list is re-linked to change the LRU element to the MRU element <highlight><bold>822</bold></highlight>, and the dictionary index <highlight><bold>604</bold></highlight> is updated. Finally, the flow diagram <highlight><bold>800</bold></highlight> is exited <highlight><bold>826</bold></highlight> by using the EAGLES information <highlight><bold>824</bold></highlight>. </paragraph>
<paragraph id="P-0068" lvl="7"><number>&lsqb;0068&rsqb;</number> Glossary of Terms Used in this Disclosure </paragraph>
<paragraph id="P-0069" lvl="1"><number>&lsqb;0069&rsqb;</number> ALPHA LIST&mdash;is an indexed list that has been sorted with respect to the (usually) ascending alphabet. With an alpha list a search for an entry name based on the corresponding index order, which in the alpha list is the alphabetical position. It should be noted that if an alpha list search does not find the entry at the correct alphabetical location on the alpha list, then the entry is not in the alpha list. </paragraph>
<paragraph id="P-0070" lvl="1"><number>&lsqb;0070&rsqb;</number> B-Tree&mdash;is a method of placing and locating records in a database. The B-tree algorithm minimizes the number of times a medium must be accessed to locate a desired record, thereby speeding up the process. </paragraph>
<paragraph id="P-0071" lvl="7"><number>&lsqb;0071&rsqb;</number> B-trees are preferred when decision points, called nodes, are on hard disk rather than in random-access memory (RAM). It takes thousands of times longer to access a data element from hard disk as compared with accessing it from RAM, because a disk drive has mechanical parts, which read and write data far more slowly than purely electronic media. B-trees save time by using nodes with many branches (called children), compared with binary trees, in which each node has only two children. When there are many children per node, a record can be found by passing through fewer nodes than if there are two children per node. </paragraph>
<paragraph id="P-0072" lvl="1"><number>&lsqb;0072&rsqb;</number> B&plus;-tree&mdash;is an improved version of the B-Tree algorithm that optimizes search times by ensuring that the tree is always balanced. A B-tree may easily become &ldquo;unbalanced&rdquo;, meaning that some branches (search paths) are much longer than others, for example if a lot of records were stored under the letter &ldquo;S&rdquo;. The B&plus;-tree ensures a balanced tree by enforcing a max. height and splitting the nodes (pages) as needed to ensure that the max. height is never exceeded. To use the example where many records are stored under the letter &ldquo;S&rdquo;, page splitting would ensure that there were a number of top level pages for the letter &ldquo;S&rdquo; instead of a single one sitting on top of a tall tree. </paragraph>
<paragraph id="P-0073" lvl="1"><number>&lsqb;0073&rsqb;</number> CACHE&mdash;is a memory location to store information temporarily. One example of a cache used is where Web pages are stored on a browser&apos;s cache directory on a system&apos;s hard disk. If a user returns to a page previously visited, the browser may access the page from the cache rather than the original server, saving both time and the network the burden of some additional traffic. Another example of a cache used is a RAM disk cache that contains the data most recently read in from the hard disk. A cache can comprise many different memory technologies including volatile, non-volatile, electronic, mechanical, chemical and organic. </paragraph>
<paragraph id="P-0074" lvl="1"><number>&lsqb;0074&rsqb;</number> DATABASE&mdash;is a collection of data that is organized so that its contents can easily be accessed, managed, and updated. The most prevalent type of database is the relational database. A relational database is a tabular database in which data is defined so that it can be reorganized and accessed in a number of different ways. A distributed database is one that can be dispersed or replicated among different points in a network. An object-oriented programming database is one that is congruent with the data defined in object classes and subclasses. </paragraph>
<paragraph id="P-0075" lvl="1"><number>&lsqb;0075&rsqb;</number> DICTIONARY&mdash;is a collection of data objects or items in a data model that are indexed for rapid accessing. This indexing may use any of a number of standard schemes, such as a hash table, a B&plus;Tree, or any other method that allows quick lookups using a key value. This collection can be organized for reference into a database. </paragraph>
<paragraph id="P-0076" lvl="1"><number>&lsqb;0076&rsqb;</number> DUAL INDEXING&mdash;is a scheme whereby two indexes are combined to simultaneously optimize two different types of lookups. Each index would contain the information that is normal for its type (alphabetic index or double-linked list), but in addition each entry also points to a corresponding entry in the other list. An example is an element in a dictionary index that contains a pointer to an entry in a double-linked list (a usage index) that contains the usage order of each item in the dictionary. </paragraph>
<paragraph id="P-0077" lvl="1"><number>&lsqb;0077&rsqb;</number> ENCYCLOPEDIA&mdash;is a source of information that is used to construct a collection of data objects. These data objects, taken together are also known as a DICTIONARY. The DICTIONARY, once constructed is used during caching to improve performance and to point to the specific location of additional data that resides in the ENCYCLOPEDIA. </paragraph>
<paragraph id="P-0078" lvl="1"><number>&lsqb;0078&rsqb;</number> HASHING&mdash;is the transformation of a string of characters into a usually shorter fixed-length value or key that represents the original string. Hashing is used to index and retrieve items in a database because it is faster to find the item using the shorter hashed key than to find it using the original value. It is also used in many encryption algorithms. </paragraph>
<paragraph id="P-0079" lvl="7"><number>&lsqb;0079&rsqb;</number> As a simple example of the using of hashing in databases, a group of animals could be arranged in a database like this: </paragraph>
<paragraph id="P-0080" lvl="2"><number>&lsqb;0080&rsqb;</number> Aardvarks </paragraph>
<paragraph id="P-0081" lvl="2"><number>&lsqb;0081&rsqb;</number> Birds </paragraph>
<paragraph id="P-0082" lvl="2"><number>&lsqb;0082&rsqb;</number> Cats </paragraph>
<paragraph id="P-0083" lvl="2"><number>&lsqb;0083&rsqb;</number> Dogs </paragraph>
<paragraph id="P-0084" lvl="2"><number>&lsqb;0084&rsqb;</number> (and many more sorted into alphabetical order) </paragraph>
<paragraph id="P-0085" lvl="7"><number>&lsqb;0085&rsqb;</number> Each of these animals would be the key in the database for that animal&apos;s data. A database search mechanism would first have to start looking character-by-character across the name for matches until it found the match (or ruled the other entries out). But if each of the names were hashed, it might be possible (depending on the number of animals in the database) to generate a unique four-digit key for each name. For example: </paragraph>
<paragraph id="P-0086" lvl="2"><number>&lsqb;0086&rsqb;</number> <highlight><bold>7864</bold></highlight> Aardvarks </paragraph>
<paragraph id="P-0087" lvl="2"><number>&lsqb;0087&rsqb;</number> <highlight><bold>9802</bold></highlight> Birds </paragraph>
<paragraph id="P-0088" lvl="2"><number>&lsqb;0088&rsqb;</number> <highlight><bold>1990</bold></highlight> Cats </paragraph>
<paragraph id="P-0089" lvl="2"><number>&lsqb;0089&rsqb;</number> <highlight><bold>8822</bold></highlight> Dogs </paragraph>
<paragraph id="P-0090" lvl="2"><number>&lsqb;0090&rsqb;</number> (and so forth) </paragraph>
<paragraph id="P-0091" lvl="7"><number>&lsqb;0091&rsqb;</number> A search for any name would first consist of computing the hash value (using the same hash function used to store the item) and then comparing for a match using that value. It would, in general, be much faster to find a match across four digits, each having only 10 possibilities, than across an unpredictable value length where each character had 26 possibilities. </paragraph>
<paragraph id="P-0092" lvl="1"><number>&lsqb;0092&rsqb;</number> INFORMATION&mdash;is any data and code that is used by a computer including text, graphics, audio, video and multimedia content. </paragraph>
<paragraph id="P-0093" lvl="1"><number>&lsqb;0093&rsqb;</number> LINKED LIST&mdash;is list, sometimes called a chained list in which the elements of the list may be dispersed but in which each element contains information, typically a pointer, for locating the next element in the list. Two examples of linked lists are a single-linked list, where each element (in addition to the data it holds) only has a pointer to the next element, and a double-linked list, where each element also has a pointer to the previous element. In applications where the ordering of the elements often has to be changed, it is more efficient to use a double-linked list since a single-linked list forces you to scan from the beginning of the list every time you need to find the previous element to re-link the chain. </paragraph>
<paragraph id="P-0094" lvl="7"><number>&lsqb;0094&rsqb;</number> Non-limiting Examples </paragraph>
<paragraph id="P-0095" lvl="0"><number>&lsqb;0095&rsqb;</number> Although a specific embodiment of the invention has been disclosed. It will be understood by those having skill in the art that changes can be made to this specific embodiment without departing from the spirit and scope of the invention. The scope of the invention is not to be restricted, therefore, to the specific embodiment, and it is intended that the appended claims cover any and all such applications, modifications, and embodiments within the scope of the present invention. </paragraph>
<paragraph id="P-0096" lvl="7"><number>&lsqb;0096&rsqb;</number> Link</paragraph>
</section>
</detailed-description>
</subdoc-description>
<subdoc-claims>
<heading lvl="1">What is claimed is: </heading>
<claim id="CLM-00001">
<claim-text><highlight><bold>1</bold></highlight>. A method for organizing information into a cache memory subsystem, the subsystem containing a database of information elements stored in memory, and a subset of the database of information elements stored in a cache memory, the method comprising the steps of: 
<claim-text>forming an indexed list for holding information elements in a predetermined order based on the indexing method being used; </claim-text>
<claim-text>forming a linked usage list based on the historic use of information elements including a most-recently-used (MRU) information element and a least-recently-used (LRU) element so that the indexed list and the linked usage list together form a dual indexing scheme for the information elements; </claim-text>
<claim-text>receiving a request for an information element; </claim-text>
<claim-text>determining if the requested information element is not in cache by searching the indexed list, and if the requested information element is not in cache, then determining if there is cache memory space available and if there is no cache memory space available then performing the sub-steps of: 
<claim-text>determining a location of the LRU information element from the linked usage list; </claim-text>
<claim-text>updating the pointer to the LRU informational element in the indexed list; </claim-text>
<claim-text>updating the LRU informational element entry in the linked usage list; </claim-text>
<claim-text>updating the information element requested from a information source; and </claim-text>
<claim-text>storing the requested information in the cache. </claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002">
<claim-text><highlight><bold>2</bold></highlight>. The method according to <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, further comprising the sub-steps of: 
<claim-text>updating the indexed list based on an index order; and </claim-text>
<claim-text>updating the linked usage list based on the historic use of the information elements. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00003">
<claim-text><highlight><bold>3</bold></highlight>. The method according to <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, further comprising the sub-step of: 
<claim-text>updating the linked usage list based on the usage order, and; </claim-text>
<claim-text>providing the new MRU and LRU entries. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00004">
<claim-text><highlight><bold>4</bold></highlight>. The method according to <dependent-claim-reference depends_on="CLM-00002">claim 2</dependent-claim-reference>, wherein the step of updating the indexed list based on an index order includes updating the indexed list based upon an alphabetical order or a numerical order or some combination of both an alphabetical and numeric order. </claim-text>
</claim>
<claim id="CLM-00005">
<claim-text><highlight><bold>5</bold></highlight>. The method according to <dependent-claim-reference depends_on="CLM-00002">claim 2</dependent-claim-reference>, further comprising the sub-steps of: 
<claim-text>updating the linked usage list so that the location in the cache memory subsystem of the requested information element is now identified as the MRU location. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00006">
<claim-text><highlight><bold>6</bold></highlight>. A method for management of a memory cache system comprising a plurality of information elements, comprising the steps of: 
<claim-text>placing a plurality of information elements in an indexed list to enable searching of the information elements based on a position in the indexed list; </claim-text>
<claim-text>creating a linked usage list with a usage history from a most-recently-used (MRU) to a least-recently-used (LRU) based upon a set of information elements in the indexed list, wherein each member of the linked usage list contains a pointer back to a member in the indexed list so that the order of usage of the individual members of the indexed list is searchable by searching the linked usage list; and </claim-text>
<claim-text>receiving a request for an information element that is not in the indexed list, whereby an information element representing the requested information is placed in the indexed list replacing the least recently used information element as determined by the LRU element in the linked usage list. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00007">
<claim-text><highlight><bold>7</bold></highlight>. The method according to <dependent-claim-reference depends_on="CLM-00006">claim 6</dependent-claim-reference>, wherein the step of placing information elements includes placing pointers or keys into a database holding additional information elements. </claim-text>
</claim>
<claim id="CLM-00008">
<claim-text><highlight><bold>8</bold></highlight>. The method according to <dependent-claim-reference depends_on="CLM-00007">claim 7</dependent-claim-reference>, wherein the step of placing information elements includes placing pointers or keys into a database, further comprises the steps of: 
<claim-text>associating each of the informational elements with a particular pointer, wherein the pointer is a number; </claim-text>
<claim-text>associating a location in the database holding additional information elements stored in memory with each of the informational elements, and; </claim-text>
<claim-text>associating each of the informational elements with a location in the indexed list. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00009">
<claim-text><highlight><bold>9</bold></highlight>. The method according to <dependent-claim-reference depends_on="CLM-00007">claim 7</dependent-claim-reference>, wherein the step of placing information elements uses a list of a predetermined and fixed size, allowing the linked usage list to be allocated as a single contiguous memory block and each element of the linked list to be accessed by its fixed position in the index, rather than by its physical memory address. </claim-text>
</claim>
<claim id="CLM-00010">
<claim-text><highlight><bold>10</bold></highlight>. A method for addressing cached information in a memory cache sub-system, the method comprising the steps of: 
<claim-text>loading one or more information elements using a list of a predetermined and fixed size, and allocating the linked usage list as a single contiguous memory block, allowing each element of the linked list to be accessed by its fixed position in the index, rather than by its physical memory address. </claim-text>
<claim-text>creating a linked usage list in cache with a usage history from a most-recently-used (MRU) to a least-recently-used (LRU) based upon a set of information elements in the indexed list, wherein each member of the linked usage list contains a pointer back to a member in the indexed list so that the order of usage of the individual members of the indexed list is searchable by searching the linked usage list; and </claim-text>
<claim-text>receiving a request for an information element that is not in the indexed list, whereby an information element representing the requested information is placed in the indexed list replacing the element identified by the LRU element in the linked usage list. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00011">
<claim-text><highlight><bold>11</bold></highlight>. A computer readable medium containing programming instructions for organizing information into a cache memory subsystem, the subsystem containing a database of information elements stored in memory, and a subset of the database of information elements stored in a cache memory, the method comprising the steps of: 
<claim-text>forming an indexed list for holding information elements in a predetermined order based on the indexing method being used; </claim-text>
<claim-text>forming a linked usage list based on the historic use of information elements including a most-recently-used (MRU) information element and a least-recently-used (LRU) element so that the indexed list and the linked usage list together form a dual indexing scheme for the information elements; </claim-text>
<claim-text>receiving a request for an information element; </claim-text>
<claim-text>determining if the requested information element is not in cache by searching the indexed list, and if the requested information element is not in cache, then determining if there is cache memory space available and if there is no cache memory space available then performing the sub-steps of: 
<claim-text>determining a location of the LRU information element from the linked usage list; </claim-text>
<claim-text>updating the pointer to the LRU informational element in the indexed list; </claim-text>
<claim-text>updating the LRU informational element entry in the linked usage list; </claim-text>
<claim-text>updating the information element requested from a information source; and </claim-text>
<claim-text>storing the requested information in the cache. </claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00012">
<claim-text><highlight><bold>12</bold></highlight>. The computer readable medium, according to <dependent-claim-reference depends_on="CLM-00011">claim 11</dependent-claim-reference>, further comprising the sub-steps of: 
<claim-text>updating the indexed list based on an index order, and; </claim-text>
<claim-text>updating the linked usage list based on the historic use of the information elements. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00013">
<claim-text><highlight><bold>13</bold></highlight>. The computer readable medium according to <dependent-claim-reference depends_on="CLM-00011">claim 11</dependent-claim-reference>, further comprising the sub-step of: 
<claim-text>updating the linked usage list based on the usage order; and </claim-text>
<claim-text>providing the new MRU and LRU entries. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00014">
<claim-text><highlight><bold>14</bold></highlight>. The computer readable medium according to <dependent-claim-reference depends_on="CLM-00011">claim 12</dependent-claim-reference>, wherein the step of updating the indexed list based on an index order includes updating the indexed list consisting of an alphabetical order , or a numerical order or some combination of both an alphabetical order and a numerical order. </claim-text>
</claim>
<claim id="CLM-00015">
<claim-text><highlight><bold>15</bold></highlight>. The computer readable medium according to <dependent-claim-reference depends_on="CLM-00011">claim 12</dependent-claim-reference>, further comprising the sub-steps of: 
<claim-text>updating the linked usage list so that the location in the cache memory subsystem of the requested information element is now identified as the MRU location. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00016">
<claim-text><highlight><bold>16</bold></highlight>. A computer readable medium containing programming instructions for management of a memory cache system comprising a plurality of information elements, the programming instructions comprising: 
<claim-text>placing a plurality of information elements in an indexed list to enable searching of the information elements based on a position in the indexed list; </claim-text>
<claim-text>creating a linked usage list with a usage history from a most-recently-used (MRU) to a least-recently-used (LRU) based upon a set of information elements in the indexed list, wherein each member of the linked usage list contains a pointer back to a member in the indexed list so that the order of usage of the individual members of the indexed list is searchable by searching the linked usage list; and </claim-text>
<claim-text>receiving a request for an information element that is not in the indexed list, whereby an information element representing the requested information is placed in the indexed list replacing an element determined by the LRU element in the linked usage list. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00017">
<claim-text><highlight><bold>17</bold></highlight>. The computer readable medium according to <dependent-claim-reference depends_on="CLM-00011">claim 16</dependent-claim-reference>, wherein the programming instruction of placing information elements includes placing pointers/keys into a database holding additional information elements. </claim-text>
</claim>
<claim id="CLM-00018">
<claim-text><highlight><bold>18</bold></highlight>. The computer readable medium according to <dependent-claim-reference depends_on="CLM-00011">claim 17</dependent-claim-reference>, wherein the programming instruction of placing information elements includes placing pointers or keys into a database, further comprises the steps of: 
<claim-text>associating each of the informational elements with a particular pointer, wherein the pointer is a number; </claim-text>
<claim-text>associating a location in the database of information elements stored in memory with each of the informational elements, and; </claim-text>
<claim-text>associating each of the informational elements with a location in the indexed list. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00019">
<claim-text><highlight><bold>19</bold></highlight>. The computer readable medium according to <dependent-claim-reference depends_on="CLM-00011">claim 17</dependent-claim-reference>, wherein the programming instruction of placing information elements includes using a list of a predetermined and fixed size, and allocating the linked usage list as a single contiguous memory block, allowing each element of the linked list to be accessed by its fixed position in the index, rather than by its physical memory address. </claim-text>
</claim>
<claim id="CLM-00020">
<claim-text><highlight><bold>20</bold></highlight>. A computer readable medium containing programming instructions for addressing cached information in a memory cache sub-system, the programming instructions comprising: 
<claim-text>loading one or more information elements into an indexed list in cache memory using a list of a predetermined and fixed size, and allocating the linked usage list as a single contiguous memory block, allowing each element of the linked list to be accessed by its fixed position in the index, rather than by its physical memory address. </claim-text>
<claim-text>creating a linked usage list in cache with a usage history from a most-recently-used (MRU) to a least-recently-used (LRU) based upon a set of information elements in the indexed list, wherein each member of the linked usage list contains a pointer back to a member in the indexed list so that the order of usage of the individual members of the indexed list is searchable by searching the linked usage list; and </claim-text>
<claim-text>receiving a request for an information element that is not in the indexed list, whereby an information element representing the requested information is placed in the indexed replacing an element determined by the LRU element in the linked usage list. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00021">
<claim-text><highlight><bold>21</bold></highlight>. A cache memory sub-system comprising: 
<claim-text>a memory storage cache; </claim-text>
<claim-text>an indexed list formed in cache with a plurality of informational elements loaded in the indexed list whereby the information elements are searchable based on a position in the indexed list; </claim-text>
<claim-text>an linked usage list with a usage history from a most-recently-used (MRU) to a least-recently-used (LRU) based upon a set of information elements in the indexed list, wherein each member of the linked usage list contains a pointer back to a member in the indexed list so that the order of usage of the individual members of the indexed list is searchable by searching the linked usage list; and </claim-text>
<claim-text>means for receiving a request for an information element that is not in the indexed list, whereby a information element representing the requested information is placed in the indexed list replacing an element determined by the LRU element in the linked usage list.</claim-text>
</claim-text>
</claim>
</subdoc-claims>
<subdoc-drawings id="DRAWINGS">
<heading lvl="0" align="CENTER">Drawings</heading>
<representative-figure>7</representative-figure>
<figure id="figure-D00000">
<image id="EMI-D00000" file="US20030005233A1-20030102-D00000.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00001">
<image id="EMI-D00001" file="US20030005233A1-20030102-D00001.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00002">
<image id="EMI-D00002" file="US20030005233A1-20030102-D00002.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00003">
<image id="EMI-D00003" file="US20030005233A1-20030102-D00003.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00004">
<image id="EMI-D00004" file="US20030005233A1-20030102-D00004.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00005">
<image id="EMI-D00005" file="US20030005233A1-20030102-D00005.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00006">
<image id="EMI-D00006" file="US20030005233A1-20030102-D00006.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00007">
<image id="EMI-D00007" file="US20030005233A1-20030102-D00007.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00008">
<image id="EMI-D00008" file="US20030005233A1-20030102-D00008.TIF" imf="TIFF" ti="DR"/>
</figure>
</subdoc-drawings>
</patent-application-publication>
