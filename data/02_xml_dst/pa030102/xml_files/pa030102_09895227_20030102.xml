<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE patent-application-publication SYSTEM "pap-v16-2002-01-01.dtd" [
<!ENTITY US20030004683A1-20030102-D00000.TIF SYSTEM "US20030004683A1-20030102-D00000.TIF" NDATA TIF>
<!ENTITY US20030004683A1-20030102-D00001.TIF SYSTEM "US20030004683A1-20030102-D00001.TIF" NDATA TIF>
<!ENTITY US20030004683A1-20030102-D00002.TIF SYSTEM "US20030004683A1-20030102-D00002.TIF" NDATA TIF>
<!ENTITY US20030004683A1-20030102-D00003.TIF SYSTEM "US20030004683A1-20030102-D00003.TIF" NDATA TIF>
<!ENTITY US20030004683A1-20030102-D00004.TIF SYSTEM "US20030004683A1-20030102-D00004.TIF" NDATA TIF>
<!ENTITY US20030004683A1-20030102-D00005.TIF SYSTEM "US20030004683A1-20030102-D00005.TIF" NDATA TIF>
<!ENTITY US20030004683A1-20030102-D00006.TIF SYSTEM "US20030004683A1-20030102-D00006.TIF" NDATA TIF>
<!ENTITY US20030004683A1-20030102-D00007.TIF SYSTEM "US20030004683A1-20030102-D00007.TIF" NDATA TIF>
]>
<patent-application-publication>
<subdoc-bibliographic-information>
<document-id>
<doc-number>20030004683</doc-number>
<kind-code>A1</kind-code>
<document-date>20030102</document-date>
</document-id>
<publication-filing-type>new</publication-filing-type>
<domestic-filing-data>
<application-number>
<doc-number>09895227</doc-number>
</application-number>
<application-number-series-code>09</application-number-series-code>
<filing-date>20010629</filing-date>
</domestic-filing-data>
<technical-information>
<classification-ipc>
<classification-ipc-primary>
<ipc>G06F011/30</ipc>
</classification-ipc-primary>
<classification-ipc-edition>07</classification-ipc-edition>
</classification-ipc>
<classification-us>
<classification-us-primary>
<uspc>
<class>702</class>
<subclass>186000</subclass>
</uspc>
</classification-us-primary>
</classification-us>
<title-of-invention>Instruction pre-fetching mechanism for a multithreaded program execution</title-of-invention>
</technical-information>
<inventors>
<first-named-inventor>
<name>
<given-name> Shashank</given-name>
<family-name>Nemawarkar</family-name>
</name>
<residence>
<residence-us>
<city>Austin</city>
<state>TX</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</first-named-inventor>
</inventors>
<assignee>
<organization-name>International Business Machines Corp.</organization-name>
<address>
<city>Armonk</city>
<state>NY</state>
<country>
<country-code>US</country-code>
</country>
</address>
<assignee-type>02</assignee-type>
</assignee>
<correspondence-address>
<name-1>BRACEWELL &amp; PATTERSON, L.L.P.</name-1>
<name-2></name-2>
<address>
<address-1>P.O. BOX 969</address-1>
<city>AUSTIN</city>
<state>TX</state>
<postalcode>78767-0969</postalcode>
<country>
<country-code>US</country-code>
</country>
</address>
</correspondence-address>
</subdoc-bibliographic-information>
<subdoc-abstract>
<paragraph id="A-0001" lvl="0">A method and processor architecture are provided that enables efficient pre-fetching of instructions for multithreaded program executions in a data processing system. The processor architecture comprises an instruction pre-fetch unit, which includes a pre-fetch request engine, a pre-fetch request buffer, and additional logic components for the correct implementation of a thread-identifiable pre-fetching scheme. A number of pre-defined triggers initiates the generation of a pre-fetch request, which includes an identification (ID) of the particular thread from which the request is generated. Two counters associated with the pre-fetch request engine are utilized to track the number of threads and the number of executed instructions within the threads, respectively. The pre-fetch request is issued to the lower level cache or memory and returns with a corresponding cache line, tagged with the thread ID. The cache line is stored in the pre-fetch request buffer along with its thread ID. When the particular thread later requires the instruction, the instruction is provided from within the pre-fetch request buffer at a short access latency than if the instruction had to be fetched from the lower level cache or memory. </paragraph>
</subdoc-abstract>
<subdoc-description>
<summary-of-invention>
<section>
<heading lvl="1">BACKGROUND OF THE INVENTION </heading>
<paragraph id="P-0001" lvl="0"><number>&lsqb;0001&rsqb;</number> 1. Field of the Invention </paragraph>
<paragraph id="P-0002" lvl="0"><number>&lsqb;0002&rsqb;</number> The present invention generally relates to computer systems, and in particular to an instruction fetching within a processor of a data processing system. Still more particularly, the present invention relates to a method and system for providing efficient instruction pre-fetching for a multithreaded program. </paragraph>
<paragraph id="P-0003" lvl="0"><number>&lsqb;0003&rsqb;</number> 2. Description of the Related Art </paragraph>
<paragraph id="P-0004" lvl="0"><number>&lsqb;0004&rsqb;</number> The basic structure of a conventional computer system includes a system bus or a direct channel that connects one or more processors to input/output (I/O) devices (e.g., display monitor, keyboard and mouse), a permanent memory device for storing the operating system and user applications, and a temporary memory device that is utilized by the processors to execute program instructions. </paragraph>
<paragraph id="P-0005" lvl="0"><number>&lsqb;0005&rsqb;</number> When a user program is executed on a computer, the computer&apos;s operating system (OS) first loads the program files into system memory. The program files include data objects and instructions for handling the data and other parameters which may be inputted during program execution. </paragraph>
<paragraph id="P-0006" lvl="0"><number>&lsqb;0006&rsqb;</number> The operating system creates a process to run a user program. The process comprises a set of resources, including (but not limited to) values in RAM, process limits, permissions, registers, and at least one execution stream, which is commonly termed a &ldquo;thread.&rdquo; The utilization of threads in user applications is well known. Threads allow multiple execution paths within a single address space to run on a processor. This process is called &ldquo;multithreading&rdquo; and increases throughput and modularity in both multiprocessor and uniprocessor systems. For example, if a first thread of an executing program has to wait for the occurrence of an event, then the processor halts its execution, and the computer processor executes another thread to prevent stoppages in processor operation and thus optimize utilization of processor resources. The event which causes a switching of the execution from one thread to another is typically a long latency operation, such as disk/remote memory access or producer-consumer type data exchange. In a multiprocessor computer system, multithreaded programs may exploit the availability of multiple processors by running different threads of the application program in parallel. The wait associated with long latency operations is masked by the computation performed on other threads available to the processor. Parallel execution reduces response time and improves throughput in multiprocessor systems. </paragraph>
<paragraph id="P-0007" lvl="0"><number>&lsqb;0007&rsqb;</number> In a superscalar processor operating at high frequencies, execution of a program typically involves pre-fetching of instructions from the memory or instruction cache to enable a continuous flow of instructions to the processor&apos;s execution units. Instructions are &ldquo;pipelined&rdquo; utilizing an instruction fetching unit (IFU) that is a hardware component of the processor. The operational characteristics of the IFU are dependent on changes to the flow of instruction execution due to branches, the depth of processing core, and the memory access latency to fetch the new sets of instructions. Further, the IFU is hardware extensive and is typically not scalable for high frequency processor designs. Also, current IFUs typically fetch instructions in a unithread fashion, i.e., fetch all instructions for a first thread before fetching the instructions for another thread. With the movement towards multithreaded programs and multiprocessor computer systems, this later characteristic of IFU operation, along with the other limitations, results in a dampening of overall processing efficiency and reduced throughput. </paragraph>
<paragraph id="P-0008" lvl="0"><number>&lsqb;0008&rsqb;</number> Typically, instruction pre-fetching is used on single-threaded executions. Given that a multi-threaded execution involves maintenance of separate (and at times shared) address space among threads, the single-threaded pre-fetching technique is not easily extended to execution of a multithreaded program. Two approaches to providing multithreaded architectures are the von Neumann execution based multithreading and the dataflow based multithreading. For dataflow based multithreading, all inputs of a thread are fetched before the execution on that thread commences. Thus, on a probable context switch a set of fetch operations are issued to bring the thread (code and data) to the on-chip caches, and the whole thread has to be brought in. This approach is very hardware and compiler intensive because there needs to be a mechanism to determine possible input sources of the thread, and all inputs have to arrive before a thread can be scheduled for execution. Also, the performance is inhibited because of the required synchronization to ensure that all input sources have been received. Such threads tend to be small, and the number of inputs for each thread is small as well to reduce the performance degradation. However, the simpler pre-fetching scheme cannot be easily extended to current multithreading operations. </paragraph>
<paragraph id="P-0009" lvl="0"><number>&lsqb;0009&rsqb;</number> Von Neumann execution based multithreading is exemplified by a Simultaneous Multithreading technique. This type of multithreading uses a program counter to track the program execution, and each thread is assumed independent of another. That is, benefits of warm caches (due to execution on one thread) on the execution on another thread are limited. Such multithreading can benefit from simple pre-fetching schemes. U.S. Pat. No. 5,809,450 offers one proposed pre-fetch scheme. According to patent, the latency of a remote memory access is calibrated using an on-chip performance measurement scheme and is utilized to insert the pre-fetches at empirically determined places in the code. This approach is also hardware extensive, and results vary with the configuration of the processor system due to changes in the memory and network access latencies. </paragraph>
<paragraph id="P-0010" lvl="0"><number>&lsqb;0010&rsqb;</number> The present invention recognizes that it would be desirable to have a method, system and processor that enables greater efficiency in handling execution of multithreaded programs. A method, system, and processor architecture that provides more efficient pre-fetching of instructions for multithreaded program execution would be a welcomed improvement. It would be further desirable to have such a method which was also scalable to adapt to higher frequency processor designs without requiring significant hardware upgrades. These and other benefits are provided in the present invention as described herein. </paragraph>
</section>
<section>
<heading lvl="1">SUMMARY OF THE INVENTION </heading>
<paragraph id="P-0011" lvl="0"><number>&lsqb;0011&rsqb;</number> Disclosed is a method, system, and processor architecture that enables efficient pre-fetching of instructions for multithreaded program execution in a data processing system. The processor architecture comprises an instruction pre-fetch unit that includes a pre-fetch request engine, a pre-fetch request buffer, and additional logic components for the correct implementation of a thread-identifiable pre-fetching scheme. </paragraph>
<paragraph id="P-0012" lvl="0"><number>&lsqb;0012&rsqb;</number> A number of pre-defined triggers initiates the generation of a pre-fetch request, which includes an identification (ID) of the particular thread from which the request is generated. In a preferred embodiment, the tagging of the pre-fetch request for later identification of which thread the instruction belongs to is completed with the assistance of two counter mechanisms associated with the pre-fetch request engine which track the number of threads and the number of executed instruction within the threads, respectively. </paragraph>
<paragraph id="P-0013" lvl="0"><number>&lsqb;0013&rsqb;</number> The pre-fetch request is issued to the lower level cache or memory and returns with a corresponding cache line. The cache line is tagged with the thread ID. A comparison of the cache line address is made with any address in the IFAR, which has a miss in the instruction cache and, when the addresses match, the cache line is immediately provided to the processor execution units. </paragraph>
<paragraph id="P-0014" lvl="0"><number>&lsqb;0014&rsqb;</number> In the preferred embodiment, when the cache line returns, it is stored in the pre-fetch request buffer along with its thread ID. When the particular thread later requires the instruction, the instruction is provided from within the pre-fetch request buffer at a short access latency than if the instruction had to be fetched from the lower level cache or memory. </paragraph>
<paragraph id="P-0015" lvl="0"><number>&lsqb;0015&rsqb;</number> The above, as well as additional objects, features, and advantages of the present invention will become apparent in the following detailed written description. </paragraph>
</section>
</summary-of-invention>
<brief-description-of-drawings>
<section>
<heading lvl="1">BRIEF DESCRIPTION OF THE DRAWINGS </heading>
<paragraph id="P-0016" lvl="0"><number>&lsqb;0016&rsqb;</number> The novel features believed characteristic of the invention are set forth in the appended claims. The invention itself, however, as well as a preferred mode of use, further objectives, and advantages thereof, will best be understood by reference to the following detailed description of an illustrative embodiment when read in conjunction with the accompanying drawings, wherein: </paragraph>
<paragraph id="P-0017" lvl="0"><number>&lsqb;0017&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 1</cross-reference> is a block diagram of a conventional data processing system, which is utilized to implement multithread programming execution; </paragraph>
<paragraph id="P-0018" lvl="0"><number>&lsqb;0018&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 2</cross-reference> is a block diagram depicting a modified processor design that includes additional logic for completing multi-threaded pre-fetching according to the present invention; </paragraph>
<paragraph id="P-0019" lvl="0"><number>&lsqb;0019&rsqb;</number> <cross-reference target="DRAWINGS">FIGS. 3A and 3B</cross-reference> are block diagrams illustrating the components and logic structure of the multi-threaded pre-fetching mechanism for pre-fetching instructions of a multithreaded program in accordance with a preferred embodiment of the present invention; and </paragraph>
<paragraph id="P-0020" lvl="0"><number>&lsqb;0020&rsqb;</number> FIGS. <highlight><bold>4</bold></highlight>A-<highlight><bold>4</bold></highlight>C are flow charts depicting the logic flow of the method of pre-fetching instructions of a multithreaded program in accordance with the implementation of the present invention. </paragraph>
</section>
</brief-description-of-drawings>
<detailed-description>
<section>
<heading lvl="1">DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENT </heading>
<paragraph id="P-0021" lvl="0"><number>&lsqb;0021&rsqb;</number> With reference now to the figures and in particular with reference to <cross-reference target="DRAWINGS">FIG. 1</cross-reference>, there is illustrated a block diagram of the basic structure of a data processing system <highlight><bold>100</bold></highlight> utilized in the preferred embodiment of the invention. Data processing system <highlight><bold>100</bold></highlight> has at least one central processing unit (CPU) or processor <highlight><bold>10</bold></highlight> which is connected to several peripheral devices, including input/output devices <highlight><bold>114</bold></highlight> (such as a display monitor, keyboard, and graphical pointing device) for user interface, a non-volatile memory device <highlight><bold>116</bold></highlight> (such as a hard disk) for storing the data processing system&apos;s operating system and user programs/applications, and a temporary memory device <highlight><bold>118</bold></highlight> (such as random access memory or RAM) that is utilized by processor <highlight><bold>10</bold></highlight> to implement program instructions. Processor <highlight><bold>10</bold></highlight> communicates with the peripheral devices by various means, including a bus <highlight><bold>120</bold></highlight> or a direct channel <highlight><bold>122</bold></highlight> (more than one bus may be provided utilizing a bus bridge or a network of buses). </paragraph>
<paragraph id="P-0022" lvl="0"><number>&lsqb;0022&rsqb;</number> Those skilled in the art will further appreciate that there are other components that might be utilized in conjunction with those shown in the block diagram of <cross-reference target="DRAWINGS">FIG. 1</cross-reference>; for example, a display adapter connected to processor <highlight><bold>10</bold></highlight> might be utilized to control a video display monitor, and a memory controller may be utilized as an interface between temporary memory device <highlight><bold>118</bold></highlight> and processor <highlight><bold>10</bold></highlight>. Data processing system <highlight><bold>100</bold></highlight> also includes firmware <highlight><bold>124</bold></highlight> whose primary purpose is to seek out and load an operating system from one of the peripherals (usually permanent memory device <highlight><bold>116</bold></highlight>) whenever the data processing system is first turned on. In the preferred embodiment, data processing system contains a relatively fast CPU or processor <highlight><bold>10</bold></highlight> along with sufficient temporary memory device <highlight><bold>118</bold></highlight> and space on permanent memory device <highlight><bold>116</bold></highlight>, and other required hardware components necessary for providing efficient execution of instructions. </paragraph>
<paragraph id="P-0023" lvl="0"><number>&lsqb;0023&rsqb;</number> While an illustrative embodiment of the present invention has been, and will continue to be, described in the context of a fully functional data processing system, those skilled in the art will appreciate that the software aspects of an illustrative embodiment of the present invention are capable of being distributed as a program product in a variety of forms, and that an illustrative embodiment of the present invention applies equally regardless of the particular type of signal bearing media used to actually carry out the distribution. </paragraph>
<paragraph id="P-0024" lvl="0"><number>&lsqb;0024&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 2</cross-reference> is a block diagram of an exemplary processor <highlight><bold>10</bold></highlight> that is utilized for processing information according to a preferred embodiment of the present invention. Processor <highlight><bold>10</bold></highlight> may be located within data processing system <highlight><bold>100</bold></highlight> as depicted in <cross-reference target="DRAWINGS">FIG. 1</cross-reference>. In the depicted embodiment, processor <highlight><bold>10</bold></highlight> comprises a single integrated circuit superscalar microprocessor. Accordingly, as discussed further below, processor <highlight><bold>10</bold></highlight> includes various execution units, registers, buffers, memories, and other functional units, which are all formed by integrated circuitry. In a preferred embodiment of the present invention, processor <highlight><bold>10</bold></highlight> comprises one of the PowerPC&trade; line of microprocessors, which operates according to reduced instruction set computing (RISC) techniques. </paragraph>
<paragraph id="P-0025" lvl="0"><number>&lsqb;0025&rsqb;</number> As depicted in <cross-reference target="DRAWINGS">FIG. 1</cross-reference>, processor <highlight><bold>10</bold></highlight> is coupled to system bus <highlight><bold>120</bold></highlight> via a bus interface unit (BIU) <highlight><bold>12</bold></highlight> within processor <highlight><bold>10</bold></highlight>. BIU <highlight><bold>12</bold></highlight> controls the transfer of information between processor <highlight><bold>10</bold></highlight> and other devices coupled to system bus <highlight><bold>120</bold></highlight> such as a main memory (not illustrated). Processor <highlight><bold>10</bold></highlight>, system bus <highlight><bold>120</bold></highlight>, and the other devices coupled to system bus <highlight><bold>120</bold></highlight> together form a data processing system. </paragraph>
<paragraph id="P-0026" lvl="0"><number>&lsqb;0026&rsqb;</number> BIU <highlight><bold>12</bold></highlight> is connected to instruction cache <highlight><bold>14</bold></highlight> and data cache <highlight><bold>16</bold></highlight> within processor <highlight><bold>10</bold></highlight>. High speech caches, such as instruction cache <highlight><bold>14</bold></highlight> and data cache <highlight><bold>16</bold></highlight>, enable processor <highlight><bold>10</bold></highlight> to achieve relatively fast access time to a subset of data or instructions previously transferred from main memory to instruction cache <highlight><bold>14</bold></highlight> and data cache <highlight><bold>16</bold></highlight>, thus improving the speed of operation of the data processing system. Instruction cache <highlight><bold>14</bold></highlight> is further coupled to sequential fetcher <highlight><bold>17</bold></highlight>, which fetches instructions from instruction cache <highlight><bold>14</bold></highlight> during each cycle for execution. Sequential fetcher <highlight><bold>17</bold></highlight> stores sequential instructions within instruction queue <highlight><bold>19</bold></highlight> for execution by other execution circuitry within processor <highlight><bold>10</bold></highlight>. Branch instructions are also transmitted to a branch processing unit (BPU) <highlight><bold>18</bold></highlight> for execution. BPU <highlight><bold>18</bold></highlight> is a branch prediction and fetch redirection mechanism. </paragraph>
<paragraph id="P-0027" lvl="0"><number>&lsqb;0027&rsqb;</number> In the depicted embodiment, in addition to BPU <highlight><bold>18</bold></highlight>, the execution circuitry of processor <highlight><bold>10</bold></highlight> comprises multiple execution units, including fixed-point unit (FXU) <highlight><bold>22</bold></highlight>, load/store unit (LSU) <highlight><bold>28</bold></highlight>, and floating-point unit (FPU) <highlight><bold>30</bold></highlight>. As is well known by those skilled in the art, each of execution units FXU <highlight><bold>22</bold></highlight>, LSU <highlight><bold>28</bold></highlight>, and FPU <highlight><bold>30</bold></highlight> executes one or more instructions within a particular class of sequential instructions during each processor cycle. For example, FXU <highlight><bold>22</bold></highlight> performs fixed-point mathematical operations such as addition, subtraction, ANDing, ORing, and XORing utilizing source operands received from specified general purpose registers (GPRs) <highlight><bold>32</bold></highlight>. Following the execution of a fixed point instruction, FXU <highlight><bold>22</bold></highlight> outputs the data results of the instruction to GPR rename buffers <highlight><bold>33</bold></highlight>, which provide temporary storage for the result data until the instruction is completed by transferring the result data from GPR rename buffers <highlight><bold>33</bold></highlight> to one or more of GPRs <highlight><bold>32</bold></highlight>. Conversely, FPU <highlight><bold>30</bold></highlight> performs floating-point operations, such as floating-point multiplication and division, on source operands received from floating-point registers FPRs <highlight><bold>36</bold></highlight>. FPU <highlight><bold>30</bold></highlight> outputs data resulting from the execution of floating-point instructions to selected FPR rename buffers <highlight><bold>37</bold></highlight>, which temporarily store the result data until the instructions are completed by transferring the result data from FPR rename buffers <highlight><bold>37</bold></highlight> to selected FPRs <highlight><bold>36</bold></highlight>. As its name implies, LSU <highlight><bold>28</bold></highlight> executes floating-point and fixed-point instructions which either load data from memory (i.e., either data cache <highlight><bold>16</bold></highlight>, a lower level cache, or main memory) into selected GPRs <highlight><bold>32</bold></highlight> or FPRs <highlight><bold>36</bold></highlight> or which store data from a selected GPRs <highlight><bold>32</bold></highlight> or FPRs <highlight><bold>36</bold></highlight> to memory. Completion unit <highlight><bold>40</bold></highlight> informs IFU <highlight><bold>17</bold></highlight> when execution of a particular instruction or operation is completed. </paragraph>
<paragraph id="P-0028" lvl="0"><number>&lsqb;0028&rsqb;</number> Processor <highlight><bold>10</bold></highlight> employs both pipelining and out-of-order execution of instructions to further improve the performance of its superscalar architecture. Accordingly, instructions can by executed by FXU <highlight><bold>22</bold></highlight>, LSU <highlight><bold>28</bold></highlight>, and FPU <highlight><bold>30</bold></highlight> in any order as long as data dependencies are observed. In addition, instructions are processed by each of FXU <highlight><bold>22</bold></highlight>, LSU <highlight><bold>28</bold></highlight> and FPU <highlight><bold>30</bold></highlight> at a sequence of pipeline stages. As is typical of high performance processors, each instruction is processed at five distinct pipeline stages, namely, fetch, decode/dispatch, execute, finish and completion. </paragraph>
<paragraph id="P-0029" lvl="0"><number>&lsqb;0029&rsqb;</number> During the fetch stage, sequential fetcher <highlight><bold>17</bold></highlight> retrieves one or more instructions associated with one or more memory addresses from instruction cache <highlight><bold>14</bold></highlight>. Sequential instructions fetched from instruction cache <highlight><bold>14</bold></highlight> are stored by sequential fetcher <highlight><bold>17</bold></highlight> within registers such as instruction queue <highlight><bold>19</bold></highlight>. Additionally, sequential fetcher <highlight><bold>17</bold></highlight> also forwards branch instructions from within the instruction stream to BPU <highlight><bold>18</bold></highlight> for execution. </paragraph>
<paragraph id="P-0030" lvl="0"><number>&lsqb;0030&rsqb;</number> BPU <highlight><bold>18</bold></highlight> includes a branch prediction mechanism (hardware), which in one embodiment comprises a dynamic prediction mechanism such as a branch history table, that enables BPU <highlight><bold>18</bold></highlight> to speculatively execute unresolved conditional branch instruction s by predicting whether the path will be taken. Alternatively, in other embodiments of the present invention, a static, compiler-based prediction mechanism is implemented. </paragraph>
<paragraph id="P-0031" lvl="0"><number>&lsqb;0031&rsqb;</number> According to one embodiment, a bifurcated instruction pre-fetch unit is also provided within processor <highlight><bold>10</bold></highlight>. The instruction pre-fetch unit comprises pre-fetch request engine <highlight><bold>50</bold></highlight> and pre-fetch address/data buffer <highlight><bold>51</bold></highlight>. As illustrated in <cross-reference target="DRAWINGS">FIG. 2</cross-reference>, pre-fetch request engine <highlight><bold>50</bold></highlight> is coupled between I-Cache <highlight><bold>14</bold></highlight> and BIU <highlight><bold>12</bold></highlight> via request <highlight><bold>5</bold></highlight> interconnect, while pre-fetch address/data buffer <highlight><bold>51</bold></highlight> is coupled between I-Cache <highlight><bold>14</bold></highlight> and BIU <highlight><bold>12</bold></highlight> via addr/data return interconnect. The pre-fetch request engine <highlight><bold>50</bold></highlight> receives signals (triggers), which assist in the determination of when to switch processing from one thread to another. These signals/triggers include: a thread switch signal, an I-Cache miss, a D-cache miss, a decoded signal for software pre-fetch, etc. Counters are maintained in the pre-fetch request engine <highlight><bold>50</bold></highlight> and utilized as described below in the description of <cross-reference target="DRAWINGS">FIGS. 3A and 3B</cross-reference>. In an alternate embodiment, component parts of the instruction pre-fetch unit exists within the instruction fetch unit <highlight><bold>17</bold></highlight>. Likewise, other configurations may be possible, utilizing similarly featured components within a processor to achieve the same functional results and it is contemplated that all such configurations fall within the scope of the present invention. </paragraph>
<paragraph id="P-0032" lvl="0"><number>&lsqb;0032&rsqb;</number> The present invention provides a pre-fetching mechanism for handling instruction fetch addresses that are predicted to potentially result in a miss at the instruction cache. The instruction pre-fetch mechanism helps the instruction fetcher <highlight><bold>17</bold></highlight> to request instructions from memory before the instruction fetch mechanism misses in the instruction cache. When the instruction fetch addresses changes the path of fetching, the instruction pre-fetch adapts quickly. The result is that by the time the instruction fetch address misses in the cache, the instructions are likely to be found in the next level of buffers or caches and the fetch does not have to be conducted at the memory thus reducing latency of the operation and improving processor efficiency. </paragraph>
<paragraph id="P-0033" lvl="0"><number>&lsqb;0033&rsqb;</number> Implementation of the invention involves the utilization of additional logic (i.e., the hardware and software enabled structures) to monitor instruction fetching and instruction execution characteristics and direct the pre-fetching scheme of the invention. <cross-reference target="DRAWINGS">FIGS. 3A, 3B</cross-reference> illustrate high level representations of the pre-fetching mechanism including logic components utilized and interconnections between the logic components. </paragraph>
<paragraph id="P-0034" lvl="0"><number>&lsqb;0034&rsqb;</number> Instruction pre-fetch unit (PFU) <highlight><bold>300</bold></highlight> is located between the instruction fetcher <highlight><bold>17</bold></highlight> and a lower level cache (such as an L2 or L3 cache). The primary components include pre-fetch predictor logic, two counters (or counting mechanisms) <highlight><bold>317</bold></highlight>A, <highlight><bold>317</bold></highlight>B, and a pre-fetch buffer (PFB) <highlight><bold>315</bold></highlight>. Other logic components, such as MUXes, etc. are utilized to couple these components along with other components of processor to complete the functional features provided by the invention. </paragraph>
<paragraph id="P-0035" lvl="0"><number>&lsqb;0035&rsqb;</number> Pre-fetch request address generator <highlight><bold>303</bold></highlight> is a pre-fetch predictor that tracks the instruction fetcher <highlight><bold>17</bold></highlight> and requests instructions from lower level cache (or memory) before the instruction fetcher <highlight><bold>17</bold></highlight> may request them. Generation of the pre-fetch request address involves utilization of an address translation mechanism <highlight><bold>302</bold></highlight> and cache look up logic <highlight><bold>304</bold></highlight> coupled to instruction fetch address register (IFAR) <highlight><bold>301</bold></highlight>. Pre-fetch request trigger <highlight><bold>305</bold></highlight> initiates the pre-fetch request address generator <highlight><bold>303</bold></highlight> when a trigger event occurs. Thus, generation of the pre-fetch request occurs in response to the pre-fetch request trigger <highlight><bold>305</bold></highlight> in obtaining an input identified with one or more of the following events: an I-Cache miss on the current thread, a D-Cache miss on the current thread, a software directed pre-fetch, a value of accuracy for the branch prediction, and a thread change mechanism for the instruction fetcher <highlight><bold>17</bold></highlight>. The pre-fetch request trigger <highlight><bold>305</bold></highlight> initiates the issuance of the pre-fetch address by pre-fetch request address generator <highlight><bold>303</bold></highlight> and also provides a thread ID <highlight><bold>309</bold></highlight> to accompany the request. The thread ID <highlight><bold>309</bold></highlight> is provided to the pre-fetch request trigger <highlight><bold>305</bold></highlight> from the thread counter <highlight><bold>317</bold></highlight>B described below. </paragraph>
<paragraph id="P-0036" lvl="0"><number>&lsqb;0036&rsqb;</number> A memory request counter <highlight><bold>317</bold></highlight>A is associated with the trigger mechanism <highlight><bold>305</bold></highlight> and is utilized to count the consecutive memory requests on a particular thread. Thus, the memory request counter <highlight><bold>317</bold></highlight>A tracks the number of threads in the last N (e.g., one thousand) cycles (or memory requests). The counter is reset whenever the context changes. During execution, when the counter <highlight><bold>317</bold></highlight>A reaches a preset threshold value, the instruction fetch is classified as being in the single-threaded mode. </paragraph>
<paragraph id="P-0037" lvl="0"><number>&lsqb;0037&rsqb;</number> In the preferred embodiment, a second counter, thread counter <highlight><bold>317</bold></highlight>B, is utilized to track the number of instructions executed within each thread. In one embodiment, thread counter <highlight><bold>317</bold></highlight>B is an array of counters, with one counter dedicated to each thread, and which counts the number of instructions executed within that specific thread. Collectively, both counters <highlight><bold>317</bold></highlight>A, <highlight><bold>317</bold></highlight>B provide an indication to the processor whether to devote the resources within the IFU mainly for single thread execution or to share them among multiple threads indicated by the thread counter <highlight><bold>317</bold></highlight>B. Both counters <highlight><bold>317</bold></highlight>A, <highlight><bold>317</bold></highlight>B are reset when a new I-Cache miss request is sent to memory. </paragraph>
<paragraph id="P-0038" lvl="0"><number>&lsqb;0038&rsqb;</number> The memory request address selection is performed utilizing the trigger mechanism <highlight><bold>305</bold></highlight> and the counters <highlight><bold>317</bold></highlight>A, <highlight><bold>317</bold></highlight>B described above to detect single or multithreaded execution. This address selection process allows the pre-fetching to begin from when a change of context (i.e., a change of a thread) is expected to occur during an instruction fetch. Notably, for block multithreading, the instruction fetch can occur from only one thread, and this simplifies the fetch operation with respect to fine grain multithreading. </paragraph>
<paragraph id="P-0039" lvl="0"><number>&lsqb;0039&rsqb;</number> Once a request is sent to the lower level cache (or memory), the next probable request address for that thread is generated and stored for later use. These pre-fetch requests help to prime the memory stages such that possible I-Cache misses for these addresses may find the cache line already on its way from lower level cache or memory towards pre-fetch buffer (PFB) <highlight><bold>351</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 3B</cross-reference>. According to the preferred embodiment, a table of pre-fetched real addresses <highlight><bold>315</bold></highlight> is maintained as illustrated in <cross-reference target="DRAWINGS">FIG. 3A</cross-reference>. The table entries are accessed by reference to the associated thread identifier (ID). The architecture thus includes a pre-fetch buffer <highlight><bold>351</bold></highlight> in which the address tag, data, valid bits, and thread-id are maintained. Updates to the pre-fetch buffer <highlight><bold>351</bold></highlight> are completed from received information that includes: the active threads in use by the instruction fetch mechanism; the output of the counters, which indicates single-threaded or multithreaded execution; the thread-id for the data that is returned from the memory system; the address tags for the returning data; and the address tags and valid bits for the pre-fetch buffer entry. </paragraph>
<paragraph id="P-0040" lvl="0"><number>&lsqb;0040&rsqb;</number> Referring now to <cross-reference target="DRAWINGS">FIG. 3</cross-reference>B, the PFB <highlight><bold>351</bold></highlight> (same as pre-fetch address/data buffer <highlight><bold>51</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 2</cross-reference>) and supporting logic structures are provided in greater detail. PFB <highlight><bold>351</bold></highlight> comprises multiple row entries with each entry having a thread ID, valid indicators, cache line real address, and cache line data. When a cache line returns, the lower level cache updates a row of the PFB <highlight><bold>351</bold></highlight> with this information as shown in <cross-reference target="DRAWINGS">FIG. 3B</cross-reference>. </paragraph>
<paragraph id="P-0041" lvl="0"><number>&lsqb;0041&rsqb;</number> When instruction fetch address register (IFAR) <highlight><bold>301</bold></highlight> is waiting on this cache line, i.e., an I-Cache miss address <highlight><bold>311</bold></highlight> matches the returned cache line address, the instructions are immediately forwarded to the processor core (execution units). The cache line also updates the PFB <highlight><bold>351</bold></highlight> and I-Cache <highlight><bold>14</bold></highlight>. In the absence of an immediate I-Cache miss, the data is stored in the PFB <highlight><bold>351</bold></highlight>. The decision on whether to replace an entry in the PFB <highlight><bold>351</bold></highlight> and which entry to replace by the incoming cache line depends on the thread-id for the incoming line, the associativity of PFB <highlight><bold>351</bold></highlight>, and the importance of the line in the PFB <highlight><bold>351</bold></highlight> (i.e., whether the PFB entry is yet to be fully written to I-Cache <highlight><bold>14</bold></highlight>). </paragraph>
<paragraph id="P-0042" lvl="0"><number>&lsqb;0042&rsqb;</number> Thus, on a cache miss, the instructions are supplied by the PFB <highlight><bold>351</bold></highlight> if the cache line exists within the PFB <highlight><bold>351</bold></highlight>. If the IFAR accesses the cache line, the cache line is considered important enough to be written to the I-Cache. Finally, the buffer allocation for threads is governed by the mode determination for single or multithreaded execution <highlight><bold>305</bold></highlight> in <cross-reference target="DRAWINGS">FIG. 3</cross-reference>A, and the details are discussed below in <cross-reference target="DRAWINGS">FIG. 4A</cross-reference>. </paragraph>
<paragraph id="P-0043" lvl="0"><number>&lsqb;0043&rsqb;</number> The fine grain multithreading approach of the invention allows a simultaneous instruction fetch and issue from multiple threads. Simultaneous Multithreading (SMT) is one example of fine grain multithreading. The current invention is applicable to fine grain multithreading as well as block multithreading. </paragraph>
<paragraph id="P-0044" lvl="0"><number>&lsqb;0044&rsqb;</number> According to the preferred embodiment, memory request address generation is performed for an active thread for which the instruction fetch occurs and for one or more threads which are likely to be selected for execution (i.e., these selected threads are currently allocated some of the processor resources such as instruction and/or data caches, registers, buffers, instructions queues, branch arrays, etc.). </paragraph>
<paragraph id="P-0045" lvl="0"><number>&lsqb;0045&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 4A</cross-reference> illustrates the process of pre-fetching instructions of a multithreaded program according to the invention. The process begins at block <highlight><bold>401</bold></highlight> and thereafter proceeds to block <highlight><bold>403</bold></highlight> which illustrates activation of the trigger mechanism to send a pre-fetch request to the memory system (i.e., the next level caches and/or the memory). A determination is made at block <highlight><bold>405</bold></highlight> whether the counters indicate that the execution is primarily a single-threaded one. If the counters indicate that the execution is primarily a single threaded one, a next determination is made at block <highlight><bold>407</bold></highlight> whether the branch prediction accuracy is reasonably high. Following, if the branch prediction accuracy is not reasonably high, a final determination is made as illustrated at block <highlight><bold>409</bold></highlight> whether the software directed pre-fetch asks for the same thread ID. When any one of the above determination steps provides a positive result, a pre-fetch request is sent unless it is first filtered out due to the presence of the line in the I-Cache. </paragraph>
<paragraph id="P-0046" lvl="0"><number>&lsqb;0046&rsqb;</number> According to the preferred embodiment, and as illustrated in block <highlight><bold>411</bold></highlight>, the pre-fetch request is sent for an address for the same thread as that is being currently executed if any of the above three determinations results in an affirmative response. Otherwise, the pre-fetching mechanism assumes that a change of context (i.e., thread) may occur in the instruction fetch, and accordingly, begins a pre-fetch from a different thread as indicated in block <highlight><bold>413</bold></highlight>. </paragraph>
<paragraph id="P-0047" lvl="0"><number>&lsqb;0047&rsqb;</number> The change of context/thread for instruction pre-fetching is initiated if the triggering mechanism indicates so. One major trigger to cause the triggering mechanism to indicate a change is when the counters indicate a multithreaded execution. The pre-fetching from a different thread is also initiated when on-core caches are missed. Similarly, when the branch prediction is not of high accuracy, or the target address is likely to be far (such as occurs with a new function call), a change of thread occurs for the pre-fetch. </paragraph>
<paragraph id="P-0048" lvl="0"><number>&lsqb;0048&rsqb;</number> According to the preferred embodiment, the new thread, for which the pre-fetch request is issued, already has some processor resources allocated to it. The new thread is selected from among the available threads based on age and relevance. Additionally, selection of the new thread is influenced by the indication received from the thread counter about the number of threads that are allocated to processor resources at a given time. </paragraph>
<paragraph id="P-0049" lvl="0"><number>&lsqb;0049&rsqb;</number> The change of context/thread for pre-fetching is also closely linked to the change of context/thread for the instruction fetch so that whenever the instruction fetch changes the course of execution to a new thread, the pre-fetching mechanism adapts right away. Returning now to <cross-reference target="DRAWINGS">FIG. 4</cross-reference>A, once the new thread is selected, the various processor and other resources are updated to support the new thread as shown at block <highlight><bold>415</bold></highlight>. The pre-fetch address table <highlight><bold>315</bold></highlight> is updated as indicated in block <highlight><bold>416</bold></highlight> and then the process ends as indicated at block <highlight><bold>417</bold></highlight>. </paragraph>
<paragraph id="P-0050" lvl="0"><number>&lsqb;0050&rsqb;</number> The invention provides an address generation for a memory request (I-Cache miss and pre-fetch). With an I-Cache miss, i.e., when an instruction fetch address (IFAR) misses the instruction cache, the translated real address is sent to the memory as the request address for the demanded cache line. The tag to the memory system along with this address includes the thread-identifier, part of the effective page address from the IFAR, and other information such as whether the request is cache-inhibited. </paragraph>
<paragraph id="P-0051" lvl="0"><number>&lsqb;0051&rsqb;</number> For a pre-fetch request address, when a pre-fetch request is sent following a demand request (or on a cache miss), the tag for the pre-fetch request is left identical to that of the preceding demand request, with the exception of the real address of the requested cache line. The real address of the pre-fetch request is computed as follows. First, when next sequential address (NSA) pre-fetch algorithm is adopted, the real address is incremented by 1 after sending a demand request or a pre-fetch request for that thread-id (the number of outstanding pre-fetch requests issued after a demand request is implementation dependent and can be software-controlled). Second, the real address from the table of pre-fetch real addresses is chosen based on the thread ID. The thread ID is selected at the time of the change of the context/thread for instruction pre-fetching. </paragraph>
<paragraph id="P-0052" lvl="0"><number>&lsqb;0052&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 4B</cross-reference> illustrates the process of handling data that is returned from the memory system. The process begins at block <highlight><bold>451</bold></highlight> and then proceeds to block <highlight><bold>453</bold></highlight> where the memory system returns the data (cache line) along with the thread ID. A determination is made at block <highlight><bold>455</bold></highlight> whether there is a thread waiting on the cache line. When a particular thread is waiting on the cache line, the data is forwarded for that thread and is noted in the pre-fetch buffer for subsequent write to the cache as illustrated at block <highlight><bold>457</bold></highlight>. If, however, no thread is waiting on the data returning from the memory system, the pre-fetch buffer is updated as indicated at block <highlight><bold>459</bold></highlight>. Then the process ends as shown at block <highlight><bold>463</bold></highlight> </paragraph>
<paragraph id="P-0053" lvl="0"><number>&lsqb;0053&rsqb;</number> The pre-fetch table is hashed as per the thread-id. For each thread at least one pre-fetch request address is maintained. In the preferred embodiment, the pre-fetch request address is computed immediately when the last demand or pre-fetch request for that thread is sent to the memory system. The old entries are replaced by the new entries when a write occurs. </paragraph>
<paragraph id="P-0054" lvl="0"><number>&lsqb;0054&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 4C</cross-reference> illustrates the process of updating the pre-fetch buffer. The cacheable data sent by the memory system updates the pre-fetch buffer directory. The process begins at block <highlight><bold>471</bold></highlight> and then proceeds to block <highlight><bold>473</bold></highlight> where data is received from memory. A determination is made at block <highlight><bold>475</bold></highlight> whether a thread is waiting for that data (i.e., there is an outstanding demand data for that thread). When there is a thread waiting for the data, a pre-fetch buffer entry is targeted (typically a direct-mapped) for replacement with the newly arriving cache line as shown at block <highlight><bold>477</bold></highlight>. If there is no thread waiting, however, then a next determination is made as shown at block <highlight><bold>479</bold></highlight> whether the incoming cache line is partially present in the pre-fetch buffer. If the incoming cache line is partially present in the pre-fetch buffer, then the pre-fetch buffer entry is updated as shown at block <highlight><bold>481</bold></highlight>. Otherwise, the cache line is updated in the pre-fetch buffer directory if the target entry does not hold a demand cache line (i.e. a miss in I-Cache) as shown at block <highlight><bold>483</bold></highlight>. Then the process ends as indicated at block <highlight><bold>485</bold></highlight>. </paragraph>
<paragraph id="P-0055" lvl="0"><number>&lsqb;0055&rsqb;</number> For incoming cache line to be compared against a demand request as well as against a pre-fetch buffer entry, the real address and thread ID&apos;s are utilized. Moreover, for the pre-fetch buffer entry comparison, an additional comparison with effective address tags (for cache write address purpose) is necessary. </paragraph>
<paragraph id="P-0056" lvl="0"><number>&lsqb;0056&rsqb;</number> The invention takes advantage of the simplicity of data flow based multithreading in deciding when to initiate pre-fetch request, and does not require an on-chip runtime performance measurement scheme as other proposed schemes. The invention provides several advantages including: (1) the features of the invention meshes well with both single-threaded program execution as well as multithreaded program executions; (2) implementation of the invention does not require extensive hardware unlike other mechanisms that are dependent on measurements of memory access latencies to insert pre-fetch requests; (3) in a conventional multithreaded program execution, a change of context for the instruction fetch would occur before the pre-fetching from the new context. The invention goes a step further and initiates pre-fetches that speculate a change of context. </paragraph>
<paragraph id="P-0057" lvl="0"><number>&lsqb;0057&rsqb;</number> Although the invention has been described with reference to specific embodiments, this description is not meant to be construed in a limiting sense. Various modifications of the disclosed embodiment, as well as alternative embodiments of the invention, will become apparent to persons skilled in the art upon reference to the description of the invention. It is therefore contemplated that such modifications can be made without departing from the spirit or scope of the present invention as defined in the appended claims. </paragraph>
</section>
</detailed-description>
</subdoc-description>
<subdoc-claims>
<heading lvl="1">What is claimed is: </heading>
<claim id="CLM-00001">
<claim-text><highlight><bold>1</bold></highlight>. A method of pre-fetching instructions of a multithreaded program executing in a processor of a data processing system, said method comprising: 
<claim-text>determining when an instruction within a particular thread from among multiple threads executing on said processor may be pre-fetched; and </claim-text>
<claim-text>pre-fetching said instruction from a lower level memory structure, wherein said instruction is tagged with an identifier of said particular thread; and </claim-text>
<claim-text>providing said instruction to an executing unit of said processor when said instruction is required during execution of said particular thread. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00002">
<claim-text><highlight><bold>2</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, further comprising generating a pre-fetch request for said instruction, wherein said pre-fetch request includes a thread identifier (ID); and 
<claim-text>linking said thread ID to said instruction when said instruction is returned from said lower level memory structure, whereby said instruction is identified as belonging to said particular thread. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00003">
<claim-text><highlight><bold>3</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00002">claim 2</dependent-claim-reference>, further comprising initiating said generating step in response to a receipt of at least one trigger from among (1) an I-Cache miss on a current thread, (2) a D-Cache miss on a current thread, (3) a software directed pre-fetch, (4) a value of accuracy for the branch prediction, and (5) a thread change mechanism for an instruction fetcher of said processor. </claim-text>
</claim>
<claim id="CLM-00004">
<claim-text><highlight><bold>4</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00003">claim 3</dependent-claim-reference>, wherein said processor comprises an instruction pre-fetch buffer, said method further comprising storing said instruction in said instruction pre-fetch buffer. </claim-text>
</claim>
<claim id="CLM-00005">
<claim-text><highlight><bold>5</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00004">claim 4</dependent-claim-reference>, further comprising: 
<claim-text>determining whether execution of a thread is waiting on a return of said instruction; and </claim-text>
<claim-text>when execution of a thread is waiting on a return of said instruction; </claim-text>
<claim-text>immediately forwarding said instruction to said thread execution; and </claim-text>
<claim-text>marking said pre-fetch buffer for subsequent write to a higher level memory. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00006">
<claim-text><highlight><bold>6</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00005">claim 5</dependent-claim-reference>, further comprising updating said pre-fetch buffer only when no thread is determined to be waiting on said return of said instruction. </claim-text>
</claim>
<claim id="CLM-00007">
<claim-text><highlight><bold>7</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00006">claim 6</dependent-claim-reference>, further comprising, concurrently with said immediately forwarding step, targeting an entry of said pre-fetch buffer for replacement with said newly arriving instruction. </claim-text>
</claim>
<claim id="CLM-00008">
<claim-text><highlight><bold>8</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00006">claim 6</dependent-claim-reference>, further comprising: 
<claim-text>determining whether an incoming instruction is partially present in said pre-fetch buffer; and </claim-text>
<claim-text>responsive to said instruction being partially present in said pre-fetch buffer, updating said pre-fetch buffer with said instruction. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00009">
<claim-text><highlight><bold>9</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, further comprising: 
<claim-text>tagging said pre-fetch request with a tag that is identical to a tag of a demand request issued prior to said pre-fetch request; and </claim-text>
<claim-text>calculating a real address of said pre-fetch request by incrementing said real address by 1 and selecting said address from said pre-fetch buffer according to said thread identifier. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00010">
<claim-text><highlight><bold>10</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00009">claim 9</dependent-claim-reference>, further comprising selecting a different thread identifier when a change of a context for instruction pre-fetching is made. </claim-text>
</claim>
<claim id="CLM-00011">
<claim-text><highlight><bold>11</bold></highlight>. A data processing system having at least a processor and a memory connected via an interconnect, wherein said processor includes: 
<claim-text>an instruction fetcher; </claim-text>
<claim-text>an instruction cache; </claim-text>
<claim-text>a plurality of execution units; </claim-text>
<claim-text>a plurality of buses providing interconnection amongst said instruction fetcher, said instruction cache and said plurality of execution units, and to a lower level memory structure; and </claim-text>
<claim-text>logic components pre-fetching instructions from a multithreaded application, wherein instructions from a particular thread among said multithreaded application is pre-fetched and provided during execution of said particular thread by said plurality of execution units. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00012">
<claim-text><highlight><bold>12</bold></highlight>. The data processing system of <dependent-claim-reference depends_on="CLM-00011">claim 11</dependent-claim-reference>, wherein said logic components comprises: 
<claim-text>an instruction pre-fetch trigger that is activated in response to a receipt of at least one input signal from among an I-Cache miss on a current thread, a D-Cache miss on a current thread, a software directed pre-fetch, a value of accuracy for the branch prediction, and a thread change mechanism for an instruction fetcher of said processor, and which initiates creation of each pre-fetch request. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00013">
<claim-text><highlight><bold>13</bold></highlight>. The data processing system of <dependent-claim-reference depends_on="CLM-00011">claim 12</dependent-claim-reference>, further comprising wherein said mechanism includes a pair of counters associated with said instruction pre-fetch trigger, said pair of counters comprising a first counter that tracks a number of consecutive memory request on a particular thread and a second counting component that tracks a number of threads accessed during a pre-determined number of cycles. </claim-text>
</claim>
<claim id="CLM-00014">
<claim-text><highlight><bold>14</bold></highlight>. The data processing system of <dependent-claim-reference depends_on="CLM-00011">claim 13</dependent-claim-reference>, wherein said second counting component is an array having a plurality of counters that each tracks a number of instructions executed within a particular one of said number of threads. </claim-text>
</claim>
<claim id="CLM-00015">
<claim-text><highlight><bold>15</bold></highlight>. The data processing system of <dependent-claim-reference depends_on="CLM-00011">claim 13</dependent-claim-reference>, wherein said logic components further comprises a pre-fetch request generator that provides an address and thread identifier of said pre-fetch request responsive to an activation of said instruction pre-fetch trigger. </claim-text>
</claim>
<claim id="CLM-00016">
<claim-text><highlight><bold>16</bold></highlight>. The data processing system of <dependent-claim-reference depends_on="CLM-00011">claim 15</dependent-claim-reference>, wherein said processor further comprises a pre-fetch buffer in which an instruction returned by said pre-fetch request is stored until required by its associated thread. </claim-text>
</claim>
<claim id="CLM-00017">
<claim-text><highlight><bold>17</bold></highlight>. The data processing system of <dependent-claim-reference depends_on="CLM-00011">claim 16</dependent-claim-reference>, wherein said processor further comprises logic for determining when to place said instruction in said pre-fetch buffer and when to forward said instruction to a thread which is waiting on a return of said instruction. </claim-text>
</claim>
<claim id="CLM-00018">
<claim-text><highlight><bold>18</bold></highlight>. The data processing system of <dependent-claim-reference depends_on="CLM-00011">claim 17</dependent-claim-reference>, wherein said logic compares an address of a previous I-Cache miss with an address of said instruction and immediately forwards said instruction to said execution units when said addresses match.</claim-text>
</claim>
</subdoc-claims>
<subdoc-drawings id="DRAWINGS">
<heading lvl="0" align="CENTER">Drawings</heading>
<representative-figure>2</representative-figure>
<figure id="figure-D00000">
<image id="EMI-D00000" file="US20030004683A1-20030102-D00000.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00001">
<image id="EMI-D00001" file="US20030004683A1-20030102-D00001.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00002">
<image id="EMI-D00002" file="US20030004683A1-20030102-D00002.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00003">
<image id="EMI-D00003" file="US20030004683A1-20030102-D00003.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00004">
<image id="EMI-D00004" file="US20030004683A1-20030102-D00004.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00005">
<image id="EMI-D00005" file="US20030004683A1-20030102-D00005.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00006">
<image id="EMI-D00006" file="US20030004683A1-20030102-D00006.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00007">
<image id="EMI-D00007" file="US20030004683A1-20030102-D00007.TIF" imf="TIFF" ti="DR"/>
</figure>
</subdoc-drawings>
</patent-application-publication>
