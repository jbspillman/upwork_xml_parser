<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE patent-application-publication SYSTEM "pap-v16-2002-01-01.dtd" [
<!ENTITY US20030005358A1-20030102-D00000.TIF SYSTEM "US20030005358A1-20030102-D00000.TIF" NDATA TIF>
<!ENTITY US20030005358A1-20030102-D00001.TIF SYSTEM "US20030005358A1-20030102-D00001.TIF" NDATA TIF>
<!ENTITY US20030005358A1-20030102-D00002.TIF SYSTEM "US20030005358A1-20030102-D00002.TIF" NDATA TIF>
<!ENTITY US20030005358A1-20030102-D00003.TIF SYSTEM "US20030005358A1-20030102-D00003.TIF" NDATA TIF>
<!ENTITY US20030005358A1-20030102-D00004.TIF SYSTEM "US20030005358A1-20030102-D00004.TIF" NDATA TIF>
]>
<patent-application-publication>
<subdoc-bibliographic-information>
<document-id>
<doc-number>20030005358</doc-number>
<kind-code>A1</kind-code>
<document-date>20030102</document-date>
</document-id>
<publication-filing-type>new</publication-filing-type>
<domestic-filing-data>
<application-number>
<doc-number>09895231</doc-number>
</application-number>
<application-number-series-code>09</application-number-series-code>
<filing-date>20010629</filing-date>
</domestic-filing-data>
<technical-information>
<classification-ipc>
<classification-ipc-primary>
<ipc>H02H003/05</ipc>
</classification-ipc-primary>
<classification-ipc-edition>07</classification-ipc-edition>
</classification-ipc>
<classification-us>
<classification-us-primary>
<uspc>
<class>714</class>
<subclass>020000</subclass>
</uspc>
</classification-us-primary>
</classification-us>
<title-of-invention>Decentralized, self-regulating system for automatically discovering optimal configurations in a failure-rich environment</title-of-invention>
</technical-information>
<inventors>
<first-named-inventor>
<name>
<given-name>David</given-name>
<middle-name>Michael</middle-name>
<family-name>Koelle</family-name>
</name>
<residence>
<residence-us>
<city>Marlborough</city>
<state>MA</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</first-named-inventor>
<inventor>
<name>
<given-name>Brian</given-name>
<middle-name>James</middle-name>
<family-name>Tarbox</family-name>
</name>
<residence>
<residence-us>
<city>Littleton</city>
<state>MA</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
</inventors>
<assignee>
<organization-name>International Business Machines Corporation</organization-name>
<address>
<city>Armonk</city>
<state>NY</state>
<country>
<country-code>US</country-code>
</country>
</address>
<assignee-type>03</assignee-type>
</assignee>
<correspondence-address>
<name-1>Duke W. Yee</name-1>
<name-2>Carstens, Yee &amp; Cahoon, LLP</name-2>
<address>
<address-1>P.O. Box 802334</address-1>
<city>Dallas</city>
<state>TX</state>
<postalcode>75380</postalcode>
<country>
<country-code>US</country-code>
</country>
</address>
</correspondence-address>
</subdoc-bibliographic-information>
<subdoc-abstract>
<paragraph id="A-0001" lvl="0">A method, system and computer program for managing a set of data by a distributed set of services is provided. The set of data is organized into a plurality of related sets of data. Management of the related set of data is assigned, by a set of services, to a service within the distributed set of services based on an optimization criteria. Responsive to a failed service within the distributed set of services, management of the related set of data managed by the failed service is transferred to another service within the distributed set of services. </paragraph>
</subdoc-abstract>
<subdoc-description>
<summary-of-invention>
<section>
<heading lvl="1">BACKGROUND OF THE INVENTION </heading>
<paragraph id="P-0001" lvl="0"><number>&lsqb;0001&rsqb;</number> 1. Technical Field </paragraph>
<paragraph id="P-0002" lvl="0"><number>&lsqb;0002&rsqb;</number> The present invention relates to an improved data processing system. More particularly, the present invention relates to providing a decentralized, self-regulating system for automatically discovering optimal configurations in a failure rich environment. </paragraph>
<paragraph id="P-0003" lvl="0"><number>&lsqb;0003&rsqb;</number> 2. Description of Related Art </paragraph>
<paragraph id="P-0004" lvl="0"><number>&lsqb;0004&rsqb;</number> Faults are inevitable in digital computer systems due to such things as the complexity of the circuits and the associated electromechanical devices. To permit system operation, even after the occurrence of a fault, the art has developed a number of fault-tolerant designs. Improved fault-tolerant digital data processing systems include redundant functional units, e.g., duplicate CPUs, memories, and peripheral controllers interconnected along a common system bus. Each of a pair of functional units responds identically to input received from the bus. In the outputs, if a pair of functional units do not agree, that pair of units is taken off-line, and another pair of functional units (a &ldquo;spare&rdquo;) continues to function in its place. </paragraph>
<paragraph id="P-0005" lvl="0"><number>&lsqb;0005&rsqb;</number> This problem exists in any system in which services, which have the potential to fail, are responsible for maintaining items that must always be available. Prior art solutions include redundant systems, in which multiple services are kept and if one fails, others can back up the failed service. In addition, prior art solutions such as a central managing service maintains other services and can redistribute workload as needed during optimization or failure. </paragraph>
<paragraph id="P-0006" lvl="0"><number>&lsqb;0006&rsqb;</number> However, other systems that can manage fail-over and maintain optimal configurations must do so with a central authority. There are two problems associated with existing methods. The first problem is that the use of a central authority requires duplication of knowledge such as knowing which services are managing which data. Time and resources must be spent keeping this knowledge up-to-date and resources must be spent for storing this knowledge. The second problem is that the central authority itself can fail. If such a failure does occur, then no fail-over resolution or optimum configuration can occur. </paragraph>
<paragraph id="P-0007" lvl="0"><number>&lsqb;0007&rsqb;</number> Therefore, it would be advantageous to provide a mechanism for maintaining data on individual services which ensure that knowledge about which services manage which data is always current and accurate and ensures that changes to the system happen quickly and in parallel. </paragraph>
</section>
<section>
<heading lvl="1">SUMMARY OF THE INVENTION </heading>
<paragraph id="P-0008" lvl="0"><number>&lsqb;0008&rsqb;</number> The present invention provides a method, system and computer program for managing a set of data by a distributed set of services. The set of data is organized into a plurality of related sets of data. Management of the related set of data is assigned, by a set of services, to a service within the distributed set of services based on an optimization criteria. Responsive to a failed service within the distributed set of services, management of the related set of data managed by the failed service is transferred to another service within the distributed set of services. </paragraph>
</section>
</summary-of-invention>
<brief-description-of-drawings>
<section>
<heading lvl="1">BRIEF DESCRIPTION OF THE DRAWINGS </heading>
<paragraph id="P-0009" lvl="0"><number>&lsqb;0009&rsqb;</number> The novel features believed characteristic of the invention are set forth in the appended claims. The invention itself, however, as well as a preferred mode of use, further objectives and advantages thereof, will best be understood by reference to the following detailed description of an illustrative embodiment when read in conjunction with the accompanying drawings, wherein: </paragraph>
<paragraph id="P-0010" lvl="0"><number>&lsqb;0010&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 1</cross-reference> is a pictorial representation of a distributed data processing system in which the present invention may be implemented; </paragraph>
<paragraph id="P-0011" lvl="0"><number>&lsqb;0011&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 2</cross-reference> is a block diagram depicting a data processing system that may be implemented as a server in accordance with a preferred embodiment of the present invention; </paragraph>
<paragraph id="P-0012" lvl="0"><number>&lsqb;0012&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 3</cross-reference> is a block diagram illustrating a data processing system in which the present invention may be implemented; </paragraph>
<paragraph id="P-0013" lvl="0"><number>&lsqb;0013&rsqb;</number> <cross-reference target="DRAWINGS">FIGS. 4A and 4B</cross-reference> are exemplary illustrations of services managing sets of data containing a sensing mechanism for detecting the failure of a service in accordance with a preferred embodiment of the present invention; </paragraph>
<paragraph id="P-0014" lvl="0"><number>&lsqb;0014&rsqb;</number> <cross-reference target="DRAWINGS">FIGS. 5A and 5B</cross-reference> are exemplary illustrations of services managing sets of data containing a sensing mechanism for handling sets of data more optimally when an additional service comes online in accordance with a preferred embodiment of the present invention; </paragraph>
<paragraph id="P-0015" lvl="0"><number>&lsqb;0015&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 6</cross-reference> is an exemplary flowchart illustrating maintenance of resiliency in a failure-rich environment in accordance with a preferred embodiment of the present invention; and </paragraph>
<paragraph id="P-0016" lvl="0"><number>&lsqb;0016&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 7</cross-reference> is an exemplary flowchart illustrating a determination of an optimal configuration in accordance with a preferred embodiment of the present invention. </paragraph>
</section>
</brief-description-of-drawings>
<detailed-description>
<section>
<heading lvl="1">DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENT </heading>
<paragraph id="P-0017" lvl="0"><number>&lsqb;0017&rsqb;</number> With reference now to the figures, <cross-reference target="DRAWINGS">FIG. 1</cross-reference> is a pictorial representation of a distributed data processing system in which the present invention may be implemented. Distributed data processing system <highlight><bold>100</bold></highlight> is a network of computers in which the present invention may be implemented. Distributed data processing system <highlight><bold>100</bold></highlight> contains a network <highlight><bold>102</bold></highlight>, which is the medium used to provide communications links between various devices and computers connected together within distributed data processing system <highlight><bold>100</bold></highlight>. Network <highlight><bold>102</bold></highlight> may include permanent connections, such as wire or fiber optic cables, or temporary connections made through telephone connections. </paragraph>
<paragraph id="P-0018" lvl="0"><number>&lsqb;0018&rsqb;</number> In the depicted example, a server <highlight><bold>104</bold></highlight> is connected to network <highlight><bold>102</bold></highlight> along with storage unit <highlight><bold>106</bold></highlight>. In addition, clients <highlight><bold>108</bold></highlight>, <highlight><bold>110</bold></highlight> and <highlight><bold>112</bold></highlight> also are connected to network <highlight><bold>102</bold></highlight>. These clients <highlight><bold>108</bold></highlight>, <highlight><bold>110</bold></highlight> and <highlight><bold>112</bold></highlight> may be, for example, personal computers or network computers. For purposes of this application, a network computer is any computer coupled to a network, which receives a program or other application from another computer coupled to the network. In the depicted example, server <highlight><bold>104</bold></highlight> provides data, such as boot files, operating system images, and applications to clients <highlight><bold>108</bold></highlight>, <highlight><bold>110</bold></highlight> and <highlight><bold>112</bold></highlight>. Clients <highlight><bold>108</bold></highlight>, <highlight><bold>110</bold></highlight> and <highlight><bold>112</bold></highlight> are clients to server <highlight><bold>104</bold></highlight>. Distributed data processing system <highlight><bold>100</bold></highlight> may include additional servers, clients, and other devices not shown. </paragraph>
<paragraph id="P-0019" lvl="0"><number>&lsqb;0019&rsqb;</number> In the depicted example, distributed data processing system <highlight><bold>100</bold></highlight> is the Internet, with network <highlight><bold>102</bold></highlight> representing a worldwide collection of networks and gateways that use the TCP/IP suite of protocols to communicate with one another. At the heart of the Internet is a backbone of high-speed data communication lines between major nodes or host computers, consisting of thousands of commercial, government, education, and other computer systems that route data and messages. Of course, distributed data processing system <highlight><bold>100</bold></highlight> also may be implemented as a number of different types of networks, such as, for example, an intranet, a local area network (LAN), or a wide area network (WAN). <cross-reference target="DRAWINGS">FIG. 1</cross-reference> is intended as an example and not as an architectural limitation for the present invention. </paragraph>
<paragraph id="P-0020" lvl="0"><number>&lsqb;0020&rsqb;</number> Data processing system <highlight><bold>200</bold></highlight> may be a symmetric multiprocessor (SMP) system including a plurality of processors <highlight><bold>202</bold></highlight> and <highlight><bold>204</bold></highlight> connected to system bus <highlight><bold>206</bold></highlight>. Alternatively, a single processor system may be employed. Also connected to system bus <highlight><bold>206</bold></highlight> is memory controller/cache <highlight><bold>208</bold></highlight>, which provides an interface to local memory <highlight><bold>209</bold></highlight>. I/O bus bridge <highlight><bold>210</bold></highlight> is connected to system bus <highlight><bold>206</bold></highlight> and provides an interface to I/O bus <highlight><bold>212</bold></highlight>. Memory controller/cache <highlight><bold>208</bold></highlight> and I/O bus bridge <highlight><bold>210</bold></highlight> may be integrated as depicted. </paragraph>
<paragraph id="P-0021" lvl="0"><number>&lsqb;0021&rsqb;</number> Peripheral component interconnect (PCI) bus bridge <highlight><bold>214</bold></highlight> connected to I/O bus <highlight><bold>212</bold></highlight> provides an interface to PCI local bus <highlight><bold>216</bold></highlight>. A number of modems <highlight><bold>218</bold></highlight> and <highlight><bold>220</bold></highlight> may be connected to PCI bus <highlight><bold>216</bold></highlight>. Typical PCI bus implementations will support four PCI expansion slots or add-in connectors. Communications links to network computers <highlight><bold>108</bold></highlight>-<highlight><bold>112</bold></highlight> in <cross-reference target="DRAWINGS">FIG. 1</cross-reference> may be provided through modem <highlight><bold>218</bold></highlight> and network adapter <highlight><bold>220</bold></highlight> connected to PCI local bus <highlight><bold>216</bold></highlight> through add-in boards. </paragraph>
<paragraph id="P-0022" lvl="0"><number>&lsqb;0022&rsqb;</number> Additional PCI bus bridges <highlight><bold>222</bold></highlight> and <highlight><bold>224</bold></highlight> provide interfaces for additional PCI buses <highlight><bold>226</bold></highlight> and <highlight><bold>228</bold></highlight>, from which additional modems or network adapters may be supported. In this manner, server <highlight><bold>200</bold></highlight> allows connections to multiple network computers. A memory mapped graphics adapter <highlight><bold>230</bold></highlight> and hard disk <highlight><bold>232</bold></highlight> may also be connected to I/O bus <highlight><bold>212</bold></highlight> as depicted, either directly or indirectly. </paragraph>
<paragraph id="P-0023" lvl="0"><number>&lsqb;0023&rsqb;</number> Those of ordinary skill in the art will appreciate that the hardware depicted in <cross-reference target="DRAWINGS">FIG. 2</cross-reference> may vary. For example, other peripheral devices, such as optical disk drive and the like also may be used in addition or in place of the hardware depicted. The depicted example is not meant to imply architectural limitations with respect to the present invention. The data processing system depicted in <cross-reference target="DRAWINGS">FIG. 2</cross-reference> may be, for example, an IBM eServer pSeries, a product of International Business Machines Corporation in Armonk, N.Y., running the Advanced Interactive Executive (AIX) operating system. </paragraph>
<paragraph id="P-0024" lvl="0"><number>&lsqb;0024&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 3</cross-reference> is a block diagram illustrating a data processing system in which the present invention may be implemented. Data processing system <highlight><bold>300</bold></highlight> is an example of a client computer, such as, for example, client computers <highlight><bold>108</bold></highlight>, <highlight><bold>110</bold></highlight> and <highlight><bold>112</bold></highlight> in <cross-reference target="DRAWINGS">FIG. 1</cross-reference>. Data processing system <highlight><bold>300</bold></highlight> employs a peripheral component interconnect (PCI) local bus architecture. Although the depicted example employs a PCI bus, other bus architectures, such as Micro Channel and ISA, may be used. Processor <highlight><bold>302</bold></highlight> and main memory <highlight><bold>304</bold></highlight> are connected to PCI local bus <highlight><bold>306</bold></highlight> through PCI bridge <highlight><bold>308</bold></highlight>. PCI bridge <highlight><bold>308</bold></highlight> also may include an integrated memory controller and cache memory for processor <highlight><bold>302</bold></highlight>. Additional connections to PCI local bus <highlight><bold>306</bold></highlight> may be made through direct component interconnection or through add-in boards. In the depicted example, local area network (LAN) adapter <highlight><bold>310</bold></highlight>, SCSI host bus adapter <highlight><bold>312</bold></highlight>, and expansion bus interface <highlight><bold>314</bold></highlight> are connected to PCI local bus <highlight><bold>306</bold></highlight> by direct component connection. In contrast, audio adapter <highlight><bold>316</bold></highlight>, graphics adapter <highlight><bold>318</bold></highlight>, and audio/video adapter <highlight><bold>319</bold></highlight> are connected to PCI local bus <highlight><bold>306</bold></highlight> by add-in boards inserted into expansion slots. Expansion bus interface <highlight><bold>314</bold></highlight> provides a connection for a keyboard and mouse adapter <highlight><bold>320</bold></highlight>, modem <highlight><bold>322</bold></highlight>, and additional memory <highlight><bold>324</bold></highlight>. SCSI host bus adapter <highlight><bold>312</bold></highlight> provides a connection for hard disk drive <highlight><bold>326</bold></highlight>, tape drive <highlight><bold>328</bold></highlight>, and CD-ROM drive <highlight><bold>330</bold></highlight>. Typical PCI local bus implementations support three or four PCI expansion slots or add-in connectors. </paragraph>
<paragraph id="P-0025" lvl="0"><number>&lsqb;0025&rsqb;</number> An operating system runs on processor <highlight><bold>302</bold></highlight> and is used to coordinate and provide control of various components within data processing system <highlight><bold>300</bold></highlight> in <cross-reference target="DRAWINGS">FIG. 3</cross-reference>. The operating system may be a commercially available operating system such as a UNIX based operating system, AIX for instance, which is available from International Business Machines Corporation. &ldquo;AIX&rdquo; is a trademark of International Business Machines Corporation. An object oriented programming system, such as Java, may run in conjunction with the operating system and provide calls to the operating system from Java programs or applications executing on data processing system <highlight><bold>300</bold></highlight>. &ldquo;Java&rdquo; is a trademark of Sun Microsystems, Inc. Instructions for the operating system, the object-oriented operating system, and applications or programs are located on storage devices, such as hard disk drive <highlight><bold>326</bold></highlight>, and may be loaded into main memory <highlight><bold>304</bold></highlight> for execution by processor <highlight><bold>302</bold></highlight>. </paragraph>
<paragraph id="P-0026" lvl="0"><number>&lsqb;0026&rsqb;</number> Those of ordinary skill in the art will appreciate that the hardware in <cross-reference target="DRAWINGS">FIG. 3</cross-reference> may vary depending on the implementation. Other internal hardware or peripheral devices, such as flash ROM (or equivalent nonvolatile memory) or optical disk drives and the like, may be used in addition to or in place of the hardware depicted in <cross-reference target="DRAWINGS">FIG. 3</cross-reference>. Also, the processes of the present invention may be applied to a multiprocessor data processing system. </paragraph>
<paragraph id="P-0027" lvl="0"><number>&lsqb;0027&rsqb;</number> For example, data processing system <highlight><bold>300</bold></highlight>, if optionally configured as a network computer, may not include SCSI host bus adapter <highlight><bold>312</bold></highlight>, hard disk drive <highlight><bold>326</bold></highlight>, tape drive <highlight><bold>328</bold></highlight>, and CD-ROM <highlight><bold>330</bold></highlight>, as noted by dotted line <highlight><bold>332</bold></highlight> in <cross-reference target="DRAWINGS">FIG. 3</cross-reference>, denoting optional inclusion. In that case, the computer, to be properly called a client computer, must include some type of network communication interface, such as LAN adapter <highlight><bold>310</bold></highlight>, modem <highlight><bold>322</bold></highlight>, or the like. As another example, data processing system <highlight><bold>300</bold></highlight> may be a stand-alone system configured to be bootable without relying on some type of network communication interface, whether or not data processing system <highlight><bold>300</bold></highlight> comprises some type of network communication interface. As a further example, data processing system <highlight><bold>300</bold></highlight> may be a Personal Digital Assistant (PDA) device which is configured with ROM and/or flash ROM in order to provide nonvolatile memory for storing operating system files and/or user-generated data. The depicted example in <cross-reference target="DRAWINGS">FIG. 3</cross-reference>, as well as above-described examples, are not meant to imply architectural limitations. For example, the processes of the present invention may be applied to a multiprocessor data processing system. </paragraph>
<paragraph id="P-0028" lvl="0"><number>&lsqb;0028&rsqb;</number> The present invention provides a mechanism for individual services working together in a type of &ldquo;swarm&rdquo; arrangement. If one service fails, the other services within the swarm may pick up the failed service&apos;s workload. By using this type of mechanism, redundancy, which is expensive and requires a multiple of every service within a system, is not required. In prior art systems, redundant systems maintain secondary systems so that if one system fails, a secondary system may back it up. However, in the present invention, other services within the swarm are already monitoring each service within the swarm, and if one of the monitoring services happens to notice that data is not being properly managed, another service will pick up the data. </paragraph>
<paragraph id="P-0029" lvl="0"><number>&lsqb;0029&rsqb;</number> The difference between the present invention and prior art systems is that in prior art systems a backup service is sitting idle. In contrast, the present invention does not need a backup service, but uses components within the system that are operating in a normal fashion, and if required to do so, these components may take on extra load if a need arises. The individual services working together also means that each individual service within the swarm knows how to optimize and deal with a failure and the service does not need to rely on a central managing mechanism. </paragraph>
<paragraph id="P-0030" lvl="0"><number>&lsqb;0030&rsqb;</number> As an example of a specific implementation, the present invention may use, for example, smart sets. Smart sets are a process which allows for grouping and managing of systems that have similar characteristics. Examples of smart sets may be operating system types and the like. The smart set architecture may be designed to be employed with, for example, a swarm of Table Cache Services (TCSs) which manage smart set groups in the TCS architecture. TCSs are a main part of a SmartSet system, the part of the SmartSet system which performs an operation. A TCS is a program that runs on any machine and performs an operation without user intervention. The TCS also responds to requests for data. The Table Cache part of the TCSs describes a specific part of a system that allows the system to store SmartSet results, which may be in a table form, and to send out the stored results when requested. Each TCS may be responsible for an arbitrary number of smart sets. </paragraph>
<paragraph id="P-0031" lvl="0"><number>&lsqb;0031&rsqb;</number> In general, TCSs may be most effective when they maintain smart sets which belong to a TCS&apos;s geopolitical location, which may be indicated by using, for example, a database and a table name. A geopolitical location is an area separated by other areas by virtue of physical forms, such as, for example, mountains, oceans, governmental boundaries, and the like. In this disclosure, a geopolitical location is used to indicate a common way in which databases are managed, for example, a &ldquo;Texas&rdquo; database, an &ldquo;East Coast&rdquo; database, and the like. By using a geopolitical location, this allows for the differentiation between, for example, routers in a first location as opposed to routers in a second location. A location corresponds to a database that serves a local network. For example, &ldquo;Building 4&rdquo; may be a location if &ldquo;Building 4&rdquo; is served by its own database. </paragraph>
<paragraph id="P-0032" lvl="0"><number>&lsqb;0032&rsqb;</number> When a new TCS comes online, the new TCS may check which existing TCSs own which existing smart sets. If the new TCS finds a SmartSet that should be, for example, in its own geopolitical location, the new TCS will ask the existing TCS owning the SmartSet for control of the SmartSet. This process allows the TCSs to maintain an optimal configuration of smart sets without the need for a centralized authority which would then dictate to each TCS which smart sets they should own. </paragraph>
<paragraph id="P-0033" lvl="0"><number>&lsqb;0033&rsqb;</number> Furthermore, if a TCS fails, the smart sets belonging to the failed TCS are released and the remaining TCSs pick up orphan smart sets which belonged to the failed TCS. If there are no TCSs specific for a SmartSet&apos;s geopolitical location, for example, then any of the remaining TCSs may pick up the orphaned SmartSet. If a more optimal TCS appears online at a later time, the new TCS may ask for the smart sets the new TCS should own and thereby providing an optimal system in addition to protection against failures within the system. In this way, the orphaned smart sets may continue to be managed. This process allows for a highly resilient behavior in a potential failure-rich environment. </paragraph>
<paragraph id="P-0034" lvl="0"><number>&lsqb;0034&rsqb;</number> The above example provides only an illustration of a specific use of the present invention. The present invention is not limited to using Table Cache Services and smart sets. The present invention may utilize any type of service and any type of data or data set. The present invention may also utilize any optimization criteria. For example, location need not be the only determining factor when selecting optimal services for the data set. Other factors may also be considered. For example, an optimal service may be one that has access to specific software tools or manipulative capability. In addition, an optimal service may be one that has a higher level of security for sensitive data or that is programmed to save volatile data files more often. Furthermore, an optimal service may be capable of more calculations per unit of time for computing complex mathematical functions. In addition, an optimal service may be used for load balancing data between services. </paragraph>
<paragraph id="P-0035" lvl="0"><number>&lsqb;0035&rsqb;</number> <cross-reference target="DRAWINGS">FIGS. 4A and 4B</cross-reference> are exemplary illustrations of services managing sets of data containing a sensing mechanism for detecting the failure of a service in accordance with a preferred embodiment of the present invention. Services often have attached data and the service may manage the attached data. A service is an application program that performs some task. Data may be organized into a set of data within a plurality of related sets of data. However, prior to the present invention, if a service fails, the data may become unmanaged and is subsequently unavailable for use. The present invention solves this problem. </paragraph>
<paragraph id="P-0036" lvl="0"><number>&lsqb;0036&rsqb;</number> In <cross-reference target="DRAWINGS">FIG. 4</cross-reference>A, service <highlight><bold>402</bold></highlight> contains attached data sets <highlight><bold>406</bold></highlight> and <highlight><bold>408</bold></highlight>. Likewise, service <highlight><bold>404</bold></highlight> contains attached data sets <highlight><bold>410</bold></highlight> and <highlight><bold>412</bold></highlight>. Services <highlight><bold>402</bold></highlight> and <highlight><bold>404</bold></highlight> may be located in different locations. Data sets <highlight><bold>406</bold></highlight> and <highlight><bold>408</bold></highlight> may or may not be in a similar location as service <highlight><bold>402</bold></highlight>. Similarly, data sets <highlight><bold>410</bold></highlight> and <highlight><bold>412</bold></highlight> may or may not be in a similar location as service <highlight><bold>404</bold></highlight>. </paragraph>
<paragraph id="P-0037" lvl="0"><number>&lsqb;0037&rsqb;</number> In <cross-reference target="DRAWINGS">FIG. 4</cross-reference>B, as indicated, service <highlight><bold>404</bold></highlight> has failed. In such a situation, data sets <highlight><bold>410</bold></highlight> and <highlight><bold>412</bold></highlight> are in an unmanaged state and may be unavailable for use. However, service <highlight><bold>402</bold></highlight> detects that service <highlight><bold>404</bold></highlight> has failed and service <highlight><bold>402</bold></highlight> detects the fact that data sets <highlight><bold>410</bold></highlight> and <highlight><bold>412</bold></highlight> are in an unmanaged state. Service <highlight><bold>402</bold></highlight> then reaches out and attaches itself to data sets <highlight><bold>410</bold></highlight> and <highlight><bold>412</bold></highlight> and takes over the management of data sets <highlight><bold>410</bold></highlight> and <highlight><bold>412</bold></highlight>. Service <highlight><bold>402</bold></highlight> may reach out and attach itself to data sets <highlight><bold>410</bold></highlight> and <highlight><bold>412</bold></highlight> by using a database instruction service <highlight><bold>402</bold></highlight> to connect to data sets <highlight><bold>410</bold></highlight> and <highlight><bold>412</bold></highlight>. With this process, even though service <highlight><bold>404</bold></highlight> has failed leaving data sets <highlight><bold>410</bold></highlight> and <highlight><bold>412</bold></highlight> unmanaged, service <highlight><bold>402</bold></highlight> senses this failure and attaches to unmanaged data sets <highlight><bold>410</bold></highlight> and <highlight><bold>412</bold></highlight>. By attaching to data sets <highlight><bold>410</bold></highlight> and <highlight><bold>412</bold></highlight>, service <highlight><bold>402</bold></highlight> takes over the management of data sets <highlight><bold>410</bold></highlight> and <highlight><bold>412</bold></highlight>. <cross-reference target="DRAWINGS">FIGS. 4A and 4B</cross-reference> are a simple illustration of the present invention and many more services may be used to manage many more data sets within the spirit and scope of the present invention than that illustrated in <cross-reference target="DRAWINGS">FIGS. 4A and 4B</cross-reference>. </paragraph>
<paragraph id="P-0038" lvl="0"><number>&lsqb;0038&rsqb;</number> <cross-reference target="DRAWINGS">FIGS. 5A and 5B</cross-reference> are exemplary illustrations of services managing sets of data containing a sensing mechanism for handling sets of data more optimally when an additional service comes online in accordance with a preferred embodiment of the present invention. Services within a service group allocate data between the services based on many factors. However, as stated above, a service is most efficient when the service is responsible for a data set that is included in the service&apos;s location. This effectiveness stems from the fact that transferring data within a location may not be subject to, for example, extended distances between a service and the data set, network latencies, network blockages, network outages and time consuming network routing that is often encountered in transferring data across large distances. However, prior to the present invention, if a new service joined the group of services, data within the group may not be arranged such that each service is responsible for data sets in the service&apos;s location. The present invention also solves this problem. </paragraph>
<paragraph id="P-0039" lvl="0"><number>&lsqb;0039&rsqb;</number> In <cross-reference target="DRAWINGS">FIG. 5</cross-reference>A, service <highlight><bold>502</bold></highlight> contains data sets <highlight><bold>506</bold></highlight>, <highlight><bold>508</bold></highlight> <highlight><bold>510</bold></highlight> and <highlight><bold>512</bold></highlight>. All of data sets <highlight><bold>506</bold></highlight>, <highlight><bold>508</bold></highlight>, <highlight><bold>510</bold></highlight> and <highlight><bold>512</bold></highlight> may be in the same location as service <highlight><bold>502</bold></highlight>. However, one or more of data sets <highlight><bold>506</bold></highlight>, <highlight><bold>508</bold></highlight>, <highlight><bold>510</bold></highlight> and <highlight><bold>512</bold></highlight> may not be in the same location as service <highlight><bold>502</bold></highlight>. When one or more of data sets <highlight><bold>506</bold></highlight>, <highlight><bold>508</bold></highlight>, <highlight><bold>510</bold></highlight> and <highlight><bold>512</bold></highlight> are not in the same location as service <highlight><bold>502</bold></highlight>, the configuration of data sets <highlight><bold>506</bold></highlight>, <highlight><bold>508</bold></highlight>, <highlight><bold>510</bold></highlight> and <highlight><bold>512</bold></highlight> is not optimized. Optimal configuration occurs when services manage data sets that are in the same location or database. Consequences of less than optimized configurations may include, for example, delays and latency as well as the need for data retransmission if data is distorted between a source and a destination. When a new service, such as service <highlight><bold>504</bold></highlight>, joins a group of services, then each data set <highlight><bold>506</bold></highlight>, <highlight><bold>508</bold></highlight>, <highlight><bold>510</bold></highlight> and <highlight><bold>512</bold></highlight> may be examined to determine the location of data sets <highlight><bold>506</bold></highlight>, <highlight><bold>508</bold></highlight>, <highlight><bold>510</bold></highlight> and <highlight><bold>512</bold></highlight>. </paragraph>
<paragraph id="P-0040" lvl="0"><number>&lsqb;0040&rsqb;</number> In <cross-reference target="DRAWINGS">FIG. 5</cross-reference>B, if new service <highlight><bold>504</bold></highlight> is in the same location as one or more of data sets <highlight><bold>506</bold></highlight>, <highlight><bold>508</bold></highlight>, <highlight><bold>510</bold></highlight> and <highlight><bold>512</bold></highlight>, then service <highlight><bold>504</bold></highlight> will request that service <highlight><bold>502</bold></highlight> release control of one or more data sets <highlight><bold>506</bold></highlight>, <highlight><bold>508</bold></highlight>, <highlight><bold>510</bold></highlight> and <highlight><bold>512</bold></highlight>. In this example, data sets <highlight><bold>510</bold></highlight>, and <highlight><bold>512</bold></highlight> are in the same location as service <highlight><bold>504</bold></highlight> but not service <highlight><bold>502</bold></highlight>. When released by service <highlight><bold>502</bold></highlight>, data sets <highlight><bold>510</bold></highlight> and <highlight><bold>512</bold></highlight> will be attached to service <highlight><bold>504</bold></highlight> thereby providing optimal configuration of data sets <highlight><bold>506</bold></highlight>, <highlight><bold>508</bold></highlight>, <highlight><bold>510</bold></highlight> and <highlight><bold>512</bold></highlight>. Attaching to a service may be as simple as the service indicating a desire to connect to a data set by using a database instruction. </paragraph>
<paragraph id="P-0041" lvl="0"><number>&lsqb;0041&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 6</cross-reference> is an exemplary flowchart illustrating maintenance of resiliency in a failure-rich environment in accordance with a preferred embodiment of the present invention. In this example, the operation begins by a determination as to whether or not a service has failed (step <highlight><bold>602</bold></highlight>). If a service has not failed (step <highlight><bold>602</bold></highlight>:NO), the operation terminates. At this point, it is important to note, there may be no determination that a failed service is part of a service swarm. The service swarm has no explicit membership. The services detect unmanaged data and begin to manage the data, and when managing the data, the services may exhibit a group behavior and work together. If a service has failed (step <highlight><bold>602</bold></highlight>:YES), then a determination is made as to whether or not the failed service owns data sets (step <highlight><bold>604</bold></highlight>). A data set contains a variable indicating the data set&apos;s owner. If this variable is nonexistent in the data set or is set to null, the data set is unmanaged. If the failed service does not own data sets (step <highlight><bold>604</bold></highlight>:NO), the operation terminates. </paragraph>
<paragraph id="P-0042" lvl="0"><number>&lsqb;0042&rsqb;</number> Otherwise, if the failed service does own data sets (step <highlight><bold>604</bold></highlight>:YES), then the remaining services in the service swarm detect that a service within the service swarm has failed (step <highlight><bold>606</bold></highlight>). Detection of a failure of a service within a service swarm may be made by use of a central directory service. Both the services and data sets have entries in the central directory service. Changes within this directory are monitored by other services, so when a change occurs in this directory, the other services can recognize these changes in the directory and thereby determine that a failure has occurred. </paragraph>
<paragraph id="P-0043" lvl="0"><number>&lsqb;0043&rsqb;</number> The remaining services in the service swarm then examine the data sets owned by the failed service which are classified as orphaned data sets (step <highlight><bold>608</bold></highlight>). The remaining services in the service swarm examine the orphaned data sets by looking at a variable associated with the data sets which indicates the owner of the data set. </paragraph>
<paragraph id="P-0044" lvl="0"><number>&lsqb;0044&rsqb;</number> Then a determination is made as to whether or not any remaining services in the service swarm are in the same location as any of the orphaned data sets (step <highlight><bold>610</bold></highlight>). The determination that services are in the same location as the data sets is performed by means of a variable associated with the service that indicates what location or database the service serves. Data sets also contain a variable indicating to which location the data set belongs. If the variables from a service and a data set match, then the service and the data set are in the same location. In such a case, an optimal configuration may be achieved when the service with the same location variable as the data set are connected. </paragraph>
<paragraph id="P-0045" lvl="0"><number>&lsqb;0045&rsqb;</number> If any of the remaining services in the service swarm are in the same location as any of the orphaned data sets (step <highlight><bold>610</bold></highlight>:YES), the service in the same location as any of the orphaned data sets attaches to these data sets by using a database instruction (step <highlight><bold>612</bold></highlight>) and thereafter the operation terminates. </paragraph>
<paragraph id="P-0046" lvl="0"><number>&lsqb;0046&rsqb;</number> If any of the remaining services in the service swarm are not in the same location as any of the orphaned data sets (step <highlight><bold>610</bold></highlight>:NO), the remaining services in the service swarm attach to the orphaned data sets (step <highlight><bold>614</bold></highlight>). At this point it is important to note that there is often a service in the same location as an orphaned data set, however, the most optimal service will always get a chance to connect to the data set. If a more optimal service realizes that it can improve on a connection, the more optimal service has the authority and ability to demand that a currently connected service break the connection to the data and the more optimal service can then connect itself to the data. </paragraph>
<paragraph id="P-0047" lvl="0"><number>&lsqb;0047&rsqb;</number> As an example, assume there are services containing three databases located in &ldquo;Japan&rdquo;, &ldquo;Texas&rdquo; and &ldquo;California&rdquo;. Each of the three databases is connected in which a transmission latency could be measured. A service in Texas ceases operation, thereby orphaning the Texas data sets. A service in Japan detects this failure of the Texas service and connects to the orphaned Texas data sets. Subsequently, a service in California also detects this failure of the Texas service and determines that the California service can handle the orphaned Texas data sets more optimally than the Japan service since the California service is closer in distance to the Texas service and may experience less latency because of this reduced distance. As the more optimal service out of the two between the Japan service and the California service, the California service asks the Japan service to disconnect from the orphaned Texas data sets, and the Japan service relinquishes control over the orphaned Texas data sets. The California service then connects to the orphaned Texas data sets. Eventually the Texas service returns to an operational mode and realizes that data sets in the Texas location are being managed by the California service. Because of the reduced distance between the Texas service and the Texas data sets, the Texas service can manage these Texas data sets more optimally than the California service. The connection between the California service and the Texas data sets is broken and the Texas service connects to the Texas data sets resulting in a more optimal configuration. The way services may attach to data sets is, for example, by indicating the data set with a database Universal Resource Location (URL), which may include the database name. If a service tries to attach to a data set in a different location than that of the service, the service may specify the location&apos;s database name in the URL. Databases are implemented using a URL. Any table in a database has a URL that refers to that database. </paragraph>
<paragraph id="P-0048" lvl="0"><number>&lsqb;0048&rsqb;</number> Then a determination is made as to whether or not a new service joining the service swarm is in the same location as any of the previously orphaned data sets (step <highlight><bold>616</bold></highlight>). If the new service joining the service swarm is not in the same location as any of the previously orphaned data sets (step <highlight><bold>616</bold></highlight>:NO), the operation terminates. If the new service joining the service swarm is in the same location as any of the previously orphaned data sets (step <highlight><bold>616</bold></highlight>:YES), the service presently attached to the previously orphaned data sets releases those data sets not in the same location and the new service in the same location as the previously orphaned data sets attaches to these data sets (step <highlight><bold>618</bold></highlight>). The service presently attached to the previously orphaned data sets releases the orphaned data sets by setting the data set&apos;s variable indicating ownership to null. The new service in the same location as the data sets then establishes the data set&apos;s variable indicating that the owner of the data sets is itself (step <highlight><bold>620</bold></highlight>) and thereafter the operation terminates. </paragraph>
<paragraph id="P-0049" lvl="0"><number>&lsqb;0049&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 7</cross-reference> is an exemplary flowchart illustrating determination of an optimal configuration in accordance with a preferred embodiment of the present invention. In this example, the operation begins by a determination as to whether or not there is a new service joining the service swarm (step <highlight><bold>702</bold></highlight>). If there is not a new service joining the service swarm (step <highlight><bold>702</bold></highlight>:NO), the operation terminates. If there is a new service joining the service swarm (step <highlight><bold>702</bold></highlight>:YES), the new service examines variables associated with data sets within the service swarm to determine the data sets&apos; location (step <highlight><bold>704</bold></highlight>). The new service examines the data sets&apos; ownership variables to determine which service owns the data set, the data set&apos;s location variable, and the currently connected service&apos;s location variable. Then a determination is made as to whether or not there are any data sets in the service swarm in the same location as that of the new service (step <highlight><bold>706</bold></highlight>). </paragraph>
<paragraph id="P-0050" lvl="0"><number>&lsqb;0050&rsqb;</number> If there are no data sets in the service swarm in the same location as the new service (step <highlight><bold>706</bold></highlight>:NO), the operation terminates. If there are data sets in the service swarm in the same location as that of the new service (step <highlight><bold>706</bold></highlight>:YES), then a determination is made as to whether or not the data sets in the same location as the new service are currently attached to a service in the same location as the data sets (step <highlight><bold>708</bold></highlight>). If the data sets in the same location as the new service are currently attached to a service in the same location as the data sets (step <highlight><bold>708</bold></highlight>:YES), the operation terminates. If the data sets in the same location as the new service are not currently attached to a service in the same location as the data sets (step <highlight><bold>708</bold></highlight>:NO), then the new service asks the existing service to release control of the data sets in the same location as the new service (step <highlight><bold>710</bold></highlight>). </paragraph>
<paragraph id="P-0051" lvl="0"><number>&lsqb;0051&rsqb;</number> Then a determination is made as to whether or not the existing service has released control of the data sets in the same location as the new service (step <highlight><bold>712</bold></highlight>). This determination is made by determining the existing service&apos;s location and the location of the data sets. If these two locations do not match, and the new service is a more optimal service in which to connect to the data sets, the new service will send a message to the existing service demanding the existing service release control of the data sets. When the existing service agrees to release control (disagreement is not an option because the new service has determined an optimal configuration), the existing service&apos;s variable is set to null and the new service connects to the data sets. If the existing service has not released control of the data sets in the same location as the new service (step <highlight><bold>712</bold></highlight>:NO), then operation returns to step <highlight><bold>710</bold></highlight> in which the new service asks the existing service to release control of the data set. The existing service will release control of the data when asked to do so. If the existing service has released control of the data sets in the same location as the new service (step <highlight><bold>712</bold></highlight>:YES), the data sets in the same location as the new service then attach to the new service (step <highlight><bold>714</bold></highlight>) and thereafter the operation terminates. </paragraph>
<paragraph id="P-0052" lvl="0"><number>&lsqb;0052&rsqb;</number> Therefore, the present invention provides mechanism for maintaining data which ensure that knowledge about which services manage which data is always current and accurate and ensures that changes to the system happen quickly and in parallel. No central authority is needed. Each service within a service swarm detects the failure of an associated service within the service swarm. If such a failure occurs, then the remaining services within the service swarm analyze data sets that were attached to the failed service. If any of the data sets are in a similar location as any of the remaining services in the service swarm, then a similarly located service attaches to orphaned data sets. This mechanism provides for resiliency in a failure-rich environment. If any of the data sets are attached to a service not in a similar location as the service to which they are attached, and if a new service joins the service swarm, any data sets in the same location as the new service are released by an existing service and attach to the new service. This mechanism provides for an optimal configuration of data sets. </paragraph>
<paragraph id="P-0053" lvl="0"><number>&lsqb;0053&rsqb;</number> It is important to note that, while the present invention has been described in the context of a fully functioning data processing system, those of ordinary skill in the art will appreciate that the processes of the present invention are capable of being distributed in the form of a computer readable medium of instructions in a variety of forms, and that the present invention applies equally regardless of the particular type of signal bearing media actually used to carry out the distribution. Examples of computer readable media include recordable-type media, such as floppy discs, hard disk drives, RAM, and CD-ROMs and transmission-type media, such as digital and analog communications links. </paragraph>
<paragraph id="P-0054" lvl="0"><number>&lsqb;0054&rsqb;</number> The description of the present invention has been presented for purposes of illustration and description but is not intended to be exhaustive or limited to the invention in the form disclosed. Many modifications and variations will be apparent to those of ordinary skill in the art. For example, the present invention, while describing management of data sets based on location may also perform management of data sets based on a load balancing criteria. For example, if a local data set can be managed more efficiently by a non-local service with excess capacity versus a local service with no excess capacity, the present invention allows management of the data set by the non-local service. Furthermore, the present invention may also allow negotiation between services attempting to manage a data set. The embodiment was chosen and described in order to best explain the principles of the invention and the practical application, and to enable others of ordinary skill in the art to understand the invention for various embodiments with various modifications as are suited to the particular use contemplated. </paragraph>
</section>
</detailed-description>
</subdoc-description>
<subdoc-claims>
<heading lvl="1">What is claimed is: </heading>
<claim id="CLM-00001">
<claim-text><highlight><bold>1</bold></highlight>. A method of managing a set of data by a distributed set of services, comprising the steps of: 
<claim-text>organizing the set of data into a plurality of related sets of data; </claim-text>
<claim-text>assigning, by a set of services, management of a related set of data to a service within the distributed set of services based on an optimization criteria; and </claim-text>
<claim-text>responsive to failure of a service within the distributed set of services, transferring management of the related set of data managed by the failed service to another service within the distributed set of services. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00002">
<claim-text><highlight><bold>2</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein the optimization criteria is based on location of the service within the distributed set of services. </claim-text>
</claim>
<claim id="CLM-00003">
<claim-text><highlight><bold>3</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, further comprising: 
<claim-text>detecting the failed service by a set of remaining services within the distributed set of services; and </claim-text>
<claim-text>examining, by the set of remaining services within the distributed set of services, the related set of data managed by the failed service. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00004">
<claim-text><highlight><bold>4</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00003">claim 3</dependent-claim-reference>, further comprising: 
<claim-text>determining whether data within the related set of data are at the same location as a service within the set of remaining services; and </claim-text>
<claim-text>responsive to data within the related set of data at the same location as a service within the set of remaining services, attaching the data to the service. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00005">
<claim-text><highlight><bold>5</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, further comprising: 
<claim-text>responsive to an additional service joining the distributed set of services, querying management of the data within the related sets of data; and </claim-text>
<claim-text>assigning management of a related set of data to the additional service within the distributed set of services based on the optimization criteria. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00006">
<claim-text><highlight><bold>6</bold></highlight>. A method of managing a set of data by a distributed set of services, comprising the steps of: 
<claim-text>organizing the set of data into a plurality of related sets of data; </claim-text>
<claim-text>assigning, by a set of services, management of a related set of data to a service within the distributed set of services based on an optimization criteria; </claim-text>
<claim-text>responsive to an additional service joining the distributed set of services, querying management of the data within the related sets of data; and </claim-text>
<claim-text>assigning management of a related set of data to the additional service within the distributed set of services based on the optimization criteria. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00007">
<claim-text><highlight><bold>7</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00006">claim 6</dependent-claim-reference>, wherein the optimization criteria is based on location of the service within the distributed set of services. </claim-text>
</claim>
<claim id="CLM-00008">
<claim-text><highlight><bold>8</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00006">claim 6</dependent-claim-reference>, further comprising: 
<claim-text>detecting a failed service in the distributed set of services by a set of remaining services within the distributed set of services; and </claim-text>
<claim-text>examining, by the set of remaining services within the distributed set of services, the related set of data managed by the failed service. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00009">
<claim-text><highlight><bold>9</bold></highlight>. The method as recited in <dependent-claim-reference depends_on="CLM-00008">claim 8</dependent-claim-reference>, further comprising: 
<claim-text>determining whether data within the related set of data are at the same location as a service within the set of remaining services; and </claim-text>
<claim-text>responsive to data within the related set of data at the same location as a service within the set of remaining services, attaching the data to the service. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00010">
<claim-text><highlight><bold>10</bold></highlight>. A data processing system, comprising: 
<claim-text>a system bus; </claim-text>
<claim-text>a memory, including a set of instructions, functionally connected to the system bus; and </claim-text>
<claim-text>a processing unit functionally connected to the system bus, wherein the processing unit executes the set of instructions from the memory to organize a set of data into a plurality of related sets of data, wherein the data in each related set of data has at least one attribute between members, the processing unit assigns, by a set of services, management of a related set of data to a service within the distributed set of services based on an optimization criteria, and, responsive to a failed service within the distributed set of services, the processing unit transfers management of the related set of data managed by the failed service to another service within the distributed set of services. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00011">
<claim-text><highlight><bold>11</bold></highlight>. A data processing system, comprising: 
<claim-text>a system bus; </claim-text>
<claim-text>a memory, including a set of instructions, functionally connected to the system bus; and </claim-text>
<claim-text>a processing unit functionally connected to the system bus, wherein the processing unit executes the set of instructions from the memory to organize a set of data into a plurality of related sets of data, wherein the data in each related set of data has at least one attribute between members, the processing unit assigns, by a set of services, management of a related set of data to a service within the distributed set of services based on an optimization criteria, responsive to an additional service joining the distributed set of services, the processing unit queries management of the data within the related sets of data, and the processing unit assigns management of a related set of data to the additional service within the distributed set of services based on the optimization criteria. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00012">
<claim-text><highlight><bold>12</bold></highlight>. A data processing system for managing a set of data by a distributed set of services, comprising: 
<claim-text>organizing means for organizing the set of data into a plurality of related sets of data, wherein the data in each related set of data has at least one attribute between members; </claim-text>
<claim-text>assigning means for assigning, by a set of services, management of a related set of data to a service within the distributed set of services based on an optimization criteria; and </claim-text>
<claim-text>transferring means, responsive to a failed service within the distributed set of services, for transferring management of the related set of data managed by the failed service to another service within the distributed set of services. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00013">
<claim-text><highlight><bold>13</bold></highlight>. The data processing system as recited in <dependent-claim-reference depends_on="CLM-00011">claim 12</dependent-claim-reference>, wherein the optimization criteria is based on location of the service within the distributed set of services. </claim-text>
</claim>
<claim id="CLM-00014">
<claim-text><highlight><bold>14</bold></highlight>. The data processing system as recited in <dependent-claim-reference depends_on="CLM-00011">claim 12</dependent-claim-reference>, further comprising: 
<claim-text>detecting means for detecting the failed service by a set of remaining services within the distributed set of services; and </claim-text>
<claim-text>examining means for examining, by the set of remaining services within the distributed set of services, the related set of data managed by the failed service. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00015">
<claim-text><highlight><bold>15</bold></highlight>. The data processing system as recited in <dependent-claim-reference depends_on="CLM-00011">claim 14</dependent-claim-reference>, further comprising: 
<claim-text>determining means for determining whether data within the related set of data are at the same location as a service within the set of remaining services; and </claim-text>
<claim-text>attaching means, responsive to data within the related set of data at the same location as a service within the set of remaining services, for attaching the data to the services. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00016">
<claim-text><highlight><bold>16</bold></highlight>. The data processing system as recited in <dependent-claim-reference depends_on="CLM-00011">claim 12</dependent-claim-reference>, further comprising: 
<claim-text>querying means, responsive to an additional service joining the distributed set of services, for querying management of the data within the related sets of data; and </claim-text>
<claim-text>assigning means for assigning management of a related set of data to the additional service within the distributed set of services based on the optimization criteria. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00017">
<claim-text><highlight><bold>17</bold></highlight>. A data processing system for managing a set of data by a distributed set of services, comprising: 
<claim-text>organizing means for organizing the set of data into a plurality of related sets of data; </claim-text>
<claim-text>assigning means for assigning, by a set of services, management of a related set of data to a service within the distributed set of services based on an optimization criteria; </claim-text>
<claim-text>querying means, responsive to an additional service joining the distributed set of services, for querying management of the data within the related sets of data; and </claim-text>
<claim-text>assigning means for assigning management of a related set of data to the additional service within the distributed set of services based on the optimization criteria. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00018">
<claim-text><highlight><bold>18</bold></highlight>. The data processing system as recited in <dependent-claim-reference depends_on="CLM-00011">claim 17</dependent-claim-reference>, wherein the optimization criteria is based on location of the service within the distributed set of services. </claim-text>
</claim>
<claim id="CLM-00019">
<claim-text><highlight><bold>19</bold></highlight>. The data processing system as recited in <dependent-claim-reference depends_on="CLM-00011">claim 17</dependent-claim-reference>, further comprising: 
<claim-text>detecting means for detecting a failed service in the distributed set of services by a set of remaining services within the distributed set of services; and </claim-text>
<claim-text>examining means for examining, by the set of remaining services within the distributed set of services, the related set of data managed by the failed service. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00020">
<claim-text><highlight><bold>20</bold></highlight>. The data processing system as recited in <dependent-claim-reference depends_on="CLM-00011">claim 19</dependent-claim-reference>, further comprising: 
<claim-text>determining means for determining whether data within the related set of data are at the same location as a service within the set of remaining services; and </claim-text>
<claim-text>attaching means, responsive to data within the related set of data at the same location as a service within the set of remaining service, attaching the data to the service. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00021">
<claim-text><highlight><bold>21</bold></highlight>. A computer program product in a computer readable medium for managing a set of data by a distributed set of services, comprising: 
<claim-text>instructions for organizing the set of data into a plurality of related sets of data; </claim-text>
<claim-text>instructions for assigning, by a set of services, management of a related set of data to a service within the distributed set of services based on an optimization criteria; and </claim-text>
<claim-text>instructions, responsive to a failed service within the distributed set of services, for transferring management of the related set of data managed by the failed service to another service within the distributed set of services. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00022">
<claim-text><highlight><bold>22</bold></highlight>. The computer program product as recited in <dependent-claim-reference depends_on="CLM-00022">claim 21</dependent-claim-reference>, wherein the optimization criteria is based on location of the service within the distributed set of services. </claim-text>
</claim>
<claim id="CLM-00023">
<claim-text><highlight><bold>23</bold></highlight>. The computer program product as recited in <dependent-claim-reference depends_on="CLM-00022">claim 21</dependent-claim-reference>, further comprising: 
<claim-text>instructions for detecting the failed service by a set of remaining service within the distributed set of services; and </claim-text>
<claim-text>instructions for examining, by the set of remaining services within the distributed set of services, the related set of data managed by the failed service. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00024">
<claim-text><highlight><bold>24</bold></highlight>. The computer program product as recited in <dependent-claim-reference depends_on="CLM-00022">claim 23</dependent-claim-reference>, further comprising: 
<claim-text>instructions for determining whether data within the related set of data are at the same location as a service within the set of remaining services; and </claim-text>
<claim-text>instructions, responsive to data within the related set of data at the same location as a service within the set of remaining services, for attaching the data to the service. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00025">
<claim-text><highlight><bold>25</bold></highlight>. The computer program product as recited in <dependent-claim-reference depends_on="CLM-00022">claim 21</dependent-claim-reference>, further comprising: 
<claim-text>instructions, responsive to an additional service joining the distributed set of service, for querying management of the data within the related sets of data; and </claim-text>
<claim-text>instructions for assigning management of a related set of data to the additional service within the distributed set of services based on the optimization criteria. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00026">
<claim-text><highlight><bold>26</bold></highlight>. A computer program product in a computer readable medium for managing a set of data by a distributed set of services, comprising the steps of: 
<claim-text>instructions for organizing the set of data into a plurality of related sets of data; </claim-text>
<claim-text>instructions for assigning, by a set of services, management of a related set of data to a service within the distributed set of services based on an optimization criteria; </claim-text>
<claim-text>instructions, responsive to an additional service joining the distributed set of services, for querying management of the data within the related sets of data; and </claim-text>
<claim-text>instructions for assigning management of a related set of data to the additional service within the distributed set of services based on the optimization criteria. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00027">
<claim-text><highlight><bold>27</bold></highlight>. The computer program product as recited in <dependent-claim-reference depends_on="CLM-00022">claim 26</dependent-claim-reference>, wherein the optimization criteria is based on location of the service within the distributed set of services. </claim-text>
</claim>
<claim id="CLM-00028">
<claim-text><highlight><bold>28</bold></highlight>. The computer program product as recited in <dependent-claim-reference depends_on="CLM-00022">claim 26</dependent-claim-reference>, further comprising: 
<claim-text>instructions for detecting a failed service in the distributed set of services by a set of remaining services within the distributed set of services; and </claim-text>
<claim-text>instructions for examining, by the set of remaining services within the distributed set of services, the related set of data managed by the failed service. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00029">
<claim-text><highlight><bold>29</bold></highlight>. The computer program product as recited in <dependent-claim-reference depends_on="CLM-00022">claim 28</dependent-claim-reference>, further comprising: 
<claim-text>instructions for determining whether data within the related set of data are at the same location as a service within the set of remaining services; and </claim-text>
<claim-text>instructions, responsive to data within the related set of data at the same location as a service within the set of remaining services, for attaching the data to the service.</claim-text>
</claim-text>
</claim>
</subdoc-claims>
<subdoc-drawings id="DRAWINGS">
<heading lvl="0" align="CENTER">Drawings</heading>
<representative-figure>1</representative-figure>
<figure id="figure-D00000">
<image id="EMI-D00000" file="US20030005358A1-20030102-D00000.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00001">
<image id="EMI-D00001" file="US20030005358A1-20030102-D00001.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00002">
<image id="EMI-D00002" file="US20030005358A1-20030102-D00002.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00003">
<image id="EMI-D00003" file="US20030005358A1-20030102-D00003.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00004">
<image id="EMI-D00004" file="US20030005358A1-20030102-D00004.TIF" imf="TIFF" ti="DR"/>
</figure>
</subdoc-drawings>
</patent-application-publication>
