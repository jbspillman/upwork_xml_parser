<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE patent-application-publication SYSTEM "pap-v16-2002-01-01.dtd" [
<!ENTITY US20030005232A1-20030102-D00000.TIF SYSTEM "US20030005232A1-20030102-D00000.TIF" NDATA TIF>
<!ENTITY US20030005232A1-20030102-D00001.TIF SYSTEM "US20030005232A1-20030102-D00001.TIF" NDATA TIF>
<!ENTITY US20030005232A1-20030102-D00002.TIF SYSTEM "US20030005232A1-20030102-D00002.TIF" NDATA TIF>
<!ENTITY US20030005232A1-20030102-D00003.TIF SYSTEM "US20030005232A1-20030102-D00003.TIF" NDATA TIF>
<!ENTITY US20030005232A1-20030102-D00004.TIF SYSTEM "US20030005232A1-20030102-D00004.TIF" NDATA TIF>
<!ENTITY US20030005232A1-20030102-D00005.TIF SYSTEM "US20030005232A1-20030102-D00005.TIF" NDATA TIF>
<!ENTITY US20030005232A1-20030102-D00006.TIF SYSTEM "US20030005232A1-20030102-D00006.TIF" NDATA TIF>
<!ENTITY US20030005232A1-20030102-D00007.TIF SYSTEM "US20030005232A1-20030102-D00007.TIF" NDATA TIF>
]>
<patent-application-publication>
<subdoc-bibliographic-information>
<document-id>
<doc-number>20030005232</doc-number>
<kind-code>A1</kind-code>
<document-date>20030102</document-date>
</document-id>
<publication-filing-type>new</publication-filing-type>
<domestic-filing-data>
<application-number>
<doc-number>09895232</doc-number>
</application-number>
<application-number-series-code>09</application-number-series-code>
<filing-date>20010629</filing-date>
</domestic-filing-data>
<technical-information>
<classification-ipc>
<classification-ipc-primary>
<ipc>G06F013/00</ipc>
</classification-ipc-primary>
<classification-ipc-edition>07</classification-ipc-edition>
</classification-ipc>
<classification-us>
<classification-us-primary>
<uspc>
<class>711</class>
<subclass>135000</subclass>
</uspc>
</classification-us-primary>
</classification-us>
<title-of-invention>High performance data processing system via cache victimization protocols</title-of-invention>
</technical-information>
<inventors>
<first-named-inventor>
<name>
<given-name>Guy</given-name>
<middle-name>Lynn</middle-name>
<family-name>Guthrie</family-name>
</name>
<residence>
<residence-us>
<city>Austin</city>
<state>TX</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</first-named-inventor>
<inventor>
<name>
<given-name>Ravi</given-name>
<middle-name>Kumar</middle-name>
<family-name>Arimilli</family-name>
</name>
<residence>
<residence-us>
<city>Austin</city>
<state>TX</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
<inventor>
<name>
<given-name>James</given-name>
<middle-name>Stephen</middle-name>
<family-name>Fields, Jr.</family-name>
</name>
<residence>
<residence-us>
<city>Austin</city>
<state>TX</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
<inventor>
<name>
<given-name>John</given-name>
<middle-name>Steven</middle-name>
<family-name>Dodson</family-name>
</name>
<residence>
<residence-us>
<city>Austin</city>
<state>TX</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
</inventors>
<assignee>
<organization-name>International Business Machines Corp.</organization-name>
<address>
<city>Armonk</city>
<state>NY</state>
<country>
<country-code>US</country-code>
</country>
</address>
<assignee-type>02</assignee-type>
</assignee>
<correspondence-address>
<name-1>BRACEWELL &amp; PATTERSON LLP</name-1>
<name-2>INTELLECTUAL PROPERTY LAW</name-2>
<address>
<address-1>POST OFFICE BOX 969</address-1>
<city>AUSTIN</city>
<state>TX</state>
<postalcode>78767-0969</postalcode>
<country>
<country-code>US</country-code>
</country>
</address>
</correspondence-address>
</subdoc-bibliographic-information>
<subdoc-abstract>
<paragraph id="A-0001" lvl="0">A cache controller for a processor in a remote node of a system bus in a multiway multiprocessor link sends out a cache deallocate address transaction (CDAT) for a given cache line when that cache line is flushed and information from memory in a home node is no longer deemed valid for that cache line of that remote node processor. A local snoop of that CDAT transaction is then performed as a background function by other processors in the same remote node. If the snoop results indicate that same information is valid in another cache, and that cache decides it better to keep it valid in that remote node, then the information remains there. If the snoop results indicate that the information is not valid among caches in that remote node, or will be flushed due to the CDAT, the system memory directory in the home node of the multiprocessor link is notified and changes state in response to this. The system has higher performance due to the cache line maintenance functions being performed in the background rather than based on mainstream demand. </paragraph>
</subdoc-abstract>
<subdoc-description>
<summary-of-invention>
<section>
<heading lvl="1">BACKGROUND OF THE INVENTION </heading>
<paragraph id="P-0001" lvl="0"><number>&lsqb;0001&rsqb;</number> 1. Technical Field </paragraph>
<paragraph id="P-0002" lvl="0"><number>&lsqb;0002&rsqb;</number> The present invention relates to data processing systems, and particularly to processors operating in nodes of multiway multiprocessor links. More specifically, the present invention relates to improving the performance of such data processing systems during flushes of cache memory in remote nodes of data obtained from memory in a home node on the link. </paragraph>
<paragraph id="P-0003" lvl="0"><number>&lsqb;0003&rsqb;</number> 2. Description of the Related Art </paragraph>
<paragraph id="P-0004" lvl="0"><number>&lsqb;0004&rsqb;</number> A widely used high performance data processing system is a multiway multiprocessor link with several nodes. During operation of such a prior art data processing system, system memory for the overall data processing system was typically partitioned among memory components of the several nodes. It was thus common for cache memory in one node, called a remote node, to access/cache information resident in the memory of another node, termed a home node, for processing. </paragraph>
<paragraph id="P-0005" lvl="0"><number>&lsqb;0005&rsqb;</number> A memory directory in the home node kept record of the transfer of that information to the cache memory in the remote node. During data processing in the remote node, the transferred information in the cache memory of the remote node would periodically be designated as a victim and flushed from that cache, based on lack of recent usage or other reasons. The system memory in the home node of prior art data processing systems would at some subsequent time also perform a home memory address flush directed towards the transferred information in the remote node cache. This required transfers of requests and flush commands over the system links, being in effect what is known as a mainstream operation. In addition, it was often the case that the remote node cache memory had actually been flushed in the remote node some time before, making the home memory address flush a redundant operation. </paragraph>
<paragraph id="P-0006" lvl="0"><number>&lsqb;0006&rsqb;</number> This form of cache memory flush had undesirable effects, reducing system speed and performance and increasing system latency. This was undesirable in high performance data processing systems. It would, therefore, be desirable to reduce system latency in multiway multiprocessor links. It would also be desirable to have cache maintenance purges in multiway multiprocessor links be done on a basis that required less usage of the system links. </paragraph>
</section>
<section>
<heading lvl="1">SUMMARY OF THE INVENTION </heading>
<paragraph id="P-0007" lvl="0"><number>&lsqb;0007&rsqb;</number> It is therefore an object of the invention to provide a method and system for high performance data processing in multiway multiprocessor links for cache maintenance purges with reduced usage of system links. </paragraph>
<paragraph id="P-0008" lvl="0"><number>&lsqb;0008&rsqb;</number> It is another object of the invention to provide a method and system for high performance data processing with reduced home memory address flushes to remote nodes in multiprocessor links. </paragraph>
<paragraph id="P-0009" lvl="0"><number>&lsqb;0009&rsqb;</number> It is still another object of the invention to provide a method and system for high performance data processing with reduced system latency by removing unnecessary memory purges from transmission over system links. </paragraph>
<paragraph id="P-0010" lvl="0"><number>&lsqb;0010&rsqb;</number> The above and other objects are achieved as is now described. A high performance data processing system and method are provided which improve operation of a multinode processor system by providing protocols for organized purges of cache memory in remote nodes when the cache memory is selected as a victim for purging. When a cache associated in a remote node (e.g. L2 cache) of the system identified as a victim is purged, its cache controller sends a cache deallocate address transaction over the system bus of that remote node. An inclusivity indicator for the associated cache is also provided in the L3 cache directory on the system bus for that remote node. The inclusivity indicator for the additional cache contains bits representing the valid/invalid status of each cache line in the associated cache on the system bus in the remote node. The inclusivity indicator changes state for the associated cache having its memory purged. An L3 cache directory in the node snoops the system bus for cache deallocate address transactions from other cache controllers on the node. The remote node notifies the home node of a cache deallocate address transaction when all cache memories of that remote node are indicated invalid. An inclusivity indicator in the remote L3 cache directory of the remote node changes state in response to such a notification. In addition, the home node maintains a system memory directory which consists of inclusivity bits that track which remote nodes have lines checked out from this home nodes system memory. The home node updates the inclusivity bits in its system memory directory when it receives a cache deallocate address transaction from the remote node. Performance of cache line maintenance functions over system links in the multinode system are thus substantially reduced. </paragraph>
<paragraph id="P-0011" lvl="0"><number>&lsqb;0011&rsqb;</number> The foregoing and other objects and advantages of the present invention will be apparent to those skilled in the art, in view of the following detailed description of the preferred embodiment of the present invention, taken in conjunction with the appended claims and the accompanying drawings. </paragraph>
<paragraph id="P-0012" lvl="0"><number>&lsqb;0012&rsqb;</number> The above as well as additional objectives, features, and advantages of the present invention will become apparent in the following detailed written description. </paragraph>
</section>
</summary-of-invention>
<brief-description-of-drawings>
<section>
<heading lvl="1">BRIEF DESCRIPTION OF THE DRAWINGS </heading>
<paragraph id="P-0013" lvl="0"><number>&lsqb;0013&rsqb;</number> The novel features believed characteristic of the invention are set forth in the appended claims. The invention itself, however, as well as a preferred mode of use, further objectives, and advantages thereof, will best be understood by reference to the following detailed description of an illustrative embodiment when read in conjunction with the accompanying drawings, wherein: </paragraph>
<paragraph id="P-0014" lvl="0"><number>&lsqb;0014&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 1</cross-reference> is a block diagram of a high performance data processing system of a multiway multiprocessor link of several nodes. </paragraph>
<paragraph id="P-0015" lvl="0"><number>&lsqb;0015&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 2</cross-reference> is a block diagram of a node according to the prior art in the data processing system of <cross-reference target="DRAWINGS">FIG. 1</cross-reference>. </paragraph>
<paragraph id="P-0016" lvl="0"><number>&lsqb;0016&rsqb;</number> <cross-reference target="DRAWINGS">FIGS. 3A and 3B</cross-reference> are schematic diagrams of an example sequence flow of a cache memory flush according to the prior art in the remote node of <cross-reference target="DRAWINGS">FIG. 2</cross-reference>. </paragraph>
<paragraph id="P-0017" lvl="0"><number>&lsqb;0017&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 4</cross-reference> is a block diagram of a high performance data processing system of a multiway multiprocessor link according to the present invention. </paragraph>
<paragraph id="P-0018" lvl="0"><number>&lsqb;0018&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 5</cross-reference> is a block diagram of a node according to the present invention in the data processing system of <cross-reference target="DRAWINGS">FIG. 4</cross-reference>. </paragraph>
<paragraph id="P-0019" lvl="0"><number>&lsqb;0019&rsqb;</number> <cross-reference target="DRAWINGS">FIGS. 6A and 6B</cross-reference> are schematic diagrams of an example sequence flow of a cache memory flush according to the present invention in the remote node of <cross-reference target="DRAWINGS">FIG. 5</cross-reference>. </paragraph>
</section>
</brief-description-of-drawings>
<detailed-description>
<section>
<heading lvl="1">DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENT </heading>
<paragraph id="P-0020" lvl="0"><number>&lsqb;0020&rsqb;</number> At the outset, an example of a cache memory purge according to the prior art is set forth in order that the operation of a high performance data processing system can be more easily understood. <cross-reference target="DRAWINGS">FIG. 1</cross-reference> in the drawings shows an example multiway high performance data processing system <highlight><bold>10</bold></highlight> with several nodes <highlight><bold>12</bold></highlight>. Each of the nodes <highlight><bold>12</bold></highlight> has a system bus <highlight><bold>14</bold></highlight> connected through a link <highlight><bold>16</bold></highlight> of that node <highlight><bold>12</bold></highlight> to a node switch <highlight><bold>18</bold></highlight> to form the high performance data processing system <highlight><bold>10</bold></highlight>. <cross-reference target="DRAWINGS">FIG. 2</cross-reference> illustrates typical components of an example node <highlight><bold>12</bold></highlight> in the form of a four way link, composed of four processors <highlight><bold>20</bold></highlight> sharing the system bus <highlight><bold>14</bold></highlight>. Each of the processors <highlight><bold>20</bold></highlight> in the nodes <highlight><bold>12</bold></highlight> have an L2 cache memory <highlight><bold>22</bold></highlight> with a cache line <highlight><bold>24</bold></highlight>. The node <highlight><bold>12</bold></highlight> also includes an associated L3 cache <highlight><bold>26</bold></highlight> or higher level cache shared in common over the system bus <highlight><bold>14</bold></highlight> with each of the four L2 cache memories <highlight><bold>22</bold></highlight>. The cache lines <highlight><bold>24</bold></highlight> in each of the L2 cache memories <highlight><bold>22</bold></highlight> of the node <highlight><bold>12</bold></highlight> each communicate over the system bus <highlight><bold>14</bold></highlight> with a cache line <highlight><bold>28</bold></highlight> in the associated cache <highlight><bold>26</bold></highlight>. </paragraph>
<paragraph id="P-0021" lvl="0"><number>&lsqb;0021&rsqb;</number> The associated L3 cache <highlight><bold>26</bold></highlight> is connected to the system bus <highlight><bold>14</bold></highlight>, as are the link <highlight><bold>16</bold></highlight> and a system memory <highlight><bold>30</bold></highlight>, a system memory directory <highlight><bold>32</bold></highlight> and an input/output or I/O channel controller <highlight><bold>34</bold></highlight>. The I/O channel controller <highlight><bold>34</bold></highlight> permits connection of conventional I/O devices through various types of computer I/O buses, such as a PCI bus, to the system bus <highlight><bold>14</bold></highlight> to form a part of the node <highlight><bold>12</bold></highlight>. </paragraph>
<paragraph id="P-0022" lvl="0"><number>&lsqb;0022&rsqb;</number> The system memory <highlight><bold>30</bold></highlight> of each node <highlight><bold>12</bold></highlight> serves as a permanent storage location for a portion of the data resident in the data processing system <highlight><bold>10</bold></highlight> and is called the home node for such purposes. The system memory directory <highlight><bold>32</bold></highlight> for system memory <highlight><bold>30</bold></highlight> includes an associated memory location or storage register <highlight><bold>36</bold></highlight> for inclusivity bits for each cache line that makes up system memory <highlight><bold>30</bold></highlight> in the home node. The inclusivity bits in storage location <highlight><bold>36</bold></highlight> correspond in number to the number of other nodes <highlight><bold>12</bold></highlight> of the data processing system <highlight><bold>10</bold></highlight> that may currently have that line cached. During the operation of data processing system <highlight><bold>10</bold></highlight>, memory units in other nodes <highlight><bold>12</bold></highlight> may request and use information permanently resident in another or home node <highlight><bold>12</bold></highlight>. In this situation the requesting node is referenced to as a remote node. The inclusivity bits in storage register <highlight><bold>36</bold></highlight> of each home node <highlight><bold>12</bold></highlight> indicate which of the other remote nodes have temporarily received and stored in their L3 cache <highlight><bold>26</bold></highlight> data which is permanently resident in the memory <highlight><bold>30</bold></highlight> of the home node <highlight><bold>12</bold></highlight>. </paragraph>
<paragraph id="P-0023" lvl="0"><number>&lsqb;0023&rsqb;</number> The associated or L3 cache <highlight><bold>26</bold></highlight> in each node <highlight><bold>12</bold></highlight> has a directory <highlight><bold>38</bold></highlight> including a storage location or register <highlight><bold>40</bold></highlight> for inclusivity bits corresponding in number to the number of associated L2 cache memories <highlight><bold>22</bold></highlight> sharing that L3 cache <highlight><bold>26</bold></highlight>. </paragraph>
<paragraph id="P-0024" lvl="0"><number>&lsqb;0024&rsqb;</number> During the conventional prior art operation of cache memory in nodes <highlight><bold>12</bold></highlight> of data processing system <highlight><bold>10</bold></highlight>, the L2 cache <highlight><bold>22</bold></highlight> periodically cleared or flushed the L2 cache memories <highlight><bold>22</bold></highlight> associated with it. Based on each of recent usage demands, the L2 cache <highlight><bold>22</bold></highlight> flushed cache memory on a least recently used (LRU) basis from an L2 cache memory <highlight><bold>22</bold></highlight>, known as a victim, associated with one of the processors <highlight><bold>20</bold></highlight>. When this occurred, the inclusivity bit in the storage location <highlight><bold>36</bold></highlight> for that associated L2 cache memory <highlight><bold>22</bold></highlight> in the remote node would not change state. In addition, the system memory directory <highlight><bold>32</bold></highlight> in the home node would not be informed of this change. Rather, at some later time the home node system memory flushed the L3 cache of that remote node based on a demand request by a processor to store to that cache line. For multiway multiprocessor links, however, this technique required that home memory address flushes be performed over the system links or busses before allowing the processor to complete the store. As such, it had undesirable effects on system performance, speed and latency. </paragraph>
<paragraph id="P-0025" lvl="0"><number>&lsqb;0025&rsqb;</number> <cross-reference target="DRAWINGS">FIGS. 3A and 3B</cross-reference> in the drawings show an example operating sequence of the prior art data processing system <highlight><bold>10</bold></highlight> using nodes <highlight><bold>12</bold></highlight>. In this example, the system memory <highlight><bold>30</bold></highlight> of each node <highlight><bold>12</bold></highlight> in <cross-reference target="DRAWINGS">FIG. 1</cross-reference> contains a partitioned segment of the overall system memory of data processing system <highlight><bold>10</bold></highlight>. The four nodes <highlight><bold>12</bold></highlight> of like structure in system <highlight><bold>10</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 2</cross-reference> are identified as nodes 00, 01, 02 and 03 for purposes of separate reference. Further, in this example, the system memory <highlight><bold>30</bold></highlight> of node 01 manages in its system memory, and maintains in its storage location <highlight><bold>36</bold></highlight>, a set of inclusivity bits indicating the state of cache lines checked out of a home node 01 by the other three nodes 00, 02 and 03. </paragraph>
<paragraph id="P-0026" lvl="0"><number>&lsqb;0026&rsqb;</number> In step <highlight><bold>51</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 3A, a</cross-reference> request for data block A, termed a Ld-A request operation or step, is sent from cache line <highlight><bold>24</bold></highlight> for a requesting one, designated P00, of the processors <highlight><bold>20</bold></highlight> in remote node 00. In step <highlight><bold>52</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 3</cross-reference>A, the Ld-A request is sent over the link <highlight><bold>16</bold></highlight> to the system memory <highlight><bold>30</bold></highlight> of home node 01. In step <highlight><bold>53</bold></highlight>, the system memory <highlight><bold>30</bold></highlight> of the node 00 prepares to transfer the requested data to the requesting cache and the system memory directory <highlight><bold>36</bold></highlight> in the node 00 indicates this transaction. In step <highlight><bold>54</bold></highlight>, the requested data is sent and stored in the associated L2 cache <highlight><bold>22</bold></highlight> and the additional L3 cache <highlight><bold>26</bold></highlight> of the remote node 00. In step <highlight><bold>55</bold></highlight>, the data is transferred from the associated L2 cache <highlight><bold>22</bold></highlight> to its requesting processor <highlight><bold>20</bold></highlight> (designated P00), completing the Ld-A transaction. </paragraph>
<paragraph id="P-0027" lvl="0"><number>&lsqb;0027&rsqb;</number> After some period of time, the data A transferred in the foregoing manner is flushed from L2 cache <highlight><bold>22</bold></highlight> in the node 00 as indicated at step <highlight><bold>56</bold></highlight> and subsequently flushed from L3 cache <highlight><bold>26</bold></highlight> in node 00, as indicated in step <highlight><bold>57</bold></highlight>. At this point, the data originally requested and used in the remote node 00 is no longer resident in either cache of that remote node, but the system memory directory <highlight><bold>36</bold></highlight> of the home node 01 still indicates the data as resident in the cache in that remote node 00. </paragraph>
<paragraph id="P-0028" lvl="0"><number>&lsqb;0028&rsqb;</number> At some future later point in time, another processor in the system <highlight><bold>10</bold></highlight>, such as a processor <highlight><bold>20</bold></highlight> (designated P30) in node 03 sends an indication as shown in step <highlight><bold>60</bold></highlight> (<cross-reference target="DRAWINGS">FIG. 3B</cross-reference>) to store the same data A to the L2 cache <highlight><bold>22</bold></highlight> of node 03. In step <highlight><bold>61</bold></highlight>, that indication is sent to system memory directory <highlight><bold>32</bold></highlight> in remote node 00. During step <highlight><bold>62</bold></highlight>, the system memory directory node <highlight><bold>32</bold></highlight> in home node 01 sends a flush_A command to node 00. As a result of step <highlight><bold>53</bold></highlight> above, the system memory directory <highlight><bold>32</bold></highlight> in Node 01 still indicates data A as resident in cache memory in node 00. </paragraph>
<paragraph id="P-0029" lvl="0"><number>&lsqb;0029&rsqb;</number> During step <highlight><bold>63</bold></highlight>, the L2 cache <highlight><bold>22</bold></highlight> in Node 00 indicates that the flush has been performed, since the associated cache line for that node 00 is indicated invalid as a result of step <highlight><bold>56</bold></highlight> previously being performed. During step <highlight><bold>64</bold></highlight>, the L3 cache <highlight><bold>26</bold></highlight> in node 00 sends a indication to the system memory directory <highlight><bold>32</bold></highlight> in node 01. As a result, during step <highlight><bold>65</bold></highlight>, the system memory directory <highlight><bold>32</bold></highlight> in node 01 can now indicate authorization to Node 03 to perform the requested Store A operation. During step <highlight><bold>66</bold></highlight>, the system memory directory <highlight><bold>32</bold></highlight> in home node 01 now permits data A to be sent to the L3 cache <highlight><bold>26</bold></highlight> and the L2 cache in node 03. As a result, during step <highlight><bold>67</bold></highlight>, data A is stored by processor <highlight><bold>20</bold></highlight> (designatedP30). As can be understood from the foregoing operations, a number of mainstream operations over system links are required to be performed. </paragraph>
<paragraph id="P-0030" lvl="0"><number>&lsqb;0030&rsqb;</number> Turning to the present invention, with reference now to the figures and in particular with reference to <cross-reference target="DRAWINGS">FIG. 4</cross-reference>, an example multiway high performance data processing system <highlight><bold>110</bold></highlight> with several nodes <highlight><bold>112</bold></highlight> is shown. The data processing system <highlight><bold>110</bold></highlight> operates differently from the data processing system <highlight><bold>10</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 1</cross-reference> as will be set forth below despite structural similarities. Each of the nodes <highlight><bold>112</bold></highlight> has a system bus <highlight><bold>114</bold></highlight> for connection through a link <highlight><bold>116</bold></highlight> of that node <highlight><bold>112</bold></highlight> to a node switch <highlight><bold>118</bold></highlight> to form the high performance data processing system <highlight><bold>110</bold></highlight>. <cross-reference target="DRAWINGS">FIG. 5</cross-reference> illustrates typical components of an example node <highlight><bold>112</bold></highlight> in the form of a four way link, composed of four processors <highlight><bold>120</bold></highlight> sharing the system bus <highlight><bold>114</bold></highlight>. Each of the processors <highlight><bold>120</bold></highlight> in the nodes <highlight><bold>112</bold></highlight> have an L2 cache memory <highlight><bold>122</bold></highlight> with a cache line <highlight><bold>124</bold></highlight>. The node <highlight><bold>112</bold></highlight> also includes an associated L3 cache <highlight><bold>126</bold></highlight> or higher level cache shared in common over the system bus <highlight><bold>114</bold></highlight> with each of the four L2 cache memories <highlight><bold>122</bold></highlight>. The cache lines <highlight><bold>124</bold></highlight> in each of the L2 cache memories <highlight><bold>122</bold></highlight> of the node <highlight><bold>112</bold></highlight> each communicate over the system bus <highlight><bold>114</bold></highlight> with a cache line <highlight><bold>128</bold></highlight> in the associated cache <highlight><bold>126</bold></highlight>. </paragraph>
<paragraph id="P-0031" lvl="0"><number>&lsqb;0031&rsqb;</number> The associated L3 cache <highlight><bold>126</bold></highlight> is connected to the system bus <highlight><bold>114</bold></highlight>, as are the link <highlight><bold>116</bold></highlight> and a system memory <highlight><bold>130</bold></highlight>, a system memory directory <highlight><bold>132</bold></highlight> and an input/output or I/O channel controller <highlight><bold>134</bold></highlight>. The I/O channel controller <highlight><bold>134</bold></highlight> permits connection of conventional I/O devices through various types of computer I/O buses, such as a PCI bus, to the system bus <highlight><bold>114</bold></highlight> to form a part of the node <highlight><bold>112</bold></highlight>. </paragraph>
<paragraph id="P-0032" lvl="0"><number>&lsqb;0032&rsqb;</number> The system memory <highlight><bold>130</bold></highlight> of each node <highlight><bold>112</bold></highlight> serves as a permanent storage location for a portion of the data resident in the data processing system <highlight><bold>110</bold></highlight> and is called the home node for such purposes. The system memory directory <highlight><bold>132</bold></highlight> for system memory <highlight><bold>30</bold></highlight> includes an associated memory location or storage register <highlight><bold>136</bold></highlight> for inclusivity bits for each cache line that makes up system memory <highlight><bold>30</bold></highlight> in the home node. The inclusivity bits in storage location <highlight><bold>136</bold></highlight> correspond in number to the number of other nodes <highlight><bold>112</bold></highlight> of the data processing system <highlight><bold>110</bold></highlight> that may currently have that line cached. During the operation of data processing system <highlight><bold>110</bold></highlight>, memory units in other nodes <highlight><bold>112</bold></highlight> may request and use information permanently resident in another or home node <highlight><bold>112</bold></highlight>. In this situation the requesting node is referenced to as a remote node. The inclusivity bits in storage register <highlight><bold>136</bold></highlight> of each home node <highlight><bold>112</bold></highlight> indicate which other nodes have temporarily received and stored in the L3 cache <highlight><bold>126</bold></highlight> of those remote nodes <highlight><bold>112</bold></highlight> data which is permanently resident in the memory <highlight><bold>130</bold></highlight> of that home node <highlight><bold>112</bold></highlight>. </paragraph>
<paragraph id="P-0033" lvl="0"><number>&lsqb;0033&rsqb;</number> The associated or L3 cache <highlight><bold>126</bold></highlight> in each node <highlight><bold>112</bold></highlight> has a directory <highlight><bold>138</bold></highlight> including a storage location or register <highlight><bold>140</bold></highlight> for inclusivity bits corresponding in number to the number of associated L2 cache memories <highlight><bold>122</bold></highlight> sharing that L3 cache <highlight><bold>126</bold></highlight>. </paragraph>
<paragraph id="P-0034" lvl="0"><number>&lsqb;0034&rsqb;</number> During the operation of cache memory in nodes <highlight><bold>112</bold></highlight> of data processing system <highlight><bold>110</bold></highlight> according to the present invention, the L2 cache <highlight><bold>122</bold></highlight> in the remote node periodically clears or flushes the L2 cache memories <highlight><bold>122</bold></highlight> associated with it. Based on each of recent usage demands, the L2 cache <highlight><bold>122</bold></highlight> flushes cache memory on a least recently used (LRU) basis from an L2 cache memory <highlight><bold>122</bold></highlight>, known as a victim, associated with one of the processors <highlight><bold>120</bold></highlight>. </paragraph>
<paragraph id="P-0035" lvl="0"><number>&lsqb;0035&rsqb;</number> A cache controller resident in the L2 cache <highlight><bold>122</bold></highlight> on the remote node sends a cache deallocate address transaction (CDAT) as indicated by arrows <highlight><bold>125</bold></highlight> to the L3 cache <highlight><bold>126</bold></highlight> for the remote node <highlight><bold>112</bold></highlight>. The cache controllers for each of the L2 caches <highlight><bold>122</bold></highlight> in the remote node snoop the system bus for CDAT&apos;s from the other cache controllers of that remote node. The CDAT from the cache controller performing the flush transaction is provided to the L3 cache <highlight><bold>126</bold></highlight> for the remote node, causing the status of the inclusivity bit in the L3 cache <highlight><bold>136</bold></highlight> to change, indicating that the data previously cached is no longer valid in the L2 caches. The L3 cache controller for the remote node <highlight><bold>112</bold></highlight> in response to the CDAT scans the snoop responses of each of the L2 caches in the remote node <highlight><bold>112</bold></highlight>. So long as any one of the L2 (or the L3) caches indicates that the data being flushed from another L2 cache is still valid, no flush status coherency exists and the inclusivity bit in the L3 cache <highlight><bold>126</bold></highlight> for that L2 cache does not change state. Rather, the inclusivity bit in L3 cache <highlight><bold>126</bold></highlight> for that L2 cache indicates that data is still valid. </paragraph>
<paragraph id="P-0036" lvl="0"><number>&lsqb;0036&rsqb;</number> In the event, however, the snoop responses of each of the L2 caches <highlight><bold>120</bold></highlight> and L3 cache <highlight><bold>126</bold></highlight> in the remote node indicate that the data being flushed is not present in any of such L2/L3 caches cache flush status coherency exists. As a result, as indicated by arrow <highlight><bold>135</bold></highlight>, the link <highlight><bold>116</bold></highlight> for the remote node transmits the CDAT to the system memory directory <highlight><bold>132</bold></highlight> in the home node. In this manner, the home node is informed of the cache flush in a remote node. Further, this flush is done as a background function. </paragraph>
<paragraph id="P-0037" lvl="0"><number>&lsqb;0037&rsqb;</number> <cross-reference target="DRAWINGS">FIGS. 6A and 6B</cross-reference> in the drawings show an example operating sequence of the data processing system <highlight><bold>110</bold></highlight> according to the present invention, operating in response to a similar set of data transfer requests to those shown in <cross-reference target="DRAWINGS">FIGS. 3A and 3B</cross-reference>. Steps <highlight><bold>151</bold></highlight> through <highlight><bold>155</bold></highlight> in <cross-reference target="DRAWINGS">FIG. 6A</cross-reference> are performed in a like manner to the operation described above in <cross-reference target="DRAWINGS">FIG. 3</cross-reference>A and accordingly their description is not repeated here. </paragraph>
<paragraph id="P-0038" lvl="0"><number>&lsqb;0038&rsqb;</number> With the present invention, during step <highlight><bold>156</bold></highlight> as shown in <cross-reference target="DRAWINGS">FIG. 6</cross-reference>A, the L2 cache <highlight><bold>122</bold></highlight> of processor P00 in node 00 being flushed sends the cache address deallocate transaction (CDAT) described with reference to the structure of <cross-reference target="DRAWINGS">FIGS. 4 and 5</cross-reference> to its L3 cache <highlight><bold>126</bold></highlight>. For purposes of this example, the snoop performed by the other L2 caches <highlight><bold>122</bold></highlight> indicates that the data is also invalid for those caches. As a result, during step <highlight><bold>157</bold></highlight>, the L3 cache indicates the change in status of inclusivity bit to the system memory directory <highlight><bold>130</bold></highlight> of the home node 01. At this time, the home node 01 changes the state of its inclusivity bit in its system memory directory <highlight><bold>130</bold></highlight>. </paragraph>
<paragraph id="P-0039" lvl="0"><number>&lsqb;0039&rsqb;</number> At some subsequent time, another processor in another node, such as processor <highlight><bold>120</bold></highlight> (designated P30) in node 03 sends an indication as shown in step <highlight><bold>160</bold></highlight> to store data A in its L2 cache. In step <highlight><bold>161</bold></highlight>, that indication is sent to system memory directory <highlight><bold>132</bold></highlight> in home node 01. As a result of the operation of the data processing system <highlight><bold>110</bold></highlight> of the present invention (<cross-reference target="DRAWINGS">FIGS. 4 and 5</cross-reference>) during steps <highlight><bold>156</bold></highlight> and <highlight><bold>157</bold></highlight> as described above, the system memory directory <highlight><bold>132</bold></highlight> in home node 01 during step <highlight><bold>157</bold></highlight> has been informed that the requested data has been previously purged from the remote node 00. This is indicated by steps <highlight><bold>162</bold></highlight>, <highlight><bold>163</bold></highlight> and <highlight><bold>164</bold></highlight> being indicated NOP in <cross-reference target="DRAWINGS">FIG. 6</cross-reference>. The system memory directory <highlight><bold>132</bold></highlight> for the home node 01 can as a result now indicate authorization, as shown at step <highlight><bold>165</bold></highlight>, for the requested operation. During step <highlight><bold>166</bold></highlight>, the requested data is transferred to the L3 cache and the L2 cache of node 03 which now is the remote node. During step <highlight><bold>167</bold></highlight>, the data from L2 cache is then stored to processor P30 on node 03. </paragraph>
<paragraph id="P-0040" lvl="0"><number>&lsqb;0040&rsqb;</number> It should be understood that the system of the present invention described in the foregoing embodiment could be used in systems with a larger hierarchy. However, in most of those higher order hierarchies, the L3 cache line does not need to send flushes to high order CPU&apos;s and the foregoing example of <cross-reference target="DRAWINGS">FIGS. 5 and 6</cross-reference> is still applicable for the higher order hierarchies. </paragraph>
<paragraph id="P-0041" lvl="0"><number>&lsqb;0041&rsqb;</number> It is to be noted that with the present invention, a transaction of the type illustrated in <cross-reference target="DRAWINGS">FIGS. 6A and 6B</cross-reference> is not a retriable operation, and can be thus deemed a weak or imprecise bus transaction. Should any problem occur during the course of such a transaction with receipt of the CDAT and proper response of the system, the CDAT is ignored. If desired, however, the processing system of the present may be programmed to send the CDAT in a multishot manner on occurrence of a victim cache purge. In such a case, the cache controller for the victim cache would send several CDAT transmissions rather than a single one as described. </paragraph>
<paragraph id="P-0042" lvl="0"><number>&lsqb;0042&rsqb;</number> Further, the cache deallocate access transaction (CDAT) may be enlarged in content to carry information beyond the indication that a victim cache purge has taken place. Examples of the additional information that can be added as code bits to the cache deallocate access transaction are: whether the information purged from the cache was address, instructions or data; whether the information purged had been subject to multiple previous uses while in cache; and the like. </paragraph>
<paragraph id="P-0043" lvl="0"><number>&lsqb;0043&rsqb;</number> With the present invention, a home memory and thus the entire system exhibit higher performance. The contents of the inclusivity indicators in the system memory directory are maintained more current and accurate. Thus, the cache lines for the home memory more accurately reflect the status of memory contents in cache in another node at any particular time. </paragraph>
<paragraph id="P-0044" lvl="0"><number>&lsqb;0044&rsqb;</number> Accordingly, the data processing system of the present invention offers reduced latency. The number of home memory flushes to remote nodes is fewer. Further, the home memory when necessary can use information gained from its system directory inclusivity indicator bits to select a cache as victim to be purged rather than being forced to rely solely on the LRU (or least recently used) basis. This allows more cache lines to be active and also reduces artificial invalidation of remote mode cable lines. </paragraph>
<paragraph id="P-0045" lvl="0"><number>&lsqb;0045&rsqb;</number> Further, operations of the system of the present invention assist in recovery. In the event that a node crashes, the home node during system recovery has access in its system directory to what data was checked out to the crashed remote node at the time the remote node crashed. If desired, the information made available with the present invention may be used to adapt processor usage according to cache demand experience. For example, if a remote node processor has been continuously borrowing data from the home node memory, the processing functions could be transferred to the home node. </paragraph>
<paragraph id="P-0046" lvl="0"><number>&lsqb;0046&rsqb;</number> Although the invention has been described with reference to specific embodiments, this description is not meant to be construed in a limiting sense. Various modifications of the disclosed embodiment, as well as alternative embodiments of the invention, will become apparent to persons skilled in the art upon reference to the description of the invention. It is therefore contemplated that such modifications can be made without departing from the spirit or scope of the present invention as defined in the appended claims. </paragraph>
</section>
</detailed-description>
</subdoc-description>
<subdoc-claims>
<heading lvl="1">What is claimed is: </heading>
<claim id="CLM-00001">
<claim-text><highlight><bold>1</bold></highlight>. A data processing system comprising: 
<claim-text>a plurality of nodes interconnected for transfer of data for processing in the nodes; </claim-text>
<claim-text>each node including one or more processors and associated memory; </claim-text>
<claim-text>each node serving as a home node for portions of data stored in its associated memory and transferring segments of that data to another node for processing in that other node as a remote node; </claim-text>
<claim-text>at least one of the processors in the remote node periodically flushing its memory; </claim-text>
<claim-text>the remote node inquiring whether each of its processors has further need for the data being flushed; and </claim-text>
<claim-text>the remote node notifying the home node in the event flush status coherency exists for data for each associated memory in the remote node. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00002">
<claim-text><highlight><bold>2</bold></highlight>. The data processing system of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein the remote node includes inclusivity indicators which indicate whether data from other nodes is present in memory of the remote node. </claim-text>
</claim>
<claim id="CLM-00003">
<claim-text><highlight><bold>3</bold></highlight>. The data processing system of <dependent-claim-reference depends_on="CLM-00002">claim 2</dependent-claim-reference>, further including the remote node sending a cache deallocate address transaction to change the status of the inclusivity indicator for memory of other nodes when the memory of the remote node is flushed. </claim-text>
</claim>
<claim id="CLM-00004">
<claim-text><highlight><bold>4</bold></highlight>. The data processing system of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein each of the nodes serves as a remote node and includes inclusivity indicators which indicate whether data from other nodes is present in memory of the remote node. </claim-text>
</claim>
<claim id="CLM-00005">
<claim-text><highlight><bold>5</bold></highlight>. The data processing system of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein the home nodes include inclusivity indicators which indicate whether other nodes are storing data from the memory of the home node. </claim-text>
</claim>
<claim id="CLM-00006">
<claim-text><highlight><bold>6</bold></highlight>. A data processing node connectable in a multinode data processing system, said node comprising: 
<claim-text>one or more processors and associated memory; </claim-text>
<claim-text>the node serving as a remote node when receiving in the associated memory segments of data transferred to the node; </claim-text>
<claim-text>at least one of the processors periodically flushing data from the associated memory; </claim-text>
<claim-text>the remote node inquiring whether each of its processors has further need for the data being flushed; and </claim-text>
<claim-text>the remote node notifying the home node in the event flush status coherency exists for each associated memory in the remote node for the data being flushed. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00007">
<claim-text><highlight><bold>7</bold></highlight>. The data processing node of <dependent-claim-reference depends_on="CLM-00006">claim 6</dependent-claim-reference>, further including: 
<claim-text>inclusivity indicators which indicate whether data from other nodes is present in memory of the remote node. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00008">
<claim-text><highlight><bold>8</bold></highlight>. The data processing node of <dependent-claim-reference depends_on="CLM-00007">claim 7</dependent-claim-reference>, further including the remote node sending a cache deallocate address transaction to change the status of the inclusivity indicator for memory of other nodes when the memory of the remote node is flushed. </claim-text>
</claim>
<claim id="CLM-00009">
<claim-text><highlight><bold>9</bold></highlight>. A data processing node connectable in a multinode data processing system, said node comprising: 
<claim-text>one or more processors and associated memory; </claim-text>
<claim-text>the node serving as a home node when transferring segments of data from the associated memory; </claim-text>
<claim-text>the node including inclusivity indicators which indicate whether other nodes are storing data from the associated memory of the home node. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00010">
<claim-text><highlight><bold>10</bold></highlight>. A method of optimizing performance of a multinode data processing system during purges of memory in a remote node of the system, comprising the steps of: 
<claim-text>flushing the memory in one of the nodes; </claim-text>
<claim-text>inquiring whether each of its processors has further need for the data being flushed; and </claim-text>
<claim-text>notifying the home node in the event flush status coherency exists for data for each associated memory in the remote node. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00011">
<claim-text><highlight><bold>11</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00011">claim 10</dependent-claim-reference>, further including the step of: 
<claim-text>changing the status of an inclusivity indicator for the remote node in the event flush status coherency exists for data for each associated memory in the remote node. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00012">
<claim-text><highlight><bold>12</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00011">claim 10</dependent-claim-reference>, further including the step of: 
<claim-text>sending a cache deallocate transaction to other nodes in the system in the event flush status coherency exists for data for each associated memory in the remote node. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00013">
<claim-text><highlight><bold>13</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00011">claim 12</dependent-claim-reference>, wherein the nodes have inclusivity indicators and further including the step of: 
<claim-text>changing the status of inclusivity indicators in the other nodes in response to the cache deallocate access transaction from the remote node. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00014">
<claim-text><highlight><bold>14</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00011">claim 12</dependent-claim-reference>, further including the step of: 
<claim-text>indicating at the home node whether other nodes are storing data from the home node. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00015">
<claim-text><highlight><bold>15</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00011">claim 14</dependent-claim-reference>, further including the step of: 
<claim-text>changing the indications at the home node in the event flush status coherency exists for data for each associated memory in the remote node.</claim-text>
</claim-text>
</claim>
</subdoc-claims>
<subdoc-drawings id="DRAWINGS">
<heading lvl="0" align="CENTER">Drawings</heading>
<representative-figure>4</representative-figure>
<figure id="figure-D00000">
<image id="EMI-D00000" file="US20030005232A1-20030102-D00000.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00001">
<image id="EMI-D00001" file="US20030005232A1-20030102-D00001.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00002">
<image id="EMI-D00002" file="US20030005232A1-20030102-D00002.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00003">
<image id="EMI-D00003" file="US20030005232A1-20030102-D00003.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00004">
<image id="EMI-D00004" file="US20030005232A1-20030102-D00004.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00005">
<image id="EMI-D00005" file="US20030005232A1-20030102-D00005.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00006">
<image id="EMI-D00006" file="US20030005232A1-20030102-D00006.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00007">
<image id="EMI-D00007" file="US20030005232A1-20030102-D00007.TIF" imf="TIFF" ti="DR"/>
</figure>
</subdoc-drawings>
</patent-application-publication>
