<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE patent-application-publication SYSTEM "pap-v16-2002-01-01.dtd" [
<!ENTITY US20030004991A1-20030102-D00000.TIF SYSTEM "US20030004991A1-20030102-D00000.TIF" NDATA TIF>
<!ENTITY US20030004991A1-20030102-D00001.TIF SYSTEM "US20030004991A1-20030102-D00001.TIF" NDATA TIF>
<!ENTITY US20030004991A1-20030102-D00002.TIF SYSTEM "US20030004991A1-20030102-D00002.TIF" NDATA TIF>
<!ENTITY US20030004991A1-20030102-D00003.TIF SYSTEM "US20030004991A1-20030102-D00003.TIF" NDATA TIF>
<!ENTITY US20030004991A1-20030102-D00004.TIF SYSTEM "US20030004991A1-20030102-D00004.TIF" NDATA TIF>
]>
<patent-application-publication>
<subdoc-bibliographic-information>
<document-id>
<doc-number>20030004991</doc-number>
<kind-code>A1</kind-code>
<document-date>20030102</document-date>
</document-id>
<publication-filing-type>new</publication-filing-type>
<domestic-filing-data>
<application-number>
<doc-number>09896123</doc-number>
</application-number>
<application-number-series-code>09</application-number-series-code>
<filing-date>20010629</filing-date>
</domestic-filing-data>
<technical-information>
<classification-ipc>
<classification-ipc-primary>
<ipc>G06F017/24</ipc>
</classification-ipc-primary>
<classification-ipc-edition>07</classification-ipc-edition>
</classification-ipc>
<classification-us>
<classification-us-primary>
<uspc>
<class>707</class>
<subclass>512000</subclass>
</uspc>
</classification-us-primary>
<classification-us-secondary>
<uspc>
<class>707</class>
<subclass>541000</subclass>
</uspc>
</classification-us-secondary>
</classification-us>
<title-of-invention>Correlating handwritten annotations to a document</title-of-invention>
</technical-information>
<inventors>
<first-named-inventor>
<name>
<given-name>Dhananjay</given-name>
<middle-name>V.</middle-name>
<family-name>Keskar</family-name>
</name>
<residence>
<residence-us>
<city>Beaverton</city>
<state>OR</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</first-named-inventor>
<inventor>
<name>
<given-name>John</given-name>
<middle-name>J.</middle-name>
<family-name>Light</family-name>
</name>
<residence>
<residence-us>
<city>Beaverton</city>
<state>OR</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
<inventor>
<name>
<given-name>Alan</given-name>
<middle-name>B.</middle-name>
<family-name>McConkie</family-name>
</name>
<residence>
<residence-us>
<city>Gaston</city>
<state>OR</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
</inventors>
<correspondence-address>
<name-1>FISH &amp; RICHARDSON, PC</name-1>
<name-2></name-2>
<address>
<address-1>4350 LA JOLLA VILLAGE DRIVE</address-1>
<address-2>SUITE 500</address-2>
<city>SAN DIEGO</city>
<state>CA</state>
<postalcode>92122</postalcode>
<country>
<country-code>US</country-code>
</country>
</address>
</correspondence-address>
</subdoc-bibliographic-information>
<subdoc-abstract>
<paragraph id="A-0001" lvl="0">An electronic image of a document that includes a printed text portion and a handwritten portion is formed, and a part of the printed text portion in the image is identified as being associated with the handwritten portion. A correlation between a digital version of the handwritten portion and digital text representing the previously-identified part of the printed text portion is stored. </paragraph>
</subdoc-abstract>
<subdoc-description>
<summary-of-invention>
<section>
<heading lvl="1">BACKGROUND </heading>
<paragraph id="P-0001" lvl="0"><number>&lsqb;0001&rsqb;</number> The invention relates to correlating handwritten annotations to a document. </paragraph>
<paragraph id="P-0002" lvl="0"><number>&lsqb;0002&rsqb;</number> Writing on paper is a common technique for making comments and other annotations with respect to paper-based content. For example, persons attending a corporate meeting during which a document is discussed may find it convenient to write their comments or other annotations directly on the document. Although the annotations may be intended solely for use by the person making them, the annotations also may be useful for other persons.</paragraph>
</section>
</summary-of-invention>
<brief-description-of-drawings>
<section>
<heading lvl="1">BRIEF DESCRIPTION OF THE DRAWINGS </heading>
<paragraph id="P-0003" lvl="0"><number>&lsqb;0003&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 1</cross-reference> shows a document with printed text. </paragraph>
<paragraph id="P-0004" lvl="0"><number>&lsqb;0004&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 2</cross-reference> illustrates a system for use in correlating handwritten annotations on the document to an electronic version of the document. </paragraph>
<paragraph id="P-0005" lvl="0"><number>&lsqb;0005&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 3</cross-reference> shows a printed document with handwritten annotations. </paragraph>
<paragraph id="P-0006" lvl="0"><number>&lsqb;0006&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 4</cross-reference> illustrates additional details for correlating handwritten annotations to an electronic version of the document. </paragraph>
<paragraph id="P-0007" lvl="0"><number>&lsqb;0007&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 5</cross-reference> is a flow chart of a method of correlating a handwritten annotation to an electronic version of the document.</paragraph>
</section>
</brief-description-of-drawings>
<detailed-description>
<section>
<heading lvl="1">DETAILED DESCRIPTION </heading>
<paragraph id="P-0008" lvl="0"><number>&lsqb;0008&rsqb;</number> As shown in <cross-reference target="DRAWINGS">FIG. 1</cross-reference>, an original printed document <highlight><bold>10</bold></highlight> includes a printed text portion <highlight><bold>12</bold></highlight>. The document can be printed, for example, on paper. In some implementations, the document <highlight><bold>10</bold></highlight> includes a unique machine-readable identifier <highlight><bold>14</bold></highlight> such as a bar code. If the document includes multiple pages, a different, machine-readable identifier can be placed on each page. </paragraph>
<paragraph id="P-0009" lvl="0"><number>&lsqb;0009&rsqb;</number> As indicated by <cross-reference target="DRAWINGS">FIG. 2</cross-reference>, an electronic version <highlight><bold>32</bold></highlight> of the text portion <highlight><bold>12</bold></highlight> of the original document is stored in memory <highlight><bold>34</bold></highlight> such as a hard-disk of a word processor, personal computer or other computer system <highlight><bold>36</bold></highlight>. The electronic version <highlight><bold>32</bold></highlight> includes digital text corresponding to the printed text portion <highlight><bold>12</bold></highlight> of the original document. The machine-readable identifiers <highlight><bold>18</bold></highlight>, if any, are stored in the memory <highlight><bold>34</bold></highlight> and are associated with the electronic version <highlight><bold>32</bold></highlight> of the document. An optical scanner <highlight><bold>18</bold></highlight> is coupled to the processor <highlight><bold>36</bold></highlight>. </paragraph>
<paragraph id="P-0010" lvl="0"><number>&lsqb;0010&rsqb;</number> For purposes of illustration, it is assumed that an individual makes one or more handwritten annotations on the original printed document <highlight><bold>10</bold></highlight> resulting in an annotated document <highlight><bold>10</bold></highlight>A (<cross-reference target="DRAWINGS">FIG. 3</cross-reference>). The annotations <highlight><bold>16</bold></highlight> may include, for example, comments or suggestions by a person reviewing the document. In another scenario, the annotations <highlight><bold>16</bold></highlight> may include notes made on a document handed out at a meeting. The annotations <highlight><bold>16</bold></highlight> may include other handwritten notes, comments or suggestions that relate in some way to the printed text portion <highlight><bold>12</bold></highlight> of the document. </paragraph>
<paragraph id="P-0011" lvl="0"><number>&lsqb;0011&rsqb;</number> As shown in <cross-reference target="DRAWINGS">FIGS. 4 and 5</cross-reference>, the printed version of the document <highlight><bold>10</bold></highlight>A with the handwritten annotation <highlight><bold>16</bold></highlight> is scanned <highlight><bold>100</bold></highlight> by the scanner <highlight><bold>18</bold></highlight>. An electronic image <highlight><bold>20</bold></highlight> of the scanned document is retained by the system&apos;s memory <highlight><bold>34</bold></highlight>. A keypad (not shown) coupled to the scanner <highlight><bold>18</bold></highlight> can be used to enter information that identifies the document as well as the person who made the annotations. </paragraph>
<paragraph id="P-0012" lvl="0"><number>&lsqb;0012&rsqb;</number> In an alternative implementation, instead of scanning the document, the electronic image <highlight><bold>20</bold></highlight> can be formed by using high resolution digital photographic techniques. </paragraph>
<paragraph id="P-0013" lvl="0"><number>&lsqb;0013&rsqb;</number> Instructions, which may be implemented, for example, as a software program <highlight><bold>22</bold></highlight> residing in memory, cause the system <highlight><bold>36</bold></highlight> to process the image <highlight><bold>20</bold></highlight> of the scanned document <highlight><bold>10</bold></highlight>A as described below. The program <highlight><bold>22</bold></highlight> identifies <highlight><bold>102</bold></highlight> printed portions of the scanned document <highlight><bold>10</bold></highlight>A from the image <highlight><bold>20</bold></highlight> and also identifies <highlight><bold>104</bold></highlight> handwritten portions of the document. The printed portions <highlight><bold>12</bold></highlight> of the document <highlight><bold>10</bold></highlight>A can be identified based, for example, on characteristics that tend to distinguish printed information from handwritten information. In some situations, the printed information <highlight><bold>12</bold></highlight> is likely to be uniform. Thus, spacings between words, between lines and between paragraphs are likely to be consistent throughout the document. Similarly, the printed letters are likely to share font attributes such as ascenders, descenders and curves. Furthermore, the printed information <highlight><bold>12</bold></highlight> is likely to be neat. One or both margins are likely to be aligned, and lines are likely to be horizontal and parallel. Those or similar characteristics can be used to identify the printed portions of the annotated document <highlight><bold>10</bold></highlight>A based on the stored electronic image <highlight><bold>20</bold></highlight>. </paragraph>
<paragraph id="P-0014" lvl="0"><number>&lsqb;0014&rsqb;</number> To facilitate analysis of the electronic image <highlight><bold>20</bold></highlight>, image processing techniques can be applied in conjunction with Hough transforms so that each line of text printed in a particular size is transformed into a horizontal line. The software <highlight><bold>22</bold></highlight> then would analyze the resulting lines to determine their uniformity. Similarly, templates based on font attributes can be applied to each line of text to ascertain uniformity and, thereby, classify elements as printed or non-printed text. Some templates may be based, for example, on the curves of letters such as &ldquo;d,&rdquo; &ldquo;b,&rdquo; and &ldquo;p,&rdquo; on the descenders in letters such as &ldquo;g&rdquo; and &ldquo;j,&rdquo; or on the ascenders in letters such as &ldquo;h,&rdquo; &ldquo;d&rdquo; and &ldquo;b.&rdquo;</paragraph>
<paragraph id="P-0015" lvl="0"><number>&lsqb;0015&rsqb;</number> The handwritten annotations can be identified, for example, by a lack of some or all of the foregoing characteristics. </paragraph>
<paragraph id="P-0016" lvl="0"><number>&lsqb;0016&rsqb;</number> The software <highlight><bold>22</bold></highlight> identifies <highlight><bold>106</bold></highlight> a part of the printed portion <highlight><bold>12</bold></highlight> of the scanned document <highlight><bold>10</bold></highlight>A with which a particular annotation is associated. The part of the printed document with which the annotation is associated may be, for example, a particular page, a particular paragraph, a particular sentence, a particular phrase or a particular word. The machine-readable identifiers <highlight><bold>14</bold></highlight> (if any) can be used in conjunction with the information previously stored in memory <highlight><bold>34</bold></highlight> to facilitate identification of the document and page <highlight><bold>24</bold></highlight> (<cross-reference target="DRAWINGS">FIG. 4</cross-reference>) on which the annotation appears. Proofing conventions can be used to associate the annotation with a particular line or other section of the printed text <highlight><bold>12</bold></highlight>. </paragraph>
<paragraph id="P-0017" lvl="0"><number>&lsqb;0017&rsqb;</number> For example, as illustrated in <cross-reference target="DRAWINGS">FIG. 3</cross-reference>, underlining may indicate that the annotation <highlight><bold>16</bold></highlight> is associated with the underlined text <highlight><bold>17</bold></highlight>. Proofing conventions, such as vertical lines in the margin and highlighted or circled words, can be used to associate the annotation <highlight><bold>16</bold></highlight> with a particular section of the printed text <highlight><bold>12</bold></highlight>. Other proofing conventions may include the use of a caret to indicate an insertion point, an arrow to associate comments with particular words or phrases. A combination of line recognition and pattern recognition techniques can be used to find and interpret such symbols. In the absence of such marks, the annotation <highlight><bold>16</bold></highlight> simply can be associated with an adjacent or closest line of printed text <highlight><bold>12</bold></highlight>. </paragraph>
<paragraph id="P-0018" lvl="0"><number>&lsqb;0018&rsqb;</number> After identifying a particular location of the text portion <highlight><bold>12</bold></highlight> of the scanned image <highlight><bold>20</bold></highlight> that is associated with a specific annotation <highlight><bold>16</bold></highlight>, an optical character recognition (OCR) technique can be applied <highlight><bold>108</bold></highlight> to the text in the identified location. The OCR technique transforms the text in the particular location of the image to digital text. For example, if the software program <highlight><bold>22</bold></highlight> identifies the underlined text <highlight><bold>17</bold></highlight> (<cross-reference target="DRAWINGS">FIG. 3</cross-reference>) as the location in the scanned image with which the annotation <highlight><bold>16</bold></highlight> is associated, an optical character recognition technique can be used to transform that part of the image to digital text. In the illustrated example, the underlined section of the image would be transformed into digital text that reads &ldquo;printed text m.&rdquo; The software program <highlight><bold>22</bold></highlight> then searches <highlight><bold>110</bold></highlight> the electronic version <highlight><bold>32</bold></highlight> of the original document <highlight><bold>10</bold></highlight> to locate the text or selective word pattern <highlight><bold>26</bold></highlight> (<cross-reference target="DRAWINGS">FIG. 4</cross-reference>) corresponding to the digital text. </paragraph>
<paragraph id="P-0019" lvl="0"><number>&lsqb;0019&rsqb;</number> The previously-identified handwritten annotation <highlight><bold>16</bold></highlight> in the scanned image <highlight><bold>20</bold></highlight> is transformed <highlight><bold>112</bold></highlight> to a digital form <highlight><bold>28</bold></highlight> (<cross-reference target="DRAWINGS">FIG. 4</cross-reference>). Preferably, handwriting recognition is applied to the handwritten portion <highlight><bold>16</bold></highlight>. The handwritten portion <highlight><bold>16</bold></highlight> is thereby transformed to digital text. Handwriting recognition software packages are available, for example, from Parascript LLC in Niwot Colo., although other handwriting recognition software can be used as well. To improve the handwriting recognition, skew analysis can be applied to determine the orientation of the handwritten portion <highlight><bold>16</bold></highlight>. The corresponding image can be rotated before applying handwriting recognition. Hough transforms also can be used to facilitate application of the handwriting recognition. </paragraph>
<paragraph id="P-0020" lvl="0"><number>&lsqb;0020&rsqb;</number> In some cases, the handwriting recognition software may be unable to determine the text corresponding to the handwritten annotation <highlight><bold>16</bold></highlight>. In situations where the handwritten portion <highlight><bold>16</bold></highlight> cannot be transformed to corresponding digital text, a digital image corresponding to the handwritten portion can be used instead. </paragraph>
<paragraph id="P-0021" lvl="0"><number>&lsqb;0021&rsqb;</number> The software <highlight><bold>22</bold></highlight> relates <highlight><bold>114</bold></highlight> the digital text or image <highlight><bold>28</bold></highlight> of the handwritten annotation <highlight><bold>16</bold></highlight> to the text in the electronic version <highlight><bold>32</bold></highlight> of the original document <highlight><bold>10</bold></highlight>. The digital form <highlight><bold>28</bold></highlight> of the annotation, as well as the correlation between the digital form of the annotation and the corresponding section of the original document, can be stored in the system&apos;s memory <highlight><bold>34</bold></highlight>. That allows an electronic version of the annotated document <highlight><bold>30</bold></highlight> (<cross-reference target="DRAWINGS">FIG. 4</cross-reference>) to be stored, where each annotation is correlated to the particular part of the digital text associated with that annotation. </paragraph>
<paragraph id="P-0022" lvl="0"><number>&lsqb;0022&rsqb;</number> In some implementations, one or more of the following advantages may be provided. Handwritten notes, comments, suggestions and other annotations from multiple sources can be stored electronically and can be associated with the corresponding digital text of the original document. Annotations associated with a particular portion of the original document can be accessed and viewed on a display <highlight><bold>38</bold></highlight>. For example, when the text of the original document <highlight><bold>10</bold></highlight> is viewed on the display <highlight><bold>38</bold></highlight>, the portion of the text associated with an annotation can appear in highlighted form to indicate that an annotation has been stored in connection with that part of the text. The annotation can be viewed by pointing at the highlighted text using an electronic mouse to cause the text or image of the annotation to appear, for example, in a pop-up screen on the display <highlight><bold>38</bold></highlight>. The name of the person who made the annotation also can appear in the pop-up screen. If the annotation has been transformed to digital text, it can be edited and/or incorporated into a revised electronic version of the original document. The techniques can, therefore, facilitate storage and retrieval of handwritten annotations as well as editing of electronically-stored documents. </paragraph>
<paragraph id="P-0023" lvl="0"><number>&lsqb;0023&rsqb;</number> Various features of the system can be implemented in hardware, software, or a combination of hardware and software. For example, some features of the system can be implemented in computer programs executing on programmable computers. Each program can be implemented in a high level procedural or object-oriented programming language to communicate with a computer system. Furthermore, each such computer program can be stored on a storage medium, such as read-only-memory (ROM) readable by a general or special purpose programmable computer or processor, for configuring and operating the computer when the storage medium is read by the computer to perform the function described above. </paragraph>
<paragraph id="P-0024" lvl="0"><number>&lsqb;0024&rsqb;</number> Other implementations are within the scope of the following claims. </paragraph>
</section>
</detailed-description>
</subdoc-description>
<subdoc-claims>
<heading lvl="1">what is claimed is: </heading>
<claim id="CLM-00001">
<claim-text><highlight><bold>1</bold></highlight>. An apparatus comprising: 
<claim-text>memory; </claim-text>
<claim-text>a processor coupled to the memory and configured to: 
<claim-text>receive an electronic image of a document that includes a printed text portion and a handwritten portion; </claim-text>
<claim-text>identify a part of the printed text portion in the image as being associated with the handwritten portion; and </claim-text>
<claim-text>store in the memory a correlation between a digital version of the handwritten portion and digital text representing the previously-identified part of the printed text portion. </claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002">
<claim-text><highlight><bold>2</bold></highlight>. The apparatus of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference> wherein the processor is configured to identify a portion of the electronic image that represents printed text and identify a portion of the electronic image that represents a handwritten annotation. </claim-text>
</claim>
<claim id="CLM-00003">
<claim-text><highlight><bold>3</bold></highlight>. The apparatus of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference> wherein the processor is configured to apply optical character recognition to transform the previously-identified part of the printed text portion to digital text. </claim-text>
</claim>
<claim id="CLM-00004">
<claim-text><highlight><bold>4</bold></highlight>. The apparatus of <dependent-claim-reference depends_on="CLM-00003">claim 3</dependent-claim-reference> wherein the processor is configured to search a digital text version stored in the memory for the digital text corresponding to the previously-identified part of the printed text portion. </claim-text>
</claim>
<claim id="CLM-00005">
<claim-text><highlight><bold>5</bold></highlight>. The apparatus of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference> wherein the processor is configured to: 
<claim-text>generate a digital image corresponding to the handwritten portion; and </claim-text>
<claim-text>store in the memory a correlation between the digital image and the digital text that represents the previously-identified part of the printed text portion. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00006">
<claim-text><highlight><bold>6</bold></highlight>. The apparatus of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference> wherein the processor is configured to: 
<claim-text>generate digital text corresponding to the handwritten portion; and </claim-text>
<claim-text>store in the memory a correlation between the digital text representing the handwritten portion and the digital text representing the previously-identified part of the printed text portion. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00007">
<claim-text><highlight><bold>7</bold></highlight>. The apparatus of <dependent-claim-reference depends_on="CLM-00006">claim 6</dependent-claim-reference> wherein the processor is configured to apply handwriting recognition to the handwritten portion to generate the digital text representing the handwritten portion. </claim-text>
</claim>
<claim id="CLM-00008">
<claim-text><highlight><bold>8</bold></highlight>. The apparatus of <dependent-claim-reference depends_on="CLM-00007">claim 7</dependent-claim-reference> wherein the processor is configured to apply skew analysis to the handwritten portion prior to applying handwriting recognition. </claim-text>
</claim>
<claim id="CLM-00009">
<claim-text><highlight><bold>9</bold></highlight>. The apparatus of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference> wherein the processor is configured to: 
<claim-text>identify a portion of the scanned image that represents the printed text and identify a portion of the scanned image that represents the handwritten portion; </claim-text>
<claim-text>apply optical character recognition to transform the previously-identified part of the printed text portion of the image to digital text; </claim-text>
<claim-text>search a digital text version stored in the memory for the digital text representing the previously-identified part of the printed text portion; </claim-text>
<claim-text>transform the handwritten portion to digital text; and </claim-text>
<claim-text>store in the memory a correlation between the digital text representing the handwritten portion and the particular digital text corresponding to the previously-identified part of the printed text portion. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00010">
<claim-text><highlight><bold>10</bold></highlight>. The apparatus of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference> wherein the processor is configured to identify a particular paragraph, a particular sentence, a particular phrase or a particular word in the printed text portion of the image as the part of the printed text portion associated with the handwritten portion. </claim-text>
</claim>
<claim id="CLM-00011">
<claim-text><highlight><bold>11</bold></highlight>. A method comprising: 
<claim-text>forming an electronic image of a document comprising a printed text portion and a handwritten portion; </claim-text>
<claim-text>identifying a part of the printed text portion in the image as being associated with the handwritten portion; and </claim-text>
<claim-text>storing a correlation between a digital version of the handwritten portion and digital text representing the previously-identified part of the printed text portion. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00012">
<claim-text><highlight><bold>12</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00011">claim 11</dependent-claim-reference> including identifying a portion of the electronic image that represents printed text and identifying a portion of the electronic image that represents a handwritten annotation. </claim-text>
</claim>
<claim id="CLM-00013">
<claim-text><highlight><bold>13</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00011">claim 11</dependent-claim-reference> including applying optical character recognition to transform the previously-identified part of the printed text portion to digital text. </claim-text>
</claim>
<claim id="CLM-00014">
<claim-text><highlight><bold>14</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00011">claim 13</dependent-claim-reference> including searching a digital text version that represents the printed text portion of the document for the digital text corresponding to the previously-identified part of the printed text portion. </claim-text>
</claim>
<claim id="CLM-00015">
<claim-text><highlight><bold>15</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00011">claim 11</dependent-claim-reference> including: 
<claim-text>generating a digital image corresponding to the handwritten portion; and </claim-text>
<claim-text>storing a correlation between the digital image and the digital text that represents the previously-identified part of the printed text portion. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00016">
<claim-text><highlight><bold>16</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00011">claim 11</dependent-claim-reference> including: 
<claim-text>generating digital text corresponding to the handwritten portion; and </claim-text>
<claim-text>storing a correlation between the digital text representing the handwritten portion and the digital text representing the previously-identified part of the printed text portion. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00017">
<claim-text><highlight><bold>17</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00011">claim 16</dependent-claim-reference> wherein generating digital text representing the handwritten portion includes applying handwriting recognition to the handwritten portion. </claim-text>
</claim>
<claim id="CLM-00018">
<claim-text><highlight><bold>18</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00011">claim 17</dependent-claim-reference> including applying skew analysis to the handwritten portion prior to applying the handwriting recognition. </claim-text>
</claim>
<claim id="CLM-00019">
<claim-text><highlight><bold>19</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00011">claim 11</dependent-claim-reference> including: 
<claim-text>identifying a portion of the electronic image that represents the printed text and identifying a portion of the electronic image that represents the handwritten portion; </claim-text>
<claim-text>applying optical character recognition to transform the previously-identified part of the printed text portion of the image to digital text; </claim-text>
<claim-text>searching a digital text version that represents the printed text portion of the document for the digital text representing the previously-identified part of the printed text portion; </claim-text>
<claim-text>transforming the handwritten portion to digital text; and </claim-text>
<claim-text>storing a correlation between the digital text representing the handwritten portion and the digital text corresponding to the previously-identified part of the printed text portion. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00020">
<claim-text><highlight><bold>20</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00011">claim 11</dependent-claim-reference> wherein identifying a part of the printed text portion in the image as being associated with the handwritten portion includes identifying a particular paragraph, a particular sentence, a particular phrase or a particular word in the printed text portion of the image. </claim-text>
</claim>
<claim id="CLM-00021">
<claim-text><highlight><bold>21</bold></highlight>. An apparatus comprising: 
<claim-text>a scanner for generating an electronic image of a document that includes a printed text portion and a handwritten portion; and </claim-text>
<claim-text>a processor coupled to the scanner and configured to: 
<claim-text>identify a part of the printed text portion in the image as being associated with the handwritten portion; and </claim-text>
<claim-text>store a correlation between a digital version of the handwritten portion and digital text representing the previously-identified part of the printed text portion. </claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00022">
<claim-text><highlight><bold>22</bold></highlight>. The apparatus of <dependent-claim-reference depends_on="CLM-00022">claim 21</dependent-claim-reference> wherein the processor is configured to identify a portion of the electronic image that represents printed text and identify a portion of the electronic image that represents a handwritten annotation. </claim-text>
</claim>
<claim id="CLM-00023">
<claim-text><highlight><bold>23</bold></highlight>. The apparatus of <dependent-claim-reference depends_on="CLM-00022">claim 21</dependent-claim-reference> wherein the processor is configured to apply optical character recognition to transform the previously-identified part of the printed text portion to digital text. </claim-text>
</claim>
<claim id="CLM-00024">
<claim-text><highlight><bold>24</bold></highlight>. The apparatus of <dependent-claim-reference depends_on="CLM-00022">claim 23</dependent-claim-reference> wherein the processor is configured to search a digital text version that represents the printed text portion of the document for the digital text corresponding to the previously-identified part of the printed text portion. </claim-text>
</claim>
<claim id="CLM-00025">
<claim-text><highlight><bold>25</bold></highlight>. The apparatus of <dependent-claim-reference depends_on="CLM-00022">claim 21</dependent-claim-reference> wherein the processor is configured to: 
<claim-text>generate a digital image corresponding to the handwritten portion; and </claim-text>
<claim-text>store a correlation between the digital image and the digital text that represents the previously-identified part of the printed text portion. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00026">
<claim-text><highlight><bold>26</bold></highlight>. The apparatus of <dependent-claim-reference depends_on="CLM-00022">claim 21</dependent-claim-reference> wherein the processor is configured to: 
<claim-text>generate digital text corresponding to the handwritten portion; and </claim-text>
<claim-text>store a correlation between the digital text representing the handwritten portion and the digital text representing the previously-identified part of the printed text portion. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00027">
<claim-text><highlight><bold>27</bold></highlight>. The apparatus of <dependent-claim-reference depends_on="CLM-00022">claim 26</dependent-claim-reference> wherein the processor is configured to apply handwriting recognition to the handwritten portion to generate the digital text representing the handwritten portion. </claim-text>
</claim>
<claim id="CLM-00028">
<claim-text><highlight><bold>28</bold></highlight>. The apparatus of <dependent-claim-reference depends_on="CLM-00022">claim 27</dependent-claim-reference> wherein the processor is configured to apply skew analysis to the handwritten portion prior to applying handwriting recognition. </claim-text>
</claim>
<claim id="CLM-00029">
<claim-text><highlight><bold>29</bold></highlight>. The apparatus of <dependent-claim-reference depends_on="CLM-00022">claim 21</dependent-claim-reference> wherein the processor is configured to: 
<claim-text>identify a portion of the scanned image that represents the printed text and identify a portion of the scanned image that represents the handwritten portion; </claim-text>
<claim-text>apply optical character recognition to transform the previously-identified part of the printed text portion of the image to digital text; </claim-text>
<claim-text>search a digital text version that represents the printed text portion of the document for the digital text representing the previously-identified part of the printed text portion; </claim-text>
<claim-text>transform the handwritten portion to digital text; and </claim-text>
<claim-text>store a correlation between the digital text representing the handwritten portion and the particular digital text corresponding to the previously-identified part of the printed text portion. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00030">
<claim-text><highlight><bold>30</bold></highlight>. The apparatus of <dependent-claim-reference depends_on="CLM-00022">claim 21</dependent-claim-reference> wherein the processor is configured to identify a particular paragraph, a particular sentence, a particular phrase or a particular word in the printed text portion of the image as the part of the printed text portion associated with the handwritten portion. </claim-text>
</claim>
<claim id="CLM-00031">
<claim-text><highlight><bold>31</bold></highlight>. An article comprising a computer-readable medium storing computer-executable instructions for causing a computer system to: 
<claim-text>in response to obtaining an electronic image of a document that includes a printed text portion and a handwritten portion, identify a part of the printed text portion in the image as being associated with the handwritten portion; and </claim-text>
<claim-text>store a correlation between a digital version of the handwritten portion and digital text representing the previously-identified part of the printed text portion. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00032">
<claim-text><highlight><bold>32</bold></highlight>. The article of <dependent-claim-reference depends_on="CLM-00033">claim 31</dependent-claim-reference> including instructions for causing the computer system to identify a portion of the electronic image that represents printed text and identify a portion of the electronic image that represents a handwritten annotation. </claim-text>
</claim>
<claim id="CLM-00033">
<claim-text><highlight><bold>33</bold></highlight>. The article of <dependent-claim-reference depends_on="CLM-00033">claim 31</dependent-claim-reference> including instructions for causing the computer system to apply optical character recognition to transform the previously-identified part of the printed text portion to digital text. </claim-text>
</claim>
<claim id="CLM-00034">
<claim-text><highlight><bold>34</bold></highlight>. The article of <dependent-claim-reference depends_on="CLM-00033">claim 33</dependent-claim-reference> including instructions for causing the computer system to search a digital text version that represents the printed text portion of the document for the digital text corresponding to the previously-identified part of the printed text portion. </claim-text>
</claim>
<claim id="CLM-00035">
<claim-text><highlight><bold>35</bold></highlight>. The article of <dependent-claim-reference depends_on="CLM-00033">claim 31</dependent-claim-reference> including instructions for causing the computer system: 
<claim-text>generate a digital image corresponding to the handwritten portion; and </claim-text>
<claim-text>store a correlation between the digital image and the digital text that represents the previously-identified part of the printed text portion. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00036">
<claim-text><highlight><bold>36</bold></highlight>. The article of <dependent-claim-reference depends_on="CLM-00033">claim 31</dependent-claim-reference> including instructions for causing the computer system to: 
<claim-text>generate digital text corresponding to the handwritten portion; and </claim-text>
<claim-text>store a correlation between the digital text representing the handwritten portion and the digital text representing the previously-identified part of the printed text portion. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00037">
<claim-text><highlight><bold>37</bold></highlight>. The article of <dependent-claim-reference depends_on="CLM-00033">claim 36</dependent-claim-reference> including instructions for causing the computer system to apply handwriting recognition to the handwritten portion to generate the digital text representing the handwritten portion. </claim-text>
</claim>
<claim id="CLM-00038">
<claim-text><highlight><bold>38</bold></highlight>. The article of <dependent-claim-reference depends_on="CLM-00033">claim 37</dependent-claim-reference> including instructions for causing the computer system to apply skew analysis to the handwritten portion prior to applying handwriting recognition. </claim-text>
</claim>
<claim id="CLM-00039">
<claim-text><highlight><bold>39</bold></highlight>. The article of <dependent-claim-reference depends_on="CLM-00033">claim 31</dependent-claim-reference> including instructions for causing the computer system to: 
<claim-text>identify a portion of the scanned image that represents the printed text and identify a portion of the scanned image that represents the handwritten portion; </claim-text>
<claim-text>apply optical character recognition to transform the previously-identified part of the printed text portion of the image to digital text; </claim-text>
<claim-text>search a digital text version that represents the printed text portion of the document for the digital text representing the previously-identified part of the printed text portion; </claim-text>
<claim-text>transform the handwritten portion to digital text; and </claim-text>
<claim-text>store a correlation between the digital text representing the handwritten portion and the particular digital text corresponding to the previously-identified part of the printed text portion. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00040">
<claim-text><highlight><bold>40</bold></highlight>. The article of <dependent-claim-reference depends_on="CLM-00033">claim 31</dependent-claim-reference> including instructions for causing the computer system to identify a particular paragraph, a particular sentence, a particular phrase or a particular word in the printed text portion of the image as the part of the printed text portion associated with the handwritten portion.</claim-text>
</claim>
</subdoc-claims>
<subdoc-drawings id="DRAWINGS">
<heading lvl="0" align="CENTER">Drawings</heading>
<representative-figure>3</representative-figure>
<figure id="figure-D00000">
<image id="EMI-D00000" file="US20030004991A1-20030102-D00000.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00001">
<image id="EMI-D00001" file="US20030004991A1-20030102-D00001.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00002">
<image id="EMI-D00002" file="US20030004991A1-20030102-D00002.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00003">
<image id="EMI-D00003" file="US20030004991A1-20030102-D00003.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00004">
<image id="EMI-D00004" file="US20030004991A1-20030102-D00004.TIF" imf="TIFF" ti="DR"/>
</figure>
</subdoc-drawings>
</patent-application-publication>
