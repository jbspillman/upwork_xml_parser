<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE patent-application-publication SYSTEM "pap-v16-2002-01-01.dtd" [
<!ENTITY US20030001908A1-20030102-D00000.TIF SYSTEM "US20030001908A1-20030102-D00000.TIF" NDATA TIF>
<!ENTITY US20030001908A1-20030102-D00001.TIF SYSTEM "US20030001908A1-20030102-D00001.TIF" NDATA TIF>
<!ENTITY US20030001908A1-20030102-D00002.TIF SYSTEM "US20030001908A1-20030102-D00002.TIF" NDATA TIF>
<!ENTITY US20030001908A1-20030102-D00003.TIF SYSTEM "US20030001908A1-20030102-D00003.TIF" NDATA TIF>
]>
<patent-application-publication>
<subdoc-bibliographic-information>
<document-id>
<doc-number>20030001908</doc-number>
<kind-code>A1</kind-code>
<document-date>20030102</document-date>
</document-id>
<publication-filing-type>new</publication-filing-type>
<domestic-filing-data>
<application-number>
<doc-number>09896199</doc-number>
</application-number>
<application-number-series-code>09</application-number-series-code>
<filing-date>20010629</filing-date>
</domestic-filing-data>
<technical-information>
<classification-ipc>
<classification-ipc-primary>
<ipc>G09G005/00</ipc>
</classification-ipc-primary>
<classification-ipc-edition>07</classification-ipc-edition>
</classification-ipc>
<classification-us>
<classification-us-primary>
<uspc>
<class>345</class>
<subclass>863000</subclass>
</uspc>
</classification-us-primary>
</classification-us>
<title-of-invention>Picture-in-picture repositioning and/or resizing based on speech and gesture control</title-of-invention>
</technical-information>
<inventors>
<first-named-inventor>
<name>
<given-name>Eric</given-name>
<family-name>Cohen-solal</family-name>
</name>
<residence>
<residence-us>
<city>Ossining</city>
<state>NY</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</first-named-inventor>
</inventors>
<assignee>
<organization-name>Koninklijke Philips Electronics N.V.</organization-name>
<assignee-type>02</assignee-type>
</assignee>
<correspondence-address>
<name-1>Corporate Patent Counsel;</name-1>
<name-2>U.S. Philips Corporation</name-2>
<address>
<address-1>580 White Plains Road</address-1>
<city>Tarrytown</city>
<state>NY</state>
<postalcode>10591</postalcode>
<country>
<country-code>US</country-code>
</country>
</address>
</correspondence-address>
</subdoc-bibliographic-information>
<subdoc-abstract>
<paragraph id="A-0001" lvl="0">A video display device having a picture-in-picture (PIP) display, an audio input device, an image input device, and a processor. The device utilizes a combination of an audio indication and a related gesture from a user to control PIP display characteristics such as a position of the PIP within a display and the size of the PIP. A microphone captures the audio indication and the processor performs a recognition act to determine that a PIP control command is intended from the user. Thereafter, the camera captures an image or a series of images of the user including at least some portion of the user containing a gesture. The processor then identifies the gesture and affects a PIP display characteristic in response to the combined audio indication and gesture. </paragraph>
</subdoc-abstract>
<subdoc-description>
<summary-of-invention>
<section>
<heading lvl="1">FIELD OF THE INVENTION </heading>
<paragraph id="P-0001" lvl="0"><number>&lsqb;0001&rsqb;</number> This invention generally relates to a method and device to enhance home television usage. Specifically, the present invention relates to a picture-in-picture display (PIP) that may be repositioned and/or resized. </paragraph>
</section>
<section>
<heading lvl="1">BACKGROUND OF THE INVENTION </heading>
<paragraph id="P-0002" lvl="0"><number>&lsqb;0002&rsqb;</number> It is very common for televisions to have a capability of displaying more than one video display on the television display at the same time. Typically, the display is separated into two or more portions wherein a main portion of the display is dedicated to a first video data stream (e.g., a given television channel). A second video data stream is simultaneously shown in a display box that is shown as an inset over the display of the first data stream. This inset box is typically denoted as a picture-in-picture display (&ldquo;PIP&rdquo;). This PIP provides the functionality for a television viewer to monitor two or more video data streams at the same time. This may be desirable for instance at a time when a commercial segment has started on a given television channel and a viewer wishes to &ldquo;surf&rdquo; additional selected television channels during the commercial segment, yet does not wish to miss a return from the commercial segment. At other times, a viewer may wish to search for other video content or just view the other content without missing content on another selected channel. </paragraph>
<paragraph id="P-0003" lvl="0"><number>&lsqb;0003&rsqb;</number> In any event, PIP has a problem in that the PIP is typically shown in an inset box that is overlaid on top of a primary display. The overlaid PIP has the undesirable effect of obscuring a portion of the primary display. </paragraph>
<paragraph id="P-0004" lvl="0"><number>&lsqb;0004&rsqb;</number> In prior art systems, the PIP may be resized utilizing a remote control input so that the user may decide what size to make the PIP to avoid obscuring portions of the underlying video images. In other systems, a user may utilize the remote control to move the PIP to pre-selected or variably selectable portions of the video screen. However, these systems are unwieldy and confusing for a user to operate. </paragraph>
<paragraph id="P-0005" lvl="0"><number>&lsqb;0005&rsqb;</number> In some systems, it is shown that a television may be responsive to voice control to control television functions such as channel selection and volume control. However, these systems have problems in that users are not familiar with voice control and the voice recognition systems have problems in discerning between different control features. In addition, oftentimes there may be voice signals that are not intended as control commands. </paragraph>
<paragraph id="P-0006" lvl="0"><number>&lsqb;0006&rsqb;</number> In the art of computer vision there are known systems that respond to gestures of a user to control features of a given system but again these systems are difficult to manipulate and may erroneously detect gestures by users that may not be intended as a control gesture. </paragraph>
<paragraph id="P-0007" lvl="0"><number>&lsqb;0007&rsqb;</number> Accordingly, it is an object of the present invention to overcome the disadvantages of the prior art. </paragraph>
</section>
<section>
<heading lvl="1">SUMMARY OF THE INVENTION </heading>
<paragraph id="P-0008" lvl="0"><number>&lsqb;0008&rsqb;</number> The present invention is a system having a video display device, such as a television, with a picture-in-picture (PIP) display and a processor. The system further has both an audio input device, such as a microphone, and a video input device, such as a camera for operation in accordance with the present invention. </paragraph>
<paragraph id="P-0009" lvl="0"><number>&lsqb;0009&rsqb;</number> The system utilizes a combination of an audio indication and a related gesture from a user to control PIP display characteristics such as a position of the PIP within the display and the size of the PIP. The microphone captures the audio indication and the processor performs a recognition act to determine that a PIP control command is intended from the user. Thereafter, the camera captures an image or a series of images of the user including at least some portion of the user containing a gesture. The processor then identifies the gesture and affects a PIP display characteristic in response to the combined audio indication and gesture.</paragraph>
</section>
</summary-of-invention>
<brief-description-of-drawings>
<section>
<heading lvl="1">BRIEF DESCRIPTION OF THE DRAWINGS </heading>
<paragraph id="P-0010" lvl="0"><number>&lsqb;0010&rsqb;</number> The following are descriptions of embodiments of the present invention that when taken in conjunction with the following drawings will demonstrate the above noted features and advantages, as well as further ones. It should be expressly understood that the drawings are included for illustrative purposes and do not represent the scope of the present invention that is defined by the appended claims. The invention is best understood in conjunction with the accompanying drawings in which: </paragraph>
<paragraph id="P-0011" lvl="0"><number>&lsqb;0011&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 1</cross-reference> shows an illustrative system in accordance with an embodiment of the present invention; </paragraph>
<paragraph id="P-0012" lvl="0"><number>&lsqb;0012&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 2</cross-reference> shows a flow diagram illustrating an operation in accordance with an embodiment of the present invention; and </paragraph>
<paragraph id="P-0013" lvl="0"><number>&lsqb;0013&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 3</cross-reference> shows a flow diagram illustrating a setup procedure that may be utilized in accordance with an embodiment of the present invention for training the system to recognize audio indications and/or gestures.</paragraph>
</section>
</brief-description-of-drawings>
<detailed-description>
<section>
<heading lvl="1">DETAILED DESCRIPTION OF THE INVENTION </heading>
<paragraph id="P-0014" lvl="0"><number>&lsqb;0014&rsqb;</number> In the discussion to follow, certain terms will be illustratively discussed in regard to specific embodiments or systems to facilitate the discussion. As would be readily apparent to a person of ordinary skill in the art, these terms should be understood to encompass other similar known terms wherein the present invention may be readily applied. </paragraph>
<paragraph id="P-0015" lvl="0"><number>&lsqb;0015&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 1</cross-reference> shows an illustrative system <highlight><bold>100</bold></highlight> in accordance with an embodiment of the present invention including a display <highlight><bold>110</bold></highlight>, operatively coupled to a processor <highlight><bold>120</bold></highlight>, and a remote control device <highlight><bold>130</bold></highlight>. The processor <highlight><bold>120</bold></highlight> and the remote control device <highlight><bold>130</bold></highlight> are operatively coupled as is known in the art via an infrared (IR) receiver <highlight><bold>125</bold></highlight>, operatively coupled to the processor <highlight><bold>120</bold></highlight>, and an IR transmitter <highlight><bold>131</bold></highlight>, operatively coupled to the remote control device <highlight><bold>130</bold></highlight>. </paragraph>
<paragraph id="P-0016" lvl="0"><number>&lsqb;0016&rsqb;</number> The display <highlight><bold>110</bold></highlight> may be a television receiver or other device enabled to reproduce audiovisual content for a user to view and listen to. The processor <highlight><bold>120</bold></highlight> is operable to produce a picture-in-picture display (PIP) on the display <highlight><bold>110</bold></highlight> as is known by a person of ordinary skill in the art. Further, the processor <highlight><bold>120</bold></highlight> is operable to provide, position, and size a PIP display in accordance with the present invention. </paragraph>
<paragraph id="P-0017" lvl="0"><number>&lsqb;0017&rsqb;</number> The remote control device <highlight><bold>130</bold></highlight> contains buttons that operate as is known in the art. Specifically, the remote control device <highlight><bold>130</bold></highlight> contains a PIP button <highlight><bold>134</bold></highlight>, a swap button <highlight><bold>132</bold></highlight>, and PIP position control buttons <highlight><bold>137</bold></highlight>A, <highlight><bold>137</bold></highlight>B, <highlight><bold>137</bold></highlight>C, <highlight><bold>137</bold></highlight>D. The PIP button <highlight><bold>134</bold></highlight> may be utilized to initiate a PIP function to open a PIP on the display <highlight><bold>110</bold></highlight>. The swap button <highlight><bold>132</bold></highlight> swaps each of a PIP image and a primary display image which may be shown on the display <highlight><bold>110</bold></highlight>. The PIP position control buttons <highlight><bold>137</bold></highlight>A, <highlight><bold>137</bold></highlight>B, <highlight><bold>137</bold></highlight>C, <highlight><bold>137</bold></highlight>D enable a user to manually reposition the PIP over selectable portions of the display <highlight><bold>110</bold></highlight>. The remote control <highlight><bold>130</bold></highlight> may also contain other control buttons, as is known in the art, such as channel selector keys <highlight><bold>139</bold></highlight>A, <highlight><bold>139</bold></highlight>B and <highlight><bold>138</bold></highlight>A, <highlight><bold>138</bold></highlight>B for selecting the video data streams respectively for the PIP and a primary display image. </paragraph>
<paragraph id="P-0018" lvl="0"><number>&lsqb;0018&rsqb;</number> As would be obvious to a person of ordinary skill in the art, although the buttons <highlight><bold>138</bold></highlight>A, <highlight><bold>138</bold></highlight>B, <highlight><bold>139</bold></highlight>A, <highlight><bold>139</bold></highlight>B are illustratively shown as channel selector buttons, the buttons <highlight><bold>138</bold></highlight>A, <highlight><bold>138</bold></highlight>B, <highlight><bold>139</bold></highlight>A, <highlight><bold>139</bold></highlight>B may also select from amongst a plurality of video data streams from one or more other sources of video. For instance, one source of either video data stream (e.g., the PIP and the primary display image) may be a broadcast video data stream while another source may be a storage device. The storage device may be a tape storage device (e.g., VHS analog tape), a digital storage device such as a hard drive, an optical storage device, etc., or any other type of known device for storing a video data stream. In fact, any source of a video data stream for either of the PIP and the primary display image may be utilized in accordance with the present invention without deviating from the scope of the present invention. </paragraph>
<paragraph id="P-0019" lvl="0"><number>&lsqb;0019&rsqb;</number> However, as stated above, the remote control device is confusing and difficult to utilize for manipulation of the PIP. In addition, oftentimes, the PIP needs to be manipulated, such as resized or moved, in response to changes in the primary display image. For example, the area of interest in the primary display image may change as transitions in scenes of the primary display image occur. </paragraph>
<paragraph id="P-0020" lvl="0"><number>&lsqb;0020&rsqb;</number> In accordance with the present invention, to facilitate manipulation of the PIP and more specifically, a display characteristic of the PIP (e.g., size, position, etc.), the processor is also operatively coupled to an audio input device, such as a microphone <highlight><bold>122</bold></highlight> and an image input device, such as a camera <highlight><bold>124</bold></highlight>. The microphone <highlight><bold>122</bold></highlight> and the camera <highlight><bold>124</bold></highlight> are respectively utilized to capture audio indications and related gestures from a user <highlight><bold>140</bold></highlight> to facilitate control of the PIP. </paragraph>
<paragraph id="P-0021" lvl="0"><number>&lsqb;0021&rsqb;</number> Specifically, in accordance with the present invention, a combination of an audio indication <highlight><bold>142</bold></highlight> followed by a related gesture <highlight><bold>144</bold></highlight> are utilized by the system <highlight><bold>100</bold></highlight> to control the PIP. This series of the audio indication <highlight><bold>142</bold></highlight> followed by the gesture <highlight><bold>144</bold></highlight> may also be utilized to activate (e.g., turn on) the PIP. The audio indication <highlight><bold>142</bold></highlight> and the gesture <highlight><bold>144</bold></highlight> are related such that the system <highlight><bold>100</bold></highlight> can distinguish between audio indications and gestures of a user that are not intended for PIP control. Specifically, this combination of the audio indication <highlight><bold>142</bold></highlight> followed by the gesture <highlight><bold>144</bold></highlight> helps prevent false activation of the system <highlight><bold>100</bold></highlight> in response to spurious background audio and gesture indications that may occur due to the users activity in and around the area where the system <highlight><bold>100</bold></highlight> is located. </paragraph>
<paragraph id="P-0022" lvl="0"><number>&lsqb;0022&rsqb;</number> Further, the audio indication <highlight><bold>142</bold></highlight> and the gesture <highlight><bold>144</bold></highlight> are related such that the system <highlight><bold>100</bold></highlight> may distinguish between PIP size and position related commands. Specifically, a given gesture may be related to two or more different audio indications. For example, an audio indication of &ldquo;PIP SIZE&rdquo; followed by a &ldquo;THUMBS UP&rdquo; gesture may be utilized by a user to increase the size of the PIP. However, an audio indication of &ldquo;PIP POSITION&rdquo; followed by a &ldquo;THUMBS UP&rdquo; gesture may be utilized to reposition the PIP in an upward direction. Further operation of the present invention will be described herein with regard to <cross-reference target="DRAWINGS">FIGS. 2 and 3</cross-reference>. <cross-reference target="DRAWINGS">FIG. 2</cross-reference> shows a flow diagram <highlight><bold>200</bold></highlight> in accordance with an embodiment of the present invention. As illustrated in the flow diagram in <cross-reference target="DRAWINGS">FIG. 2</cross-reference>, during act <highlight><bold>205</bold></highlight>, the user <highlight><bold>140</bold></highlight> provides the audio indication <highlight><bold>142</bold></highlight> to the system <highlight><bold>100</bold></highlight> and specifically, to the microphone input <highlight><bold>122</bold></highlight>. The audio indication indicates to the system <highlight><bold>100</bold></highlight> that a PIP related command is intended by the user and specifically, indicates which PIP manipulation is desired. The system <highlight><bold>100</bold></highlight> will continue to receive and interpret audio input until a recognized audio indication is received. By the term recognized, what is intended is that the system <highlight><bold>100</bold></highlight> must receive an audio indication that is known by the system <highlight><bold>100</bold></highlight> to be related to PIP display characteristic manipulations. </paragraph>
<paragraph id="P-0023" lvl="0"><number>&lsqb;0023&rsqb;</number> The audio indication <highlight><bold>142</bold></highlight> may be a simple one-word term such as an utterance of &ldquo;PIP&rdquo; by the user <highlight><bold>140</bold></highlight> to simply indicate that a PIP related gesture <highlight><bold>144</bold></highlight> would follow. As stated above, the combinations of audio indications and gestures are related such that for a given audio indication, one or more following gestures are expected by the system <highlight><bold>100</bold></highlight>. In the case of a simple audio indication such as &ldquo;PIP&rdquo;, a following gesture should indicate to the system the PIP related manipulation expected. For example, a finger (e.g., thumb) indication pointing up, down, left, right, diagonal, etc. may be a gesture to indicate a desired position for the PIP. </paragraph>
<paragraph id="P-0024" lvl="0"><number>&lsqb;0024&rsqb;</number> This combination of an audio indication followed by a related gesture may also turn on a PIP that has not previously been turned on by a separate audio indication and related gesture, or by the remote control <highlight><bold>130</bold></highlight>. Other gestures may be utilized to indicate that a PIP size related command is intended such as two fingers held close together to indicate a desire to reduce the size of the PIP, etc. The user may utilize two fingers held far apart to indicate a desire to increase the size of the PIP. </paragraph>
<paragraph id="P-0025" lvl="0"><number>&lsqb;0025&rsqb;</number> It should be understood that the above examples of audio indications and gestures are presented merely to facilitate the explanation of the operation of the present invention and should not be considered limitations thereto. Many combinations of audio indications and corresponding gestures would be readily apparent to a person of ordinary skill in the art. Accordingly, the above examples should not be understood to limite the scope of the appended claims. </paragraph>
<paragraph id="P-0026" lvl="0"><number>&lsqb;0026&rsqb;</number> The audio indication may also be more complex multiple word utterances, such &ldquo;PIP SIZE&rdquo; that indicates to the system <highlight><bold>100</bold></highlight> that the following related gesture is intended as a command to change the PIP sizing. In any event, in act <highlight><bold>210</bold></highlight> the processor <highlight><bold>120</bold></highlight> tries to recognize the audio indication as a PIP related audio indication. This recognition act in addition to a gesture recognition act will be further described below. In the event wherein the audio indication is not recognized as a PIP related audio indication, then as shown in <cross-reference target="DRAWINGS">FIG. 2</cross-reference>, the processor <highlight><bold>120</bold></highlight> returns to act <highlight><bold>205</bold></highlight> and continues to monitor audio indications until a PIP related audio indication is recognized. </paragraph>
<paragraph id="P-0027" lvl="0"><number>&lsqb;0027&rsqb;</number> When an audio indication is recognized by the system <highlight><bold>100</bold></highlight>, then during act <highlight><bold>230</bold></highlight> the processor <highlight><bold>120</bold></highlight> may acquire an image or a sequence of images of the user <highlight><bold>140</bold></highlight> through use of the camera <highlight><bold>124</bold></highlight>. There are known systems for acquiring and recognizing a gesture of a user. For example, a publication entitled &ldquo;Vision-Based Gesture Recognition: A Review&rdquo; by Ying Wu and Thomas S. Huang, from Proceedings of International Gesture Workshop 1999 on Gesture-Based Communication in Human Computer Interaction, describes a use of gestures for control functions. This article is incorporated herein by reference as if set forth in its entirety herein. </paragraph>
<paragraph id="P-0028" lvl="0"><number>&lsqb;0028&rsqb;</number> In general, there are two general types of systems for recognizing a gesture. In one system, generally referred to as hand posture recognition, the camera <highlight><bold>124</bold></highlight> may acquire one image or a sequence of a few images to determine an intended gesture by the user. This type of system generally makes a static assessment of a gesture by a user. In another known system, the camera <highlight><bold>124</bold></highlight> may acquire a sequence of images to dynamically determine a gesture. This type of recognition system is generally referred to as dynamic/temporal gesture recognition. In some systems, dynamic gesture recognition is performed by analyzing the trajectory of the hand and thereafter comparing this trajectory to learned models of trajectories corresponding to specific gestures. A general overview of the process of learning gestures and audio indications will be discussed further herein below with references to <cross-reference target="DRAWINGS">FIG. 3</cross-reference>. </paragraph>
<paragraph id="P-0029" lvl="0"><number>&lsqb;0029&rsqb;</number> As should be clear to a person of ordinary skill in the art, there are many known ways of training systems to recognize speech. There are also many known ways for training a system to recognize gestures, both statically and dynamically. The below discussion is presented herein merely for illustrative purposes. Accordingly, the present invention should be understood to encompass these other known systems. </paragraph>
<paragraph id="P-0030" lvl="0"><number>&lsqb;0030&rsqb;</number> In any event, after the camera <highlight><bold>124</bold></highlight> acquires an image or a sequence of images, during act <highlight><bold>240</bold></highlight>, the processor <highlight><bold>120</bold></highlight> tries to identify the gesture. When the processor <highlight><bold>120</bold></highlight> does not identify the gesture, the processor returns to act <highlight><bold>230</bold></highlight> to acquire an additional image or sequence of images of the user <highlight><bold>140</bold></highlight>. After a predetermined number of attempts at determining a known gesture from the image or sequence of images without a known gesture being recognized, the processor <highlight><bold>120</bold></highlight> may during act <highlight><bold>250</bold></highlight> provide an indication to the user <highlight><bold>140</bold></highlight> that the gesture was not recognized. This indication may be in the form of an audio signal from a speaker <highlight><bold>128</bold></highlight> or may be a visual signal from the display <highlight><bold>110</bold></highlight>. In this or other embodiments, after a number of tries, the system may return to act <highlight><bold>205</bold></highlight> to await an other audio indication. </paragraph>
<paragraph id="P-0031" lvl="0"><number>&lsqb;0031&rsqb;</number> When the processor <highlight><bold>120</bold></highlight> identifies the gesture, during act <highlight><bold>260</bold></highlight> the processor <highlight><bold>120</bold></highlight> determines a requested PIP manipulation by querying a memory <highlight><bold>126</bold></highlight>. The memory <highlight><bold>126</bold></highlight> may be configured as a look-up table that stores gestures that the system <highlight><bold>100</bold></highlight> may recognize along with corresponding PIP manipulations. During act <highlight><bold>270</bold></highlight>, after the requested PIP manipulation is retrieved from the memory <highlight><bold>126</bold></highlight>, the processor <highlight><bold>120</bold></highlight> performs the requested PIP manipulation. The system then returns to act <highlight><bold>205</bold></highlight> to await a further audio indication from the user <highlight><bold>140</bold></highlight>. </paragraph>
<paragraph id="P-0032" lvl="0"><number>&lsqb;0032&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 3</cross-reference> shows an illustrative flow diagram of acts that may be utilized in training the system <highlight><bold>100</bold></highlight> to recognize speech and gesture inputs. Although the specific systems, algorithms, etc. for recognizing speech and voice are very different, the general acts are somewhat similar. Specifically, in act <highlight><bold>310</bold></highlight> the speech or gesture training system elicits and captures one or more input samples for each expected audio indication or recognizable gesture. What is intended by the term &ldquo;elicits&rdquo; is that the system prompts the user to provide a particular input sample. </paragraph>
<paragraph id="P-0033" lvl="0"><number>&lsqb;0033&rsqb;</number> Thereafter, in act <highlight><bold>320</bold></highlight>, the system associates the one or more captured input samples for each expected audio indication or recognizable gesture with a label identifying the one or more input samples. In act, <highlight><bold>330</bold></highlight>, the one or more labeled input samples are provided to a classifier (e.g., processor <highlight><bold>120</bold></highlight>) to derive models that are then utilized for recognizing user indications. </paragraph>
<paragraph id="P-0034" lvl="0"><number>&lsqb;0034&rsqb;</number> In one embodiment, this training may be performed directly by the system <highlight><bold>100</bold></highlight> interacting with a user during a setup procedure. In another embodiment, this training may by performed generally once for a group of systems and the results of the training (e.g., the models derived therefrom) may be stored in the memory <highlight><bold>126</bold></highlight>. In yet another embodiment, the group of systems may be trained once with the results stored in the memory <highlight><bold>126</bold></highlight>, and thereafter, each system may elicit further input/training from the user to refine the models. </paragraph>
<paragraph id="P-0035" lvl="0"><number>&lsqb;0035&rsqb;</number> Finally, the above-discussion is intended to be merely illustrative of the present invention. Numerous alternative embodiments may be devised by those having ordinary skill in the art without departing from the spirit and scope of the following claims. For example, although the processor <highlight><bold>120</bold></highlight> is shown separate from the display <highlight><bold>110</bold></highlight>, clearly both may be combined in a single display device such as a television. In addition, the processor may be a dedicated processor for performing in accordance with the present invention or may be a general purpose processor wherein only one of many functions operate for performing in accordance with the present invention. In addition, the processor may operate utilizing a program portion, multiple program segments, or may be a hardware device utilizing a dedicated or multi-purpose integrated circuit. </paragraph>
<paragraph id="P-0036" lvl="0"><number>&lsqb;0036&rsqb;</number> Also, although the invention is described above with regard to a PIP on a television display, the present invention may be suitably utilized with any display device that has the ability to display a primary image and a PIP including a computer monitor or any other known display device. </paragraph>
<paragraph id="P-0037" lvl="0"><number>&lsqb;0037&rsqb;</number> Numerous alternative embodiments may be devised by those having ordinary skill in the art without departing from the spirit and scope of the following claims. In interpreting the appended claims, it should be understood that: </paragraph>
<paragraph id="P-0038" lvl="2"><number>&lsqb;0038&rsqb;</number> a) the word &ldquo;comprising&rdquo; does not exclude the presence of other elements or acts than those listed in a given claim; </paragraph>
<paragraph id="P-0039" lvl="2"><number>&lsqb;0039&rsqb;</number> b) the word &ldquo;a&rdquo; or &ldquo;an&rdquo; preceding an element does not exclude the presence of a plurality of such elements; </paragraph>
<paragraph id="P-0040" lvl="2"><number>&lsqb;0040&rsqb;</number> c) any reference signs in the claims do not limit their scope; and </paragraph>
<paragraph id="P-0041" lvl="2"><number>&lsqb;0041&rsqb;</number> d) several &ldquo;means&rdquo; may be represented by the same item or hardware or software implemented structure or function. </paragraph>
</section>
</detailed-description>
</subdoc-description>
<subdoc-claims>
<heading lvl="1">The claimed invention is: </heading>
<claim id="CLM-00001">
<claim-text><highlight><bold>1</bold></highlight>. A video display device comprising: 
<claim-text>a display configured to display a primary image and a picture-in-picture image (PIP) overlaying the primary image; </claim-text>
<claim-text>a processor operatively coupled to the display and configured to receive a first video data stream for the primary image, to receive a second video data stream for the PIP, and to change a PIP display characteristic in response to a received audio indication and a related gesture from a user. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00002">
<claim-text><highlight><bold>2</bold></highlight>. The video display device of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein the PIP display characteristic is at least one of a position of the PIP on the display and a display size of the PIP. </claim-text>
</claim>
<claim id="CLM-00003">
<claim-text><highlight><bold>3</bold></highlight>. The video display device of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, comprising: 
<claim-text>a microphone for receiving the audio indication from the user; and </claim-text>
<claim-text>a camera for acquiring an image of the user containing the related gesture. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00004">
<claim-text><highlight><bold>4</bold></highlight>. The video display device of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference> wherein the processor is configured to analyze audio information received from the user to identify when a PIP related audio indication is intended by the user. </claim-text>
</claim>
<claim id="CLM-00005">
<claim-text><highlight><bold>5</bold></highlight>. The video display device of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein the processor is configured to analyze image information received from the user after the audio indication is received to identify the change in the PIP display characteristic that is expressed by the received gesture. </claim-text>
</claim>
<claim id="CLM-00006">
<claim-text><highlight><bold>6</bold></highlight>. The video display device of <dependent-claim-reference depends_on="CLM-00005">claim 5</dependent-claim-reference>, wherein the image information is contained in a sequence of images and wherein the processor is configured to analyze the sequence of images to determine the received gesture. </claim-text>
</claim>
<claim id="CLM-00007">
<claim-text><highlight><bold>7</bold></highlight>. The video display device of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein the image information is contained in a sequence of images and wherein the processor is configured to determine the received gesture by analyzing the sequence of images and determining a trajectory of a hand of the user. </claim-text>
</claim>
<claim id="CLM-00008">
<claim-text><highlight><bold>8</bold></highlight>. The video display device of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein the processor is configured to determine the received gesture by analyzing an image of the user and determining a posture of a hand of the user. </claim-text>
</claim>
<claim id="CLM-00009">
<claim-text><highlight><bold>9</bold></highlight>. The video display device of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein the video display device is a television. </claim-text>
</claim>
<claim id="CLM-00010">
<claim-text><highlight><bold>10</bold></highlight>. The video display device of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein the image is a sequence of images of the user containing the user gesture, the video display device comprising a camera for acquiring the sequence of images of the user. </claim-text>
</claim>
<claim id="CLM-00011">
<claim-text><highlight><bold>11</bold></highlight>. A method of controlling a display characteristic of a picture-in-picture display (PIP) overlaying a primary display, the method comprising: 
<claim-text>receiving an audio indication from a user; </claim-text>
<claim-text>determining whether the received audio indication is one of a plurality of expected audio indications; </claim-text>
<claim-text>analyzing a gesture of the user if the received audio indication is one of the plurality of expected audio indications; and </claim-text>
<claim-text>controlling the display characteristic if the gesture is a gesture related to the received audio indication. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00012">
<claim-text><highlight><bold>12</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00011">claim 11</dependent-claim-reference>, wherein analyzing the gesture comprises: 
<claim-text>receiving a sequence of images; and </claim-text>
<claim-text>analyzing the sequence of images to determine the gesture. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00013">
<claim-text><highlight><bold>13</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00011">claim 11</dependent-claim-reference>, wherein analyzing the gesture comprises: 
<claim-text>receiving a sequence of images; </claim-text>
<claim-text>analyzing the sequence of images to determine a trajectory of a hand of the user; and </claim-text>
<claim-text>determining the gesture by the determined trajectory. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00014">
<claim-text><highlight><bold>14</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00011">claim 11</dependent-claim-reference>, wherein analyzing the gesture comprises: 
<claim-text>analyzing an image of the user to determine a posture of a hand of the user; and </claim-text>
<claim-text>determining the gesture by the determined posture. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00015">
<claim-text><highlight><bold>15</bold></highlight>. A program segment stored on a processor readable medium for controlling a display characteristic of a picture-in-picture display (PIP) overlaying a primary display, the program segment comprising: 
<claim-text>a program segment for controlling receipt of an audio indication; </claim-text>
<claim-text>a program segment for determining whether a received audio indication is one of a plurality of stored audio indications; </claim-text>
<claim-text>a program segment for analyzing a gesture of the user if the received audio indication is one of the plurality of stored audio indications; and </claim-text>
<claim-text>a program segment for controlling the display characteristic if the gesture is a gesture related to the received audio indication. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00016">
<claim-text><highlight><bold>16</bold></highlight>. The program segment of <dependent-claim-reference depends_on="CLM-00011">claim 15</dependent-claim-reference>, wherein the program segment for analyzing the gesture comprises: 
<claim-text>a program segment for controlling receipt of a sequence of images; and </claim-text>
<claim-text>a program segment for analyzing the sequence of images to determine the gesture. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00017">
<claim-text><highlight><bold>17</bold></highlight>. The program segment of <dependent-claim-reference depends_on="CLM-00011">claim 15</dependent-claim-reference>, wherein the program segment for analyzing the gesture comprises: 
<claim-text>a program segment for controlling receipt of a sequence of images; </claim-text>
<claim-text>a program segment for analyzing the sequence of images to determine a trajectory of a hand of the user; and </claim-text>
<claim-text>a program segment for determining the gesture by the determined trajectory. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00018">
<claim-text><highlight><bold>18</bold></highlight>. The program segment of <dependent-claim-reference depends_on="CLM-00011">claim 15</dependent-claim-reference>, wherein the program segment for analyzing the gesture comprises: 
<claim-text>a program segment for analyzing an image of the user to determine a posture of a hand of the user; and </claim-text>
<claim-text>a program segment for determining the gesture by the determined posture.</claim-text>
</claim-text>
</claim>
</subdoc-claims>
<subdoc-drawings id="DRAWINGS">
<heading lvl="0" align="CENTER">Drawings</heading>
<representative-figure>2</representative-figure>
<figure id="figure-D00000">
<image id="EMI-D00000" file="US20030001908A1-20030102-D00000.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00001">
<image id="EMI-D00001" file="US20030001908A1-20030102-D00001.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00002">
<image id="EMI-D00002" file="US20030001908A1-20030102-D00002.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00003">
<image id="EMI-D00003" file="US20030001908A1-20030102-D00003.TIF" imf="TIFF" ti="DR"/>
</figure>
</subdoc-drawings>
</patent-application-publication>
