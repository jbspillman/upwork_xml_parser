<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE patent-application-publication SYSTEM "pap-v16-2002-01-01.dtd" [
<!ENTITY US20030005439A1-20030102-D00000.TIF SYSTEM "US20030005439A1-20030102-D00000.TIF" NDATA TIF>
<!ENTITY US20030005439A1-20030102-D00001.TIF SYSTEM "US20030005439A1-20030102-D00001.TIF" NDATA TIF>
<!ENTITY US20030005439A1-20030102-D00002.TIF SYSTEM "US20030005439A1-20030102-D00002.TIF" NDATA TIF>
<!ENTITY US20030005439A1-20030102-D00003.TIF SYSTEM "US20030005439A1-20030102-D00003.TIF" NDATA TIF>
<!ENTITY US20030005439A1-20030102-D00004.TIF SYSTEM "US20030005439A1-20030102-D00004.TIF" NDATA TIF>
<!ENTITY US20030005439A1-20030102-D00005.TIF SYSTEM "US20030005439A1-20030102-D00005.TIF" NDATA TIF>
<!ENTITY US20030005439A1-20030102-D00006.TIF SYSTEM "US20030005439A1-20030102-D00006.TIF" NDATA TIF>
<!ENTITY US20030005439A1-20030102-D00007.TIF SYSTEM "US20030005439A1-20030102-D00007.TIF" NDATA TIF>
<!ENTITY US20030005439A1-20030102-D00008.TIF SYSTEM "US20030005439A1-20030102-D00008.TIF" NDATA TIF>
]>
<patent-application-publication>
<subdoc-bibliographic-information>
<document-id>
<doc-number>20030005439</doc-number>
<kind-code>A1</kind-code>
<document-date>20030102</document-date>
</document-id>
<publication-filing-type>new</publication-filing-type>
<domestic-filing-data>
<application-number>
<doc-number>09896391</doc-number>
</application-number>
<application-number-series-code>09</application-number-series-code>
<filing-date>20010629</filing-date>
</domestic-filing-data>
<technical-information>
<classification-ipc>
<classification-ipc-primary>
<ipc>G06F003/00</ipc>
</classification-ipc-primary>
<classification-ipc-secondary>
<ipc>H04N005/445</ipc>
</classification-ipc-secondary>
<classification-ipc-secondary>
<ipc>G06F013/00</ipc>
</classification-ipc-secondary>
<classification-ipc-secondary>
<ipc>G09G005/00</ipc>
</classification-ipc-secondary>
<classification-ipc-edition>07</classification-ipc-edition>
</classification-ipc>
<classification-us>
<classification-us-primary>
<uspc>
<class>725</class>
<subclass>037000</subclass>
</uspc>
</classification-us-primary>
<classification-us-secondary>
<uspc>
<class>345</class>
<subclass>664000</subclass>
</uspc>
</classification-us-secondary>
<classification-us-secondary>
<uspc>
<class>345</class>
<subclass>679000</subclass>
</uspc>
</classification-us-secondary>
<classification-us-secondary>
<uspc>
<class>345</class>
<subclass>706000</subclass>
</uspc>
</classification-us-secondary>
<classification-us-secondary>
<uspc>
<class>345</class>
<subclass>757000</subclass>
</uspc>
</classification-us-secondary>
<classification-us-secondary>
<uspc>
<class>345</class>
<subclass>836000</subclass>
</uspc>
</classification-us-secondary>
<classification-us-secondary>
<uspc>
<class>345</class>
<subclass>810000</subclass>
</uspc>
</classification-us-secondary>
<classification-us-secondary>
<uspc>
<class>345</class>
<subclass>852000</subclass>
</uspc>
</classification-us-secondary>
</classification-us>
<title-of-invention>Subscriber television system user interface with a virtual reality media space</title-of-invention>
</technical-information>
<inventors>
<first-named-inventor>
<name>
<given-name>Luis</given-name>
<middle-name>A.</middle-name>
<family-name>Rovira</family-name>
</name>
<residence>
<residence-us>
<city>Atlanta</city>
<state>GA</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</first-named-inventor>
</inventors>
<correspondence-address>
<name-1>SCIENTIFIC-ATLANTA, INC.</name-1>
<name-2>INTELLECTUAL PROPERTY DEPARTMENT</name-2>
<address>
<address-1>5030 SUGARLOAF PARKWAY</address-1>
<city>LAWRENCEVILLE</city>
<state>GA</state>
<postalcode>30044</postalcode>
<country>
<country-code>US</country-code>
</country>
</address>
</correspondence-address>
</subdoc-bibliographic-information>
<subdoc-abstract>
<paragraph id="A-0001" lvl="0">This invention provides a method for a subscriber television system client device to provide a three-dimensional user interface comprising a virtual reality media space. The three-dimensional user interface allows the user to navigate a three dimensional environment, participate in activities, and interact with other users. The three-dimensional user interface enables the user to associate personal characteristics with an avatar which represents the user in the system, such personal characteristics comprising a symbol, a picture, and video. </paragraph>
</subdoc-abstract>
<subdoc-description>
<summary-of-invention>
<section>
<heading lvl="1">TECHNICAL FIELD </heading>
<paragraph id="P-0001" lvl="0"><number>&lsqb;0001&rsqb;</number> The present invention relates generally to the field of subscriber television systems, and in particular, to user interfaces for subscriber television systems. </paragraph>
</section>
<section>
<heading lvl="1">BACKGROUND OF THE INVENTION </heading>
<paragraph id="P-0002" lvl="0"><number>&lsqb;0002&rsqb;</number> The explosion of technology in the world of communications and computing has enabled an amazing trend in the systems that people use for enjoyment, interaction, and communication. The trend involves recreating the environment in which we live. This has been made possible by the advancement of the technology that drives communication and computing. This general trend of reality based computer interaction is evident in most areas of advancing technology. Recent advancements in processing power and transmission capability have made it possible to take one giant step closer to recreating a realistic setting in a computer enabled system. </paragraph>
<paragraph id="P-0003" lvl="0"><number>&lsqb;0003&rsqb;</number> As people begin to use computing systems to effectuate everyday occurrences, such as working, shopping, and visiting friends, a desire emanates to have these systems more closely model reality. If these systems are to enhance and expedite day-to-day activities, then they should bring about the same feeling of interaction one achieves in reality. In order for a person to truly interact in a system, they need to feel that they are a part of that system. To enable this feeling of interaction some highly advanced systems allow the users to see images of themselves and their surroundings in a simulated world, a virtual reality. Highly advanced virtual reality systems create three dimensional (3D) worlds where a user navigates and interacts in a simulated 3D environment. </paragraph>
<paragraph id="P-0004" lvl="0"><number>&lsqb;0004&rsqb;</number> In the early days of virtual reality only bulky and expensive computing devices were powerful enough to enable a 3D user interface. The recent explosion of technology in the field of computing has advanced such that it is possible to have relatively inexpensive computing devices enable 3D virtual reality user interfaces. These 3D virtual reality worlds allow a user to explore a simulated environment. Such environments could be anything from an ordinary street scene with walkways, roads, and buildings to a completely fictitious landscape of an outer space planet. The end goal with most virtual reality interfaces is to give the user the most realistic experience possible. </paragraph>
<paragraph id="P-0005" lvl="0"><number>&lsqb;0005&rsqb;</number> One significant advancement in the effort to bring realism to virtual reality applications involves the use of avatars. As mentioned previously, one cannot truly interact within a system unless they feel they are a part of that system. An avatar is a tool that allows the user to have an icon inside the virtual reality world to represent the user. The word avatar specifically means an incarnation of a Hindu deity or an incarnation in human form. The term avatar applied to the world of virtual reality defines an entity generated by the virtual reality software application to represent a user. In current systems the avatar is an action or comic figure chosen by the user to represent the user&apos;s personality. The avatars can take human shape, the shape of an animal, or even a monster. When using an avatar-enabled 3D virtual reality interface, a user selects an avatar that can be unique with respect to that user. In this manner, the user can see the user as the avatar in the system and watch as the user navigates the landscape and interacts with the avatars of the other users. The avatar enables the user to feel part of the reality of the 3D environment. </paragraph>
<paragraph id="P-0006" lvl="0"><number>&lsqb;0006&rsqb;</number> Numerous 3D virtual reality worlds as described above exist in various types of applications. The first widespread use of such 3D worlds was in localized games and soon expanded into multi-user virtual worlds with the advent of the internet. Many of the internet systems that enable a user to experience 3D virtual worlds today involve such things as shopping and chatting. Some internet sites allow a user to pick or create an avatar and join the virtual world for a set fee per month. As a subscriber to this world the user can log on as the user&apos;s avatar, walk through the aisles and shops of a virtual mall, and purchase items. Other internet sites allow a user to assume an avatar representation and walk the halls of a live 3D chat environment. In such instances the user can logon and walk to certain desired rooms to see and meet the avatar representations of their friends. </paragraph>
<paragraph id="P-0007" lvl="0"><number>&lsqb;0007&rsqb;</number> Although numerous 3D virtual worlds exist on the internet today, they are severely limited. As previously mentioned, it is the goal of these systems to recreate normal every day occurrences and interactions as close to reality as possible. The systems that exist today are far from reality. Although the technology dictating computing processing power has significantly advanced, many other factors plague attempts at widely available virtual reality. In order to provide a realistic experience, one needs high quality communications and multimedia equipment. The majority of users accessing the internet today do so with a computer over a limited internet connection using an analog modem operating in the miniscule baseband spectrum of Plain Old Telephone Service (POTS). Creating a realistic virtual world interface is extremely difficult using the connection speeds of an analog modem. Reality comes through interaction with a multitude of users and events, thus virtual reality is significantly improved in systems where high bandwidth is available. </paragraph>
<paragraph id="P-0008" lvl="0"><number>&lsqb;0008&rsqb;</number> A realistic experience and interaction involves appealing to as many senses as possible. The most important of these senses are sight and sound. If a user is not visually and audibly stimulated by the given surroundings, then that user can have a harder time feeling as if the user is a part of a system. </paragraph>
<paragraph id="P-0009" lvl="0"><number>&lsqb;0009&rsqb;</number> Another problem with 3D virtual reality systems today is the fact that so many variations exist. With numerous vendors, versions of software, and client modifications it is difficult to achieve a standard for numerous users. The voluminous nature of virtual reality (VR) applications makes it difficult for users to congregate. A user essentially has no consistent and controlled arena in which to work, shop, and play. </paragraph>
</section>
<section>
<heading lvl="1">SUMMARY OF THE INVENTION </heading>
<paragraph id="P-0010" lvl="0"><number>&lsqb;0010&rsqb;</number> This invention provides a system and/or method for a subscriber television system client device to provide a user interface comprising a three dimensional virtual reality media space. The client device exists within a three dimensional virtual reality media space system implemented in a subscriber television network. The user interface enables the user to navigate a three dimensional environment. </paragraph>
<paragraph id="P-0011" lvl="0"><number>&lsqb;0011&rsqb;</number> Other systems, methods, features, and advantages of the invention will be or become apparent to one with skill in the art upon examination of the following figures and detailed description. It is intended that all such additional systems, methods, features, and advantages be included within this description, be within the scope of the invention, and be protected by the accompanying claims.</paragraph>
</section>
</summary-of-invention>
<brief-description-of-drawings>
<section>
<heading lvl="1">BRIEF DESCRIPTION OF THE SEVERAL VIEWS OF THE DRAWINGS </heading>
<paragraph id="P-0012" lvl="0"><number>&lsqb;0012&rsqb;</number> The accompanying drawings, incorporated in and forming a part of the specification, illustrate several aspects of the present invention, and together with the description serve to explain the principles of the invention. The components in the drawings are not necessarily to scale, emphasis instead being placed upon clearly illustrating the principles of the present invention. Moreover, in the drawings, like reference numerals designate corresponding parts throughout the several views. The reference numbers in the drawings have at least three digits with the two rightmost digits being reference numbers within a figure. The digits to the left of those digits are the number of the figure in which the item identified by the reference number first appears. For example, an item with reference number <highlight><bold>209</bold></highlight> first appears in <cross-reference target="DRAWINGS">FIG. 2</cross-reference>. In the drawings: </paragraph>
<paragraph id="P-0013" lvl="0"><number>&lsqb;0013&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 1A</cross-reference> is a diagram of a high level view of the architecture in accordance with one preferred embodiment of the present invention; </paragraph>
<paragraph id="P-0014" lvl="0"><number>&lsqb;0014&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 1B</cross-reference> is a diagram depicting the lower level details of a portion of the high level view of the architecture depicted in <cross-reference target="DRAWINGS">FIG. 1</cross-reference> in accordance with one preferred embodiment of the present invention; </paragraph>
<paragraph id="P-0015" lvl="0"><number>&lsqb;0015&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 1C</cross-reference> is a diagram depicting the lower level details of a portion of the high level view of the architecture depicted in <cross-reference target="DRAWINGS">FIG. 1</cross-reference> in accordance with one preferred embodiment of the present invention; </paragraph>
<paragraph id="P-0016" lvl="0"><number>&lsqb;0016&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 2</cross-reference> is a diagram of a snapshot of the user interface displayed by the system depicted in <cross-reference target="DRAWINGS">FIG. 1</cross-reference> in accordance with one preferred embodiment of the present invention; </paragraph>
<paragraph id="P-0017" lvl="0"><number>&lsqb;0017&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 3</cross-reference> is a diagram of a snapshot demonstrating a continuation of movement by the user from <cross-reference target="DRAWINGS">FIG. 2</cross-reference>; </paragraph>
<paragraph id="P-0018" lvl="0"><number>&lsqb;0018&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 4</cross-reference> is a diagram of a snapshot of an alternative user interface displayed by the system depicted in FIG.<highlight><bold>1</bold></highlight> in accordance with one preferred embodiment of the present invention; </paragraph>
<paragraph id="P-0019" lvl="0"><number>&lsqb;0019&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 5</cross-reference> is a diagram of a snapshot of a portal to another user interface displayed by the system depicted in <cross-reference target="DRAWINGS">FIG. 1</cross-reference> in accordance with one preferred embodiment of the present invention; </paragraph>
<paragraph id="P-0020" lvl="0"><number>&lsqb;0020&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 6</cross-reference> is a diagram describing a modification of the architecture in <cross-reference target="DRAWINGS">FIG. 1A</cross-reference> to allow additional and alternative functionality in accordance with one preferred embodiment of the present invention.</paragraph>
</section>
</brief-description-of-drawings>
<detailed-description>
<section>
<heading lvl="1">DETAILED DESCRIPTION OF THE INVENTION </heading>
<paragraph id="P-0021" lvl="0"><number>&lsqb;0021&rsqb;</number> Having summarized various aspects of the present invention, reference will now be made in detail to the description of the invention as illustrated in the drawings. While the invention will be described in connection with these drawings, there is no intent to limit it to the embodiment or embodiments disclosed therein. On the contrary, the intent is to cover all alternatives, modifications, and equivalents included within the spirit and scope of the invention as defined by the appended claims. In addition, all examples given herein are intended to be non-limiting, i.e., among other examples considered to be within the scope of the present invention. </paragraph>
<paragraph id="P-0022" lvl="0"><number>&lsqb;0022&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 1A</cross-reference> describes a general hierarchy of the architecture employed in accordance with one embodiment of the current invention. Generally <cross-reference target="DRAWINGS">FIG. 1A</cross-reference> depicts a Subscriber Television System (STS). An STS can be implemented in a variety of different environments, non-limiting examples including a cable television system, a satellite television system, a RF system, and a wireless transmission system. More specifically, the architecture displayed in <cross-reference target="DRAWINGS">FIG. 1A</cross-reference> depicts a system that enables a user to interact with a 3D Virtual Reality Media Space System <highlight><bold>170</bold></highlight>. The particular embodiment depicted in <cross-reference target="DRAWINGS">FIG. 1A</cross-reference> applies specifically to a deployment of the 3D Virtual Reality Media Space System <highlight><bold>170</bold></highlight> in an STS. The STS Headend <highlight><bold>110</bold></highlight> is responsible for most activities involving the operations, administration, maintenance, and provisioning of the 3D Virtual Reality Media Space System <highlight><bold>170</bold></highlight>. The STS Headend <highlight><bold>110</bold></highlight> transmits and receives information with the Client Devices <highlight><bold>130</bold></highlight> in the 3D Virtual Reality Media Space System <highlight><bold>170</bold></highlight> over the STS Transmission System <highlight><bold>120</bold></highlight>. In one embodiment, the Client Device <highlight><bold>1</bold></highlight> <highlight><bold>130</bold></highlight>A receives and interprets information from the STS Transmission System <highlight><bold>120</bold></highlight> regarding a 3D Virtual Reality Media Space System <highlight><bold>170</bold></highlight> communication and in turn transmits user information regarding its user activity to the network. The Client Device <highlight><bold>1</bold></highlight> <highlight><bold>130</bold></highlight>A is responsible for processing and arranging data such that it can be transmitted to a Presentation System <highlight><bold>1</bold></highlight> <highlight><bold>150</bold></highlight>A to be experienced by the user. </paragraph>
<paragraph id="P-0023" lvl="0"><number>&lsqb;0023&rsqb;</number> The Presentation System <highlight><bold>1</bold></highlight> <highlight><bold>150</bold></highlight>A can be any system that enables a user to experience a session provided by the 3D Virtual Reality Media Space System <highlight><bold>170</bold></highlight>. The Presentation System <highlight><bold>1</bold></highlight> <highlight><bold>150</bold></highlight>A can be, for example but not limited to, a television, a Personal Computer, a projection unit, a system including 3D goggles and headphones, or a simulator providing visual, audible, and physical stimulation. The Presentation System <highlight><bold>1</bold></highlight> <highlight><bold>150</bold></highlight>A processes information from the Client Device <highlight><bold>1</bold></highlight> <highlight><bold>130</bold></highlight>A and thereby the 3D VR Module <highlight><bold>140</bold></highlight>A. The Presentation System <highlight><bold>1</bold></highlight> <highlight><bold>150</bold></highlight>A processes the information such that it can be viewed, heard, felt, or otherwise presented to the senses of the user. It is through the Presentation System <highlight><bold>1</bold></highlight> <highlight><bold>150</bold></highlight>A that the user is able to experience the Virtual Reality Media Space User Interface (&ldquo;VR-MS-UI&rdquo;) <highlight><bold>100</bold></highlight>. The user is able to give commands to Client Device <highlight><bold>1</bold></highlight> <highlight><bold>130</bold></highlight>A to effectively interact with the VR-MS-UI <highlight><bold>100</bold></highlight> through a Client Command Device <highlight><bold>160</bold></highlight>A. The commands given by the Client Command Device <highlight><bold>160</bold></highlight>A dictate, among other things, the execution of certain actions within the VR-MS-UI <highlight><bold>100</bold></highlight>. With the use of the Client Command Device <highlight><bold>1</bold></highlight> <highlight><bold>160</bold></highlight>A and the Presentation System <highlight><bold>1</bold></highlight> <highlight><bold>150</bold></highlight>A the user can experience and interact with the 3D Virtual Reality Media Space System <highlight><bold>170</bold></highlight>. In an alternate embodiment of the system depicted in FIG <highlight><bold>1</bold></highlight>A, the Client Device <highlight><bold>1</bold></highlight> <highlight><bold>130</bold></highlight>A and the Presentation System <highlight><bold>1</bold></highlight> <highlight><bold>150</bold></highlight>A can be implemented in the same device. In addition, the Client Command Device <highlight><bold>1</bold></highlight> <highlight><bold>160</bold></highlight>A could be incorporated into the entity containing the Client Device <highlight><bold>1</bold></highlight> <highlight><bold>130</bold></highlight>A and Presentation System <highlight><bold>1</bold></highlight> <highlight><bold>150</bold></highlight>A. </paragraph>
<paragraph id="P-0024" lvl="0"><number>&lsqb;0024&rsqb;</number> In accordance with one embodiment, the Client Device <highlight><bold>1</bold></highlight> <highlight><bold>130</bold></highlight>A is responsible for numerous functionalities. One functionality is the operation of the 3D VR Module <highlight><bold>140</bold></highlight>A. The 3D VR Module <highlight><bold>140</bold></highlight>A is a module that is executed by a combination of software and hardware components in one embodiment of the current invention. In one embodiment, the 3D VR Module <highlight><bold>140</bold></highlight>A is completely separate from the other modules in the Client Device <highlight><bold>1</bold></highlight> <highlight><bold>130</bold></highlight>A. In one implementation of this embodiment, the 3D VR Module <highlight><bold>140</bold></highlight>A is a completely independent entity in hardware that can be inserted and removed from an expansion slot in the Client Device <highlight><bold>1</bold></highlight> <highlight><bold>130</bold></highlight>A. Thereby, the 3D VR Module <highlight><bold>140</bold></highlight>A can be upgraded through a hardware replacement. In an alternate implementation of this embodiment, the 3D VR module exists as a block of software within the Client Device <highlight><bold>1</bold></highlight> <highlight><bold>130</bold></highlight>A which can be remotely upgraded and modified by the 3D Virtual Reality Media Space System <highlight><bold>170</bold></highlight>. </paragraph>
<paragraph id="P-0025" lvl="0"><number>&lsqb;0025&rsqb;</number> The 3D VR Module <highlight><bold>140</bold></highlight>A handles the information and processing for the 3D Virtual Reality Media Space System <highlight><bold>170</bold></highlight> involving the Client Device <highlight><bold>1</bold></highlight> <highlight><bold>130</bold></highlight>A. In this manner, the 3D VR Module <highlight><bold>140</bold></highlight>A controls and directs the Presentation System <highlight><bold>1</bold></highlight> <highlight><bold>150</bold></highlight>A such that the user can interact with the 3D Virtual Reality Media Space System <highlight><bold>170</bold></highlight> and discover a plurality of media. </paragraph>
<paragraph id="P-0026" lvl="0"><number>&lsqb;0026&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 1B</cross-reference> depicts an implementation of the STS Headend <highlight><bold>110</bold></highlight>A in accordance with one embodiment of the present invention. STS Headend <highlight><bold>110</bold></highlight>A is configured to provide numerous services to the Client Devices <highlight><bold>130</bold></highlight>. One of the services is the 3D Virtual Reality Media Space System <highlight><bold>170</bold></highlight> (<cross-reference target="DRAWINGS">FIG. 1</cross-reference>). In a non-limiting example, the 3D Virtual Reality Media Space System <highlight><bold>170</bold></highlight> (<cross-reference target="DRAWINGS">FIG. 1</cross-reference>) is controlled from the headend by the 3D VR Server Application <highlight><bold>119</bold></highlight> (<cross-reference target="DRAWINGS">FIG. 1</cross-reference>). The 3D VR Server Application <highlight><bold>119</bold></highlight> is responsible for reserving and configuring system resources needed to provide 3D Virtual Reality Media Space System <highlight><bold>170</bold></highlight> (<cross-reference target="DRAWINGS">FIG. 1</cross-reference>) services and for providing configuration and service data to the 3D VR Module <highlight><bold>140</bold></highlight>A (<cross-reference target="DRAWINGS">FIG. 1</cross-reference>). </paragraph>
<paragraph id="P-0027" lvl="0"><number>&lsqb;0027&rsqb;</number> The Digital Network Control System (DNCS) <highlight><bold>113</bold></highlight> provides complete management, monitoring, and control of the network&apos;s elements and broadcast services provided to users. The DNCS <highlight><bold>113</bold></highlight> controls the Content Servers <highlight><bold>111</bold></highlight> which drive the Video &amp; Data Pumps providing media to the STS Transmission System <highlight><bold>120</bold></highlight>. In one implementation, the DNCS <highlight><bold>113</bold></highlight> uses a Data Insertion Multiplexer <highlight><bold>112</bold></highlight> and a Data QAM <highlight><bold>114</bold></highlight> to insert in-band broadcast file system (BFS) data in to a MPEG-2 transport stream that is broadcast over the STS Transmission System <highlight><bold>120</bold></highlight> to the Client Devices <highlight><bold>130</bold></highlight> (<cross-reference target="DRAWINGS">FIG. 1</cross-reference>). The Content Servers <highlight><bold>111</bold></highlight> house the Video &amp; Data Pumps which supply media to the Client Devices <highlight><bold>130</bold></highlight> (<cross-reference target="DRAWINGS">FIG. 1</cross-reference>) through the QAM Group <highlight><bold>115</bold></highlight>. The QPSK Modem <highlight><bold>117</bold></highlight> can be utilized to transport the out-of-band datagram traffic between the STS Headend <highlight><bold>110</bold></highlight>A and the Client Devices <highlight><bold>130</bold></highlight> (<cross-reference target="DRAWINGS">FIG. 1</cross-reference>). Through the use of the control and management devices in the STS Headend <highlight><bold>110</bold></highlight>A, an administrator can control the services provided by the system and more specifically the 3D VR Media Space System <highlight><bold>170</bold></highlight> (<cross-reference target="DRAWINGS">FIG. 1</cross-reference>). </paragraph>
<paragraph id="P-0028" lvl="0"><number>&lsqb;0028&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 1C</cross-reference> is a diagram depicting an implementation of one of the Client Devices <highlight><bold>130</bold></highlight> (<cross-reference target="DRAWINGS">FIG. 1</cross-reference>) in accordance with one embodiment of the current invention. The device depicted in <cross-reference target="DRAWINGS">FIG. 1C</cross-reference> is a Digital Home Communications Terminal (DHCT) <highlight><bold>130</bold></highlight>C, a specific implementation of one of the Client Devices <highlight><bold>130</bold></highlight> (<cross-reference target="DRAWINGS">FIG. 1</cross-reference>). The DHCT <highlight><bold>130</bold></highlight>C is typically situated within a residence or business of a user. It may be integrated into a device that has display unit, such as a television set, or it may be a stand alone unit that couples to an external display. The DHCT <highlight><bold>130</bold></highlight>C includes a processor <highlight><bold>131</bold></highlight> for controlling operations of the DHCT <highlight><bold>130</bold></highlight>C, a video output port such as an RF output system <highlight><bold>132</bold></highlight> for driving the Presentation System <highlight><bold>2</bold></highlight> <highlight><bold>150</bold></highlight>C , and Tuner System <highlight><bold>133</bold></highlight> for tuning into a particular television channel to be displayed for sending and receiving various types of data from the STS Headend <highlight><bold>110</bold></highlight>A (<cross-reference target="DRAWINGS">FIG. 1B</cross-reference>). The Tuner System <highlight><bold>133</bold></highlight> includes, in one implementation, an out-of-band tuner for bi-directional Quadrature Phase Shift Keying (QPSK) data communication and a Quadrature Amplitude Modulation (QAM) tuner for receiving television signals. Additionally, DHCT <highlight><bold>130</bold></highlight>C includes a receiver for receiving externally-generated information, such as user input from a Client Command Device <highlight><bold>2</bold></highlight> <highlight><bold>160</bold></highlight>C. In this implementation shown in <cross-reference target="DRAWINGS">FIG. 1</cross-reference>C, the Client Command Device <highlight><bold>2</bold></highlight> <highlight><bold>160</bold></highlight>C is a remote control. Other types of client command devices such as a keyboard, a mouse, or a voice command may also provide the user inputs. The DHCT <highlight><bold>130</bold></highlight>C may also include one or more wireless or wired communication interfaces, also called ports, for receiving and/or transmitting data to other devices. </paragraph>
<paragraph id="P-0029" lvl="0"><number>&lsqb;0029&rsqb;</number> Memory <highlight><bold>135</bold></highlight>, such as non-volatile (i.e., SRAM) and Dynamic Random Access Memory (DRAM), is coupled to the Processor <highlight><bold>131</bold></highlight> and stores operation parameters, such as commands that are recognized by the Processor <highlight><bold>131</bold></highlight>. The most basic functionality of the DHCT <highlight><bold>130</bold></highlight>C is provided by an Operating System <highlight><bold>136</bold></highlight> that operates in Memory <highlight><bold>135</bold></highlight>. One or more programmed software applications, herein referred to as applications, are executed by utilizing the computing resources in the DHCT <highlight><bold>130</bold></highlight>C. The application executable program stored in Memory <highlight><bold>135</bold></highlight> is executed by Processor <highlight><bold>131</bold></highlight> (e.g., a central processing unit or digital signal processor) under the auspices of the Operating System <highlight><bold>136</bold></highlight>. Data required as input by the application program is stored in Memory <highlight><bold>135</bold></highlight> and read by Processor <highlight><bold>131</bold></highlight> from Memory <highlight><bold>135</bold></highlight> as need be during the course of application program execution. Input data may be data stored in Memory <highlight><bold>135</bold></highlight> by a secondary application or other source, either internal or external to the DHCT <highlight><bold>130</bold></highlight>C, or may have been created with the application program at the time it was generated as a software application program. Data may be received via any of the communication ports of the DHCT <highlight><bold>130</bold></highlight>C, from the STS Headend <highlight><bold>110</bold></highlight>A via the DHCT&apos;s network interface (i.e., the QAM or out-of-band tuners) or as user input via Receiver <highlight><bold>134</bold></highlight>. A type of input data fulfills and serves the purpose of parameters as described below. Data generated by an application program is stored in Memory <highlight><bold>135</bold></highlight> by Processor <highlight><bold>131</bold></highlight> during the course of application program execution. </paragraph>
<paragraph id="P-0030" lvl="0"><number>&lsqb;0030&rsqb;</number> In accordance with the embodiment depicted in <cross-reference target="DRAWINGS">FIG. 1</cross-reference>C, the 3D VR Module <highlight><bold>140</bold></highlight>C is responsible for executing most functionality regarding the 3D VR Media Space System <highlight><bold>170</bold></highlight> (<cross-reference target="DRAWINGS">FIG. 1</cross-reference>) in relation to DHCT <highlight><bold>130</bold></highlight>C. The 3D VR Module <highlight><bold>140</bold></highlight>C, is enabled to execute in accordance with the aforementioned interactions with, among other things, the Memory <highlight><bold>135</bold></highlight>, the Processor <highlight><bold>131</bold></highlight>, and the Operating System <highlight><bold>136</bold></highlight>. The data inputted by the user with the Client Command Device <highlight><bold>2</bold></highlight> <highlight><bold>160</bold></highlight>C is interpreted by the Receiver <highlight><bold>134</bold></highlight>, stored in Memory <highlight><bold>135</bold></highlight>, and assigned to the 3D VR Module <highlight><bold>140</bold></highlight>C by the Operating System <highlight><bold>136</bold></highlight>. The 3D VR Module <highlight><bold>140</bold></highlight>C executes, on the Processor <highlight><bold>131</bold></highlight>, the commands provided by the user in addition to those received through the Communications Interface <highlight><bold>137</bold></highlight> provided by the STS Headend <highlight><bold>110</bold></highlight>A. In addition to the received commands, the 3D VR Module <highlight><bold>140</bold></highlight>C also requires that certain application specific stored information be executed by the Processor <highlight><bold>131</bold></highlight>. A non-limiting example is illustrated by the 3D VR Environment <highlight><bold>1</bold></highlight> <highlight><bold>138</bold></highlight> stored as part of the 3D VR Module <highlight><bold>140</bold></highlight>C. The 3D VR Environment <highlight><bold>1</bold></highlight> <highlight><bold>138</bold></highlight> contains pre-configured information that, once executed by the Processor <highlight><bold>131</bold></highlight>, can be outputted to the RF Output System <highlight><bold>132</bold></highlight> to be displayed on the Presentation System <highlight><bold>2</bold></highlight> <highlight><bold>150</bold></highlight>C as the background of the VR-MS-UI <highlight><bold>100</bold></highlight>. The user can interact with the shown VR-MS-UI <highlight><bold>100</bold></highlight> to explore the presently available media space. The 3D VR Module <highlight><bold>140</bold></highlight>C is enabled to store information pertaining to a specific user in the User Database <highlight><bold>139</bold></highlight>. Thereby, when a specific user accesses the 3D VR Module <highlight><bold>140</bold></highlight>C, the module can cause data associated with that user to be loaded into the Processor <highlight><bold>131</bold></highlight>, outputted on the RF Output System <highlight><bold>132</bold></highlight>, and displayed within the VR-MS-UI <highlight><bold>100</bold></highlight>. As will be shown below, the 3D VR Module <highlight><bold>140</bold></highlight>C is capable of providing numerous different kinds of VR-MS-UI <highlight><bold>100</bold></highlight> implementations. </paragraph>
<paragraph id="P-0031" lvl="0"><number>&lsqb;0031&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 2</cross-reference> is an illustration of a snapshot of the VR-MS-UI <highlight><bold>100</bold></highlight> (<cross-reference target="DRAWINGS">FIG. 1A</cross-reference>) in accordance with one embodiment of the current invention. In the embodiment of the current invention depicted in <cross-reference target="DRAWINGS">FIG. 2</cross-reference>, the VR-MS-UI <highlight><bold>100</bold></highlight>A consists of a vehicle navigating a 3D Landscape <highlight><bold>290</bold></highlight>. It is respectfully submitted that it will be clear to those of ordinary skill in the art that the navigation of the vehicle incorporates a maneuverability function that could be implemented in various alternative ways, such as driving, walking, or flying. In a non-limiting example, the navigation commands could be provided by the Client Command Device <highlight><bold>2</bold></highlight> <highlight><bold>160</bold></highlight>C (<cross-reference target="DRAWINGS">FIG. 1C</cross-reference>). The user could utilize the Navigation Pad <highlight><bold>161</bold></highlight> (<cross-reference target="DRAWINGS">FIG. 1C</cross-reference>) to maneuver around in the 3D Landscape <highlight><bold>290</bold></highlight> by pressing the up button to move forward and similar buttons for other directions. The Client Command Device <highlight><bold>2</bold></highlight> <highlight><bold>160</bold></highlight>C (<cross-reference target="DRAWINGS">FIG. 1C</cross-reference>) could also be other instruments, such as a joystick or a mouse. </paragraph>
<paragraph id="P-0032" lvl="0"><number>&lsqb;0032&rsqb;</number> This particular application of the VR-MS-UI <highlight><bold>100</bold></highlight> (<cross-reference target="DRAWINGS">FIG. 1A</cross-reference>) accepts commands from the user that dictate the way in which a virtual vehicle moves about. This virtual vehicle is shown in part by a VR Dashboard <highlight><bold>210</bold></highlight>. The snapshot in <cross-reference target="DRAWINGS">FIG. 2</cross-reference> shows the viewpoint of the avatar of the user sitting in the virtual vehicle, though the avatar itself is not shown from this perspective. The snapshot shown in <cross-reference target="DRAWINGS">FIG. 2</cross-reference> depicts the vehicle moving down the road with different attractions along the way. One of the attractions is indicated by a Movies <highlight><bold>270</bold></highlight> sign positioned along the side of the road. The Movies <highlight><bold>270</bold></highlight> sign indicates an area where the user can direct the virtual vehicle to pull over into a parking lot <highlight><bold>280</bold></highlight> and watch a movie. In one embodiment of the current invention, the roadside movie attraction would act similar to a drive-in-movie. The user would position the virtual vehicle such that the movie showing on the selected billboard <highlight><bold>260</bold></highlight> was in full screen view inside the VR-MS-UI <highlight><bold>100</bold></highlight>A. </paragraph>
<paragraph id="P-0033" lvl="0"><number>&lsqb;0033&rsqb;</number> It is respectfully submitted that one skilled in the art would recognize that movie billboard <highlight><bold>260</bold></highlight> could be one of a multitude of billboards. This would enable a number of movie screens with looping trailers to be visible from which the user could choose to view. In one embodiment, the Client Device <highlight><bold>1</bold></highlight> <highlight><bold>130</bold></highlight>A (<cross-reference target="DRAWINGS">FIG. 1A</cross-reference>) could interpret various signals contained in one RF channel, and output a movie stream or information concerning the movies to the Presentation System <highlight><bold>1</bold></highlight> <highlight><bold>150</bold></highlight>A (<cross-reference target="DRAWINGS">FIG. 1A</cross-reference>). In another embodiment, the Client Device <highlight><bold>1</bold></highlight> <highlight><bold>130</bold></highlight>A (<cross-reference target="DRAWINGS">FIG. 1A</cross-reference>) could have multiple tuners which simultaneously output multiple movie streams or information concerning the movies. In addition, these movies could be grouped on a movie billboard <highlight><bold>160</bold></highlight>, or sets of movie billboards, according to common themes or by customer selected favorite groupings. In an alternative embodiment the billboard <highlight><bold>260</bold></highlight> would display other media comprising the TV channels received by the Client Device <highlight><bold>1</bold></highlight> <highlight><bold>130</bold></highlight>A (<cross-reference target="DRAWINGS">FIG. 1</cross-reference>), advertisements, or web pages. </paragraph>
<paragraph id="P-0034" lvl="0"><number>&lsqb;0034&rsqb;</number> To implement the functions desired the user could, in part, make use of the controls on the VR Dashboard <highlight><bold>210</bold></highlight>. A World Map <highlight><bold>220</bold></highlight> is a feature that indicates the user&apos;s position within the entire current 3D virtual world in relation to other attractions and elements inside the world. This feature allows the user to be cognizant of those things outside of his/her view. The embodiment of invention in <cross-reference target="DRAWINGS">FIG. 2</cross-reference> shows the World Map <highlight><bold>220</bold></highlight> indicating the current user position with a star icon and the movies up ahead with the triangle icon. Alternatively, the user can jump to certain positions within the 3D world using the Destination Hyperlinks <highlight><bold>250</bold></highlight> feature. The Destinations Hyperlink <highlight><bold>250</bold></highlight> feature is depicted on the right side of the dashboard and allows the user to instantaneously jump to certain areas of the 3D world. In this manner, the user could choose not to personally navigate the 3D Landscape <highlight><bold>290</bold></highlight> but simply select the desired destination on the Destination Hyperlink <highlight><bold>250</bold></highlight> and go there immediately. If the user chooses to navigate the 3D Landscape <highlight><bold>290</bold></highlight> in the virtual vehicle, then the user can control the speed of movement of the virtual vehicle with the Throttle <highlight><bold>240</bold></highlight> on the VR Dashboard <highlight><bold>210</bold></highlight>. Moving the Throttle <highlight><bold>240</bold></highlight> forward would cause the virtual vehicle to traveler faster down the road ahead to another desired location. In a non-limiting example, the user would be enabled to move the Throttle <highlight><bold>240</bold></highlight> through the use of a Client Command Device <highlight><bold>2</bold></highlight> <highlight><bold>160</bold></highlight>C (<cross-reference target="DRAWINGS">FIG. 1C</cross-reference>). The user could use the Client Command Device <highlight><bold>160</bold></highlight>C <highlight><bold>2</bold></highlight> (<cross-reference target="DRAWINGS">FIG. 1C</cross-reference>) to select the Throttle <highlight><bold>240</bold></highlight> and then use the up arrow on the Navigation Pad <highlight><bold>161</bold></highlight> (<cross-reference target="DRAWINGS">FIG. 1C</cross-reference>) to throttle up and the down arrow to throttle down. Selection of the Throttle <highlight><bold>240</bold></highlight> or other elements of the VR Dashboard <highlight><bold>210</bold></highlight> could occur through any of a variety of methods. For example, a free floating arrow, similar to a conventional Personal Computer mouse pointer, could be displayed and controlled by the Navigation Pad <highlight><bold>161</bold></highlight> (<cross-reference target="DRAWINGS">FIG. 1C</cross-reference>) on the Client Command Device <highlight><bold>2</bold></highlight> <highlight><bold>160</bold></highlight>C (<cross-reference target="DRAWINGS">FIG. 1C</cross-reference>). Another example, among others, includes highlighting, or bringing in focus, various elements of the VR Dashboard <highlight><bold>210</bold></highlight> in a looping manner among all selectable items such that a user simply cycles through the items with an arrow key until hitting a select button in order to utilize a particular element. </paragraph>
<paragraph id="P-0035" lvl="0"><number>&lsqb;0035&rsqb;</number> The VR Dashboard <highlight><bold>210</bold></highlight> can be adapted in many different ways to add or drop numerous other features, including as non-limiting examples, a chat tool and a directional instrument. In one embodiment, the VR Dashboard <highlight><bold>210</bold></highlight> could change specific to the particular application of VR-MS-UI <highlight><bold>100</bold></highlight> (<cross-reference target="DRAWINGS">FIG. 1A</cross-reference>) that is running. The VR Dashboard <highlight><bold>210</bold></highlight> features could also change as dictated by the VR-MS-UI <highlight><bold>100</bold></highlight>A dependent upon what area of a 3D Virtual World the user is located. For example, but not limited to, the snapshot in <cross-reference target="DRAWINGS">FIG. 2</cross-reference> shows the user near the Movies <highlight><bold>270</bold></highlight> activity area. It would behoove the user in the Movies <highlight><bold>270</bold></highlight> activity area to have features on the dashboard associated with viewing a movie. In one embodiment, the VR Dashboard <highlight><bold>210</bold></highlight> might be outfitted to include trick modes such as fast forward, rewind, pause and stop. In a similar embodiment the VR Dashboard <highlight><bold>210</bold></highlight> might have such features to allow the user to select the language of a movie, see its trailers, or special additions. The VR Dashboard <highlight><bold>210</bold></highlight> might also have a chat functionality feature. In this embodiment the user might be able to select the group of other users he wants to chat with and/or what topics he desires to chat about. As previously mentioned, the VR-MS-UI <highlight><bold>100</bold></highlight>A could be configured such that a new set of features would be displayed on the VR Dashboard <highlight><bold>210</bold></highlight> upon leaving the movie area and going into a shopping area. In one embodiment, a user would be presented with VR Dashboard <highlight><bold>210</bold></highlight> features such as a credit card, a shopping list, a list of items in a shopping cart, and numerous other functionality specific to shopping. </paragraph>
<paragraph id="P-0036" lvl="0"><number>&lsqb;0036&rsqb;</number> An alternative or additional embodiment to the VR-MS-UI <highlight><bold>100</bold></highlight>A discussed above would allow a user to get involved in activities with a friend. The VR-MS-UI <highlight><bold>100</bold></highlight>A would provide the user with an option to enter an invite screen in which all users or certain sets of users available for invitation would be listed. Through the invite screen, the user could select certain friends to be prompted to join their session. For example, but not limited to, the user could invite a friend(s) to join him/her in the virtual vehicle discussed above. With a friend in the vehicle, the two companions could participate in media events together. The meeting of friends would involve two users directing their avatars to meet at a location and get in a common virtual vehicle. While exploring the 3D world, the two users could look at each others avatar in the seat next to them and converse as friends would in an ordinary car ride. The virtual vehicle would allow the two companions to experience media events together and even utilize the Destination Hyperlink <highlight><bold>250</bold></highlight> feature in group mode. Thus, when one user decided to &ldquo;warp&rdquo; to a specific location with the Destination Hyperlink <highlight><bold>250</bold></highlight>, his/her friends traveling in the same virtual vehicle would be warped to the same location. </paragraph>
<paragraph id="P-0037" lvl="0"><number>&lsqb;0037&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 3</cross-reference> is an illustration of a continuation of the movement of the user in the virtual vehicle described in conjunction with <cross-reference target="DRAWINGS">FIG. 2</cross-reference>. Examination of the two figures together will illustrate that <cross-reference target="DRAWINGS">FIG. 3</cross-reference> simply demonstrates the progression of the user&apos;s view as the user moves up the road in the 3D Landscape <highlight><bold>290</bold></highlight> (<cross-reference target="DRAWINGS">FIG. 2</cross-reference>). The parking lot <highlight><bold>280</bold></highlight> (<cross-reference target="DRAWINGS">FIG. 2</cross-reference>) for the movie attraction is now closer, along with the Movie sign <highlight><bold>270</bold></highlight> (<cross-reference target="DRAWINGS">FIG. 2</cross-reference>) and the movie display billboard <highlight><bold>260</bold></highlight> (<cross-reference target="DRAWINGS">FIG. 2</cross-reference>). In one embodiment, the user would simply direct the virtual vehicle to pull into the parking lot <highlight><bold>280</bold></highlight> and position the virtual vehicle such that the move display billboard <highlight><bold>260</bold></highlight> was in full view. It is noted that the World Map <highlight><bold>220</bold></highlight> dynamically updates in conjunction with the user&apos;s movement from the position depicted in <cross-reference target="DRAWINGS">FIG. 2</cross-reference>. The World Map <highlight><bold>220</bold></highlight> (<cross-reference target="DRAWINGS">FIG. 2</cross-reference>) shows that the triangle icon, representing the movie attraction, is now directly in front of the star icon, representing the user&apos;s location. The snapshot of <cross-reference target="DRAWINGS">FIG. 2</cross-reference> and <cross-reference target="DRAWINGS">FIG. 3</cross-reference> show the user maneuvering about the 3D Landscape <highlight><bold>290</bold></highlight>. If the user had decided to warp to the movie location instead, the user could have simply activated the movies function with the triangle icon in the Destination Hyperlink <highlight><bold>250</bold></highlight> tool. </paragraph>
<paragraph id="P-0038" lvl="0"><number>&lsqb;0038&rsqb;</number> In an alternative embodiment of the current invention, the VR-MS-UI <highlight><bold>100</bold></highlight>A would be configured to allow access to the 3D virtual world on a conditional basis. The conditional basis would be dependent upon a dynamic variable. This dynamic variable would be configurable to be dependent upon certain actions. In one embodiment, access to the virtual world would be denied when the value of the dynamic variable equaled zero. The dynamic variable could be configured to decrease in value based upon certain events comprising the viewing of certain media, entering a restricted area, and linking to another environment. The dynamic variable could also be configured to increase in value based on certain events comprising viewing of advertisements, executing purchases, linking to external environments, and purchasing an increase in value of the variable. </paragraph>
<paragraph id="P-0039" lvl="0"><number>&lsqb;0039&rsqb;</number> One implementation of this embodiment would base such limited access on a feature much like the Fuel Gauge <highlight><bold>230</bold></highlight> on the VR Dashboard <highlight><bold>210</bold></highlight>. The level of the Fuel Gauge <highlight><bold>230</bold></highlight> would indicate to the user the amount of time remaining to visit the 3D world or the remaining distance that could be traveled. The conditional nature of the access to the 3D world could be based upon many factors. For example, but not limited to, the Fuel Gauge <highlight><bold>230</bold></highlight> could be filled up when a user visits a particular store, views a particular advertisement, or pays a certain fee. In this manner, a user might pay a set fee for a certain number of access minutes per month. The user might, in another instance, have to purchase something within the 3D world to maintain ability to access. In any case, administrators of the 3D Virtual Reality Media Space <highlight><bold>170</bold></highlight> (<cross-reference target="DRAWINGS">FIG. 1A</cross-reference>) would be given the functionality to configure the system such that it performs to desired specifications. </paragraph>
<paragraph id="P-0040" lvl="0"><number>&lsqb;0040&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 4</cross-reference> is a diagram that illustrates an alternative embodiment of the VR-MS-UI <highlight><bold>100</bold></highlight> (<cross-reference target="DRAWINGS">FIG. 1A</cross-reference>) to the user interface depicted in <cross-reference target="DRAWINGS">FIG. 2</cross-reference> and <cross-reference target="DRAWINGS">FIG. 3</cross-reference>. In <cross-reference target="DRAWINGS">FIG. 4</cross-reference> the user navigates the 3D world by causing the avatar to walk around. As mentioned previously, an avatar is an icon that represents a user in an application. In <cross-reference target="DRAWINGS">FIG. 4</cross-reference> we see that Avatar One <highlight><bold>410</bold></highlight> takes the shape of a woman. In this application, one particular user is represented by Avatar One <highlight><bold>410</bold></highlight> and that user&apos;s actions are implemented by that avatar. The discretion of the user can determine whether the viewpoint will be from the eyes of the avatar character or an &ldquo;over the shoulder&rdquo; viewpoint of her avatar. In any manner, VR-MS-UI <highlight><bold>100</bold></highlight>B allows the user to cause Avatar One <highlight><bold>410</bold></highlight> to walk around the landscape. The 3D World given by VR-MS-UI <highlight><bold>100</bold></highlight>B is an area where a user can walk along and watch events on the Media Wall <highlight><bold>430</bold></highlight>. As shown in <cross-reference target="DRAWINGS">FIG. 4</cross-reference>, the Media Wall <highlight><bold>430</bold></highlight> displays numerous media events. The user can take part in viewing these media events simply by walking along the Media Wall <highlight><bold>430</bold></highlight> and bringing a certain event into view within the user interface. An alternative embodiment might allow the user to walk up to a media event and select a certain event to be shown in full screen mode, similar to a normal television display. In <cross-reference target="DRAWINGS">FIG. 4</cross-reference> we can see that Avatar One <highlight><bold>410</bold></highlight> has wandered up to Media Event <highlight><bold>440</bold></highlight> and is watching the Live Sports event that is being displayed. If the user controlling Avatar One <highlight><bold>410</bold></highlight> desires to view something else, then that user can simply continue to stroll down the extensive Media Wall <highlight><bold>430</bold></highlight> and find an event of interest. The Media Wall <highlight><bold>430</bold></highlight> can contain video streams from numerous television channels, interactive program guides, videos on demand, web pages, advertisements or any other type of media. In a non-limiting example, a Client Device containing multiple tuners could show several small billboards simultaneously displaying currently tuned TV channels that could be selected by the user for full screen viewing. In an alternative implementation, a Client Device with one tuner could tune to a frequency containing a signal with multiple lower bandwidth TV channels concatenated into that one signal. In this manner, the Client Device might simultaneously display multiple TV channels on multiple billboards of a lower bandwidth. </paragraph>
<paragraph id="P-0041" lvl="0"><number>&lsqb;0041&rsqb;</number> In addition to interacting with media, a user can also interact with other avatars. For instance, Avatar One <highlight><bold>410</bold></highlight> could walk over to Avatar Two <highlight><bold>420</bold></highlight> and converse. A user&apos;s avatar can interact with a multitude of other users&apos; avatars throughout the 3D world. </paragraph>
<paragraph id="P-0042" lvl="0"><number>&lsqb;0042&rsqb;</number> It is respectfully submitted that the VR-MS-UI <highlight><bold>100</bold></highlight> (<cross-reference target="DRAWINGS">FIG. 1A</cross-reference>) can allow access to many different kinds of media, not simply just enhanced 3D media. As alluded to above, the user has the option to link to a traditional television channel. The user might also have the option to view a web page. The dynamic nature of the VR-MS-UI <highlight><bold>100</bold></highlight> (<cross-reference target="DRAWINGS">FIG. 1A</cross-reference>) is very powerful in that a user can access whatever type of media is most suitable for a given circumstance. For example, but not limited to, if the user was browsing the showroom floor of a car dealership it would be beneficial to the see the car in 3D. On the other hand, when the user was ready to look at specifications, a traditional 2D specification list would be the most appropriate display. This 2D specification list would be accessible to the user by simply activating that feature in the virtual car dealership. In a similar manner, if a user did not want to maneuver along the Media Wall <highlight><bold>430</bold></highlight> to find a channel to watch, that user could access a more traditional 2D-programming guide to find something of interest. </paragraph>
<paragraph id="P-0043" lvl="0"><number>&lsqb;0043&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 5</cross-reference> is a diagram of snapshot from VR-MS-UI <highlight><bold>100</bold></highlight>C that shows a portal to another area. In one embodiment the user can instruct her avatar to open the door labeled Public Shopping Area <highlight><bold>510</bold></highlight>. The door symbolizes a portal into another environment. Through the use of portals, administrators of a 3D Virtual Reality Media Space System <highlight><bold>170</bold></highlight> (<cross-reference target="DRAWINGS">FIG. 1A</cross-reference>) can restrict and assign access to different areas and applications. In the embodiment pictured in <cross-reference target="DRAWINGS">FIG. 5</cross-reference>, the VR-MS-UI <highlight><bold>100</bold></highlight>C can allow Avatar One <highlight><bold>410</bold></highlight> to access a Public Shopping Area <highlight><bold>510</bold></highlight>. This Public Shopping Area <highlight><bold>510</bold></highlight> could be an application independent to the current application and also could be run on a different server or by a different company. The 3D Virtual Reality Media Space System <highlight><bold>170</bold></highlight> (<cross-reference target="DRAWINGS">FIG. 1A</cross-reference>) enables the ability to restrict access to portals based on the user or any other criteria. In one embodiment, the user might have to direct Avatar One <highlight><bold>410</bold></highlight> to pay a certain fee in order to access the Public Shopping Area <highlight><bold>510</bold></highlight>. The Public Shopping Area <highlight><bold>510</bold></highlight> could be another area of the current VR-MS-UI <highlight><bold>100</bold></highlight>C or an entirely different user interface. </paragraph>
<paragraph id="P-0044" lvl="0"><number>&lsqb;0044&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 6</cross-reference> illustrates an architecture of the 3D Virtual Reality Media Space System <highlight><bold>170</bold></highlight> (<cross-reference target="DRAWINGS">FIG. 1A</cross-reference>) in which the user can associate personal features with his/her avatar in accordance with one embodiment of the current invention. A discussion presented above concerned the fact that one cannot feel they are part of a system until they can picture themselves in that system. The architecture outlined in <cross-reference target="DRAWINGS">FIG. 6</cross-reference> allows the user to feel a part of a system by allowing that user to project real life personal traits onto the avatar that represents that user in the 3D world. Through the use of the Reality Insertion Device <highlight><bold>610</bold></highlight>A, the user can apply personal traits to their avatar. In one embodiment, the Reality Insertion Device <highlight><bold>610</bold></highlight>A is a digital camera. In this embodiment the 3D VR Module <highlight><bold>140</bold></highlight>A (<cross-reference target="DRAWINGS">FIG. 1A</cross-reference>) receives input from the digital camera in the form of a picture of the user. The 3D VR Module <highlight><bold>140</bold></highlight>A (<cross-reference target="DRAWINGS">FIG. 1A</cross-reference>) interprets this digital picture and processes the picture such that it is applied to the avatar representing the user in the 3D world. A continuation of this embodiment would allow the 3D VR Module <highlight><bold>140</bold></highlight>A (<cross-reference target="DRAWINGS">FIG. 1A</cross-reference>) to accept multiple pictures to be used at different times by the avatar. For instance, but not limited to, the avatar might show a smiling photo of the user when greeting another avatar. In an alternative embodiment, the Reality Insertion Device <highlight><bold>610</bold></highlight>A is a video camera. In this embodiment the 3D VR Module <highlight><bold>140</bold></highlight>A (<cross-reference target="DRAWINGS">FIG. 1A</cross-reference>) receives information from a video camera recording footage of the user. The 3D VR Module <highlight><bold>140</bold></highlight>A (<cross-reference target="DRAWINGS">FIG. 1A</cross-reference>) interprets the information and inserts that video upon the user&apos;s avatar. In this manner, the personality, feelings, and characteristics of the actual user in reality would appear as a part of the virtual reality in the 3D world. When walking around and conversing with other avatars in the 3D world, one could actually see real life recreations of the other users. </paragraph>
<paragraph id="P-0045" lvl="0"><number>&lsqb;0045&rsqb;</number> One feature enabled by the video avatar embodiment is that of companionship activities. For instance, but not limited to, consider two people who live a great distance from each other. Through the use of the 3D Virtual Reality Media Space System (<cross-reference target="DRAWINGS">FIG. 1A</cross-reference>), these two people could agree to meet as avatars in the 3D world. The users could partake in an event such as a concert. The avatars of the two users could sit side by side on the balcony of a concert hall, enjoy the show, converse, and watch the reaction of each other in live video projection upon their respective avatars. </paragraph>
<paragraph id="P-0046" lvl="0"><number>&lsqb;0046&rsqb;</number> Another alternative of the current invention is also depicted in <cross-reference target="DRAWINGS">FIG. 6</cross-reference> in which the 3D Virtual Reality Media Space <highlight><bold>170</bold></highlight> (<cross-reference target="DRAWINGS">FIG. 1A</cross-reference>) could connect to any network such as the Public Network <highlight><bold>620</bold></highlight>. The Client Devices <highlight><bold>130</bold></highlight> (<cross-reference target="DRAWINGS">FIG. 1A</cross-reference>) of such an embodiment would operate independently of the headend network. One implementation of this embodiment would involve enabling the Client Devices <highlight><bold>130</bold></highlight> (<cross-reference target="DRAWINGS">FIG. 1A</cross-reference>) to transmit data between other subscribers over the internet. An alternative implementation might involve the Public Network <highlight><bold>620</bold></highlight> consisting of specific 3D Virtual Reality Media Space System <highlight><bold>170</bold></highlight> (<cross-reference target="DRAWINGS">FIG. 1A</cross-reference>) servers. These servers would interpret all transmitted and received information from the Client Devices <highlight><bold>130</bold></highlight> (<cross-reference target="DRAWINGS">FIG. 1A</cross-reference>) and pass that information to distant severs or clients over the internet. </paragraph>
<paragraph id="P-0047" lvl="0"><number>&lsqb;0047&rsqb;</number> The VR-MS-UI of the present invention can be implemented in hardware, software, firmware, or a combination thereof. In the preferred embodiment(s), the VR-MS-UI is implemented in software or firmware that is stored in a memory and that is executed by a suitable instruction execution system. If implemented in hardware, as in an alternative embodiment, the VR-MS-UI can be implemented with any combination of the following technologies, which are all well known in the art: a discrete logic circuit(s) having logic gates for implementing logic functions upon data signals, an application specific integrated circuit (ASIC) having appropriate combinational logic gates, a programmable gate array(s) (PGA), a field programmable gate array (FPGA), etc. </paragraph>
<paragraph id="P-0048" lvl="0"><number>&lsqb;0048&rsqb;</number> The VR-MS-UI program, which comprises an ordered listing of executable instructions for implementing logical functions, can be embodied in any computer-readable medium for use by or in connection with an instruction execution system, apparatus, or device, such as a computer-based system, processor-containing system, or other system that can fetch the instructions from the instruction execution system, apparatus, or device and execute the instructions. In the context of this document, a &ldquo;computer-readable medium&rdquo; can be any means that can contain, store, communicate, propagate, or transport the program for use by or in connection with the instruction execution system, apparatus, or device. The computer readable medium can be, for example but not limited to, an electronic, magnetic, optical, electromagnetic, infrared, or semiconductor system, apparatus, device, or propagation medium. More specific examples (a nonexhaustive list) of the computer-readable medium would include the following: an electrical connection (electronic) having one or more wires, a portable computer diskette (magnetic), a random access memory (RAM) (electronic), a read-only memory (ROM) (electronic), an erasable programmable read-only memory (EPROM or Flash memory) (electronic), an optical fiber (optical), and a portable compact disc read-only memory (CDROM) (optical). Note that the computer-readable medium could even be paper or another suitable medium upon which the program is printed, as the program can be electronically captured, via for instance optical scanning of the paper or other medium, then compiled, interpreted or otherwise processed in a suitable manner if necessary, and then stored in a computer memory. </paragraph>
<paragraph id="P-0049" lvl="0"><number>&lsqb;0049&rsqb;</number> In concluding the detailed description, it should be noted that it will be clear to those skilled in the art that many variations and modifications can be made to the preferred embodiment without substantially departing from the principles of the present invention. All such variations are intended to be included herein within the scope of the present invention, as set forth in the following claims. </paragraph>
</section>
</detailed-description>
</subdoc-description>
<subdoc-claims>
<heading lvl="1">Therefore, having thus described the invention, at least the following is claimed: </heading>
<claim id="CLM-00001">
<claim-text><highlight><bold>1</bold></highlight>. A method in a Subscriber Television System (STS) client device of providing a user interface, the method comprising steps of: 
<claim-text>implementing the user interface to be a three-dimensional (3D) user interface; and </claim-text>
<claim-text>displaying to a user a Virtual Reality (VR) media space in the 3D user interface. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00002">
<claim-text><highlight><bold>2</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein the 3D user interface enables a plurality of navigation functions, the plurality of navigation functions enabling the user to maneuver through the VR media space. </claim-text>
</claim>
<claim id="CLM-00003">
<claim-text><highlight><bold>3</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00002">claim 2</dependent-claim-reference>, wherein the VR media space comprises a virtual world of a plurality of media. </claim-text>
</claim>
<claim id="CLM-00004">
<claim-text><highlight><bold>4</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00003">claim 3</dependent-claim-reference>, wherein the plurality of media comprises a plurality of video streams, a plurality of advertisements, a plurality of avatars, and a plurality of user communications. </claim-text>
</claim>
<claim id="CLM-00005">
<claim-text><highlight><bold>5</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00003">claim 3</dependent-claim-reference>, wherein one or more of the plurality of media is enabled to be selected by the user to be displayed in full screen mode. </claim-text>
</claim>
<claim id="CLM-00006">
<claim-text><highlight><bold>6</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00004">claim 4</dependent-claim-reference>, wherein the plurality of navigation functions enable the user to maneuver in the VR media space such that one or more of the plurality of media is viewable. </claim-text>
</claim>
<claim id="CLM-00007">
<claim-text><highlight><bold>7</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00006">claim 6</dependent-claim-reference>, wherein the VR media space comprises a plurality of billboards displaying the plurality of media. </claim-text>
</claim>
<claim id="CLM-00008">
<claim-text><highlight><bold>8</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00007">claim 7</dependent-claim-reference>, wherein the plurality of navigation functions enable the user to maneuver in the VR media space such that one or more of the plurality of billboards is viewable. </claim-text>
</claim>
<claim id="CLM-00009">
<claim-text><highlight><bold>9</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00008">claim 8</dependent-claim-reference>, wherein the plurality of billboards displaying the plurality of media comprises the plurality of video streams. </claim-text>
</claim>
<claim id="CLM-00010">
<claim-text><highlight><bold>10</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00009">claim 9</dependent-claim-reference>, wherein the plurality of video streams comprises the television channels currently being received by the STS client device. </claim-text>
</claim>
<claim id="CLM-00011">
<claim-text><highlight><bold>11</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00011">claim 10</dependent-claim-reference>, wherein the STS client device is enabled with a plurality of tuners, the amount of the plurality of tuners enabling an equal amount of the plurality of video streams to be displayed simultaneously. </claim-text>
</claim>
<claim id="CLM-00012">
<claim-text><highlight><bold>12</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00002">claim 2</dependent-claim-reference>, wherein the plurality of navigation functions enable the user to select an object in the VR media space and link to a two-dimensional (2D) user interface. </claim-text>
</claim>
<claim id="CLM-00013">
<claim-text><highlight><bold>13</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00011">claim 12</dependent-claim-reference>, wherein the object is a billboard displaying a video stream of a television channel currently being received by the STS client device. . </claim-text>
</claim>
<claim id="CLM-00014">
<claim-text><highlight><bold>14</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00011">claim 12</dependent-claim-reference>, wherein the 2D user interface is a web page. </claim-text>
</claim>
<claim id="CLM-00015">
<claim-text><highlight><bold>15</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00011">claim 12</dependent-claim-reference>, wherein the 2D user interface is an interactive program guide displaying programming information for the STS client device. </claim-text>
</claim>
<claim id="CLM-00016">
<claim-text><highlight><bold>16</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00002">claim 2</dependent-claim-reference>, wherein the plurality of navigation functions enable the user to select on an object in the VR media space and link to a 3D VR media space not contained within current the VR media space. </claim-text>
</claim>
<claim id="CLM-00017">
<claim-text><highlight><bold>17</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00002">claim 2</dependent-claim-reference>, wherein the plurality of navigation functions enable the user to selectively jump to positions in the VR media space. </claim-text>
</claim>
<claim id="CLM-00018">
<claim-text><highlight><bold>18</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein a secondary portion of the 3D user interface is a map view of a larger area of the VR media space than is currently visible in a primary portion of the 3D user interface. </claim-text>
</claim>
<claim id="CLM-00019">
<claim-text><highlight><bold>19</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00011">claim 18</dependent-claim-reference>, wherein the map view of a larger area of the VR media space displays a dynamically updating aerial view of the VR media space, the dynamically updating aerial view of the VR media space comprising an indication of the user&apos;s position in relation to a plurality of elements in the VR media space. </claim-text>
</claim>
<claim id="CLM-00020">
<claim-text><highlight><bold>20</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00004">claim 4</dependent-claim-reference>, further comprising a reality insertion device providing a plurality of character information to the STS client device. </claim-text>
</claim>
<claim id="CLM-00021">
<claim-text><highlight><bold>21</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00022">claim 20</dependent-claim-reference>, wherein the plurality of avatars are implemented to be augmented by the plurality of character information. </claim-text>
</claim>
<claim id="CLM-00022">
<claim-text><highlight><bold>22</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00022">claim 21</dependent-claim-reference>, wherein the plurality of character information comprises a picture of the user. </claim-text>
</claim>
<claim id="CLM-00023">
<claim-text><highlight><bold>23</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00022">claim 21</dependent-claim-reference>, wherein the reality insertion device comprises a digital camera enabled to provide a plurality pictures of the user as a plurality of character information. </claim-text>
</claim>
<claim id="CLM-00024">
<claim-text><highlight><bold>24</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00022">claim 21</dependent-claim-reference>, wherein the reality insertion device comprises a video camera enabled to provide a video of the user as a plurality of character information. </claim-text>
</claim>
<claim id="CLM-00025">
<claim-text><highlight><bold>25</bold></highlight>. A method in a Subscriber Television System (STS) client device of providing a user interface, the method comprising steps of: 
<claim-text>implementing the user interface to be a three-dimensional (3D) user interface; </claim-text>
<claim-text>displaying to a user a Virtual Reality (VR) media space in the 3D user interface; and </claim-text>
<claim-text>granting access to the VR media space based upon a dynamic variable. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00026">
<claim-text><highlight><bold>26</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00022">claim 25</dependent-claim-reference>, wherein access to the VR media space is denied responsive to the value of the dynamic variable being equal to zero. </claim-text>
</claim>
<claim id="CLM-00027">
<claim-text><highlight><bold>27</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00022">claim 26</dependent-claim-reference>, wherein the dynamic variable is configurable to decrease in value as the elapsed time of a session of the user increases. </claim-text>
</claim>
<claim id="CLM-00028">
<claim-text><highlight><bold>28</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00022">claim 25</dependent-claim-reference>, wherein the dynamic variable is configurable to change in value when the user engages in a debiting VR activity, the debiting VR activity comprising viewing a plurality of media, entering a particular domain, and linking to an external environment. </claim-text>
</claim>
<claim id="CLM-00029">
<claim-text><highlight><bold>29</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00022">claim 25</dependent-claim-reference>, wherein the dynamic variable is configurable to change in value when the user engages in a crediting VR activity, the crediting VR activity comprising viewing a plurality of advertisement media, executing a purchase of an element in the VR media space, linking to an external environment, and purchasing an increase in the dynamic variable. </claim-text>
</claim>
<claim id="CLM-00030">
<claim-text><highlight><bold>30</bold></highlight>. A Subscriber Television System (STS) client device for providing a user with a user interface, the STS client device comprising: 
<claim-text>logic configured to provide the user with the user interface in a three-dimensional (3D) format; and </claim-text>
<claim-text>logic configured provide a Virtual Reality (VR) Media Space in the user interface in the 3D format. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00031">
<claim-text><highlight><bold>31</bold></highlight>. The STS client device of <dependent-claim-reference depends_on="CLM-00033">claim 30</dependent-claim-reference>, wherein the 3D user interface enables a plurality of navigation functions, the plurality of navigation functions allowing the user to maneuver through the VR media space. </claim-text>
</claim>
<claim id="CLM-00032">
<claim-text><highlight><bold>32</bold></highlight>. The STS client device of <dependent-claim-reference depends_on="CLM-00033">claim 31</dependent-claim-reference>, wherein the VR media space comprises a virtual world of a plurality of media. </claim-text>
</claim>
<claim id="CLM-00033">
<claim-text><highlight><bold>33</bold></highlight>. The STS client device of <dependent-claim-reference depends_on="CLM-00033">claim 32</dependent-claim-reference>, wherein the plurality of navigation functions enable the user to maneuver in the VR media space such that at least one of the plurality of media is viewable. </claim-text>
</claim>
<claim id="CLM-00034">
<claim-text><highlight><bold>34</bold></highlight>. A system enabling a Subscriber Television System (STS) client device to provide a user with a user interface, comprising; 
<claim-text>means for providing the user with the user interface in a three-dimensional (3D) format; and </claim-text>
<claim-text>means for providing a Virtual Reality (VR) Media Space in the user interface in the 3D format. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00035">
<claim-text><highlight><bold>35</bold></highlight>. A computer readable medium having a program for providing a three-dimensional (3D) user interface to a user, the 3D user interface comprising: 
<claim-text>a means for implementing the 3D user interface in a Subscriber Television System (STS); and </claim-text>
<claim-text>a means for providing a Virtual Reality (VR) Media Space in the user interface in the 3D format. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00036">
<claim-text><highlight><bold>36</bold></highlight>. The medium of <dependent-claim-reference depends_on="CLM-00033">claim 35</dependent-claim-reference>, wherein the means for implementing the 3D user interface comprises a plurality of navigation functions, the plurality of navigation functions allowing the user to maneuver through the VR media space and interact with a plurality of media and a plurality of other users. </claim-text>
</claim>
<claim id="CLM-00037">
<claim-text><highlight><bold>37</bold></highlight>. A Subscriber Television System (STS) Headend Server for providing a user with a user interface, the STS Headend Server comprising: 
<claim-text>logic configured to provide the user with the user interface in a three-dimensional (3D) format; and </claim-text>
<claim-text>logic configured provide a Virtual Reality (VR) Media Space in the user interface in the 3D format. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00038">
<claim-text><highlight><bold>38</bold></highlight>. The STS Headend Server of <dependent-claim-reference depends_on="CLM-00033">claim 37</dependent-claim-reference>, wherein the STS Headend Server is coupled to a transmission network, the transmission network distributing the user interface to a plurality of subscriber devices. </claim-text>
</claim>
<claim id="CLM-00039">
<claim-text><highlight><bold>39</bold></highlight>. The STS of <dependent-claim-reference depends_on="CLM-00033">claim 37</dependent-claim-reference>, wherein the 3D user interface enables a plurality of navigation functions, the plurality of navigation functions allowing the user to maneuver through the VR media space. </claim-text>
</claim>
<claim id="CLM-00040">
<claim-text><highlight><bold>40</bold></highlight>. A Subscriber Television System (STS) for providing a user with a user interface, the STS comprising: 
<claim-text>a first logic module configured to provide the user with the user interface in a three-dimensional (3D) format; and </claim-text>
<claim-text>a second logic module configured provide a Virtual Reality (VR) Media Space in the user interface in the 3D format. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00041">
<claim-text><highlight><bold>41</bold></highlight>. The STS of <dependent-claim-reference depends_on="CLM-00044">claim 40</dependent-claim-reference>, wherein the first logic module and the second logic module can be implemented in any device within the STS. </claim-text>
</claim>
<claim id="CLM-00042">
<claim-text><highlight><bold>42</bold></highlight>. The STS of <dependent-claim-reference depends_on="CLM-00044">claim 40</dependent-claim-reference>, wherein the first logic module and the second logic module can be further separated into a plurality of dependent modules, the plurality of dependent modules being implemented in a plurality of devices throughout the STS. </claim-text>
</claim>
<claim id="CLM-00043">
<claim-text><highlight><bold>43</bold></highlight>. The STS of <dependent-claim-reference depends_on="CLM-00044">claim 40</dependent-claim-reference>, wherein the 3D user interface enables a plurality of navigation functions, the plurality of navigation functions allowing the user to maneuver through the VR media space. </claim-text>
</claim>
<claim id="CLM-00044">
<claim-text><highlight><bold>44</bold></highlight>. A method in a Subscriber Television System (STS) client device of providing a user interface, the method comprising steps of: 
<claim-text>implementing the user interface to be a three-dimensional (3D) user interface; </claim-text>
<claim-text>displaying to a user a Virtual Reality (VR) media space in the 3D user interface, the 3D user interface enabling a plurality of navigation functions, the plurality of navigation functions enabling the user to maneuver through the VR media space, the VR media space comprising a virtual world of a plurality of media, the plurality of navigation functions enabling the user to maneuver in the VR media space such that one or more of the plurality of media is viewable, the plurality of media comprising a plurality of video streams, a plurality of advertisements, a plurality of avatars, and a plurality of user communications; </claim-text>
<claim-text>enabling the user to select an object in the VR media space and link to a secondary interface, the secondary interface comprising a full screen view of a television channel, a video, a web page, a programming guide, and an interactive programming guide; </claim-text>
<claim-text>displaying at least one of a plurality of avatars to represent the user, said at least one plurality of avatars being implemented to be augmented by a reality insertion device providing a plurality of character information, the plurality of character information comprising a picture of the user, at least one picture from a digital camera, and at least one video from a video camera; and </claim-text>
<claim-text>granting access to said VR media space based upon a dynamic variable, the dynamic variable being configured to change in value upon the occurrence of an event, the event comprising a debiting event and a crediting event, the debiting event comprising viewing a plurality of media, entering a particular domain, and linking to an external environment, the crediting event comprising viewing a plurality of advertisement media, executing a purchase of an element in said VR media space, linking to an external environment, and purchasing an increase in said dynamic variable.</claim-text>
</claim-text>
</claim>
</subdoc-claims>
<subdoc-drawings id="DRAWINGS">
<heading lvl="0" align="CENTER">Drawings</heading>
<representative-figure>4</representative-figure>
<figure id="figure-D00000">
<image id="EMI-D00000" file="US20030005439A1-20030102-D00000.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00001">
<image id="EMI-D00001" file="US20030005439A1-20030102-D00001.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00002">
<image id="EMI-D00002" file="US20030005439A1-20030102-D00002.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00003">
<image id="EMI-D00003" file="US20030005439A1-20030102-D00003.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00004">
<image id="EMI-D00004" file="US20030005439A1-20030102-D00004.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00005">
<image id="EMI-D00005" file="US20030005439A1-20030102-D00005.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00006">
<image id="EMI-D00006" file="US20030005439A1-20030102-D00006.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00007">
<image id="EMI-D00007" file="US20030005439A1-20030102-D00007.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00008">
<image id="EMI-D00008" file="US20030005439A1-20030102-D00008.TIF" imf="TIFF" ti="DR"/>
</figure>
</subdoc-drawings>
</patent-application-publication>
