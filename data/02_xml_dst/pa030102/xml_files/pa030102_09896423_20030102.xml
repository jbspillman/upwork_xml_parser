<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE patent-application-publication SYSTEM "pap-v16-2002-01-01.dtd" [
<!ENTITY US20030005261A1-20030102-D00000.TIF SYSTEM "US20030005261A1-20030102-D00000.TIF" NDATA TIF>
<!ENTITY US20030005261A1-20030102-D00001.TIF SYSTEM "US20030005261A1-20030102-D00001.TIF" NDATA TIF>
<!ENTITY US20030005261A1-20030102-D00002.TIF SYSTEM "US20030005261A1-20030102-D00002.TIF" NDATA TIF>
<!ENTITY US20030005261A1-20030102-D00003.TIF SYSTEM "US20030005261A1-20030102-D00003.TIF" NDATA TIF>
<!ENTITY US20030005261A1-20030102-D00004.TIF SYSTEM "US20030005261A1-20030102-D00004.TIF" NDATA TIF>
<!ENTITY US20030005261A1-20030102-D00005.TIF SYSTEM "US20030005261A1-20030102-D00005.TIF" NDATA TIF>
<!ENTITY US20030005261A1-20030102-D00006.TIF SYSTEM "US20030005261A1-20030102-D00006.TIF" NDATA TIF>
]>
<patent-application-publication>
<subdoc-bibliographic-information>
<document-id>
<doc-number>20030005261</doc-number>
<kind-code>A1</kind-code>
<document-date>20030102</document-date>
</document-id>
<publication-filing-type>new</publication-filing-type>
<domestic-filing-data>
<application-number>
<doc-number>09896423</doc-number>
</application-number>
<application-number-series-code>09</application-number-series-code>
<filing-date>20010629</filing-date>
</domestic-filing-data>
<technical-information>
<classification-ipc>
<classification-ipc-primary>
<ipc>G06F015/00</ipc>
</classification-ipc-primary>
<classification-ipc-edition>07</classification-ipc-edition>
</classification-ipc>
<classification-us>
<classification-us-primary>
<uspc>
<class>712</class>
<subclass>035000</subclass>
</uspc>
</classification-us-primary>
</classification-us>
<title-of-invention>Method and apparatus for attaching accelerator hardware containing internal state to a processing core</title-of-invention>
</technical-information>
<inventors>
<first-named-inventor>
<name>
<given-name>Gad</given-name>
<family-name>Sheaffer</family-name>
</name>
<residence>
<residence-non-us>
<city>Haifa</city>
<country-code>IL</country-code>
</residence-non-us>
</residence>
<authority-applicant>INV</authority-applicant>
</first-named-inventor>
</inventors>
<correspondence-address>
<name-1>KENYON &amp; KENYON (SAN JOSE)</name-1>
<name-2></name-2>
<address>
<address-1>333 WEST SAN CARLOS ST.</address-1>
<address-2>SUITE 600</address-2>
<city>SAN JOSE</city>
<state>CA</state>
<postalcode>95110</postalcode>
<country>
<country-code>US</country-code>
</country>
</address>
</correspondence-address>
</subdoc-bibliographic-information>
<subdoc-abstract>
<paragraph id="A-0001" lvl="0">A digital signal processor system and method for improving processing speed by providing a memory file and a register file connected to an accelerator which is connected to a write-back logic bus. One or more execution units can be connected between the memory and register files and the accelerator and/or between the accelerator and the bus. The accelerator is provided with internal state. The internal state is configured to enable increasing the ratio of computation operations to the memory bandwidth available from a digital signal processor. </paragraph>
</subdoc-abstract>
<subdoc-description>
<summary-of-invention>
<section>
<heading lvl="1">FIELD OF THE INVENTION </heading>
<paragraph id="P-0001" lvl="0"><number>&lsqb;0001&rsqb;</number> The present invention relates to the acceleration of processing. More particularly, the present invention relates to attaching accelerator hardware containing internal state to a processing core. </paragraph>
</section>
<section>
<heading lvl="1">BACKGROUND INFORMATION </heading>
<paragraph id="P-0002" lvl="0"><number>&lsqb;0002&rsqb;</number> Modern microprocessors implement a variety of techniques to increase the performance of executing instructions including superscalar and pipelining execution. Superscalar microprocessors are capable of processing multiple instructions within a common clock cycle. Pipelined microprocessors divide the processing of an operation into separate pipestages and overlap the pipestage processing of subsequent instructions in an attempt to achieve single pipestage throughput performance. </paragraph>
<paragraph id="P-0003" lvl="0"><number>&lsqb;0003&rsqb;</number> In any particular processing system, it can happen that a code will consume too many cycles on the execution units within the processing core and thus is not efficient. Accelerator blocks are execution units modified to perform certain specialized tasks, for example, interleaving, more efficiently. Thus, the accelerator blocks, situated as hardware used in a processing system, optimize execution of those specialized tasks and the regular execution units execute the other tasks. For example, if there are seventeen tasks to be performed concurrently and one task takes 20% of the time, the overall processing can be reduced by using accelerator blocks focused particularly to that one task. The remaining 16 tasks can then be processed more efficiently and in fewer cycles by the regular execution units because the 17<highlight><superscript>th </superscript></highlight>task requiring 20% of the processing time has been effectively removed from that path.</paragraph>
</section>
</summary-of-invention>
<brief-description-of-drawings>
<section>
<heading lvl="1">BRIEF DESCRIPTION OF THE DRAWINGS </heading>
<paragraph id="P-0004" lvl="0"><number>&lsqb;0004&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 1</cross-reference> depicts a block diagram of accelerator blocks in a general processing core according to an embodiment of the present invention. </paragraph>
<paragraph id="P-0005" lvl="0"><number>&lsqb;0005&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 2</cross-reference> depicts a block diagram of an accelerator block having internal state according to another embodiment of the present invention. </paragraph>
<paragraph id="P-0006" lvl="0"><number>&lsqb;0006&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 3</cross-reference> depicts a block diagram of an accelerator block in a general processing core according to another embodiment of the present invention. </paragraph>
<paragraph id="P-0007" lvl="0"><number>&lsqb;0007&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 4</cross-reference> depicts a block diagram of an accelerator block according to another embodiment of the present invention. </paragraph>
<paragraph id="P-0008" lvl="0"><number>&lsqb;0008&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 5</cross-reference> depicts a block diagram of accelerator blocks in a general processing core according to another embodiment of the present invention. </paragraph>
<paragraph id="P-0009" lvl="0"><number>&lsqb;0009&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 6</cross-reference> depicts a block diagram of accelerator blocks in a general processing core according to yet another embodiment of the present invention</paragraph>
</section>
</brief-description-of-drawings>
<detailed-description>
<section>
<heading lvl="1">DETAILED DESCRIPTION </heading>
<paragraph id="P-0010" lvl="0"><number>&lsqb;0010&rsqb;</number> In the detailed description, various systems, circuits and interfaces are described in block form and certain well-known elements, devices, process steps and the like are not described in detail to avoid any unnecessary obscurement of the present invention. </paragraph>
<paragraph id="P-0011" lvl="0"><number>&lsqb;0011&rsqb;</number> When accelerator blocks are operated in parallel with the execution units in a general processing core of a signal processor, the accelerator blocks can provide a more efficient path for the processing codes/signals. Accelerator hardware can be attached to the process core on the outside of the general processing core. In such a case, the general processing core sends blocks of data to the accelerator and the accelerator then transmits that processed data back to the general processing core. In the present invention, the accelerator blocks, or hardware, may be attached within the general processing core. Further, the accelerator blocks may be provided with internal state. The internal state allows the accelerator blocks to have available memory. Further, in the present invention, the accelerator blocks can be operated in parallel with the regular execution units. The accelerator blocks and the regular execution units are connected to the same inputs/outputs. Further, one or both of the accelerator blocks and the regular execution units can provide specialized operation for the off-load work. </paragraph>
<paragraph id="P-0012" lvl="0"><number>&lsqb;0012&rsqb;</number> Generally, the regular non-pipeline execution units operate on what enters in the current cycle and do not maintain any memory. The internal state of the accelerator block according to an embodiment of the present invention provides a capacity for storing data for the accelerator block. The execution units are fed data by the same buses, write back data to the same buses and are operated in the same manner as the accelerator blocks. A further embodiment of the present invention includes making additional memory available to the accelerator block. </paragraph>
<paragraph id="P-0013" lvl="0"><number>&lsqb;0013&rsqb;</number> Embodiments of the present invention further provide an accelerator block or a plurality of accelerator blocks which may or may not have internal state and can be inserted into already existing general processing cores of digital signal processors or attached to the outside. While the regular execution units do not have memory or internal state, the accelerator block of the present invention is provided with internal state and does have memory. </paragraph>
<paragraph id="P-0014" lvl="0"><number>&lsqb;0014&rsqb;</number> Referring to <cross-reference target="DRAWINGS">FIG. 1, a</cross-reference> block diagram of accelerator blocks <highlight><bold>6</bold></highlight> in a general processing core <highlight><bold>1</bold></highlight> of a digital signal processor (DSP) according to an embodiment of the present invention is shown. In this embodiment of the present invention, the hardware accelerator blocks <highlight><bold>6</bold></highlight> can be attached between the memory file ports <highlight><bold>2</bold></highlight>, <highlight><bold>4</bold></highlight> and/or register file ports <highlight><bold>3</bold></highlight> and the write-back bus <highlight><bold>7</bold></highlight> of either the digital signal processor <highlight><bold>1</bold></highlight> or any general-purpose processor. Multiplexer units <highlight><bold>8</bold></highlight><highlight><italic>a,b,c,d, </italic></highlight>or data selectors, are used for selecting the information from the memory and register file ports <highlight><bold>2</bold></highlight>,<highlight><bold>3</bold></highlight>,<highlight><bold>4</bold></highlight> and direct the information to the regular execution units <highlight><bold>5</bold></highlight> and/or the acceleration blocks <highlight><bold>6</bold></highlight>. The accelerator blocks <highlight><bold>6</bold></highlight> can include, among other things, larger precision accumulators, temporary registers holding previous values of either outputs, inputs or intermediate results, registers arranged as FIFO structure, scratch pad memory arranged as either caches or directly addressable, accumulators containing higher precision versions of the computed results, special purpose registers containing status flags generated by the execution hardware, and registers arranged as shift registers. The regular execution units <highlight><bold>5</bold></highlight> can provide support for copying the contents of the accumulator into either registers or memory along with saturation and down-shifting for precision adjustment and packing. </paragraph>
<paragraph id="P-0015" lvl="0"><number>&lsqb;0015&rsqb;</number> The accelerator block <highlight><bold>6</bold></highlight> in <cross-reference target="DRAWINGS">FIG. 1</cross-reference> is attached to all the operand ports of the processor, and can therefore use the fall memory bandwidth of the processing core <highlight><bold>1</bold></highlight>; bandwidth being the difference between the frequency limits of a band containing the useful frequency components of a signal. The accelerator block <highlight><bold>6</bold></highlight> can also be activated by a single instruction in one of the issue slots and occupy part or all the memory bandwidth, while another instruction in a second issue slot can use the core&apos;s other resources in parallel. </paragraph>
<paragraph id="P-0016" lvl="0"><number>&lsqb;0016&rsqb;</number> In <cross-reference target="DRAWINGS">FIG. 1</cross-reference>, the core is balanced with respect to the number of execution units so that there is no overabundance of execution units or of a number of operands. In <cross-reference target="DRAWINGS">FIG. 1</cross-reference>, the number of operands <highlight><bold>2</bold></highlight><highlight><italic>a,b, </italic></highlight><highlight><bold>3</bold></highlight><highlight><italic>a,b,c,d, </italic></highlight><highlight><bold>4</bold></highlight><highlight><italic>a,b </italic></highlight>from the memory and register units can be used by the regular execution units <highlight><bold>5</bold></highlight>, without any operands remaining idle or unused. If an additional accelerator block having no internal state was attached in parallel to the regular execution units, then the accelerator block would be idle or unused because there are no additional operands to be used. Thus, if the accelerator blocks are to be run in parallel, they need to be provided with bandwidth. </paragraph>
<paragraph id="P-0017" lvl="0"><number>&lsqb;0017&rsqb;</number> Referring to <cross-reference target="DRAWINGS">FIG. 2</cross-reference>, an exemplary accelerator block <highlight><bold>31</bold></highlight> having internal state according to an embodiment of the present invention is shown. The accelerator block <highlight><bold>31</bold></highlight> may contain a FIFO register <highlight><bold>32</bold></highlight>, other temporary registers <highlight><bold>33</bold></highlight>, execution blocks <highlight><bold>34</bold></highlight>, a cache <highlight><bold>35</bold></highlight>, and a scratch pad memory <highlight><bold>36</bold></highlight>. </paragraph>
<paragraph id="P-0018" lvl="0"><number>&lsqb;0018&rsqb;</number> An exemplary embodiment of the present invention includes a processor having an accelerator which is provided with internal state. For example, in <cross-reference target="DRAWINGS">FIG. 2</cross-reference>, an exemplary accelerator having a FIFO (First In First Out) register <highlight><bold>32</bold></highlight> according to the present invention is shown. In this embodiment, the FIFO register <highlight><bold>32</bold></highlight> samples operands entering the execution blocks <highlight><bold>34</bold></highlight> so that there are copies stored in the memory of the accelerator block <highlight><bold>31</bold></highlight> of the input operand from the execution unit blocks <highlight><bold>34</bold></highlight> from the previous, e.g., three cycles. Thus, a regular execution unit operating outside the accelerator block <highlight><bold>31</bold></highlight> can operate on the input operand during a current cycle while the accelerator block works on input operand from a previous cycle. For example, a first vector set of operands is A<highlight><subscript>1</subscript></highlight>, B<highlight><subscript>1 </subscript></highlight>and a second vector set of operands is A<highlight><subscript>2</subscript></highlight>, B<highlight><subscript>2</subscript></highlight>. When the first set of operands enter the regular execution units, the regular execution units operate on that current vector set, that is, the first vector set of operands A<highlight><subscript>1</subscript></highlight>, B<highlight><subscript>1</subscript></highlight>. Likewise, when the second set of operands enter the regular execution units, the regular execution units operate on that current set, that is, the second vector set of operands A<highlight><subscript>2</subscript></highlight>, B<highlight><subscript>2</subscript></highlight>. However, the accelerator block <highlight><bold>31</bold></highlight> can store operand A<highlight><subscript>1 </subscript></highlight>from the first set and then operate on operand A<highlight><subscript>1 </subscript></highlight>with operand B<highlight><subscript>2 </subscript></highlight>while the regular execution units are operating, e.g., multiplying, on operands A<highlight><subscript>2 </subscript></highlight>and B<highlight><subscript>2</subscript></highlight>. </paragraph>
<paragraph id="P-0019" lvl="0"><number>&lsqb;0019&rsqb;</number> Referring to <cross-reference target="DRAWINGS">FIG. 3</cross-reference>, an exemplary system and method of an accelerator block <highlight><bold>41</bold></highlight> having internal state according to an embodiment of the present invention is shown. Operand A <highlight><bold>42</bold></highlight> is sent to an execution unit <highlight><bold>44</bold></highlight> and to a multiplexer <highlight><bold>46</bold></highlight>. Operand B <highlight><bold>43</bold></highlight> is sent to the multiplexer <highlight><bold>47</bold></highlight>. A possibly delayed operand is sent to the same multiplexer from the execution unit <highlight><bold>45</bold></highlight>. The outputs of both multiplexers <highlight><bold>46</bold></highlight>, <highlight><bold>47</bold></highlight> are sent to an execution unit <highlight><bold>48</bold></highlight>. The execution unit <highlight><bold>48</bold></highlight> forwards the result from a cycle to the execution unit <highlight><bold>45</bold></highlight>. Further, execution unit <highlight><bold>44</bold></highlight> may store the input operand from Operand A <highlight><bold>42</bold></highlight> from a previous cycle and then forward it to the multiplexer <highlight><bold>46</bold></highlight> in a later cycle. </paragraph>
<paragraph id="P-0020" lvl="0"><number>&lsqb;0020&rsqb;</number> The multiplexers in the general processing core can select the source of data, e.g., register or memory, and forward that data to the regular execution units and the accelerator block(s). </paragraph>
<paragraph id="P-0021" lvl="0"><number>&lsqb;0021&rsqb;</number> In a further example of the present invention, when there are several intermediate variables, the accelerator block can be provided with additional memory to handle the variables. In this example, the memory inside the accelerator block also appears to serve as a scratchpad for the accelerator block. If the accelerator block did not have internal state, then one would not be able to use the execution unit in the accelerator block because of the data and the memory requirements. </paragraph>
<paragraph id="P-0022" lvl="0"><number>&lsqb;0022&rsqb;</number> According to an example of the present invention, an accelerator block can be plugged into the general processing core to handle n bits of data, where n is less than m. So, for the accelerator block, there is a mismatched m and n, so when kernals differ in m and n, it is useful to use the accelerator block having internal state and connected in the general processing core as described in the examples of the present embodiment. Kernal A requires more than n bits of data. If kernal A requires m data/cycle, where m&gt;n. The accelerator block can use its internal state to fill in the difference between m and n. For example, if 64 bytes are needed at the input and output to do X at a Y rate, but the code gives only 32 bytes in and 32 bytes out, the remaining number of bytes needed can come from the internal state of the accelerator. The internal state of the accelerator can add some bytes from the previous cycles. </paragraph>
<paragraph id="P-0023" lvl="0"><number>&lsqb;0023&rsqb;</number> Referring to <cross-reference target="DRAWINGS">FIG. 4</cross-reference>, an exemplary accelerator block <highlight><bold>57</bold></highlight> that can be plugged into the general processing core is shown. Input <highlight><bold>51</bold></highlight> of M operand data bits and input <highlight><bold>55</bold></highlight> of N operand data bits are inputted to the execution hardware <highlight><bold>52</bold></highlight>. The output <highlight><bold>23</bold></highlight> of the execution hardware <highlight><bold>52</bold></highlight> may include any K result data bits. The output <highlight><bold>56</bold></highlight> of the execution hardware <highlight><bold>52</bold></highlight> may include any L result data bits. The output <highlight><bold>56</bold></highlight> of the L result data bits may be inputted into internal storage <highlight><bold>54</bold></highlight>. The output <highlight><bold>55</bold></highlight> of the internal storage <highlight><bold>54</bold></highlight> is then fed to the execution hardware. </paragraph>
<paragraph id="P-0024" lvl="0"><number>&lsqb;0024&rsqb;</number> Specifically in the embodiments of the present invention, the accelerator blocks contain internal state. That internal state enables increasing the ratio of computation operations to memory bandwidth and enable adding more execution resources onto a given micro-architecture, without increasing the memory and register file available bandwidth. </paragraph>
<paragraph id="P-0025" lvl="0"><number>&lsqb;0025&rsqb;</number> Assuming that the ratio of computation to memory bandwidth in the micro-architecture to which the accelerator blocks <highlight><bold>6</bold></highlight> are attached (the host processing core) is already balanced, accelerator blocks which perform specialized operations designed to perform among other things can be added into any embodiments described herein of the present invention. If some operands and/or intermediate results are latched and stored inside the accelerator blocks <highlight><bold>6</bold></highlight>, then a much larger number of computation units can be attached to an existing micro-architecture, within a given memory bandwidth. </paragraph>
<paragraph id="P-0026" lvl="0"><number>&lsqb;0026&rsqb;</number> Referring to <cross-reference target="DRAWINGS">FIG. 5, a</cross-reference> block diagram of accelerator blocks <highlight><bold>16</bold></highlight>A, <highlight><bold>16</bold></highlight>B in a general processing core of a digital signal processor system <highlight><bold>11</bold></highlight> according to an embodiment of the present invention is shown. In this embodiment, the memory and register files <highlight><bold>12</bold></highlight>, <highlight><bold>13</bold></highlight>, <highlight><bold>14</bold></highlight> are connected via multiplexers <highlight><bold>18</bold></highlight><highlight><italic>a,b,c,d </italic></highlight>to the regular execution units <highlight><bold>15</bold></highlight> and/or the accelerator blocks A and B <highlight><bold>16</bold></highlight>A, <highlight><bold>16</bold></highlight>B. In this embodiment of the present invention, the two accelerator blocks <highlight><bold>16</bold></highlight>A, <highlight><bold>16</bold></highlight>B, are attached, each to an execution pipeline of the execution units <highlight><bold>15</bold></highlight>. These two execution units/pipelines can be either identical or different. In a general purpose superscalar architecture, if the two execution units/pipelines are different, such asymmetry can be accounted for using additional hardware or algorithms to recognize the difference and adjust accordingly for the different processing times and other differences of the pipelines employed. Having two distinct but identical accelerator blocks can provide another measure of flexibility, at a cost of a higher fetch bandwidth. The two accelerator blocks can each communicate with writeback logic/bus <highlight><bold>17</bold></highlight>. </paragraph>
<paragraph id="P-0027" lvl="0"><number>&lsqb;0027&rsqb;</number> Referring to <cross-reference target="DRAWINGS">FIG. 6, a</cross-reference> block diagram of accelerator blocks <highlight><bold>26</bold></highlight> in a general processing core <highlight><bold>21</bold></highlight> according to an embodiment of the present invention is shown. In this embodiment of the present invention, an accelerator block <highlight><bold>26</bold></highlight> is attached to both the memory <highlight><bold>22</bold></highlight>, <highlight><bold>24</bold></highlight> and register file ports of the processing core, plying it with even greater bandwidth. The bandwidth can effectively be almost doubled in size. Each operand can be from the memory or the register. In this embodiment of the present invention, additional multipliers and/or multiplexers <highlight><bold>28</bold></highlight><highlight><italic>a,b,c,d </italic></highlight>can be used for shifting and sorting in embodiments of the present invention. In effect, the accelerators having internal state according to the present invention can be modified in their architecture to perform any number of operations, including multiplication, shifting and sorting. </paragraph>
<paragraph id="P-0028" lvl="0"><number>&lsqb;0028&rsqb;</number> In <cross-reference target="DRAWINGS">FIG. 6</cross-reference>, there are eight input operands. The regular execution units <highlight><bold>25</bold></highlight> can take four of the eight input operands. The accelerator blocks <highlight><bold>26</bold></highlight> are controlled by the same instructions as the regular execution units <highlight><bold>25</bold></highlight>. That is, a single instruction can control both the regular execution units and the accelerator blocks (having internal state), unlike in the past when a bus forwarded chunks of data outside of the general processing core to an accelerator block which then worked on the chunk of data separately and with special instructions. </paragraph>
<paragraph id="P-0029" lvl="0"><number>&lsqb;0029&rsqb;</number> Embodiments of the present invention also can include accelerator blocks which read external data sources in addition to previous options. </paragraph>
<paragraph id="P-0030" lvl="0"><number>&lsqb;0030&rsqb;</number> Embodiments of the present invention introduce methods to attach accelerator blocks to the existing buses in order to increase the efficiency of the executed operations. </paragraph>
<paragraph id="P-0031" lvl="0"><number>&lsqb;0031&rsqb;</number> Although several embodiments are specifically illustrated and described herein, it will be appreciated that modifications and variations of the present invention are covered by the above teachings and within the purview of the appended claims without departing from the spirit and intended scope of the present invention. For example, the present invention can be expanded, for example, to involve additional accelerator blocks having internal state attached to execution pipes <highlight><bold>25</bold></highlight> and/or memory and register file ports <highlight><bold>22</bold></highlight>, <highlight><bold>23</bold></highlight>, <highlight><bold>24</bold></highlight> of a processing core. </paragraph>
</section>
</detailed-description>
</subdoc-description>
<subdoc-claims>
<heading lvl="1">What is claimed is: </heading>
<claim id="CLM-00001">
<claim-text><highlight><bold>1</bold></highlight>. A digital signal processor system comprising: 
<claim-text>at least one accelerator having internal state and being connected to a bus; and </claim-text>
<claim-text>at least one of a memory file and a register file, wherein the at least one of the memory file and the register file is connected to the at least one accelerator. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00002">
<claim-text><highlight><bold>2</bold></highlight>. The system of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein the bus has write-back logic. </claim-text>
</claim>
<claim id="CLM-00003">
<claim-text><highlight><bold>3</bold></highlight>. The system of <dependent-claim-reference depends_on="CLM-00002">claim 2</dependent-claim-reference>, wherein the internal state of the accelerator is configured to add additional execution resources without increasing the memory bandwidth of the digital signal processor system. </claim-text>
</claim>
<claim id="CLM-00004">
<claim-text><highlight><bold>4</bold></highlight>. The system of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein the internal state of the accelerator includes at least one of a precision accumulator, a temporary register to hold previous values, a FIFO structure register, a scratch pad memory configured as a cache, a scratch pad memory configured as directly addressable, a special purpose register to contain status flags generated, and a shift register. </claim-text>
</claim>
<claim id="CLM-00005">
<claim-text><highlight><bold>5</bold></highlight>. The system of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein the internal state is configured to provide additional stored data bits from a previous cycle to a current cycle. </claim-text>
</claim>
<claim id="CLM-00006">
<claim-text><highlight><bold>6</bold></highlight>. The system of <dependent-claim-reference depends_on="CLM-00002">claim 2</dependent-claim-reference>, wherein the internal state of the accelerator is configured to enable increasing a ratio of computation operations to memory bandwidth of the digital signal processor system. </claim-text>
</claim>
<claim id="CLM-00007">
<claim-text><highlight><bold>7</bold></highlight>. The system of <dependent-claim-reference depends_on="CLM-00003">claim 3</dependent-claim-reference>, wherein the at least one accelerator contains at least one precision accumulator. </claim-text>
</claim>
<claim id="CLM-00008">
<claim-text><highlight><bold>8</bold></highlight>. The system of <dependent-claim-reference depends_on="CLM-00007">claim 7</dependent-claim-reference>, further comprising: 
<claim-text>at least one execution unit connected between the at least one memory file and register file and the at least one accelerator. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00009">
<claim-text><highlight><bold>9</bold></highlight>. The system of <dependent-claim-reference depends_on="CLM-00007">claim 7</dependent-claim-reference>, further comprising: 
<claim-text>at least one execution unit connected between the at least one accelerator and the bus. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00010">
<claim-text><highlight><bold>10</bold></highlight>. The system of <dependent-claim-reference depends_on="CLM-00008">claim 8</dependent-claim-reference>, wherein the at least one execution unit is configured to copy data from the at least one precision accumulator into an execution unit memory, the at least one execution unit being further configured to adjust and package the data copied from the at least one precision accumulator. </claim-text>
</claim>
<claim id="CLM-00011">
<claim-text><highlight><bold>11</bold></highlight>. The system of <dependent-claim-reference depends_on="CLM-00009">claim 9</dependent-claim-reference>, wherein the at least one execution unit is configured to copy data from the at least one precision accumulator into an execution unit memory, the at least one execution unit being further configured to adjust and package the data copied from the at least one precision accumulator. </claim-text>
</claim>
<claim id="CLM-00012">
<claim-text><highlight><bold>12</bold></highlight>. The system of <dependent-claim-reference depends_on="CLM-00004">claim 4</dependent-claim-reference>, wherein the at least one accelerator is attached to all operand ports of the digital signal processor system and is configured to use the full memory bandwidth of the digital signal processor system. </claim-text>
</claim>
<claim id="CLM-00013">
<claim-text><highlight><bold>13</bold></highlight>. The system of <dependent-claim-reference depends_on="CLM-00004">claim 4</dependent-claim-reference>, wherein the at least one accelerator has a first issue slot and a second issue slot and is configured to be activated by a first instruction in the first issue slot while a second instruction in the second issue slot executes in the digital signal processor system in parallel with the first instruction in the first issue slot. </claim-text>
</claim>
<claim id="CLM-00014">
<claim-text><highlight><bold>14</bold></highlight>. A digital signal processor system comprising: 
<claim-text>a first accelerator and a second accelerator; and </claim-text>
<claim-text>at least one of a memory file and a register file, </claim-text>
<claim-text>wherein the at least one of the memory file and the register file are connected to at least one of the first accelerator and the second accelerator via at least one multiplexer, </claim-text>
<claim-text>wherein the at least one of the first and second accelerators have an internal state and are connected to a bus. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00015">
<claim-text><highlight><bold>15</bold></highlight>. The system of <dependent-claim-reference depends_on="CLM-00011">claim 14</dependent-claim-reference>, wherein the internal state includes at least one of a precision accumulator, a temporary register to hold previous values, a FIFO structure register, a scratch pad memory configured as a cache, a scratch pad memory configured as directly addressable, a special purpose register to contain status flags generated, and a shift register. </claim-text>
</claim>
<claim id="CLM-00016">
<claim-text><highlight><bold>16</bold></highlight>. The system of <dependent-claim-reference depends_on="CLM-00011">claim 14</dependent-claim-reference>, further comprising: 
<claim-text>at least one execution unit, </claim-text>
<claim-text>wherein the first and second accelerators are attached to a first and second execution pipeline, respectively, of the at least one execution unit, the first and second execution pipelines being configured as one of identical pipelines and non-identical pipelines, the execution unit being connected to a write-back logic bus. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00017">
<claim-text><highlight><bold>17</bold></highlight>. The system of <dependent-claim-reference depends_on="CLM-00011">claim 16</dependent-claim-reference>, wherein the first and second execution pipelines are non-identical pipelines, and 
<claim-text>further comprising hardware to recognize if the first and second execution pipelines process data at different speeds. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00018">
<claim-text><highlight><bold>18</bold></highlight>. The system of <dependent-claim-reference depends_on="CLM-00011">claim 17</dependent-claim-reference>, wherein the hardware recognizes that the first and second execution pipelines process data at different speeds and then at least one of i) an alert indication is activated and ii) the first and second execution pipelines are modified so that the data is processed at similar speeds. </claim-text>
</claim>
<claim id="CLM-00019">
<claim-text><highlight><bold>19</bold></highlight>. A method for attaching accelerator hardware to a processing core of a digital signal processor, comprising: 
<claim-text>providing at least one of a memory file and a register file; </claim-text>
<claim-text>connecting an accelerator to the at least one of the memory file and the register file; </claim-text>
<claim-text>providing the accelerator with an internal state, the internal state being configured to enable increasing a ratio of computation operations to the memory bandwidth of the processor; and </claim-text>
<claim-text>connecting the accelerator to a bus. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00020">
<claim-text><highlight><bold>20</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00011">claim 19</dependent-claim-reference>, further comprising: 
<claim-text>connecting an execution unit between the accelerator and the at least one of the memory file and the register file; and </claim-text>
<claim-text>wherein the bus is configured to contain write-back logic. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00021">
<claim-text><highlight><bold>21</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00011">claim 19</dependent-claim-reference>, further comprising: 
<claim-text>connecting an execution unit between the accelerator and the bus; and </claim-text>
<claim-text>wherein the bus is configured to contain write-back logic. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00022">
<claim-text><highlight><bold>22</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00011">claim 19</dependent-claim-reference>, wherein the internal state of the accelerator includes at least one of a precision accumulator, a temporary register to hold previous values, a FIFO structure register, a scratch pad memory configured as a cache, a scratch pad memory configured as directly addressable, a special purpose register to contain status flags generated, and a shift register.</claim-text>
</claim>
</subdoc-claims>
<subdoc-drawings id="DRAWINGS">
<heading lvl="0" align="CENTER">Drawings</heading>
<representative-figure>1</representative-figure>
<figure id="figure-D00000">
<image id="EMI-D00000" file="US20030005261A1-20030102-D00000.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00001">
<image id="EMI-D00001" file="US20030005261A1-20030102-D00001.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00002">
<image id="EMI-D00002" file="US20030005261A1-20030102-D00002.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00003">
<image id="EMI-D00003" file="US20030005261A1-20030102-D00003.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00004">
<image id="EMI-D00004" file="US20030005261A1-20030102-D00004.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00005">
<image id="EMI-D00005" file="US20030005261A1-20030102-D00005.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00006">
<image id="EMI-D00006" file="US20030005261A1-20030102-D00006.TIF" imf="TIFF" ti="DR"/>
</figure>
</subdoc-drawings>
</patent-application-publication>
