<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE patent-application-publication SYSTEM "pap-v16-2002-01-01.dtd" [
<!ENTITY US20030004913A1-20030102-D00000.TIF SYSTEM "US20030004913A1-20030102-D00000.TIF" NDATA TIF>
<!ENTITY US20030004913A1-20030102-D00001.TIF SYSTEM "US20030004913A1-20030102-D00001.TIF" NDATA TIF>
<!ENTITY US20030004913A1-20030102-D00002.TIF SYSTEM "US20030004913A1-20030102-D00002.TIF" NDATA TIF>
<!ENTITY US20030004913A1-20030102-D00003.TIF SYSTEM "US20030004913A1-20030102-D00003.TIF" NDATA TIF>
<!ENTITY US20030004913A1-20030102-D00004.TIF SYSTEM "US20030004913A1-20030102-D00004.TIF" NDATA TIF>
]>
<patent-application-publication>
<subdoc-bibliographic-information>
<document-id>
<doc-number>20030004913</doc-number>
<kind-code>A1</kind-code>
<document-date>20030102</document-date>
</document-id>
<publication-filing-type>new</publication-filing-type>
<domestic-filing-data>
<application-number>
<doc-number>09897524</doc-number>
</application-number>
<application-number-series-code>09</application-number-series-code>
<filing-date>20010702</filing-date>
</domestic-filing-data>
<technical-information>
<classification-ipc>
<classification-ipc-primary>
<ipc>G06F017/00</ipc>
</classification-ipc-primary>
<classification-ipc-secondary>
<ipc>G06N005/02</ipc>
</classification-ipc-secondary>
<classification-ipc-edition>07</classification-ipc-edition>
</classification-ipc>
<classification-us>
<classification-us-primary>
<uspc>
<class>706</class>
<subclass>047000</subclass>
</uspc>
</classification-us-primary>
</classification-us>
<title-of-invention>Vision-based method and apparatus for detecting an event requiring assistance or documentation</title-of-invention>
</technical-information>
<inventors>
<first-named-inventor>
<name>
<given-name>Srinivas</given-name>
<family-name>Gutta</family-name>
</name>
<residence>
<residence-us>
<city>Buchanan</city>
<state>NY</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</first-named-inventor>
<inventor>
<name>
<given-name>Damian</given-name>
<middle-name>M.</middle-name>
<family-name>Lyons</family-name>
</name>
<residence>
<residence-us>
<city>Putnam Valley</city>
<state>NY</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
</inventors>
<assignee>
<organization-name>Koninklijke Philips Electronics N.V.</organization-name>
<assignee-type>02</assignee-type>
</assignee>
<correspondence-address>
<name-1>Corporate Patent Counsel</name-1>
<name-2>U.S. Philips Corporation</name-2>
<address>
<address-1>580 White Plains Road</address-1>
<city>Tarrytown</city>
<state>NY</state>
<postalcode>10591</postalcode>
<country>
<country-code>US</country-code>
</country>
</address>
</correspondence-address>
</subdoc-bibliographic-information>
<subdoc-abstract>
<paragraph id="A-0001" lvl="0">A method and apparatus are disclosed for monitoring a location using vision-based technologies to recognize predefined assistance-invoking events. One or more image capture devices are focused on a given location. The captured images are processed to identify one or more predefined events and to initiate an appropriate response, such as sending assistance or recording the event for evidentiary purposes. A number of rules define various assistance-invoking events. Each rule contains one or more conditions that must be satisfied and a corresponding action-item that should be performed when the rule is satisfied. At least one of the conditions for each rule identifies a feature that must be detected in an image using vision-based techniques. An event monitoring process is also disclosed that analyzes the captured images to detect one or more assistance-invoking events defined by the event rules. </paragraph>
</subdoc-abstract>
<subdoc-description>
<summary-of-invention>
<section>
<heading lvl="1">FIELD OF THE INVENTION </heading>
<paragraph id="P-0001" lvl="0"><number>&lsqb;0001&rsqb;</number> The present invention relates to computer-vision techniques, and more particularly, to a method and apparatus for detecting events using vision-based recognition techniques. </paragraph>
</section>
<section>
<heading lvl="1">BACKGROUND OF THE INVENTION </heading>
<paragraph id="P-0002" lvl="0"><number>&lsqb;0002&rsqb;</number> Due to increasing labor costs, as well as an inadequate number of qualified employee candidates, many retail businesses and other establishments must often operate with an insufficient number of employees. Thus, when there are not enough employees to perform every desired function, the management must prioritize responsibilities to ensure that the most important functions are satisfied, or find an alternate way to perform the function. For example, many retail establishments utilize automated theft detection systems to replace or supplement a security staff. </paragraph>
<paragraph id="P-0003" lvl="0"><number>&lsqb;0003&rsqb;</number> In addition, many businesses do not have enough employees to adequately monitor an entire store or other location, for example, for security purposes or to determine when a patron may require assistance. Thus, many businesses and other establishments position cameras at various locations to monitor the activities of patrons and employees. While the images generated by the cameras typically allow the various locations to be monitored by one person positioned at a central location, such a system nonetheless requires human monitoring to detect events of interest. </paragraph>
<paragraph id="P-0004" lvl="0"><number>&lsqb;0004&rsqb;</number> When such an event of interest includes an injury of an employee or patron, the business proprietor may be exposed to liability. It is therefore desirable to archive any associated images associated with the injury-related event for subsequent evidentiary purposes. With a conventional system requiring human monitoring of the images, however, such injury-related events may not be detected or reported at the time of the event, or within a sufficient period of time to ensure that the images are archived. </paragraph>
<paragraph id="P-0005" lvl="0"><number>&lsqb;0005&rsqb;</number> With an increasing trend towards false claims of &ldquo;slip and fall&rdquo; and other injuries, it is particularly beneficial for the business proprietor to record images of an injury-related event for evidentiary purposes. </paragraph>
<paragraph id="P-0006" lvl="0"><number>&lsqb;0006&rsqb;</number> A need therefore exists for a monitoring system that uses vision-based technologies to automatically recognize events suggesting that an individual may require assistance. A further need exists for an event monitoring system that employs a rule-base to define each event. Yet another need exists for a monitoring system that uses vision-based technologies to recognize predefined events and to record such events for evidentiary purposes. </paragraph>
</section>
<section>
<heading lvl="1">SUMMARY OF THE INVENTION </heading>
<paragraph id="P-0007" lvl="0"><number>&lsqb;0007&rsqb;</number> Generally, a method and apparatus are disclosed for monitoring a location using vision-based technologies to recognize predefined events where an individual may require assistance or may involve liability, referred to herein as assistance-invoking events. The disclosed event monitoring system includes one or more image capture devices that are focused on a given location. The captured images are processed by the event monitoring system to identify one or more assistance-invoking events and to initiate an appropriate response, such as sending assistance or recording the event for evidentiary purposes (or both). </paragraph>
<paragraph id="P-0008" lvl="0"><number>&lsqb;0008&rsqb;</number> According to one aspect of the invention, a number of rules are utilized to define various assistance-invoking events. Each rule contains one or more conditions that must be satisfied in order for the rule to be triggered, and, optionally, a corresponding action-item that should be performed when the rule is satisfied, such as sending assistance or recording the event for evidentiary purposes (or both). At least one condition for each rule identifies a feature that must be detected in an image using vision-based techniques. Upon detection of a predefined event, the corresponding action, if any, is performed by the event monitoring system. </paragraph>
<paragraph id="P-0009" lvl="0"><number>&lsqb;0009&rsqb;</number> When the identified assistance-invoking event is a patron requiring assistance, for example, the corresponding action item may be automatically sending store personnel or medical assistance, if appropriate. An illustrative event monitoring process is disclosed to illustrate the general concepts of the present invention that analyzes the captured images to detect one or more assistance-invoking events defined by the event rules. </paragraph>
<paragraph id="P-0010" lvl="0"><number>&lsqb;0010&rsqb;</number> A more complete understanding of the present invention, as well as further features and advantages of the present invention, will be obtained by reference to the following detailed description and drawings.</paragraph>
</section>
</summary-of-invention>
<brief-description-of-drawings>
<section>
<heading lvl="1">BRIEF DESCRIPTION OF THE DRAWINGS </heading>
<paragraph id="P-0011" lvl="0"><number>&lsqb;0011&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 1</cross-reference> illustrates an event monitoring system in accordance with the present invention; </paragraph>
<paragraph id="P-0012" lvl="0"><number>&lsqb;0012&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 2</cross-reference> illustrates a sample table from the event database of <cross-reference target="DRAWINGS">FIG. 1</cross-reference>; </paragraph>
<paragraph id="P-0013" lvl="0"><number>&lsqb;0013&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 3</cross-reference> is a flow chart describing an exemplary event monitoring process embodying principles of the present invention; and </paragraph>
<paragraph id="P-0014" lvl="0"><number>&lsqb;0014&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 4</cross-reference> is a flow chart describing an exemplary slip and fall detection process incorporating features of the present invention. </paragraph>
</section>
</brief-description-of-drawings>
<detailed-description>
<section>
<heading lvl="1">DETAILED DESCRIPTION </heading>
<paragraph id="P-0015" lvl="0"><number>&lsqb;0015&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 1</cross-reference> illustrates an event monitoring system <highlight><bold>100</bold></highlight> in accordance with the present invention. Generally, the events detected by the present invention are those events involving individuals that may require assistance or events involving liability, hereinafter collectively referred to as &ldquo;assistance-invoking events.&rdquo; As shown in <cross-reference target="DRAWINGS">FIG. 1</cross-reference>, the event monitoring system <highlight><bold>100</bold></highlight> includes one or more image capture devices <highlight><bold>150</bold></highlight>-<highlight><bold>1</bold></highlight> through <highlight><bold>150</bold></highlight>-N (hereinafter, collectively referred to as image capture devices <highlight><bold>150</bold></highlight>) that are focused on one or more monitored areas <highlight><bold>160</bold></highlight>. The monitored area <highlight><bold>160</bold></highlight> can be any location that is likely to have an individual requiring assistance, such as an aisle in a store, or to have an event involving potential liability, such as an intersection that may have a significant number of vehicle accidents. </paragraph>
<paragraph id="P-0016" lvl="0"><number>&lsqb;0016&rsqb;</number> The present invention recognizes that assistance-invoking events are often subsequently involved in litigation. Thus, according to another aspect of the invention, the images captured by the image capture devices <highlight><bold>150</bold></highlight> may be recorded and stored for evidentiary purposes, for example, in an image archive database <highlight><bold>175</bold></highlight>. As discussed further below, images associated with each detected assistance-invoking event may optionally be recorded in the image archive database <highlight><bold>175</bold></highlight> for evidentiary purposes. In one embodiment, a predefined number of image frames before and after each detected assistance-invoking event may be recorded in the image archive database <highlight><bold>175</bold></highlight>, together with a timestamp of the event, for example, for evidentiary purposes. </paragraph>
<paragraph id="P-0017" lvl="0"><number>&lsqb;0017&rsqb;</number> Each image capture device <highlight><bold>150</bold></highlight> may be embodied, for example, as a fixed or pan-tilt-zoom (PTZ) camera for capturing image or video information. The images generated by the image capture devices <highlight><bold>150</bold></highlight> are processed by the event monitoring system <highlight><bold>100</bold></highlight>, in a manner discussed below in conjunction with <cross-reference target="DRAWINGS">FIG. 3</cross-reference>, to identify one or more predefined assistance-invoking events. In one implementation, the present invention employs an event database <highlight><bold>200</bold></highlight>, discussed further below in conjunction with <cross-reference target="DRAWINGS">FIG. 2</cross-reference>, that records a number of rules defining various assistance-invoking events. </paragraph>
<paragraph id="P-0018" lvl="0"><number>&lsqb;0018&rsqb;</number> The assistance-invoking events defined by each rule may be detected by the event monitoring system <highlight><bold>100</bold></highlight> in accordance with the present invention. As discussed further below, each rule contains one or more criteria that must be satisfied in order for the rule to be triggered, and, optionally, a corresponding action-item that should be performed when the predefined criteria for initiating the rule is satisfied. At least one of the criteria for each rule is a condition detected in an image using -vision-based techniques, in accordance with the present invention. Upon detection of such a predefined traffic event, the corresponding action, if any, is performed by the event monitoring system <highlight><bold>100</bold></highlight>, such as sending assistance or recording the event for evidentiary purposes. </paragraph>
<paragraph id="P-0019" lvl="0"><number>&lsqb;0019&rsqb;</number> As shown in <cross-reference target="DRAWINGS">FIG. 1</cross-reference>, and discussed further below in conjunction with <cross-reference target="DRAWINGS">FIGS. 3 and 4</cross-reference>, the event monitoring system <highlight><bold>100</bold></highlight> also contains an event detection process <highlight><bold>300</bold></highlight> and a slip and fall detection process <highlight><bold>400</bold></highlight>. Generally, the event detection process <highlight><bold>300</bold></highlight> analyzes the images obtained by the image capture devices <highlight><bold>150</bold></highlight> and detects a number of specific, yet exemplary, assistance-invoking events defined in the traffic event database <highlight><bold>200</bold></highlight>. The slip and fall detection process <highlight><bold>400</bold></highlight> analyzes the images obtained by the image capture devices <highlight><bold>150</bold></highlight> and detects when a person has fallen. </paragraph>
<paragraph id="P-0020" lvl="0"><number>&lsqb;0020&rsqb;</number> The event monitoring system <highlight><bold>100</bold></highlight> may be embodied as any computing device, such as a personal computer or workstation, that contains a processor <highlight><bold>120</bold></highlight>, such as a central processing unit (CPU), and memory <highlight><bold>110</bold></highlight>, such as RAM and/or ROM. In an alternate implementation, the image processing system <highlight><bold>100</bold></highlight> may be embodied using an application specific integrated circuit (ASIC). </paragraph>
<paragraph id="P-0021" lvl="0"><number>&lsqb;0021&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 2</cross-reference> illustrates an exemplary table of the event database <highlight><bold>200</bold></highlight> that records each of the rules that define various assistance-invoking events. Each rule in the event database <highlight><bold>200</bold></highlight> includes predefined criteria specifying the conditions under which the rule should be initiated, and, optionally, a corresponding action item that should be triggered when the criteria associated with the rule is satisfied. Typically, the action item defines one or more appropriate step(s) that should be performed when the rule is triggered, such as sending assistance or recording the event for evidentiary purposes (or both). </paragraph>
<paragraph id="P-0022" lvl="0"><number>&lsqb;0022&rsqb;</number> As shown in <cross-reference target="DRAWINGS">FIG. 2</cross-reference>, the exemplary event database <highlight><bold>200</bold></highlight> maintains a plurality of records, such as records <highlight><bold>205</bold></highlight>-<highlight><bold>210</bold></highlight>, each associated with a different rule. For each rule, the event database <highlight><bold>200</bold></highlight> identifies the rule criteria in field <highlight><bold>250</bold></highlight> and the corresponding action item, if any, in field <highlight><bold>260</bold></highlight>. For example, the rule recorded in record <highlight><bold>206</bold></highlight> is an event corresponding to a vehicle accident. As indicated in field <highlight><bold>250</bold></highlight>, the rule in record <highlight><bold>206</bold></highlight> is triggered when two vehicles collide. The rule in record <highlight><bold>206</bold></highlight> specifies a number of independent conditions that may be detected to initiate the rule. For example, when two vehicles are within a predefined distance of one another, an accident has likely occurred. As indicated in field <highlight><bold>260</bold></highlight>, the corresponding action consists of sending assistance to the monitored location <highlight><bold>160</bold></highlight> and recording the event for evidentiary purposes. </paragraph>
<paragraph id="P-0023" lvl="0"><number>&lsqb;0023&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 3</cross-reference> is a flow chart describing an exemplary event detection process <highlight><bold>300</bold></highlight>. The event detection process <highlight><bold>300</bold></highlight> analyzes images obtained from the image capture devices <highlight><bold>150</bold></highlight> and detects a number of specific, yet exemplary, assistance-invoking events defined in the event database <highlight><bold>200</bold></highlight>. As shown in <cross-reference target="DRAWINGS">FIG. 3</cross-reference>, the event detection process <highlight><bold>300</bold></highlight> initially obtains one or more images of the monitored area <highlight><bold>160</bold></highlight> from the image capture devices <highlight><bold>150</bold></highlight> during step <highlight><bold>310</bold></highlight>. </paragraph>
<paragraph id="P-0024" lvl="0"><number>&lsqb;0024&rsqb;</number> Thereafter, the images are analyzed during step <highlight><bold>320</bold></highlight> using video content analysis (VCA) techniques. For a detailed discussion of suitable VCA techniques, see, for example, Nathanael Rota and Monique Thonnat, &ldquo;Video Sequence Interpretation for Visual Surveillance,&rdquo; in Proc. of the 3d IEEE Int&apos;l Workshop on Visual Surveillance, 59-67, Dublin, Ireland (Jul. 1, 2000), and Jonathan Owens and Andrew Hunter, &ldquo;Application of the Self-Organizing Map to Trajectory Classification,&rsquo; in Proc. of the 3d IEEE Int&apos;l Workshop on Visual Surveillance, 77-83, Dublin, Ireland (Jul. 1, 2000), incorporated by reference herein. Generally, the VCA techniques are employed to recognize various features in the images obtained by the image capture devices <highlight><bold>150</bold></highlight>. </paragraph>
<paragraph id="P-0025" lvl="0"><number>&lsqb;0025&rsqb;</number> A test is performed during step <highlight><bold>330</bold></highlight> to determine if the video content analysis detects a predefined event, as defined in the event database <highlight><bold>200</bold></highlight>. If it is determined during step <highlight><bold>330</bold></highlight> that the video content analysis does not detect a predefined event, then program control returns to step <highlight><bold>310</bold></highlight> to continue monitoring the location(s) <highlight><bold>160</bold></highlight> in the manner discussed above. </paragraph>
<paragraph id="P-0026" lvl="0"><number>&lsqb;0026&rsqb;</number> If, however, it is determined during step <highlight><bold>330</bold></highlight> that the video content analysis detects a predefined event, then the event is processed during step <highlight><bold>340</bold></highlight> as indicated in field <highlight><bold>260</bold></highlight> of the event database <highlight><bold>200</bold></highlight>. As previously indicated, according to one aspect of the invention, the images associated with a detected assistance-invoking event may optionally be recorded in the image archive database <highlight><bold>175</bold></highlight>, with a time-stamp for evidentiary purposes during step <highlight><bold>350</bold></highlight>. Program control then terminates (or returns to step <highlight><bold>310</bold></highlight> and continues monitoring location(s) <highlight><bold>160</bold></highlight> in the manner discussed above). </paragraph>
<paragraph id="P-0027" lvl="0"><number>&lsqb;0027&rsqb;</number> As previously indicated, the slip and fall detection process <highlight><bold>400</bold></highlight> analyzes the images obtained by the image capture devices <highlight><bold>150</bold></highlight> and detects when a person has fallen. As shown in <cross-reference target="DRAWINGS">FIG. 4</cross-reference>, the slip and fall detection process <highlight><bold>400</bold></highlight> initially obtains one or more images of the monitored area <highlight><bold>160</bold></highlight> from the image capture devices <highlight><bold>150</bold></highlight> during step <highlight><bold>410</bold></highlight>. </paragraph>
<paragraph id="P-0028" lvl="0"><number>&lsqb;0028&rsqb;</number> Thereafter, subsequent image frames are subtracted during step <highlight><bold>420</bold></highlight> to detect moving objects. It is noted that in the controlled environment of most retail locations, it can be assumed that a detected moving object is a person. However, well-known human classification techniques can optionally be employed for additional safeguards, if desired, as would be apparent to a person of ordinary skill in the art. The projection of each detected object is analyzed during step <highlight><bold>430</bold></highlight> to identify the orientation of the object&apos;s principal axis (vertical or horizontal). </paragraph>
<paragraph id="P-0029" lvl="0"><number>&lsqb;0029&rsqb;</number> A test is performed during step <highlight><bold>440</bold></highlight> to determine if the principal axis of a detected object has transitioned from a vertical orientation in a previous frame to a horizontal orientation in the current frame, suggesting that someone has fallen. If it is determined during step <highlight><bold>440</bold></highlight> that the principal axis of a detected object has not transitioned from a vertical orientation to a horizontal orientation, then a slip and fall has not occurred and program control returns to step <highlight><bold>410</bold></highlight> to continue monitoring the location(s) <highlight><bold>160</bold></highlight> in the manner discussed above. </paragraph>
<paragraph id="P-0030" lvl="0"><number>&lsqb;0030&rsqb;</number> If, however, it is determined during step <highlight><bold>440</bold></highlight> that the principal axis of a detected object has transitioned from a vertical orientation to a horizontal orientation, then a slip and fall has occurred and the detected slip and fall event is processed during step <highlight><bold>450</bold></highlight> as indicated in field <highlight><bold>260</bold></highlight> of the event database <highlight><bold>200</bold></highlight>. As previously indicated, according to one aspect of the invention, the images associated with a detected assistance-invoking event, such as a slip and fall, may optionally be recorded in the image archive database <highlight><bold>175</bold></highlight>, with a time-stamp for evidentiary purposes during step <highlight><bold>460</bold></highlight>. Program control then terminates (or returns to step <highlight><bold>410</bold></highlight> and continues monitoring location(s) <highlight><bold>160</bold></highlight> in the manner discussed above). </paragraph>
<paragraph id="P-0031" lvl="0"><number>&lsqb;0031&rsqb;</number> It is to be understood that the embodiments and variations shown and described herein are merely illustrative of the principles of this invention and that various modifications may be implemented by those skilled in the art without departing from the scope and spirit of the invention. </paragraph>
</section>
</detailed-description>
</subdoc-description>
<subdoc-claims>
<heading lvl="1">What is claimed is: </heading>
<claim id="CLM-00001">
<claim-text><highlight><bold>1</bold></highlight>. A method for detecting an assistance-invoking event at a monitored location, comprising: 
<claim-text>establishing a rule defining said assistance-invoking event, said rule including at least one condition; </claim-text>
<claim-text>processing at least one image of said monitored location to identify said condition; and </claim-text>
<claim-text>providing assistance if said rule is satisfied. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00002">
<claim-text><highlight><bold>2</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, further comprising the step of recording said at least one image if said rule is satisfied. </claim-text>
</claim>
<claim id="CLM-00003">
<claim-text><highlight><bold>3</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein said assistance-invoking event is an injury at said monitored location. </claim-text>
</claim>
<claim id="CLM-00004">
<claim-text><highlight><bold>4</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein said assistance-invoking event is an accident at said monitored location. </claim-text>
</claim>
<claim id="CLM-00005">
<claim-text><highlight><bold>5</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein said assistance-invoking event is a patron in need of assistance. </claim-text>
</claim>
<claim id="CLM-00006">
<claim-text><highlight><bold>6</bold></highlight>. A method for detecting an assistance-invoking event, comprising: 
<claim-text>obtaining at least one image of a monitored location; </claim-text>
<claim-text>analyzing said image using video content analysis techniques to identify at least one predefined feature in said image associated with said assistance-invoking event; and </claim-text>
<claim-text>providing assistance if said predefined feature is recognized in one of said images. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00007">
<claim-text><highlight><bold>7</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00006">claim 6</dependent-claim-reference>, further comprising the step of recording said at least one image if said predefined feature is recognized in one of said images. </claim-text>
</claim>
<claim id="CLM-00008">
<claim-text><highlight><bold>8</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00006">claim 6</dependent-claim-reference>, wherein said assistance-invoking event is an injury at said monitored location. </claim-text>
</claim>
<claim id="CLM-00009">
<claim-text><highlight><bold>9</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00006">claim 6</dependent-claim-reference>, wherein said assistance-invoking event is an accident at said monitored location. </claim-text>
</claim>
<claim id="CLM-00010">
<claim-text><highlight><bold>10</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00006">claim 6</dependent-claim-reference>, wherein said assistance-invoking event is a patron in need of assistance. </claim-text>
</claim>
<claim id="CLM-00011">
<claim-text><highlight><bold>11</bold></highlight>. A method for documenting an event that may be involved in litigation, comprising: 
<claim-text>obtaining at least one image of a monitored location; </claim-text>
<claim-text>analyzing said image using video content analysis techniques to identify at least one predefined feature in said image associated with said event; and </claim-text>
<claim-text>recording said at least one image if said predefined feature is recognized in one of said images. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00012">
<claim-text><highlight><bold>12</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein said event is an injury at said monitored location. </claim-text>
</claim>
<claim id="CLM-00013">
<claim-text><highlight><bold>13</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00011">claim 11</dependent-claim-reference>, wherein said event is an accident at said monitored location. </claim-text>
</claim>
<claim id="CLM-00014">
<claim-text><highlight><bold>14</bold></highlight>. A system for detecting an assistance-invoking event, comprising: 
<claim-text>a memory that stores computer-readable code; and </claim-text>
<claim-text>a processor operatively coupled to said memory, said processor configured to implement said computer-readable code, said computer-readable code configured to: </claim-text>
<claim-text>obtain at least one image of a monitored location; </claim-text>
<claim-text>analyze said image using video content analysis techniques to identify at least one predefined feature in said image associated with said assistance-invoking event; and </claim-text>
<claim-text>provide assistance if said predefined feature is recognized in one of said images. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00015">
<claim-text><highlight><bold>15</bold></highlight>. The system of <dependent-claim-reference depends_on="CLM-00011">claim 14</dependent-claim-reference>, wherein said feature is recorded in a rule defining said assistance-invoking event. </claim-text>
</claim>
<claim id="CLM-00016">
<claim-text><highlight><bold>16</bold></highlight>. The system of <dependent-claim-reference depends_on="CLM-00011">claim 14</dependent-claim-reference>, wherein said processor is further configured to record said at least one image if said predefined feature is recognized in one of said images. </claim-text>
</claim>
<claim id="CLM-00017">
<claim-text><highlight><bold>17</bold></highlight>. The system of <dependent-claim-reference depends_on="CLM-00011">claim 14</dependent-claim-reference>, wherein said assistance-invoking event is an injury at said monitored location. </claim-text>
</claim>
<claim id="CLM-00018">
<claim-text><highlight><bold>18</bold></highlight>. The system of <dependent-claim-reference depends_on="CLM-00011">claim 14</dependent-claim-reference>, wherein said assistance-invoking event is an accident at said monitored location. </claim-text>
</claim>
<claim id="CLM-00019">
<claim-text><highlight><bold>19</bold></highlight>. The system of <dependent-claim-reference depends_on="CLM-00011">claim 14</dependent-claim-reference>, wherein said assistance-invoking event is a patron in need of assistance. </claim-text>
</claim>
<claim id="CLM-00020">
<claim-text><highlight><bold>20</bold></highlight>. A system for documenting an event that may be involved in litigation, comprising: 
<claim-text>a memory that stores computer-readable code; and </claim-text>
<claim-text>a processor operatively coupled to said memory, said processor configured to implement said computer-readable code, said computer-readable code configured to: </claim-text>
<claim-text>obtain at least one image of a monitored location; </claim-text>
<claim-text>analyze said image using video content analysis techniques to identify at least one predefined feature in said image associated with said event; and </claim-text>
<claim-text>record said at least one image if said predefined feature is recognized in one of said images. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00021">
<claim-text><highlight><bold>21</bold></highlight>. The system of <dependent-claim-reference depends_on="CLM-00022">claim 20</dependent-claim-reference>, wherein said feature is recorded in a rule defining said assistance-invoking event. </claim-text>
</claim>
<claim id="CLM-00022">
<claim-text><highlight><bold>22</bold></highlight>. The system of <dependent-claim-reference depends_on="CLM-00022">claim 20</dependent-claim-reference>, wherein said event is an injury at said monitored location. </claim-text>
</claim>
<claim id="CLM-00023">
<claim-text><highlight><bold>23</bold></highlight>. The system of <dependent-claim-reference depends_on="CLM-00022">claim 20</dependent-claim-reference>, wherein said event is an accident at said monitored location. </claim-text>
</claim>
<claim id="CLM-00024">
<claim-text><highlight><bold>24</bold></highlight>. An article of manufacture for detecting an assistance-invoking event, comprising: 
<claim-text>a computer readable medium having computer readable code means embodied thereon, said computer readable program code means comprising: 
<claim-text>a step to obtain at least one image of a monitored location; </claim-text>
<claim-text>a step to analyze said image using video content analysis techniques to identify at least one predefined feature in said image associated with said assistance-invoking event; and </claim-text>
<claim-text>a step to provide assistance if said predefined feature is recognized in one of said images. </claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00025">
<claim-text><highlight><bold>25</bold></highlight>. An article of manufacture for documenting an event that may be involved in litigation, comprising: 
<claim-text>a computer readable medium having computer readable code means embodied thereon, said computer readable program code means comprising: 
<claim-text>a step to obtain at least one image of a monitored location; </claim-text>
<claim-text>a step to analyze said image using video content analysis techniques to identify at least one predefined feature in said image associated with said event; and </claim-text>
<claim-text>a step to record said at least one image if said predefined feature is recognized in one of said images.</claim-text>
</claim-text>
</claim-text>
</claim>
</subdoc-claims>
<subdoc-drawings id="DRAWINGS">
<heading lvl="0" align="CENTER">Drawings</heading>
<representative-figure>1</representative-figure>
<figure id="figure-D00000">
<image id="EMI-D00000" file="US20030004913A1-20030102-D00000.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00001">
<image id="EMI-D00001" file="US20030004913A1-20030102-D00001.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00002">
<image id="EMI-D00002" file="US20030004913A1-20030102-D00002.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00003">
<image id="EMI-D00003" file="US20030004913A1-20030102-D00003.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00004">
<image id="EMI-D00004" file="US20030004913A1-20030102-D00004.TIF" imf="TIFF" ti="DR"/>
</figure>
</subdoc-drawings>
</patent-application-publication>
