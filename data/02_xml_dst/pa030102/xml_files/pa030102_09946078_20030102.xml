<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE patent-application-publication SYSTEM "pap-v16-2002-01-01.dtd" [
<!ENTITY US20030005120A1-20030102-D00000.TIF SYSTEM "US20030005120A1-20030102-D00000.TIF" NDATA TIF>
<!ENTITY US20030005120A1-20030102-D00001.TIF SYSTEM "US20030005120A1-20030102-D00001.TIF" NDATA TIF>
<!ENTITY US20030005120A1-20030102-D00002.TIF SYSTEM "US20030005120A1-20030102-D00002.TIF" NDATA TIF>
<!ENTITY US20030005120A1-20030102-D00003.TIF SYSTEM "US20030005120A1-20030102-D00003.TIF" NDATA TIF>
<!ENTITY US20030005120A1-20030102-D00004.TIF SYSTEM "US20030005120A1-20030102-D00004.TIF" NDATA TIF>
<!ENTITY US20030005120A1-20030102-D00005.TIF SYSTEM "US20030005120A1-20030102-D00005.TIF" NDATA TIF>
<!ENTITY US20030005120A1-20030102-D00006.TIF SYSTEM "US20030005120A1-20030102-D00006.TIF" NDATA TIF>
<!ENTITY US20030005120A1-20030102-D00007.TIF SYSTEM "US20030005120A1-20030102-D00007.TIF" NDATA TIF>
<!ENTITY US20030005120A1-20030102-D00008.TIF SYSTEM "US20030005120A1-20030102-D00008.TIF" NDATA TIF>
<!ENTITY US20030005120A1-20030102-D00009.TIF SYSTEM "US20030005120A1-20030102-D00009.TIF" NDATA TIF>
<!ENTITY US20030005120A1-20030102-D00010.TIF SYSTEM "US20030005120A1-20030102-D00010.TIF" NDATA TIF>
<!ENTITY US20030005120A1-20030102-D00011.TIF SYSTEM "US20030005120A1-20030102-D00011.TIF" NDATA TIF>
<!ENTITY US20030005120A1-20030102-D00012.TIF SYSTEM "US20030005120A1-20030102-D00012.TIF" NDATA TIF>
<!ENTITY US20030005120A1-20030102-D00013.TIF SYSTEM "US20030005120A1-20030102-D00013.TIF" NDATA TIF>
<!ENTITY US20030005120A1-20030102-D00014.TIF SYSTEM "US20030005120A1-20030102-D00014.TIF" NDATA TIF>
<!ENTITY US20030005120A1-20030102-D00015.TIF SYSTEM "US20030005120A1-20030102-D00015.TIF" NDATA TIF>
<!ENTITY US20030005120A1-20030102-D00016.TIF SYSTEM "US20030005120A1-20030102-D00016.TIF" NDATA TIF>
<!ENTITY US20030005120A1-20030102-D00017.TIF SYSTEM "US20030005120A1-20030102-D00017.TIF" NDATA TIF>
<!ENTITY US20030005120A1-20030102-D00018.TIF SYSTEM "US20030005120A1-20030102-D00018.TIF" NDATA TIF>
]>
<patent-application-publication>
<subdoc-bibliographic-information>
<document-id>
<doc-number>20030005120</doc-number>
<kind-code>A1</kind-code>
<document-date>20030102</document-date>
</document-id>
<publication-filing-type>new</publication-filing-type>
<domestic-filing-data>
<application-number>
<doc-number>09946078</doc-number>
</application-number>
<application-number-series-code>09</application-number-series-code>
<filing-date>20010904</filing-date>
</domestic-filing-data>
<technical-information>
<classification-ipc>
<classification-ipc-primary>
<ipc>G06F015/173</ipc>
</classification-ipc-primary>
<classification-ipc-edition>07</classification-ipc-edition>
</classification-ipc>
<classification-us>
<classification-us-primary>
<uspc>
<class>709</class>
<subclass>225000</subclass>
</uspc>
</classification-us-primary>
<classification-us-secondary>
<uspc>
<class>709</class>
<subclass>223000</subclass>
</uspc>
</classification-us-secondary>
</classification-us>
<title-of-invention>Information replication system having enhanced error detection and recovery</title-of-invention>
</technical-information>
<continuity-data>
<continuations>
<continuation-in-part-of>
<parent-child>
<child>
<document-id>
<doc-number>09946078</doc-number>
<kind-code>A1</kind-code>
<document-date>20010904</document-date>
</document-id>
</child>
<parent>
<document-id>
<doc-number>09894422</doc-number>
<document-date>20010628</document-date>
<country-code>US</country-code>
</document-id>
</parent>
<parent-status>PENDING</parent-status>
</parent-child>
</continuation-in-part-of>
</continuations>
</continuity-data>
<inventors>
<first-named-inventor>
<name>
<given-name>Madhav</given-name>
<family-name>Mutalik</family-name>
</name>
<residence>
<residence-us>
<city>Southborough</city>
<state>MA</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</first-named-inventor>
<inventor>
<name>
<given-name>Ajay</given-name>
<family-name>Shekhar</family-name>
</name>
<residence>
<residence-us>
<city>Medway</city>
<state>MA</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
<inventor>
<name>
<given-name>Neil</given-name>
<family-name>Schutzman</family-name>
</name>
<residence>
<residence-us>
<city>Marlborough</city>
<state>MA</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
<inventor>
<name>
<given-name>Thomas</given-name>
<family-name>Dings</family-name>
</name>
<residence>
<residence-us>
<city>Hopkinton</city>
<state>MA</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
<inventor>
<name>
<given-name>Ananthan</given-name>
<middle-name>K.</middle-name>
<family-name>Pillai</family-name>
</name>
<residence>
<residence-us>
<city>Shrewsbury</city>
<state>MA</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
<inventor>
<name>
<given-name>John</given-name>
<middle-name>E.</middle-name>
<family-name>Stockenberg</family-name>
</name>
<residence>
<residence-us>
<city>Newport</city>
<state>RI</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
<inventor>
<name>
<given-name>Michael</given-name>
<middle-name>H.</middle-name>
<family-name>Wright</family-name>
</name>
<residence>
<residence-us>
<city>Franklin</city>
<state>MA</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
</inventors>
<correspondence-address>
<name-1>DALY, CROWLEY &amp; MOFFORD, LLP</name-1>
<name-2></name-2>
<address>
<address-1>SUITE 101</address-1>
<address-2>275 TURNPIKE STREET</address-2>
<city>CANTON</city>
<state>MA</state>
<postalcode>02021-2310</postalcode>
<country>
<country-code>US</country-code>
</country>
</address>
</correspondence-address>
</subdoc-bibliographic-information>
<subdoc-abstract>
<paragraph id="A-0001" lvl="0">An information recovery system includes mountable data volume replications that can be examined for enhanced error detection and correction. </paragraph>
</subdoc-abstract>
<subdoc-description>
<cross-reference-to-related-applications>
<heading lvl="1">CROSS REFERENCE TO RELATED APPLICATIONS </heading>
<paragraph id="P-0001" lvl="0"><number>&lsqb;0001&rsqb;</number> The present application is a continuation-in-part of U.S. patent application Ser. No. 09/894,422, filed on Jun. 28, 2001, which is incorporated herein by reference.</paragraph>
</cross-reference-to-related-applications>
<summary-of-invention>
<section>
<heading lvl="1">STATEMENT REGARDING FEDERALLY SPONSORED RESEARCH </heading>
<paragraph id="P-0002" lvl="0"><number>&lsqb;0002&rsqb;</number> Not Applicable. </paragraph>
</section>
<section>
<heading lvl="1">FIELD OF THE INVENTION </heading>
<paragraph id="P-0003" lvl="0"><number>&lsqb;0003&rsqb;</number> The present invention relates generally to data storage and, more particularly, to data replication systems. </paragraph>
</section>
<section>
<heading lvl="1">BACKGROUND OF THE INVENTION </heading>
<paragraph id="P-0004" lvl="0"><number>&lsqb;0004&rsqb;</number> As is known in the art, computer systems that process and store large amounts of data typically include one or more processors in communication with a shared data storage system in which the data is stored. The data storage system can include one or more storage devices, such as disk drives. To minimize data loss, the computer systems can also include a backup storage system in communication with the primary processor and the data storage system. </paragraph>
<paragraph id="P-0005" lvl="0"><number>&lsqb;0005&rsqb;</number> Known backup storage systems can include a backup storage device (such as tape storage or any other storage mechanism), together with a system for placing data into the storage device and recovering the data from that storage device. To perform a backup, the host copies data from the shared storage system across the network to the backup storage system. Thus, an actual data file can be communicated over the network to the backup storage device. </paragraph>
<paragraph id="P-0006" lvl="0"><number>&lsqb;0006&rsqb;</number> The shared storage system corresponds to the actual physical storage. For the host to write the backup data over the network to the backup storage system, the host first converts the backup data into file data, i.e., the host retrieves the data from the physical storage system level, and converts the data into application level format (e.g. a file) through a logical volume manager level, a file system level and the application level. When the backup storage device receives the data file, the backup storage system can take the application level data file, and convert it to its appropriate format for the backup storage system. If the backup storage system is a tape-based device, the data is converted to a serial format of blocks or segments. </paragraph>
<paragraph id="P-0007" lvl="0"><number>&lsqb;0007&rsqb;</number> The EMC Data Manager (EDM) is capable of such backup and restore over a network, as described in numerous publications available from EMC of Hopkinton, Mass., including the EDM User Guide (Network) &ldquo;Basic EDM Product Manual.&rdquo; An exemplary prior art backup storage architecture in which a direct connection is established between the shared storage system and the backup storage system is described in U.S. Pat. No. 6,047,294, assigned to assignee of the present invention, entitled Logical Restore from a Physical Backup in Computer Storage System, and incorporated herein by reference. </paragraph>
<paragraph id="P-0008" lvl="0"><number>&lsqb;0008&rsqb;</number> For large databases, tape-based data backup and restore systems, which are well known in the art, can be used. In general, files, databases and the like are copied to tape media at selected times. Typically, data is periodically backed up to prevent the loss of data due to software errors, human error, hardware failures. Upon detection of an error, in an online database, for example, the backed up data can be restored to effect recovery of the data. While restore refers to obtaining backed up data, data recovery refers to the entire process in which applications can access and use the retrieved data. Transactions since the time of backup can be recreated using so-called redo logs. </paragraph>
<paragraph id="P-0009" lvl="0"><number>&lsqb;0009&rsqb;</number> Tape-based backup and restore systems have a number of disadvantages. For example, due to the significant amount of time and overhead associated with backing up and restoring data to tape, such operations are performed relatively infrequently. The longer the period between backup and restoration, the more complicated and time consuming the overall recovery process becomes since, for example, this may render it more difficult to determine the point at which an error occurred. In addition, improvements in the data restore process, such as faster tape access times, provide only incremental advances in the overall data recovery process. </paragraph>
<paragraph id="P-0010" lvl="0"><number>&lsqb;0010&rsqb;</number> Further, data on tape cannot be accessed until it is restored to disk. Only when the data has been restored can a host computer examine the data. The data must be reformatted for each transition between tape and disk, which requires significant processing resources and elapsed time. </paragraph>
<paragraph id="P-0011" lvl="0"><number>&lsqb;0011&rsqb;</number> A further disadvantage associated with tape-based data storage systems is associated with the data recovery process itself. For example, after an error has occurred an operator, such as a database administrator, evaluates the error in an attempt to find a correct the error. However, the administrator has to deal with limitations imposed by the nature of tape-based storage. For a large mission critical database, it can be prohibitively expensive to shut down the database and perform a restoration from tape. If all possible, the administrator will attempt to perform a repair of the database. However, the risks of corrupting the entire database, causing additional errors, and failing to remedy the error, are significant. </paragraph>
<paragraph id="P-0012" lvl="0"><number>&lsqb;0012&rsqb;</number> In addition, it is not always known at what time the database became corrupted. In the case where data must be restored from tape, correction of the error can be an iterative and time-consuming process. The administrator may select a first set of tapes for restoration, after which the database can be examined to determine if the error is corrected. If it is not, another set of tapes, which is typically an earlier backup, must be restored. Data examination steps are then performed until the error is corrected. </paragraph>
<paragraph id="P-0013" lvl="0"><number>&lsqb;0013&rsqb;</number> Once the error is corrected, the error may be re-introduced into the database as post backup transactions are added to the database from the redo logs. The point at which the error occurs must be identified. The time and effort associated with iterative tape restores and error identification can be quite substantial. </paragraph>
<paragraph id="P-0014" lvl="0"><number>&lsqb;0014&rsqb;</number> One known attempt to identify errors includes so-called data scrubbing tools. These tools, which can be run periodically, are used in an endeavor to detect errors as soon as possible. While such tools may detect errors, many production databases, like those used by Internet-based vendors, are mission critical and cannot handle the loading required by such tools. In many applications, data scrubbing tools are not a practical option. </paragraph>
<paragraph id="P-0015" lvl="0"><number>&lsqb;0015&rsqb;</number> In addition, there are times at which it is desirable to recover only a portion of a database. However, known systems do not readily enable recovery of less than the entire database. While a portion of a database may be possible in conventional data backup and restore systems, a high level of skill is required to manually recover a portion of a database. </paragraph>
<paragraph id="P-0016" lvl="0"><number>&lsqb;0016&rsqb;</number> It would, therefore, be desirable to overcome the aforesaid and other disadvantages. </paragraph>
</section>
<section>
<heading lvl="1">SUMMARY OF THE INVENTION </heading>
<paragraph id="P-0017" lvl="0"><number>&lsqb;0017&rsqb;</number> The present invention provides a data recovery system having mountable data volume replications that significantly enhance error detection and correction in comparison to conventional data backup systems. While the invention is primarily shown and described in conjunction with recovering databases, it is understood that the invention is applicable to other systems in which it is desirable to detect and remove errors from stored data. </paragraph>
<paragraph id="P-0018" lvl="0"><number>&lsqb;0018&rsqb;</number> In one aspect of the invention, an information recovery system replicates one or more original data volumes to examine the integrity of the replicated or original data volumes. Upon detecting an error, the system can be used to correct the error by repair and/or data restoration. After successful error detection, the data volumes still having the error can then be corrected. </paragraph>
<paragraph id="P-0019" lvl="0"><number>&lsqb;0019&rsqb;</number> In a further aspect of the invention, an information recovery system provides mounting of partial database replications, such as one or more selected table spaces. With this arrangement, a user can select tablespaces for recovery from a replication of an original database. In one embodiment, the partial mounting can be started in a variety of modes. </paragraph>
<paragraph id="P-0020" lvl="0"><number>&lsqb;0020&rsqb;</number> In another aspect of the invention, an information recovery system provides automated replication storage selection. With this arrangement, the information recovery system automatically discovers potential storage locations that can be used to replicate an existing data volume, such as a database, and selects storage meeting predetermined requirements so as to obviate the need for a database administrator to manually identify the storage. While the invention is primarily shown and described in conjunction with replicating databases to disk, such as Oracle databases, it is understood that the invention is applicable to storage systems in general in which it is desirable to backup digital information on various replication technologies for subsequent restoration. </paragraph>
<paragraph id="P-0021" lvl="0"><number>&lsqb;0021&rsqb;</number> In one embodiment, an IR server obtains a list of potential replication storage locations, e.g., BCVs, production data volumes, e.g., standard volumes, that have been requested to be replicated. A user can specify that certain BCVs must have specified criteria, such as BCV storage previously configured by the user. The system then selects potential BCVs for each standard volume and evaluates each standard/BCV pair. In one embodiment, a pair score is determined based upon the level of resource contention, e.g., disk spindle, bus, etc., for the standard/BCV pair. The resources can be weighted to reflect the level of performance degradation due to contention on the resource. A group score is determined from the pair scores for evaluation of whether an acceptable storage solution has been found. </paragraph>
<paragraph id="P-0022" lvl="0"><number>&lsqb;0022&rsqb;</number> It is understood that certain terminology, such as BCV, standard volume, and others, are used to facilitate an understanding of the invention and should not be construed to limit the invention. </paragraph>
</section>
</summary-of-invention>
<brief-description-of-drawings>
<section>
<heading lvl="1">BRIEF DESCRIPTION OF THE DRAWINGS </heading>
<paragraph id="P-0023" lvl="0"><number>&lsqb;0023&rsqb;</number> The invention will be more fully understood from the following detailed description taken in conjunction with the accompanying drawings, in which: </paragraph>
<paragraph id="P-0024" lvl="0"><number>&lsqb;0024&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 1</cross-reference> is a schematic depiction of an information recovery system in accordance with the present invention; </paragraph>
<paragraph id="P-0025" lvl="0"><number>&lsqb;0025&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 2</cross-reference> is pictorial representation of an exemplary screen display for the system of <cross-reference target="DRAWINGS">FIG. 1</cross-reference>; </paragraph>
<paragraph id="P-0026" lvl="0"><number>&lsqb;0026&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 3</cross-reference> is a pictorial representation of a further exemplary screen display for the system of <cross-reference target="DRAWINGS">FIG. 1</cross-reference>; </paragraph>
<paragraph id="P-0027" lvl="0"><number>&lsqb;0027&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 4</cross-reference> is a schematic block diagram of a data storage network that can form a part of the system of <cross-reference target="DRAWINGS">FIG. 1</cross-reference>; </paragraph>
<paragraph id="P-0028" lvl="0"><number>&lsqb;0028&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 5</cross-reference> is a schematic representation showing further details of a data storage network; </paragraph>
<paragraph id="P-0029" lvl="0"><number>&lsqb;0029&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 6</cross-reference> is a flow diagram showing an exemplary sequence of steps for mapping logical to physical volumes for backup and restore in an information recovery system in accordance with the present invention; </paragraph>
<paragraph id="P-0030" lvl="0"><number>&lsqb;0030&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 7</cross-reference> is a flow diagram showing an exemplary sequence of steps for mapping and discovering volume information in an information recovery system in accordance with the present invention; </paragraph>
<paragraph id="P-0031" lvl="0"><number>&lsqb;0031&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 8</cross-reference> is a flow diagram showing an exemplary sequence of steps for creating a tree structure from volume information discovered for an information recovery system in accordance with the present invention; </paragraph>
<paragraph id="P-0032" lvl="0"><number>&lsqb;0032&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 9</cross-reference> is a flow diagram showing an exemplary sequence of steps for establishing and splitting mirrors in an information recovery system in accordance with the present invention; </paragraph>
<paragraph id="P-0033" lvl="0"><number>&lsqb;0033&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 10</cross-reference> is a flow diagram showing an exemplary sequence of steps for building/mounting logical information in an information recovery system in accordance with the present invention; </paragraph>
<paragraph id="P-0034" lvl="0"><number>&lsqb;0034&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 11</cross-reference> is a flow diagram showing an exemplary sequence of steps for backing up data volumes in an information recovery system in accordance with the present invention; </paragraph>
<paragraph id="P-0035" lvl="0"><number>&lsqb;0035&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 12</cross-reference> is a flow diagram showing an exemplary sequence of steps for restoring data volumes in an information recovery system in accordance with the present invention; </paragraph>
<paragraph id="P-0036" lvl="0"><number>&lsqb;0036&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 13</cross-reference> is a flow diagram showing an exemplary sequence of steps for volume dismount/cleanup in an information recovery system in accordance with the present invention; </paragraph>
<paragraph id="P-0037" lvl="0"><number>&lsqb;0037&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 14</cross-reference> is a pictorial representation of creating volume checkpoints over time in an information recovery system in accordance with the present invention; </paragraph>
<paragraph id="P-0038" lvl="0"><number>&lsqb;0038&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 15</cross-reference> is a pictorial representation of detecting and correcting errors in a data volume in an information recovery system in accordance with the present invention; and </paragraph>
<paragraph id="P-0039" lvl="0"><number>&lsqb;0039&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 16</cross-reference> is a pictorial representation of an exemplary flexible architecture for an information recovery system in accordance with the present invention. </paragraph>
<paragraph id="P-0040" lvl="0"><number>&lsqb;0040&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 17</cross-reference> is a schematic depiction of an information recovery system having partial database replication mounting in accordance with the present invention; </paragraph>
<paragraph id="P-0041" lvl="0"><number>&lsqb;0041&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 18</cross-reference> is further schematic depiction of an information recovery system mounting a partial database replication on a target mount host in accordance with the present invention; and </paragraph>
<paragraph id="P-0042" lvl="0"><number>&lsqb;0042&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 19</cross-reference> is a flow diagram showing an exemplary sequence of steps for mounting a partial replicated database in accordance with the present invention. </paragraph>
</section>
</brief-description-of-drawings>
<detailed-description>
<section>
<heading lvl="1">DETAILED DESCRIPTION OF THE INVENTION </heading>
<paragraph id="P-0043" lvl="0"><number>&lsqb;0043&rsqb;</number> In general, the information recovery system of the present invention provides users with the ability to make replications of logical objects, such as databases and mail systems, and to restore the replicated objects instantly, i.e., as fast as the underlying storage technology supports. While the invention is primarily shown and described in conjunction with an exemplary storage technology known as Symmetrix by EMC Corporation of Hopkinton, Mass., it is understood that the invention is applicable to a variety of storage technologies and operating systems. </paragraph>
<paragraph id="P-0044" lvl="0"><number>&lsqb;0044&rsqb;</number> In one particular embodiment, the information restore system targets various databases, such as Oracle and SQL Server databases. The system enables users to configure a database, or portion thereof, e.g., one or more table spaces, for replication. The configured portion of the database is referred to a replication object (RO). That is, the RO describes the application to be replicated. For each RO, one or more Activities describe how the replication of the RO should be performed. Exemplary information includes how to interact with the database, e.g., online or offline, pre and post scripts, mounting and recovery options, and storage selection criteria. Activities can be run on demand or scheduled for predetermined times. Mounting details, e.g., where and how, can be defined during activity creation. </paragraph>
<paragraph id="P-0045" lvl="0"><number>&lsqb;0045&rsqb;</number> Storage for the activity can be selected by the system in a variety of ways. For example, the system can look for free or previously established storage business continuance volumes (BCVs). As used herein, business continuance volumes refer to a mirror of a standard volume a part of the database. Also, users can define attributes on BCVs to create storage pools and select storage by attributes in the activity. The user can also explicitly assign BCVs to Standard Devices (STDs). </paragraph>
<paragraph id="P-0046" lvl="0"><number>&lsqb;0046&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 1</cross-reference> shows an exemplary information restore system <highlight><bold>100</bold></highlight> in accordance with the present invention. An IR server <highlight><bold>102</bold></highlight>, which includes persistent storage <highlight><bold>104</bold></highlight>, handles activity requests from IR users on desktop machines <highlight><bold>108</bold></highlight>. A web server <highlight><bold>102</bold></highlight><highlight><italic>a </italic></highlight>provides an interface to a Local Area Network (LAN) <highlight><bold>106</bold></highlight>, for example, to enable communication with user desktop machines <highlight><bold>108</bold></highlight>, which can include a user interface, e.g., browser <highlight><bold>108</bold></highlight><highlight><italic>a </italic></highlight>and/or command line interface (CLI) <highlight><bold>108</bold></highlight><highlight><italic>b</italic></highlight>, and an IR application <highlight><bold>108</bold></highlight><highlight><italic>c</italic></highlight>. The IR server <highlight><bold>102</bold></highlight> can also include an IR replication policy manager <highlight><bold>102</bold></highlight><highlight><italic>b </italic></highlight>within an IR daemon <highlight><bold>102</bold></highlight><highlight><italic>c </italic></highlight>for controlling and coordinating replication activity for storage units <highlight><bold>110</bold></highlight><highlight><italic>a </italic></highlight>within a storage area network (SAN) <highlight><bold>110</bold></highlight> that are visible to the respective application hosts. The IR server <highlight><bold>102</bold></highlight> can include a replication policy manager or engine <highlight><bold>102</bold></highlight><highlight><italic>b </italic></highlight>for each supported replication technology to implement decisions regarding what storage to use for a given replication. In one embodiment, the policy engine <highlight><bold>102</bold></highlight><highlight><italic>b </italic></highlight>is provided as a dynamic library that is linked with the IR Daemon. Static state information about existing replications and the corresponding storage is stored in the IR database <highlight><bold>104</bold></highlight>, which can be provided as an SQL Server database. </paragraph>
<paragraph id="P-0047" lvl="0"><number>&lsqb;0047&rsqb;</number> The system <highlight><bold>100</bold></highlight> further includes application hosts <highlight><bold>112</bold></highlight>, e.g., Oracle database server hosts, that are under client control <highlight><bold>112</bold></highlight><highlight><italic>a </italic></highlight>via an application agent <highlight><bold>112</bold></highlight><highlight><italic>b </italic></highlight>and storage service module <highlight><bold>112</bold></highlight><highlight><italic>c</italic></highlight>. The application agent <highlight><bold>112</bold></highlight><highlight><italic>b </italic></highlight>in the IR application client processes user databases, which reside in the storage area network <highlight><bold>110</bold></highlight>. This client control module <highlight><bold>112</bold></highlight><highlight><italic>a</italic></highlight>, which can be provided as a daemon, handles and dispatches client operations. Application agents <highlight><bold>112</bold></highlight>, e.g., plug-ins, for each supported application handle work requests. The IR application clients <highlight><bold>112</bold></highlight> can also communicate with third party vendors via call outs, for example, for driving a third party product to backup a replication of the user database. It is understood that hosts that are used for mounting replications can also be application hosts <highlight><bold>112</bold></highlight>. </paragraph>
<paragraph id="P-0048" lvl="0"><number>&lsqb;0048&rsqb;</number> While shown as three separate hosts, it is understood that one or more of the user, application client and IR server can reside on a single host or machine. It is further understood that the illustrated embodiments, architectures and configurations can be readily modified by one of ordinary skill in the art to meet the requirements of a particular application without departing from the present invention. </paragraph>
<paragraph id="P-0049" lvl="0"><number>&lsqb;0049&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 2</cross-reference> shows an exemplary screen display <highlight><bold>150</bold></highlight> of an IR control panel for an information recovery system in accordance with the present invention. The display <highlight><bold>150</bold></highlight> can include a hierarchy of objects <highlight><bold>152</bold></highlight> in the IR system, such as users, application hosts, applications, storage, the schedule, and replication objects (ROs). In the illustrative display, a replication object named &ldquo;Parts DB/TS1/TS2&rdquo; <highlight><bold>154</bold></highlight> is expanded to show Activities <highlight><bold>156</bold></highlight> and Instances <highlight><bold>158</bold></highlight> of this replication. The Activities include &ldquo;Create CKP on Juniper&rdquo; entry <highlight><bold>160</bold></highlight><highlight><italic>a</italic></highlight>, &ldquo;Disaster CKP&rdquo; entry <highlight><bold>160</bold></highlight><highlight><italic>b</italic></highlight>, and &ldquo;Regular Scheduled&rdquo; entry <highlight><bold>160</bold></highlight><highlight><italic>c</italic></highlight>. The Instances <highlight><bold>158</bold></highlight> show the checkpoints or replications that have been created for the RO. </paragraph>
<paragraph id="P-0050" lvl="0"><number>&lsqb;0050&rsqb;</number> In general, hosts and applications become visible to the system upon installation. When hosts and applications are installed, they are registered in the IR daemon database. </paragraph>
<paragraph id="P-0051" lvl="0"><number>&lsqb;0051&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 3</cross-reference> shows an illustrative screen display <highlight><bold>200</bold></highlight> for creating new replication objects (ROs). To create a new replication object, the user activates a pull down menu and selects &ldquo;File,&rdquo; &ldquo;New,&rdquo; and &ldquo;Replication Object&rdquo; to reach the new RO display <highlight><bold>200</bold></highlight>. The user then specifies the application host in the application host field <highlight><bold>202</bold></highlight><highlight><italic>a </italic></highlight>and application in application field <highlight><bold>202</bold></highlight><highlight><italic>b </italic></highlight>to be configured based upon information from the IR Daemon database of installed application hosts. For a given host and application, each instance of the application can be displayed in an instance field <highlight><bold>202</bold></highlight><highlight><italic>c</italic></highlight>. The user selects an instance and assigns a name to the new replication object in the name field <highlight><bold>202</bold></highlight><highlight><italic>d. </italic></highlight></paragraph>
<paragraph id="P-0052" lvl="0"><number>&lsqb;0052&rsqb;</number> Additional screen displays (not shown) can query the user for additional information. For example, a further screen display can prompt the user for application specific information about the replication object, such as how to access the database (username and password), as well as what portions of the database to backup, e.g., an entire database, tablespaces for an Oracle database, etc. Another screen display enables the user to create activities for the RO. There can be many activities that can be run individually or scheduled for specific times. More particularly, an activity can provide regularly scheduled replications, make a decision support database, and support disaster recovery. </paragraph>
<paragraph id="P-0053" lvl="0"><number>&lsqb;0053&rsqb;</number> In one embodiment, a replication or checkpoint has an expiration time, i.e., how long the replication remains valid. The user can indicate whether it is anticipated that the checkpoint (replicated database) will be mounted. This information may be required when choosing a replication technology (RT), since not all replication technologies allow mountable copies of the checkpoint. The user, when defining storage selection, can choose a replication technology or allow the system to select the RT, as mentioned above. </paragraph>
<paragraph id="P-0054" lvl="0"><number>&lsqb;0054&rsqb;</number> In addition, for each activity the user can provide attributes that are specific to mount, storage and application. Mount attributes define what should be done with the object when it is mounted, such as recovering the database, running a verify program, and doing a tape backup. Storage attributes define what storage should be used to make the replication. Application attributes define when the replication is made and what application specific things need to be done, such as putting the database in on-line backup mode, and using a user script to shut the database down. </paragraph>
<paragraph id="P-0055" lvl="0"><number>&lsqb;0055&rsqb;</number> Selecting the mount options for the replication object allows the user to specify things that should be done with a replication after it is taken. This same set of options can be displayed to the user if the user manually chooses to mount a replication. One option the user has is whether to mount and restore the replication, and where to make it visible. Running an activity can include mounting the replication, which can be fully or partially mounted. </paragraph>
<paragraph id="P-0056" lvl="0"><number>&lsqb;0056&rsqb;</number> After selecting how the application should be mounted, the user can choose what to do with the application, such as choosing which programs to run, running a backup and mounting the application after completion. Multiple programs or backups can be selected for execution. Unmounting involves bringing the application down and removing any storage references that were created to make the replication visible on the target operating system. </paragraph>
<paragraph id="P-0057" lvl="0"><number>&lsqb;0057&rsqb;</number> Storage can be made explicitly known to the IR system either by assigning attributes to it or by explicitly associating standards with replication storage. In one particular embodiment, in the define attributes storage screen, the user is presented with a list of BCVs that were discovered on the data storage device, e.g., Symmetrix, attached to the client machines. The user can then select one or more BCVs and assign attributes to them. </paragraph>
<paragraph id="P-0058" lvl="0"><number>&lsqb;0058&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 4</cross-reference> shows further details of an information recovery system <highlight><bold>300</bold></highlight> including a data storage system <highlight><bold>302</bold></highlight> in accordance with the present invention. In one embodiment, the data storage system <highlight><bold>302</bold></highlight> (which can be located within the storage area network <highlight><bold>110</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 1</cross-reference>) can be based upon a Symmetrix Integrated Cache Disk Array system available from EMC Corporation of Hopkinton, Mass. Such a data storage system and its implementation is fully described in U.S. Pat. No. 6,101,497 issued Aug. 8, 2000, and also in U.S. Pat. No. 5,206,939 issued Apr. 27, 1993, each of which is assigned to EMC, the assignee of this invention and each of which is hereby incorporated by reference. </paragraph>
<paragraph id="P-0059" lvl="0"><number>&lsqb;0059&rsqb;</number> In general, a local volume is replicated to a business continuance volume (BCV). A local system that employs mirroring allows access to production volumes while performing backup is also described in the &apos;497 patent incorporated herein. The data storage system <highlight><bold>302</bold></highlight> includes a system memory <highlight><bold>304</bold></highlight> and sets or pluralities of multiple data storage devices or data stores <highlight><bold>306</bold></highlight><highlight><italic>a,b</italic></highlight>. In an exemplary embodiment, the system memory <highlight><bold>304</bold></highlight> includes a buffer or cache memory. The storage devices <highlight><bold>306</bold></highlight> can comprise disk storage devices, optical storage devices and the like. However, in an exemplary embodiment the storage devices are disk storage devices. The storage device <highlight><bold>306</bold></highlight> represent an array of storage devices in any of a variety of known configurations. </paragraph>
<paragraph id="P-0060" lvl="0"><number>&lsqb;0060&rsqb;</number> Host adapters (HA) <highlight><bold>308</bold></highlight><highlight><italic>a,b </italic></highlight>provide communications between host systems <highlight><bold>310</bold></highlight><highlight><italic>a,b </italic></highlight>and the system memory <highlight><bold>304</bold></highlight> and disk adapters (DA) <highlight><bold>312</bold></highlight>,a,b provide pathways between the system memory <highlight><bold>114</bold></highlight> and the storage device sets <highlight><bold>306</bold></highlight><highlight><italic>a,b</italic></highlight>. A bus <highlight><bold>314</bold></highlight> interconnects the system memory <highlight><bold>304</bold></highlight>, the host adapters <highlight><bold>308</bold></highlight> and the disk adapters <highlight><bold>312</bold></highlight>. Each system memory is used by various elements within the respective systems to transfer information and interact between the respective host adapters and disk adapters. </paragraph>
<paragraph id="P-0061" lvl="0"><number>&lsqb;0061&rsqb;</number> An optional backup storage system <highlight><bold>350</bold></highlight> can be connected to the data storage system <highlight><bold>302</bold></highlight>. The backup storage system can be provided as an EMC Data Manager (EDM) connected to the data storage system as described in Symmetrix Connect User Guide, P/N 200-113-591, Rev. C, December 1997, available from EMC Corporation. The direct connection between the shared storage system and the backup storage system may be provided as a high-speed data channel <highlight><bold>352</bold></highlight> such as a SCSI cable or one or more fiber-channel cables. In this system, a user may be permitted to backup data over the network or the direct connection. </paragraph>
<paragraph id="P-0062" lvl="0"><number>&lsqb;0062&rsqb;</number> The backup system <highlight><bold>350</bold></highlight> includes a backup/restore server <highlight><bold>354</bold></highlight>, logic <highlight><bold>356</bold></highlight> as part of the server, and a tape library unit <highlight><bold>358</bold></highlight> that may include tape medium (not shown) and a robotic picker mechanism (also not shown) as is available on the preferred EDM system. </paragraph>
<paragraph id="P-0063" lvl="0"><number>&lsqb;0063&rsqb;</number> In general, the data storage system <highlight><bold>302</bold></highlight> operates in response to commands from the host systems <highlight><bold>113</bold></highlight> via the host adapters <highlight><bold>308</bold></highlight>. The host adapters <highlight><bold>308</bold></highlight> transfer commands to a command buffer that is part of system memory <highlight><bold>304</bold></highlight>. The command buffer stores data structures and write requests that the disk adapters generate. The disk adapters <highlight><bold>312</bold></highlight> respond by effecting a corresponding operation using the information in a command buffer. The selected disk adapter then initiates a data operation. Reading operations transfer data from the storage devices <highlight><bold>306</bold></highlight><highlight><italic>a,b </italic></highlight>to the system memory <highlight><bold>304</bold></highlight> through a corresponding disk adapter <highlight><bold>312</bold></highlight><highlight><italic>a,b </italic></highlight>and subsequently transfer data from the system memory <highlight><bold>304</bold></highlight> to the corresponding host adapter <highlight><bold>308</bold></highlight><highlight><italic>a,b </italic></highlight>when the host system <highlight><bold>113</bold></highlight> initiates the data writing operation. </paragraph>
<paragraph id="P-0064" lvl="0"><number>&lsqb;0064&rsqb;</number> It is understood that the computer host systems <highlight><bold>310</bold></highlight> may be any conventional computing system, each having an operating system, such as systems available from Sun Microsystems, and running the Solaris operating system (a version of Unix), an HP system running HP-UX (a Hewlett-Packard client, running a Hewlett-Packard version of the Unix operating system) or an IBM system running the AIX operating system (an IBM version of Unix) or any other system with an associated operating system such as the WINDOWS NT operating system. The storage system may be any conventional storage system, including a Symmetrix storage system, as described above. </paragraph>
<paragraph id="P-0065" lvl="0"><number>&lsqb;0065&rsqb;</number> A short description of concepts useful for understanding this invention and known in the art is now given. A physical disk is formatted into a &ldquo;physical volume&rdquo; for use by management software, such as Logical Volume Manager (LVM) software available from EMC. Each physical volume is split up into discrete chunks, called physical partitions or physical extents. Physical volumes are combined into a &ldquo;volume group.&rdquo; A volume group is thus a collection of disks, treated as one large storage area. A &ldquo;logical volume&rdquo; consists of some number of physical partitions/extents, allocated from a single volume group. A &ldquo;filesystem&rdquo; refers to a structure or a collection of files. </paragraph>
<paragraph id="P-0066" lvl="0"><number>&lsqb;0066&rsqb;</number> Below is a short description of other useful terminology which may be understood in more detail with reference to the incorporated &apos;497 patent. When a mirror is &ldquo;established&rdquo; the data storage system <highlight><bold>119</bold></highlight> creates a mirror image (copy or replication) of a source or standard volume. When using the preferred Symmetrix such a mirror is denoted as a business continuance volume (BCV), also referred to in general terms as a mirrored disk, and in such a context as a BCV device. If data on the standard volume changes, the same changes are immediately applied to the mirrored disk. </paragraph>
<paragraph id="P-0067" lvl="0"><number>&lsqb;0067&rsqb;</number> When a mirror is &ldquo;split&rdquo; the Symmetrix data storage system isolates the mirrored version of the disk and no further changes are applied to the mirrored volume. After a split is complete, the primary disk can continue to change but the mirror maintains the point-in-time data that existed at the time of the split. Mirrors can be &ldquo;synchronized&rdquo; in either direction (i.e., from the BCV to the standard or visa versa). For example, changes from the standard volume that occurred after a split to the mirror can be applied to the BCV or mirrored disk. This brings the mirrored disk current with the standard volume. If synchronized in the other direction, the primary disk can be made to match the mirror. This is often the final step during a restore. </paragraph>
<paragraph id="P-0068" lvl="0"><number>&lsqb;0068&rsqb;</number> The operation of a BCV device and its corresponding BCV volume or volumes is more readily understood in terms of data sets stored in logical volumes and is useful for understanding the present invention. Any given logical volume may be stored on a portion or all of one physical disk drive or on two or more disk drives. </paragraph>
<paragraph id="P-0069" lvl="0"><number>&lsqb;0069&rsqb;</number> Referring to <cross-reference target="DRAWINGS">FIG. 5</cross-reference>, in this particular embodiment, operations on a series of physical disks are controlled in terms of logical volumes. The segmentation or hypering of physical disks into logical volumes is well known in the art. A disk adapter (not shown) interfaces logical volumes <highlight><bold>360</bold></highlight> interface to the data storage system bus. Each of these volumes <highlight><bold>360</bold></highlight> is defined as a Business Continuation Volume and is designated a BCV device. Each BCV device comprises a standard disk controller and related disk storage devices configured to independently support applications and processes. The use of these BCV devices enables a host such as host <highlight><bold>310</bold></highlight><highlight><italic>a</italic></highlight>, described from here on as the &ldquo;source&rdquo; host computer system to utilize instantaneous copies of the data in the standard volumes <highlight><bold>362</bold></highlight>. In a conventional operation, there typically will be at least one BCV volume assigned to each host device that will operate on a data set concurrently. However, as will be explained below, the BCV volumes established for use by one host may be used by another host, such as host <highlight><bold>310</bold></highlight><highlight><italic>b</italic></highlight>, described from here on as the &ldquo;target&rdquo; host computer system. </paragraph>
<paragraph id="P-0070" lvl="0"><number>&lsqb;0070&rsqb;</number> The source host <highlight><bold>310</bold></highlight><highlight><italic>a </italic></highlight>may continue online transaction processing (such as database transaction processing) or other processing without any impact or load on the standard volumes <highlight><bold>362</bold></highlight>, while their respective mirror images on the BCVs <highlight><bold>360</bold></highlight> are used to back up data in cooperation with backup system <highlight><bold>302</bold></highlight>. However, the BCVs may be established for use on another host substantially automatically under control of a computer program, rather than requiring intervention of an operator all along the way. </paragraph>
<paragraph id="P-0071" lvl="0"><number>&lsqb;0071&rsqb;</number> The direction of data flow for backup is from the data storage system <highlight><bold>302</bold></highlight> to the backup system <highlight><bold>350</bold></highlight> as represented by arrow <highlight><bold>364</bold></highlight>. The direction of data flow for restore is to the data storage system is in the opposite direction), but the BCVs <highlight><bold>360</bold></highlight> may be mounted on another host other than the one originally established in accordance with the method of this invention. </paragraph>
<paragraph id="P-0072" lvl="0"><number>&lsqb;0072&rsqb;</number> The optional backup system <highlight><bold>350</bold></highlight>, such as the EDM system, offers several options for controlling mirror behavior before and after a backup or restore. Mirror policies are well known to one of ordinary skill in the art. Exemplary pre-backup mirror options include bring mirrors down, verify mirrors are down, bring mirrors down if needed, and bring mirrors down after establishing and post backup mirror options include bring mirrors up, leave mirrors down, and leave mirrors as found. </paragraph>
<paragraph id="P-0073" lvl="0"><number>&lsqb;0073&rsqb;</number> The system establishes one or more mirrored copies of data (BCVs) that are copies of one or more volumes of data (Standard Volumes). The BCVs are established in a conventional manner as described in the incorporated &apos;497 patent. The BCVs are separated or split from the respective one more volumes of data in a conventional manner and which is also described in the incorporated &apos;497 patent. </paragraph>
<paragraph id="P-0074" lvl="0"><number>&lsqb;0074&rsqb;</number> The system discovers logical information related to the standard volumes that are part of the volume group on the source computer system <highlight><bold>310</bold></highlight><highlight><italic>a</italic></highlight>. A map of the logical information to physical devices on the source computer system is created. In one embodiment, the map can be provided as an XML message. Alternatively, the map takes the form of a flat file that may be converted into a tree structure for fast verification of the logical information. That map is used to build a substantially identical logical configuration on the target computer system <highlight><bold>310</bold></highlight><highlight><italic>b</italic></highlight>, preferably after the logical information has been verified by using a tree structure configuration of the logical information. </paragraph>
<paragraph id="P-0075" lvl="0"><number>&lsqb;0075&rsqb;</number> The logical configuration is used to mount a duplicate of the BCVs on the target computer system (denoted as mounted target BCVs). The newly mounted target BCVs then become part of a second volume group on the target computer system <highlight><bold>310</bold></highlight><highlight><italic>b. </italic></highlight></paragraph>
<paragraph id="P-0076" lvl="0"><number>&lsqb;0076&rsqb;</number> Prior to transferring data, the backup system exercises a series of functions. A discovery/mapping function discovers and maps logical to physical devices on the source host <highlight><bold>310</bold></highlight><highlight><italic>a</italic></highlight>, and includes such information as physical and logical volumes, volume groups, and file system information. An establish/split function establishes BCVs or splits such from standard volumes, depending on the pre- and post-mirror policies in effect on source host <highlight><bold>310</bold></highlight><highlight><italic>a. </italic></highlight></paragraph>
<paragraph id="P-0077" lvl="0"><number>&lsqb;0077&rsqb;</number> A build/mount function exports the BCVs established on the source host <highlight><bold>310</bold></highlight><highlight><italic>a </italic></highlight>to the target host <highlight><bold>310</bold></highlight><highlight><italic>b</italic></highlight>. It creates volume group, logical volume, and file system objects on the target host computer system. </paragraph>
<paragraph id="P-0078" lvl="0"><number>&lsqb;0078&rsqb;</number> An optional backup/restore function performs backup of the target host BCV data that has been exported or migrated from the source host. The dismount/cleanup function removes all volume group, logical volume, and filesystem objects from the target host. </paragraph>
<paragraph id="P-0079" lvl="0"><number>&lsqb;0079&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 6</cross-reference> shows an overview of the overall replication mounting process. In step <highlight><bold>400</bold></highlight> the system maps logical to physical devices on the source host. In step <highlight><bold>402</bold></highlight>, the logic establishes and subsequently splits standard to BCVs (which may be accomplished by a call to another function on the data storage system) in accordance with the mirror policy in effect at the source host. Step <highlight><bold>404</bold></highlight>, the system builds and mounts on the target host so that the BCV&apos;s are exported or migrated to the target host. Step <highlight><bold>408</bold></highlight> is a cleanup step in which all volume group logical volume, and filesystem objects are removed from the target server. </paragraph>
<paragraph id="P-0080" lvl="0"><number>&lsqb;0080&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 7</cross-reference> is an overview of the steps of the mapping and discovery process. In step <highlight><bold>500</bold></highlight>, the discovery/mapping process begins on the source host. The filesystem is discovered on the source host in step <highlight><bold>502</bold></highlight> and the logical volume is discovered in step <highlight><bold>504</bold></highlight>. The volume group information is discovered on the source host in step <highlight><bold>506</bold></highlight>. In step <highlight><bold>508</bold></highlight>, the map can created as a flat file or other relatively efficient data structure for compiling and using the information. </paragraph>
<paragraph id="P-0081" lvl="0"><number>&lsqb;0081&rsqb;</number> Referring now to <cross-reference target="DRAWINGS">FIG. 8</cross-reference>, in one embodiment, in step <highlight><bold>600</bold></highlight> the discovered logical volume information, which can correspond to flat file, is used to create a tree structure. This structure can be built by a unix function call from information in the mapping files described above. It may be built on both the target host computer system and the source host computer system. It is referred to as a tree because the Volume group information may be placed as the root of the tree and the branches represent the device information within the group and the logical volumes within the group. The tree structure is used in step <highlight><bold>602</bold></highlight> to verify the accuracy of the map file before the map file is sent to the target host. The tree is converted to a map preferably as a flat file in step <highlight><bold>604</bold></highlight>. This flat file map is then sent back to the target in step <highlight><bold>606</bold></highlight>. </paragraph>
<paragraph id="P-0082" lvl="0"><number>&lsqb;0082&rsqb;</number> Alternatively, discovery is in manner similar to that performed by the EMC Data Manager (EDM), which is well known to one of ordinary skill in the art. In one embodiment, the map is sent as an XML message. </paragraph>
<paragraph id="P-0083" lvl="0"><number>&lsqb;0083&rsqb;</number> Referring to <cross-reference target="DRAWINGS">FIG. 9</cross-reference>, the process of establishing/splitting with a backup system is started in step <highlight><bold>700</bold></highlight>. A mirror policy, if any, is checked in step <highlight><bold>702</bold></highlight>. An inquiry is posed in step <highlight><bold>704</bold></highlight> to determine if BCVs are established in accordance with the mirror policy. If the answer is no then BCVs are established in step <highlight><bold>706</bold></highlight>. The BCVs are split from the source host in step <highlight><bold>708</bold></highlight>. The BCVs are made not ready to the host in step <highlight><bold>710</bold></highlight>. </paragraph>
<paragraph id="P-0084" lvl="0"><number>&lsqb;0084&rsqb;</number> Referring to <cross-reference target="DRAWINGS">FIG. 10</cross-reference>, the process of beginning to build/mount logical information so the BCVs can be mounted on the target is begun in step <highlight><bold>800</bold></highlight>. The volume groups are created on the target is step <highlight><bold>802</bold></highlight>. Logical volumes are verified on the target in step <highlight><bold>804</bold></highlight>. The filesystem is checked and fired up on the target in step <highlight><bold>806</bold></highlight>. The device mount may now be completed with this logical information related to the BCVs on the target host in step <highlight><bold>808</bold></highlight>. </paragraph>
<paragraph id="P-0085" lvl="0"><number>&lsqb;0085&rsqb;</number> As shown in <cross-reference target="DRAWINGS">FIG. 11</cross-reference>, the newly mounted target BCVs may now be optionally backed up in tape in step <highlight><bold>900</bold></highlight>. The application is then shut down on the target in step <highlight><bold>902</bold></highlight>. And following the backup of the target BCV&apos;s cleanup steps and notification take place in step <highlight><bold>904</bold></highlight>. </paragraph>
<paragraph id="P-0086" lvl="0"><number>&lsqb;0086&rsqb;</number> If the software application on the target host in the source host is a database, then information related to the data may also be backed up, with the effect that essentially the entire database is backed up. Important information from the database includes any transactional data performed by the database operations, and related control files, table spaces, and archives/redo logs. </paragraph>
<paragraph id="P-0087" lvl="0"><number>&lsqb;0087&rsqb;</number> Regarding databases, further terminology is now discussed. While terminology for an Oracle database is used, one skilled in the art will recognize that other databases may be used without departing from the invention. </paragraph>
<paragraph id="P-0088" lvl="0"><number>&lsqb;0088&rsqb;</number> Control files contain information about the Oracle database, including information that describes the instance where the datafiles and log files reside. Datafiles may be files on the operating system filesystem. A tablespace is the lowest logical layer of the Oracle data storage structure. The tablespace includes one or more datafiles. The tablespace provides the finest granularity for laying out data across datafiles. </paragraph>
<paragraph id="P-0089" lvl="0"><number>&lsqb;0089&rsqb;</number> In the database there are archive files known as redo log files or simply as the redo log. This is where all information that may have to be recovered is kept. Without the redo log files a system failure would render the data unrecoverable. When a log switch occurs, the log records in the filled redo log file are copied to an archive log file if archiving is enabled. </paragraph>
<paragraph id="P-0090" lvl="0"><number>&lsqb;0090&rsqb;</number> Referring now to <cross-reference target="DRAWINGS">FIG. 12</cross-reference>, the process for restoring source standard volumes is shown beginning at step <highlight><bold>1000</bold></highlight> for the optional backup system. In step <highlight><bold>1002</bold></highlight>, the system poses an inquiry to determine if the restore is to be from the BCVs on the target or somewhere else. In accordance with the answer, the standard volumes are synchronized from the target mounted BCVs or tape, for example, respectively in steps <highlight><bold>1004</bold></highlight> or <highlight><bold>1006</bold></highlight>. Step <highlight><bold>1008</bold></highlight> begins the notification and cleanup steps which are generally described in <cross-reference target="DRAWINGS">FIG. 11</cross-reference>. </paragraph>
<paragraph id="P-0091" lvl="0"><number>&lsqb;0091&rsqb;</number> The cleanup/dismount process begins in step in <highlight><bold>1100</bold></highlight> as shown in <cross-reference target="DRAWINGS">FIG. 13</cross-reference>. The BCVs are dismounted from the target in step <highlight><bold>1102</bold></highlight>. This may be accomplished for example with the UNIX umount command. The objects related to volume group, logical volume, and filesystem or move the target in steps <highlight><bold>1104</bold></highlight> and <highlight><bold>1106</bold></highlight>. The cleanup is completed in step <highlight><bold>1108</bold></highlight>. The BCVs are re-established on the source (i.e., made ready to the host) in step in <highlight><bold>1108</bold></highlight>. </paragraph>
<paragraph id="P-0092" lvl="0"><number>&lsqb;0092&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 14</cross-reference> shows an exemplary checkpoint or replication schedule for an information recovery system in accordance with the present invention. At about 8:00, a standard volume STD, which can contain an Oracle database for example, is replicated on a corresponding BCV CHKP A, as described in detail above. At 12:00, the next scheduled checkpoint occurs at which the standard volume is replicated to a further BCV CHKP B. At about 1:00 an error is detected, such as by a data scrubbing tool, so that at about 2:00, after some evaluation period, the standard volume is replicated to an on demand BCV CHKP OD. The 4:00 checkpoint to the next BCV CHKP C can occur as scheduled provided the error is corrected and the standard volume STD up to date. Further on demand checkpoints can be made as desired, as described above. </paragraph>
<paragraph id="P-0093" lvl="0"><number>&lsqb;0093&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 15</cross-reference> shows an exemplary process for recovering from a data error, such as a transaction error in a database, in accordance with the present invention. A data error occurs at a time t<highlight><subscript>e </subscript></highlight>after which the system runs for a time TLE while the error is latent, i.e., unknown to the system operators, such as the database administrator. As is well known to one of ordinary skill in the art, the errors can be caused by a variety of sources including software errors, hardware errors and/or failures, and human error. </paragraph>
<paragraph id="P-0094" lvl="0"><number>&lsqb;0094&rsqb;</number> In one embodiment, a data volume replication, e.g., a copy of the database, resides on disk as a BCV, which is mounted on a remote host and verified with so-called data scrubbing tools. Data scrubbing tools for examining data integrity are well known to one of ordinary skill in the art. For example, while it is understood that a variety of data scrubbing tools can be used to evaluate the integrity of the replications, one suitable data scrubbing tool can be provided from the Patrol family of products by BMC Software of Redwood Shores, Calif. Since data is stored on disk in accordance with the present invention, data scrubbing can significantly reduce the latent error time, i.e., the time during which errors are undiscovered. </paragraph>
<paragraph id="P-0095" lvl="0"><number>&lsqb;0095&rsqb;</number> In contrast, in many conventional systems, the error is not discovered until a person, such as a customer, queries the database operator regarding an irregularity with the customer&apos;s account, for example. As known to one of ordinary skill in the art, data living on tape cannot be examined without data restoration and recovery. </paragraph>
<paragraph id="P-0096" lvl="0"><number>&lsqb;0096&rsqb;</number> After confirmation that an error has occurred, a time TEE elapses in which the error is evaluated to determine potential corrective measures. For example, an operator can evaluate the type of error, how the error occurred, how widespread the error is, what is the impact of the error, when the error occurred, and how the error may be corrected. The level of expertise of the operator, e.g., database administrator, largely determines the time required to evaluate the error. </paragraph>
<paragraph id="P-0097" lvl="0"><number>&lsqb;0097&rsqb;</number> During error evaluation, the system can create an on-demand replica of the database for mounting on another host. This enables multiple evaluations to be performed in parallel by one or more evaluation teams. In addition, destructive evaluations can be performed on alternate hosts since the production database is available for further replications. </paragraph>
<paragraph id="P-0098" lvl="0"><number>&lsqb;0098&rsqb;</number> In general, after evaluation of the error an operator decides to correct the error during a time TCE. The operator can restore backed up data (checkpoint) or attempt repair of the production database. By creating another replication prior to attempting repair, the operator can freely attempt repair of the live database. That is, the operator can attempt risky &ldquo;shot in the dark&rdquo; repairs since multiple checkpoints exist. In the case where the operator&apos;s attempted solution fails to repair the error, or makes it worse, a checkpoint can be readily restored, recovered, and updated from the redo logs. A further copy of the database can be used for trying additional potential solutions. Alternatively, solutions can be tried on a copy of the database prior to modifying the production database. </paragraph>
<paragraph id="P-0099" lvl="0"><number>&lsqb;0099&rsqb;</number> If a restore is selected over repair, the user must decide which backup is to be restored. Ideally, this is the most recent backup before the database became corrupt. If the exact time at which the error occurred is not known, the user may have to guess which backup is to be restored. </paragraph>
<paragraph id="P-0100" lvl="0"><number>&lsqb;0100&rsqb;</number> In prior art systems, the user had to do a restore from the selected backup. After completion of the restore, which can be many hours later, the user can check if the selected backup is free of corruption. If not, the user must do another restore from a further backup. Such multiple restores are rendered unnecessary by the IR system of the present invention since the user can mount the selected backup on another host and check the backup for errors. Upon finding an error-free backup, a single restore can be performed. </paragraph>
<paragraph id="P-0101" lvl="0"><number>&lsqb;0101&rsqb;</number> In many prior art systems, so called surgical repair of the production database is the preferred option due to time and effort associated with restoring data from tape. For relatively large databases, e.g., more than a Terabyte, data restoration from tape can take many hours. In addition, for certain mission critical databases, surgical repair is virtually the only option in view of the incredible costs associated with bringing down a database. However, the concomitant risks of damaging such a database in attempting to repair an error are readily apparent. </paragraph>
<paragraph id="P-0102" lvl="0"><number>&lsqb;0102&rsqb;</number> During a further time TRF, after restore of the checkpoint, the roll forward process attempts to place the database up to date with transactions that occurred since the database copy was frozen, which are recorded in the redo log RL. The roll forward process is iterative since the database should be checked for errors as the recorded transactions injected into the database. By incrementally rolling forward, the database can be updated at discrete points. If an error is detected, the roll forward process need only revert back to the last known error-free point. Conventional tape-based systems do not provide this ability due to the incompatible formats between tape and disk. </paragraph>
<paragraph id="P-0103" lvl="0"><number>&lsqb;0103&rsqb;</number> It is understood that an operator can readily vary the particular error detection and correction steps depending upon the requirements of a particular application. For example, varying scheduled and on-demand checkpoints can be generated for use in finding an error correction solution. </paragraph>
<paragraph id="P-0104" lvl="0"><number>&lsqb;0104&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 16</cross-reference> shows an exemplary information recovery system architecture having the flexibility to readily support a variety of application agents, operating systems, and replication technologies. In general, an IR core communicates with a graphical user interface (GUI) or command line interface (CLI) via an IR enable Application Programming Interface (API). Various application agents A,B,C communicate with the IR core via an enterprise application API. The IR core can support multiple client operating systems and replication technologies. The separation of applications and replication technologies with a common interface to the core enables the addition of new applications and replication technologies without changing the core. For example, new replication technologies, which can include new hardware, new host-based software replications, and/or third party copy, e.g., SCSI XCOPY, devices, can be readily added. New databases and application can also be readily added including new filesystems, new relation database management systems (RDBMS), and new financial and E-mail applications. And new user interfaces, new scripting, and new host platforms can be readily supported. </paragraph>
<paragraph id="P-0105" lvl="0"><number>&lsqb;0105&rsqb;</number> In a further aspect of the invention, a portion of a database replication can be mounted to a host computer. While the invention is described in conjunction with tablespaces in an Oracle database, it is understood that the invention is applicable to further database types and components. In addition, it is understood that Oracle terms are used to facilitate an understanding of the invention and should not be construed as limiting the invention to a particular database type or configuration. </paragraph>
<paragraph id="P-0106" lvl="0"><number>&lsqb;0106&rsqb;</number> FIGS. <highlight><bold>17</bold></highlight>-<highlight><bold>18</bold></highlight>, which have some similarity with <cross-reference target="DRAWINGS">FIG. 1</cross-reference>, show an exemplary IR system <highlight><bold>1200</bold></highlight> having partial database replication mounting in accordance with the present invention. The IR server <highlight><bold>1202</bold></highlight> includes an IR daemon <highlight><bold>1204</bold></highlight> for handling replication requests from host computers, such as IR users <highlight><bold>1206</bold></highlight>. IR application clients <highlight><bold>1208</bold></highlight> operate as a database server DBS (<cross-reference target="DRAWINGS">FIG. 18</cross-reference>) for applications, such as Oracle databases stored on volume groups VGS in the storage area network SAN. An IR user <highlight><bold>1206</bold></highlight> or mount host MH (<cross-reference target="DRAWINGS">FIG. 18</cross-reference>) has visibility to a replicated database RDB, which was replicated from an original database ODB. </paragraph>
<paragraph id="P-0107" lvl="0"><number>&lsqb;0107&rsqb;</number> In the illustrated embodiment, the production or original database ODB is located in the storage area network SAN and runs on the database server host DBS. The original database ODB includes first, second and third tablespaces TS1, TS2, TS3, along with system information SYS. The replication RDB of the original database ODB also includes corresponding tablespaces TS1R, TS2R, TS3R and system information SYSR. Generation of the database replication is described in detail above. The replication database ODB is visible to an IR user, e.g., mount host MH (<cross-reference target="DRAWINGS">FIG. 18</cross-reference>). It is understood that the mount host MH and the database server DBS are substantially similar, e.g., running the same operating system, etc. </paragraph>
<paragraph id="P-0108" lvl="0"><number>&lsqb;0108&rsqb;</number> As described above, the IR daemon <highlight><bold>1204</bold></highlight> runs on the IR server <highlight><bold>1202</bold></highlight>. Replication information, such as mapping files, is contained in the IR database <highlight><bold>1210</bold></highlight>. The IR database has the information required to mount and start up a replication. This information is captured during creation of the replication, as is also described above. The client control module or daemon <highlight><bold>1212</bold></highlight> (<cross-reference target="DRAWINGS">FIG. 17</cross-reference>) runs on the database server DBS and the mount host MH. The client control module running on the mount host receives the data volume mapping file along with any other necessary information from the IR server <highlight><bold>1202</bold></highlight> to mount the replication database RDB. </paragraph>
<paragraph id="P-0109" lvl="0"><number>&lsqb;0109&rsqb;</number> In general, the system can start up a partial replication database, e.g., tablespaces selected by a user as described above, in a variety of modes. For example, the operator can select table space one TS1 for startup so that the second and third table spaces TS2, TS3 are ignored. After mounting, the first table space TS1 is available for user by an operator. </paragraph>
<paragraph id="P-0110" lvl="0"><number>&lsqb;0110&rsqb;</number> In one embodiment, a partial replication database, e.g., the first tablespace TS1, can be started up in recovered mode, recovered read only mode, and no recover mode, which are described below. Further modes for meeting the requirements of a particular application will be readily apparent to one of ordinary skill in the art. </paragraph>
<paragraph id="P-0111" lvl="0"><number>&lsqb;0111&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 19</cross-reference> shows an exemplary sequence of steps for starting up a partial database replication in accordance with the present invention. In step <highlight><bold>1300</bold></highlight>, the mount host retrieves the replication database mapping file from the IR database via the IR server. A database server for controlling the replicated database is then created on the mount host in step <highlight><bold>1302</bold></highlight>. Creating the database server can include, for example, importing volume groups, starting volumes, and mounting file systems. In step <highlight><bold>1303</bold></highlight>, the system copies the appropriate backup control file, as well as archive logs, the init&lt;sid&gt; file and the password file, to the required location on the mount host. In step <highlight><bold>1304</bold></highlight>, the database instance, e.g., the replicated Oracle database, is mounted on the target mount host, as described in detail above. It is understood that the replication can be mounted to a target host where the replicated instance does not exist yet. That is, the system expects the mount host to have the Oracle application installed and that the object instance to be mounted is not already running on the host. </paragraph>
<paragraph id="P-0112" lvl="0"><number>&lsqb;0112&rsqb;</number> In one embodiment, the application layer of the client control daemon receives a mapping or tree file describing what is to be recovered. The original tree is generated during the replication of the application and is stored in the IRD catalog/database on the IR server. This mapping is made available to the client control running on the mount host. The tree contains what needs to be mounted and is built from what the user requests. So the user selects a partial tree from the original tree. This partial tree file is compared to the original tree describing what is contained in the replication database. This allows the client control daemon to determine those tablespaces, e.g., TS2 and TS3, that do not need to be recovered. The log files can be copied over to the target host, as well as other information files, such as the initinstance.ora file and two backup control files (a read-only version and a regular version) for Oracle applications. </paragraph>
<paragraph id="P-0113" lvl="0"><number>&lsqb;0113&rsqb;</number> For a start up in recovery mode, in step <highlight><bold>1306</bold></highlight> the database volume groups and/or raw devices are renamed from the original host names to new names on the target mount host. For example, Oracle statements are executed to make the Oracle database aware of the name changes. It is understood that the password file is brought over from the IR database for the replication. In one embodiment, the backup control files are automatically copied to the location where the real control files are supposed to be, ready for oracle to use. The backup control files are copied to the archive log directory on the mount host. The control files are copied to the locations described by the init&lt;SID&gt;.ora file of the application host for Oracle applications, at the time of replication. The description of this location is actually cataloged, e.g., by querying the database, at the time of the replication. Depending on whether it is a read-only recover or a recover, the appropriate backup control file is copied into the above-specified location. After copying the control files, the IR Application agent software will apply the appropriate permissions and ownership. </paragraph>
<paragraph id="P-0114" lvl="0"><number>&lsqb;0114&rsqb;</number> After applying the restored password file, in step <highlight><bold>1308</bold></highlight> the unwanted information, e.g., TS2, TS3, is deleted so that only the tablespaces previously selected for recovery by the user, e.g., TS1, are recovered. The recovery of the first tablespace TS1 is then executed in step <highlight><bold>1310</bold></highlight>. After copying the appropriate control files, the database is mounted and renamed if necessary. Unwanted datafiles are then dropped (dropping datafiles updates the control files), and then the entire database is recovered. Oracle ignores the dropped datafiles and recovers only the tablespaces that are selected for mounting as defined by the control files. In step <highlight><bold>1312</bold></highlight>, the Oracle database instance is then opened and available for use. </paragraph>
<paragraph id="P-0115" lvl="0"><number>&lsqb;0115&rsqb;</number> In the no recover mode, the oracle layer does not issue any oracle recovery commands. It simply keeps the tools available for the user. That is, the initinstance.ora, the logs, the data files and the backup control files are available. The user can then recreate a password file and execute the recovery manually. </paragraph>
<paragraph id="P-0116" lvl="0"><number>&lsqb;0116&rsqb;</number> In the readonly startup mode, the system behaves similarly to the recover mode except that the backup control file that is copied to the real control file location is the read-only version, and the instance is recovered and opened in standby mode. </paragraph>
<paragraph id="P-0117" lvl="0"><number>&lsqb;0117&rsqb;</number> One skilled in the art will appreciate further features and advantages of the invention based on the above-described embodiments. Accordingly, the invention is not to be limited by what has been particularly shown and described, except as indicated by the appended claims. All publications and references cited herein are expressly incorporated herein by reference in their entirety.</paragraph>
</section>
</detailed-description>
</subdoc-description>
<subdoc-claims>
<heading lvl="1">What is claimed is: </heading>
<claim id="CLM-00001">
<claim-text><highlight><bold>1</bold></highlight>. A method for detecting errors in a data volume, comprising: 
<claim-text>storing an original data volume; </claim-text>
<claim-text>replicating the original data volume; </claim-text>
<claim-text>examining the replicated data volume for errors; </claim-text>
<claim-text>correcting an error in the original data volume based upon information obtained from examining a corresponding error in the replicated data volume. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00002">
<claim-text><highlight><bold>2</bold></highlight>. The method according to <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, further including 
<claim-text>storing the original data volume on a first plurality of disks; and </claim-text>
<claim-text>replicating the original data volume on a second plurality of disks. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00003">
<claim-text><highlight><bold>3</bold></highlight>. The method according to <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, further including mounting and examining the replicated data volume with a data scrubbing tool. </claim-text>
</claim>
<claim id="CLM-00004">
<claim-text><highlight><bold>4</bold></highlight>. The method according to <dependent-claim-reference depends_on="CLM-00003">claim 3</dependent-claim-reference>, further including enabling evaluation of the error detected in the replicated data volume to determine an error correction strategy. </claim-text>
</claim>
<claim id="CLM-00005">
<claim-text><highlight><bold>5</bold></highlight>. The method according to <dependent-claim-reference depends_on="CLM-00004">claim 4</dependent-claim-reference>, further including enabling an attempt to repair the replicated data volume. </claim-text>
</claim>
<claim id="CLM-00006">
<claim-text><highlight><bold>6</bold></highlight>. The method according to <dependent-claim-reference depends_on="CLM-00005">claim 5</dependent-claim-reference>, further including enabling implementation of a successful repair to the replicated data volume in the original dataset on the first plurality of disks. </claim-text>
</claim>
<claim id="CLM-00007">
<claim-text><highlight><bold>7</bold></highlight>. The method according to <dependent-claim-reference depends_on="CLM-00004">claim 4</dependent-claim-reference>, further including restoring the replicated data volume. </claim-text>
</claim>
<claim id="CLM-00008">
<claim-text><highlight><bold>8</bold></highlight>. The method according to <dependent-claim-reference depends_on="CLM-00007">claim 7</dependent-claim-reference>, further including updating the restored data volume with data from a redo log. </claim-text>
</claim>
<claim id="CLM-00009">
<claim-text><highlight><bold>9</bold></highlight>. The method according to <dependent-claim-reference depends_on="CLM-00008">claim 8</dependent-claim-reference>, further including incrementally examining the restored data volume for errors as the restored data volume is updated from the redo log. </claim-text>
</claim>
<claim id="CLM-00010">
<claim-text><highlight><bold>10</bold></highlight>. The method according to <dependent-claim-reference depends_on="CLM-00009">claim 9</dependent-claim-reference>, further including updating the partially restored data volume at a point where no errors are detected. </claim-text>
</claim>
<claim id="CLM-00011">
<claim-text><highlight><bold>11</bold></highlight>. The method according to <dependent-claim-reference depends_on="CLM-00011">claim 10</dependent-claim-reference>, further including replicating the partially updated, restored data volume when no errors are detected. </claim-text>
</claim>
<claim id="CLM-00012">
<claim-text><highlight><bold>12</bold></highlight>. The method according to <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, further including replicating the original data volume at predetermined intervals. </claim-text>
</claim>
<claim id="CLM-00013">
<claim-text><highlight><bold>13</bold></highlight>. The method according to <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, further including replicating the original data volume upon detection of an error. </claim-text>
</claim>
<claim id="CLM-00014">
<claim-text><highlight><bold>14</bold></highlight>. A method for recovering data, comprising: 
<claim-text>replicating a production database on a first set of disks mounted by a first host to a second set of disks to create a replicated database; </claim-text>
<claim-text>enabling mounting of the second set of disks to a second host; and </claim-text>
<claim-text>enabling examination of the production and/or replicated database for errors with a data scrubbing tool. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00015">
<claim-text><highlight><bold>15</bold></highlight>. The method according to <dependent-claim-reference depends_on="CLM-00011">claim 14</dependent-claim-reference>, further including enabling examination of the production and/or replicated database with a data scrubbing tool. </claim-text>
</claim>
<claim id="CLM-00016">
<claim-text><highlight><bold>16</bold></highlight>. The method according to <dependent-claim-reference depends_on="CLM-00011">claim 14</dependent-claim-reference>, further including generating a screen display for scheduling replications of the production database. </claim-text>
</claim>
<claim id="CLM-00017">
<claim-text><highlight><bold>17</bold></highlight>. The method according to <dependent-claim-reference depends_on="CLM-00011">claim 16</dependent-claim-reference>, further including generating a screen display for creating an on demand replication of the production database. </claim-text>
</claim>
<claim id="CLM-00018">
<claim-text><highlight><bold>18</bold></highlight>. The method according to <dependent-claim-reference depends_on="CLM-00011">claim 14</dependent-claim-reference>, further including enabling a user to replicate the production database on demand. </claim-text>
</claim>
<claim id="CLM-00019">
<claim-text><highlight><bold>19</bold></highlight>. The method according to <dependent-claim-reference depends_on="CLM-00011">claim 15</dependent-claim-reference>, further including enabling an operator to evaluate an error detected in the production and/or replicated database. </claim-text>
</claim>
<claim id="CLM-00020">
<claim-text><highlight><bold>20</bold></highlight>. The method according to <dependent-claim-reference depends_on="CLM-00011">claim 14</dependent-claim-reference>, further including enabling an operator to repair the production and/or replicated database. </claim-text>
</claim>
<claim id="CLM-00021">
<claim-text><highlight><bold>21</bold></highlight>. The method according to <dependent-claim-reference depends_on="CLM-00011">claim 14</dependent-claim-reference>, further including enabling a user to restore the replicated database. </claim-text>
</claim>
<claim id="CLM-00022">
<claim-text><highlight><bold>22</bold></highlight>. The method according to <dependent-claim-reference depends_on="CLM-00011">claim 14</dependent-claim-reference>, further including enabling a user to roll forward on the production and/or replication database with information from redo logs. </claim-text>
</claim>
<claim id="CLM-00023">
<claim-text><highlight><bold>23</bold></highlight>. The method according to <dependent-claim-reference depends_on="CLM-00022">claim 22</dependent-claim-reference>, further including incrementally updating the production and/or replication database upon determining that no errors are present. </claim-text>
</claim>
<claim id="CLM-00024">
<claim-text><highlight><bold>24</bold></highlight>. The method according to <dependent-claim-reference depends_on="CLM-00011">claim 14</dependent-claim-reference>, further including randomly accessing the replicated database from the second host. </claim-text>
</claim>
<claim id="CLM-00025">
<claim-text><highlight><bold>25</bold></highlight>. An information and recovery system, comprising: 
<claim-text>a storage area network; </claim-text>
<claim-text>a server coupled to the storage area network for receiving a request from a first host to replicate a data volume group that can be mounted by a second host and examined for errors. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00026">
<claim-text><highlight><bold>26</bold></highlight>. The system according to <dependent-claim-reference depends_on="CLM-00022">claim 25</dependent-claim-reference>, further including an IR application for generating replication requests. </claim-text>
</claim>
<claim id="CLM-00027">
<claim-text><highlight><bold>27</bold></highlight>. The system according to <dependent-claim-reference depends_on="CLM-00022">claim 26</dependent-claim-reference>, wherein the IR application resides on a host computer coupled to the server via a network. </claim-text>
</claim>
<claim id="CLM-00028">
<claim-text><highlight><bold>28</bold></highlight>. The system according to <dependent-claim-reference depends_on="CLM-00022">claim 25</dependent-claim-reference>, wherein the data volume group is controlled by an application agent. </claim-text>
</claim>
<claim id="CLM-00029">
<claim-text><highlight><bold>29</bold></highlight>. The system according to <dependent-claim-reference depends_on="CLM-00022">claim 28</dependent-claim-reference>, wherein the application agent corresponds to an Oracle database. </claim-text>
</claim>
<claim id="CLM-00030">
<claim-text><highlight><bold>30</bold></highlight>. The system according to <dependent-claim-reference depends_on="CLM-00022">claim 25</dependent-claim-reference>, further including an IR database coupled to the server. </claim-text>
</claim>
<claim id="CLM-00031">
<claim-text><highlight><bold>31</bold></highlight>. The system according to <dependent-claim-reference depends_on="CLM-00022">claim 25</dependent-claim-reference>, wherein the server further includes an IR daemon for handing data volume replication requests. </claim-text>
</claim>
<claim id="CLM-00032">
<claim-text><highlight><bold>32</bold></highlight>. The system according to <dependent-claim-reference depends_on="CLM-00022">claim 25</dependent-claim-reference>, wherein the storage area network includes a plurality of disks for storing original data volume groups and replicated data volume groups. </claim-text>
</claim>
<claim id="CLM-00033">
<claim-text><highlight><bold>33</bold></highlight>. The system according to <dependent-claim-reference depends_on="CLM-00022">claim 28</dependent-claim-reference>, wherein the server and the application agent are coupled to the storage area network. </claim-text>
</claim>
<claim id="CLM-00034">
<claim-text><highlight><bold>34</bold></highlight>. The system according to <dependent-claim-reference depends_on="CLM-00022">claim 25</dependent-claim-reference>, further including a screen display for scheduling replication activities. </claim-text>
</claim>
<claim id="CLM-00035">
<claim-text><highlight><bold>35</bold></highlight>. The system according to <dependent-claim-reference depends_on="CLM-00033">claim 34</dependent-claim-reference>, further including a screen display for generating an on demand data volume replication.</claim-text>
</claim>
</subdoc-claims>
<subdoc-drawings id="DRAWINGS">
<heading lvl="0" align="CENTER">Drawings</heading>
<representative-figure>1</representative-figure>
<figure id="figure-D00000">
<image id="EMI-D00000" file="US20030005120A1-20030102-D00000.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00001">
<image id="EMI-D00001" file="US20030005120A1-20030102-D00001.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00002">
<image id="EMI-D00002" file="US20030005120A1-20030102-D00002.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00003">
<image id="EMI-D00003" file="US20030005120A1-20030102-D00003.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00004">
<image id="EMI-D00004" file="US20030005120A1-20030102-D00004.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00005">
<image id="EMI-D00005" file="US20030005120A1-20030102-D00005.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00006">
<image id="EMI-D00006" file="US20030005120A1-20030102-D00006.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00007">
<image id="EMI-D00007" file="US20030005120A1-20030102-D00007.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00008">
<image id="EMI-D00008" file="US20030005120A1-20030102-D00008.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00009">
<image id="EMI-D00009" file="US20030005120A1-20030102-D00009.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00010">
<image id="EMI-D00010" file="US20030005120A1-20030102-D00010.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00011">
<image id="EMI-D00011" file="US20030005120A1-20030102-D00011.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00012">
<image id="EMI-D00012" file="US20030005120A1-20030102-D00012.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00013">
<image id="EMI-D00013" file="US20030005120A1-20030102-D00013.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00014">
<image id="EMI-D00014" file="US20030005120A1-20030102-D00014.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00015">
<image id="EMI-D00015" file="US20030005120A1-20030102-D00015.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00016">
<image id="EMI-D00016" file="US20030005120A1-20030102-D00016.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00017">
<image id="EMI-D00017" file="US20030005120A1-20030102-D00017.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00018">
<image id="EMI-D00018" file="US20030005120A1-20030102-D00018.TIF" imf="TIFF" ti="DR"/>
</figure>
</subdoc-drawings>
</patent-application-publication>
