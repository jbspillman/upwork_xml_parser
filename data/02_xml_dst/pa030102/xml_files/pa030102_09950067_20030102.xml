<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE patent-application-publication SYSTEM "pap-v16-2002-01-01.dtd" [
<!ENTITY US20030005068A1-20030102-M00001.NB SYSTEM "US20030005068A1-20030102-M00001.NB" NDATA NB>
<!ENTITY US20030005068A1-20030102-M00001.TIF SYSTEM "US20030005068A1-20030102-M00001.TIF" NDATA TIF>
<!ENTITY US20030005068A1-20030102-M00002.NB SYSTEM "US20030005068A1-20030102-M00002.NB" NDATA NB>
<!ENTITY US20030005068A1-20030102-M00002.TIF SYSTEM "US20030005068A1-20030102-M00002.TIF" NDATA TIF>
<!ENTITY US20030005068A1-20030102-D00000.TIF SYSTEM "US20030005068A1-20030102-D00000.TIF" NDATA TIF>
<!ENTITY US20030005068A1-20030102-D00001.TIF SYSTEM "US20030005068A1-20030102-D00001.TIF" NDATA TIF>
<!ENTITY US20030005068A1-20030102-D00002.TIF SYSTEM "US20030005068A1-20030102-D00002.TIF" NDATA TIF>
<!ENTITY US20030005068A1-20030102-D00003.TIF SYSTEM "US20030005068A1-20030102-D00003.TIF" NDATA TIF>
<!ENTITY US20030005068A1-20030102-D00004.TIF SYSTEM "US20030005068A1-20030102-D00004.TIF" NDATA TIF>
<!ENTITY US20030005068A1-20030102-D00005.TIF SYSTEM "US20030005068A1-20030102-D00005.TIF" NDATA TIF>
<!ENTITY US20030005068A1-20030102-D00006.TIF SYSTEM "US20030005068A1-20030102-D00006.TIF" NDATA TIF>
<!ENTITY US20030005068A1-20030102-D00007.TIF SYSTEM "US20030005068A1-20030102-D00007.TIF" NDATA TIF>
]>
<patent-application-publication>
<subdoc-bibliographic-information>
<document-id>
<doc-number>20030005068</doc-number>
<kind-code>A1</kind-code>
<document-date>20030102</document-date>
</document-id>
<publication-filing-type>new</publication-filing-type>
<domestic-filing-data>
<application-number>
<doc-number>09950067</doc-number>
</application-number>
<application-number-series-code>09</application-number-series-code>
<filing-date>20010912</filing-date>
</domestic-filing-data>
<technical-information>
<classification-ipc>
<classification-ipc-primary>
<ipc>G06F015/16</ipc>
</classification-ipc-primary>
<classification-ipc-secondary>
<ipc>G06F009/00</ipc>
</classification-ipc-secondary>
<classification-ipc-edition>07</classification-ipc-edition>
</classification-ipc>
<classification-us>
<classification-us-primary>
<uspc>
<class>709</class>
<subclass>208000</subclass>
</uspc>
</classification-us-primary>
<classification-us-secondary>
<uspc>
<class>709</class>
<subclass>102000</subclass>
</uspc>
</classification-us-secondary>
</classification-us>
<title-of-invention>System and method for creating a virtual supercomputer using computers working collaboratively in parallel and uses for the same</title-of-invention>
</technical-information>
<continuity-data>
<non-provisional-of-provisional>
<document-id>
<doc-number>60258354</doc-number>
<document-date>20001228</document-date>
<country-code>US</country-code>
</document-id>
</non-provisional-of-provisional>
</continuity-data>
<inventors>
<first-named-inventor>
<name>
<given-name>Ronald</given-name>
<middle-name>H.</middle-name>
<family-name>Nickel</family-name>
</name>
<residence>
<residence-us>
<city>Alexandria</city>
<state>VA</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</first-named-inventor>
<inventor>
<name>
<given-name>Igor</given-name>
<family-name>Mikolic-Torreira</family-name>
</name>
<residence>
<residence-us>
<city>Alexandria</city>
<state>VA</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
</inventors>
<correspondence-address>
<name-1>Michele M. Burris</name-1>
<name-2>SHAW PITTMAN</name-2>
<address>
<address-1>1650 Tysons Boulevard</address-1>
<city>McLean</city>
<state>VA</state>
<postalcode>22102-4859</postalcode>
<country>
<country-code>US</country-code>
</country>
</address>
</correspondence-address>
</subdoc-bibliographic-information>
<subdoc-abstract>
<paragraph id="A-0001" lvl="0">A system and method for solving a computationally intensive problem using a virtual supercomputer comprising a plurality of multipurpose workstations. The virtual supercomputer and application software for solving the computationally intensive problem are adapted for minimum impact on the multipurpose workstations which may be used for performing other tasks as needed by primary users. The computationally intensive problem is solved by the virtual supercomputer using any excess computational resources available on the multipurpose workstations. </paragraph>
</subdoc-abstract>
<subdoc-description>
<cross-reference-to-related-applications>
<paragraph id="P-0001" lvl="0"><number>&lsqb;0001&rsqb;</number> This application claims the benefit of U.S. Provisional Application No. 60/258,354, filed Dec. 28, 2000, which is herein incorporated by reference in its entirety.</paragraph>
</cross-reference-to-related-applications>
<federal-research-statement>
<paragraph-federal-research-statement id="P-0002"><number>&lsqb;0002&rsqb;</number> This invention was made with Government support under contract no. N00014-00-D-0700 awarded by the Office of Naval Research. The Government has certain rights in the invention.</paragraph-federal-research-statement>
</federal-research-statement>
<summary-of-invention>
<section>
<heading lvl="1">BACKGROUND </heading>
<paragraph id="P-0003" lvl="0"><number>&lsqb;0003&rsqb;</number> 1. Field of the Invention </paragraph>
<paragraph id="P-0004" lvl="0"><number>&lsqb;0004&rsqb;</number> The present invention relates to a system and method for creating a virtual supercomputer using computers working collaboratively in parallel and uses for the same. More specifically, the present invention pertains to an on-demand supercomputer system constructed from a group of multipurpose machines working collaboratively in parallel. The machines may be linked through a private network, but could also be linked through the Internet provided performance and security concerns are addressed. </paragraph>
<paragraph id="P-0005" lvl="0"><number>&lsqb;0005&rsqb;</number> 2. Background of the Invention </paragraph>
<paragraph id="P-0006" lvl="0"><number>&lsqb;0006&rsqb;</number> The present invention provides an on-demand supercomputer comprising a group of multipurpose machines working collaboratively in parallel. As such, the present invention falls in the category of a &ldquo;cluster&rdquo; supercomputer. Cluster supercomputers are common at the national labs and major universities. Like many other cluster supercomputers, the present invention can use the freely available &ldquo;Parallel Virtual Machine (PVM)&rdquo; software package provided by Oak Ridge National Laboratories (ORNL), Oak Ridge, Tenn., to implement the basic connectivity and data exchange mechanisms between the individual computers. Other software applications for establishing a virtual supercomputer may be used, provided that the software applications allow reconfiguration of the virtual supercomputer computer without undue interference to the overall operation of the virtual supercomputer. The present invention also uses proprietary software, as described herein, to provide various capabilities that are not provided by PVM. </paragraph>
<paragraph id="P-0007" lvl="0"><number>&lsqb;0007&rsqb;</number> Virtual supercomputers known in the art comprise job control language (JCL) processors such as the Sun Grid Engine Software (GES), from Sun Microsystems, Palo Alto, Calif., for example. In particular, with a JCL processor product for networked computers such as GES, a user submits a job (a task to be executed) to the JCL master. The JCL master then uses sophisticated rules for assigning resources to find a workstation on the network to perform the task. The task could be a single program or many programs. The JCL master will assign each program in the task to individual processors in the network in such a way that the total time to complete all the programs in the task is minimized. The GES does not, however, provide support for parallel processing in that it does not provide special mechanisms for information to be shared between the programs that might change their computations. </paragraph>
<paragraph id="P-0008" lvl="0"><number>&lsqb;0008&rsqb;</number> To the extent that it is possible for a user to develop GES applications that do parallel processing, the user must find ways to exchange data between jobs running on separate machines. Furthermore, it appears that GES has no way of knowing that the jobs running on separate computers are part of a coherent whole. It follows that if one of the parallel processing jobs drops for any reason, GES will not know that it has to do something besides simply putting that particular job &ldquo;back in the queue&rdquo;. This would make fault tolerance for a parallel-processing application under GES exceptionally hard to develop. </paragraph>
</section>
<section>
<heading lvl="1">SUMMARY OF THE INVENTION </heading>
<paragraph id="P-0009" lvl="0"><number>&lsqb;0009&rsqb;</number> A method for solving a computationally intensive problem using a plurality of multipurpose computer workstations. The method comprising the steps of: (a) building a parallel virtual machine comprising a master computer and at least one slave computer, wherein the at least one slave computer is selected from the plurality of multipurpose computer workstations; (b) dividing the computationally intensive problem into a plurality of task quantum; (c) assigning to the at least one slave computer at least one task quanta selected from the plurality of task quantum; (d) completing on the at least one slave computer the at least one task quanta; (e) receiving on the master computer a result provided by the at least one slave computer; and repeating steps (c), (d) and (e) until the computationally intensive task is completed. </paragraph>
<paragraph id="P-0010" lvl="0"><number>&lsqb;0010&rsqb;</number> In contrast to known virtual super computers such as GES, the present invention involves running a single program across multiple processors on the network. The master of the present invention not only assigns work to the processors, it uses information sent by the slave processors to direct the work of all of the processors in attacking a given problem. In other words, the programs in a task are not just sent off to run independently on processors in the network. </paragraph>
<paragraph id="P-0011" lvl="0"><number>&lsqb;0011&rsqb;</number> The differences between the present invention, conventional computer networks and networked computers controlled by GES may be illustrated by way of analogy to the handling of customer queues. Consider, for example, customers coming to a department of motor vehicles (DMV) for a variety of transactions such as licensing, vehicle registration and the like. </paragraph>
<paragraph id="P-0012" lvl="0"><number>&lsqb;0012&rsqb;</number> Conventional computer networks would be like a DMV where each teller has their own queue of customers, and the tellers are dedicated to a single kind of transaction (e.g., licenses, registrations, etc). This is very inefficient because at times one teller will have a very long queue and the other tellers will have nothing to do (e.g., everyone wants to renew licenses, but the registration teller has nothing to do). </paragraph>
<paragraph id="P-0013" lvl="0"><number>&lsqb;0013&rsqb;</number> GES is like a DMV having a bank of tellers and a single queue. Tellers can handle a wide range of transactions (but not necessarily all transactions). When you get to the front of the queue, you go to the first available teller that can handle your transaction. This is far more efficient and allows greater use of the resources at DMV. </paragraph>
<paragraph id="P-0014" lvl="0"><number>&lsqb;0014&rsqb;</number> The virtual supercomputer of the present invention is like a DMV where when you move in from out of state, handing all your out-of-state paperwork to the first person you see and having all tellers (or as many as needed) process your license, titles, registrations, address changes, etc. simultaneously. Furthermore, the tellers would automatically exchange information as required (e.g., the license teller passing your SSN to the title and registration tellers; the title teller passing the VIN and title numbers to the registration teller, etc.). In no longer than it would take to wait for just the license, you would be handed a complete package of all your updated licenses, registrations, etc. </paragraph>
<paragraph id="P-0015" lvl="0"><number>&lsqb;0015&rsqb;</number> Because each computer must perform a variety of tasks (unlike the virtual supercomputer of the present invention), the GES requires substantial loading of software on each workstation that will participate in the grid. In contrast, the concept requires a minimal change to the registry and installation of a daemon so that PVM communications can be established with the workstation. </paragraph>
<paragraph id="P-0016" lvl="0"><number>&lsqb;0016&rsqb;</number> The present invention is a supercomputer comprising a single dedicated computer (called the master computer) that coordinates and controls all the other participating computers. The formation of the supercomputer is initiated by executing the PVM master software on the master computer. The master computer will form a supercomputer by establishing connections with as many other computers as possible (or as explicitly directed by the supercomputer user). Each participating computer downloads software, data, and tasks related to the particular problem to solve as directed by the master computer. </paragraph>
<paragraph id="P-0017" lvl="0"><number>&lsqb;0017&rsqb;</number> The participating computers are preferably ordinary multipurpose computers configured with the Windows NT/2000 operating system. This operating system is common throughout both government and private industry. All that is required to make a Windows NT/<highlight><bold>2000</bold></highlight> computer capable of participating in supercomputing according to the present invention are a few simple changes to the Windows NT/2000 Registry and the installation of remote-shell software (relatively inexpensive commercial purchase; under $5,000 for a world-wide site license). The present invention is not, however, limited to Windows NT environments and, indeed, can be adapted to operate in Linux, Unix or other operating system environments. </paragraph>
<paragraph id="P-0018" lvl="0"><number>&lsqb;0018&rsqb;</number> The supercomputer of the present invention has several features that make it unique among all the cluster supercomputers. These features are. </paragraph>
<paragraph id="P-0019" lvl="0"><number>&lsqb;0019&rsqb;</number> Participating computers are ordinary multipurpose computers that are not physically dedicated to tasks related to the supercomputer. All known cluster supercomputers have dedicated participating computer (e.g., PCFARMS at Fermi National Laboratory, or Beowulf clusters developed by the National Aeronautics and Space Administration). That is, a number of computers are placed in a room, they are wired together, and the only thing they do is function as part of the cluster supercomputer. In contrast, the participating computers used in the present invention can be located in individual offices or workspaces where they can be used at any time by individual users. The present invention only requires one single computer that is dedicated to serving as the master computer. </paragraph>
<paragraph id="P-0020" lvl="0"><number>&lsqb;0020&rsqb;</number> Participating computers can be used concurrently for ordinary multipurpose computing and supercomputing tasks. Not only are the participating computers themselves not physically dedicated, but even when the supercomputer is running the individual computers can be accessed and used by ordinary users. There is no other cluster supercomputer that we know of that can do this. </paragraph>
<paragraph id="P-0021" lvl="0"><number>&lsqb;0021&rsqb;</number> Participating computers can be removed from the supercomputer while computations are in progress. Very few of the cluster supercomputers that we know of allow changes in the configuration during a computation run. In fact, cluster supercomputers based on the Message Passing Interface (MPI, an alternate to PVM) require the supercomputer to be shut down and re-started if the configuration is changed in any way. It is noted that the Unix-based cluster at Sandia National labs allows the removal of participating computers. In that system, however, participating computers can be removed only until some minimum number of participants is left (the minimum number of participants may vary from problem to problem). If any additional computers beyond the minimum drop out of the supercomputer, the computation will fail. In contrast, when a participating computer drops out of the supercomputer of the present invention, the computational workload is automatically re-distributed among remaining participants and the computations proceed uninterrupted. The supercomputer of the present invention will continue the computational tasks without interruption even if all participating computers are dropped and only the master computer remains. </paragraph>
<paragraph id="P-0022" lvl="0"><number>&lsqb;0022&rsqb;</number> Participating computers can be added to the supercomputer while computations are in progress and they will be exploited by the in-progress computation. Most PVM-based cluster supercomputers allow participating computers to be added at anytime. However, the added computers are not actually used until the next computation begins. In contrast, the supercomputer of the present invention can make immediate use of any computers added to the supercomputer. The computational workload will be re-distributed automatically to include the newly added computer. </paragraph>
<paragraph id="P-0023" lvl="0"><number>&lsqb;0023&rsqb;</number> Computers eligible to participate in the supercomputer are automatically detected and added to the supercomputer whenever such computers appear on the network. In effect, the supercomputer of the present invention will automatically seek out, find, and use any eligible computer resources it finds on the network. A computer is eligible if (a) it is configured as a potential participant and (b) the supercomputer user has indicated that the particular computer should be included in the supercomputer. The latter can be done either by providing the master computer with a list of individual computers that should make up the supercomputer or by simply instructing the master computer to use &ldquo;all available resources.&rdquo; In contrast, in distributed internet computing, such as project SETI, the individual computers must explicitly signal their availability to the master computer. That is, participation is voluntary, whereas in the present invention, participation is controlled by the master computer. </paragraph>
<paragraph id="P-0024" lvl="0"><number>&lsqb;0024&rsqb;</number> The supercomputer is highly fault tolerant. If any participating computer exits (e.g., if a hardware or software failure causes the computer to lose communication with the master computer, or if the computer is shut down), the supercomputer of the present invention will automatically detect the loss of the participant and re-balance the computational load among the remaining computers. If the network itself fails, the computation proceeds on the master computer (albeit rather slowly) until the network is restored (at which point the supercomputer automatically detects eligible participating computers and reconnects them). Only if the master computer fails will the computation also fail. In this case the computation must be restarted and the computation proceeds from an intermediate result (so not all computations are lost). Moreover, in the present invention, the robustness of the cluster supercomputer can be bolstered by implementing automatic reboot and restart procedures for the master computer. No known cluster supercomputer is as robust as that of this present invention. </paragraph>
<paragraph id="P-0025" lvl="0"><number>&lsqb;0025&rsqb;</number> The supercomputer of the present invention also has several features that make it unique from commercial products that allows parallel processing across an existing network of Linux computers. One such product, called Enfuzion, is actually quite different from the supercomputer of the present invention. The problem that Enfuzion is meant to solve is that of the need to run the same application over and over again with slightly different data sets. This is common, for example, in doing Monte Carlo modeling where lots of essentially identical runs are needed to produce useful statistical results. Needless to say, this can be a tedious and time-consuming process. Enfuzion solves this particular need by automatically and simultaneously running the application on each of the computers in the network. So, instead of running the same model <highlight><bold>100</bold></highlight> times consecutively, Enfuzion can run it once across <highlight><bold>100</bold></highlight> computers simultaneously. This lets a user obtain <highlight><bold>100</bold></highlight> model runs in the same time it normally took to do a single run. </paragraph>
<paragraph id="P-0026" lvl="0"><number>&lsqb;0026&rsqb;</number> This is far less sophisticated than the supercomputer of the present invention. The supercomputer of the present invention can perform the needed calculations as a trivial special case. In addition to the functionality provided by Enfuzion the supercomputer of the present invention can also exchange data across runs on different computers while the computation is in progress. That is, data results derived on one member of the cluster can be directly communicated to other cluster members, allowing the other nodes to make immediate use of the new data. To the best of our knowledge, Enfuzion does not have any way for the runs on different computers to exchange or share data that might affect the course of the computations. </paragraph>
<paragraph id="P-0027" lvl="0"><number>&lsqb;0027&rsqb;</number> Additionally, with the supercomputer of the present invention the participating computers can work collaboratively to solve a single computational problem. The software that Enfuzion runs on each machine is essentially independent of all the other machines. It is no different than walking up to each computer and starting up the same program on each one (perhaps with different input data for each). All Enfuzion does is automate this process so you don&apos;t have to walk from computer to computer. In essence, each is solving its own little problem. The supercomputer of the present invention allows all participating computers to work collaboratively to solve a single problem. The in-progress partial results from one computer can affect the progress of computations on any of the other computers. The programs Enfuzion runs on each computer would run equally well sequentially (they run in parallel simply to save time). In contrast, programs running on the supercomputer of the present invention beneficially interact with other computers in the cluster not only to save time, but to increase the computational capabilities.</paragraph>
</section>
</summary-of-invention>
<brief-description-of-drawings>
<section>
<heading lvl="1">BRIEF DESCRIPTION OF THE DRAWINGS </heading>
<paragraph id="P-0028" lvl="0"><number>&lsqb;0028&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 1A</cross-reference> is a schematic diagram showing a virtual supercomputer comprising a plurality of multipurpose computers. </paragraph>
<paragraph id="P-0029" lvl="0"><number>&lsqb;0029&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 1B</cross-reference> is a schematic diagram showing a configuration used in a master computer of the virtual supercomputer of the present invention. </paragraph>
<paragraph id="P-0030" lvl="0"><number>&lsqb;0030&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 1C</cross-reference> is a schematic diagram showing a configuration used in a member computer of the virtual supercomputer of the present invention. </paragraph>
<paragraph id="P-0031" lvl="0"><number>&lsqb;0031&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 1D</cross-reference> is a schematic diagram showing how the virtual supercomputer of the present invention reconfigures itself when a member host computer drops out of the virtual supercomputer. </paragraph>
<paragraph id="P-0032" lvl="0"><number>&lsqb;0032&rsqb;</number> <cross-reference target="DRAWINGS">FIGS. 2A and 2B</cross-reference> are schematic diagrams showing how the virtual supercomputer of the present invention reconfigures itself when a new member computer joins the virtual supercomputer. </paragraph>
<paragraph id="P-0033" lvl="0"><number>&lsqb;0033&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 3</cross-reference> is a flow chart showing the steps used in one embodiment of the present invention to establish and use a virtual supercomputer to rapidly complete a computationally intensive task. </paragraph>
<paragraph id="P-0034" lvl="0"><number>&lsqb;0034&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 4</cross-reference> is a flow chart showing the steps used in one embodiment of the present invention to establish a virtual supercomputer. </paragraph>
<paragraph id="P-0035" lvl="0"><number>&lsqb;0035&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 5</cross-reference> is a flow chart showing the steps used in one embodiment of the present invention to automatically reconfigure a virtual supercomputer when computer hosts join or exit the virtual supercomputer. </paragraph>
<paragraph id="P-0036" lvl="0"><number>&lsqb;0036&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 6</cross-reference> is a flow chart showing the steps used in one embodiment of the present invention when a host computer attempts to join a virtual supercomputer. </paragraph>
<paragraph id="P-0037" lvl="0"><number>&lsqb;0037&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 7</cross-reference> is a schematic diagram showing a virtual supercomputer comprising a plurality of multipurpose computers and a plurality of sub-master computers. </paragraph>
<paragraph id="P-0038" lvl="0"><number>&lsqb;0038&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 8</cross-reference> is a schematic diagram showing two virtual supercomputers comprising a common pool of multipurpose computers.</paragraph>
</section>
</brief-description-of-drawings>
<detailed-description>
<section>
<heading lvl="1">DETAILED DESCRIPTION OF THE INVENTION </heading>
<paragraph id="P-0039" lvl="0"><number>&lsqb;0039&rsqb;</number> Computer network <highlight><bold>100</bold></highlight>, shown in <cross-reference target="DRAWINGS">FIG. 1</cross-reference>A, comprises a plurality of computer systems in communication with one another. The communications path may be any network such as, for example, a local or wide area network, or the Internet. The virtual supercomputer of the present invention comprises computers selected from computer network <highlight><bold>100</bold></highlight>. One of the computers in computer network <highlight><bold>100</bold></highlight> is master computer <highlight><bold>102</bold></highlight> which serves as the master of the virtual supercomputer. Master computer <highlight><bold>102</bold></highlight> is a computer system comprising operating system <highlight><bold>104</bold></highlight>, memory <highlight><bold>106</bold></highlight>, central processor <highlight><bold>108</bold></highlight>, parallel virtual machine (PVM) daemon <highlight><bold>110</bold></highlight>, and master application software <highlight><bold>112</bold></highlight>, as shown in <cross-reference target="DRAWINGS">FIG. 1B</cross-reference>. PVM daemon <highlight><bold>110</bold></highlight> may be any software application for establishing a parallel virtual machine comprising independent computer systems working in collaboration. PVM daemon <highlight><bold>110</bold></highlight> controls the communications channels between master computer <highlight><bold>102</bold></highlight> and other member computers of the virtual supercomputer. In one embodiment of the present invention, PVM daemon <highlight><bold>110</bold></highlight> is the PVM software provided by ORNL. </paragraph>
<paragraph id="P-0040" lvl="0"><number>&lsqb;0040&rsqb;</number> Master application software <highlight><bold>112</bold></highlight> controls the operation of the virtual supercomputer in solving the computationally intensive problem. That is, master application software <highlight><bold>112</bold></highlight> is responsible for building and managing the virtual supercomputer, for assigning tasks to other member computers in the supercomputer, and for saving results of the computations as reported by the other member computers. Master computer <highlight><bold>102</bold></highlight> may be any multipurpose computer system. Preferably, master computer <highlight><bold>102</bold></highlight> is a computer system dedicated to performing tasks as the master for the virtual supercomputer. Master computer <highlight><bold>102</bold></highlight> also comprises slave application software <highlight><bold>114</bold></highlight> which performs tasks received from master application software <highlight><bold>112</bold></highlight>. </paragraph>
<paragraph id="P-0041" lvl="0"><number>&lsqb;0041&rsqb;</number> As noted above, multipurpose computers from computer network <highlight><bold>100</bold></highlight> are used to build the virtual supercomputer of the present invention. Shaded icons in <cross-reference target="DRAWINGS">FIG. 1A</cross-reference> designate computers that are members of the virtual supercomputer. That is, for example, computers <highlight><bold>102</bold></highlight>, <highlight><bold>116</bold></highlight>, <highlight><bold>118</bold></highlight> and <highlight><bold>120</bold></highlight> among others, are members of the virtual supercomputer. In contrast, computer <highlight><bold>122</bold></highlight> is not currently a member of the virtual supercomputer shown in <cross-reference target="DRAWINGS">FIG. 1A</cross-reference>. This convention is used throughout the Figures to distinguish between members and non-members of the supercomputer. </paragraph>
<paragraph id="P-0042" lvl="0"><number>&lsqb;0042&rsqb;</number> As shown in <cross-reference target="DRAWINGS">FIG. 1</cross-reference>C, member computers such as member computer <highlight><bold>120</bold></highlight> comprise operating system <highlight><bold>124</bold></highlight>, memory <highlight><bold>126</bold></highlight>, central processor <highlight><bold>128</bold></highlight>, PVM daemon <highlight><bold>130</bold></highlight> and slave application software <highlight><bold>132</bold></highlight>. Additionally, computer <highlight><bold>120</bold></highlight> may comprise other application software <highlight><bold>134</bold></highlight> allowing a user to perform multiple tasks on computer <highlight><bold>120</bold></highlight>. Such other application software may include, for example, word processing, spreadsheet, database or other commonly used office automation applications, among others. Operating system <highlight><bold>124</bold></highlight> on member computer <highlight><bold>120</bold></highlight> need not be the same as operating system <highlight><bold>104</bold></highlight> on master computer <highlight><bold>102</bold></highlight>. Similarly, operating system <highlight><bold>124</bold></highlight> on member computer <highlight><bold>120</bold></highlight> could be different from the operating system used on other member computers, such as for example, member computers <highlight><bold>116</bold></highlight> or <highlight><bold>118</bold></highlight>. Accordingly, the virtual supercomputer of the present invention may comprise a heterogeneous network of computer systems. In one embodiment of the present invention, PVM daemon <highlight><bold>130</bold></highlight> and slave application software <highlight><bold>132</bold></highlight> on member computer <highlight><bold>120</bold></highlight> are the same as PVM deamon <highlight><bold>110</bold></highlight> and slave application software <highlight><bold>113</bold></highlight>, respectively, that are implemented on master computer <highlight><bold>102</bold></highlight>. </paragraph>
<paragraph id="P-0043" lvl="0"><number>&lsqb;0043&rsqb;</number> The virtual supercomputer of the present invention provides mechanisms for automatically reconfiguring itself in the event of a failure of one of the member computers. Similarly, the virtual supercomputer of the present invention provides mechanisms for adding additional member computers as they become available. In this manner, the virtual supercomputer of the present invention is a highly robust system for completing the computationally intensive problems presented with minimal intervention from a system administrator. Moreover, the robustness provided by the virtual supercomputer of the present invention allows member computers to freely join or leave the virtual supercomputer according to the member computers availability for assuming a role in the computations. Accordingly, the virtual supercomputer of the present invention may take full advantage of the excess processing capabilities of member computers without adversely impacting the users of those computers. </paragraph>
<paragraph id="P-0044" lvl="0"><number>&lsqb;0044&rsqb;</number> The schematic diagrams shown in <cross-reference target="DRAWINGS">FIGS. 1A and 1D</cross-reference> illustrate how the virtual supercomputer of the present invention reconfigures itself when a member computer drops out of the virtual supercomputer. The processing loads for each member computer are as shown in <cross-reference target="DRAWINGS">FIG. 1A</cross-reference>. The actual processing load on each member computer is dependent on the tasks assigned to each computer by master computer <highlight><bold>102</bold></highlight>. The load may be based on an estimated instructions per time unit, or in terms of task quantum assigned or some other unit indicating the processing capacity of the member computer which is being, or will be, consumed as part of the virtual supercomputer. For example, the processing load on member computer <highlight><bold>116</bold></highlight> is represented by the symbol &ldquo;F,&rdquo; where F represents the capacity (in whatever units are chosen) of computer <highlight><bold>116</bold></highlight> devoted to solving the computationally intensive problem. Similarly, the processing load on member computer <highlight><bold>120</bold></highlight> is I, which may or may not be the same load as F. The processing load on computer <highlight><bold>122</bold></highlight> is shown as zero in <cross-reference target="DRAWINGS">FIG. 1A</cross-reference> because that computer is not currently a member of the virtual supercomputer. </paragraph>
<paragraph id="P-0045" lvl="0"><number>&lsqb;0045&rsqb;</number> If a member computer drops out of the virtual supercomputer the load must be redistributed among the remaining member computers. For example, in <cross-reference target="DRAWINGS">FIG. 1</cross-reference>D, computers <highlight><bold>116</bold></highlight> and <highlight><bold>118</bold></highlight> have dropped out of the virtual supercomputer, as evidenced by their processing loads dropping to zero. In this case, the loads F and G, formerly assigned to computers <highlight><bold>116</bold></highlight> and <highlight><bold>118</bold></highlight>, respectively, must be reallocated among the remaining member computers. As shown in <cross-reference target="DRAWINGS">FIG. 1</cross-reference>D, the load on each remaining computer is adjusted. In one embodiment, the incremental load assigned to each remaining member computer, i.e., A<highlight><subscript>1</subscript></highlight>, B<highlight><subscript>1</subscript></highlight>, etc., is evenly distributed. That is, A<highlight><subscript>1</subscript></highlight>&equals;B<highlight><subscript>1</subscript></highlight>&equals;C<highlight><subscript>1</subscript></highlight>, and so on. In another embodiment, the incremental load on each remaining member computer is proportional to the previously assigned load. For example, I<highlight><subscript>1</subscript></highlight>, the incremental load on member computer <highlight><bold>120</bold></highlight>, is given by:  
<math-cwu id="MATH-US-00001">
<number>1</number>
<math>
<mrow>
  <mrow>
    <msub>
      <mi>I</mi>
      <mn>1</mn>
    </msub>
    <mo>=</mo>
    <mrow>
      <mfrac>
        <mi>I</mi>
        <mi>T</mi>
      </mfrac>
      <mo>&times;</mo>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mi>F</mi>
          <mo>+</mo>
          <mi>G</mi>
        </mrow>
        <mo>)</mo>
      </mrow>
    </mrow>
  </mrow>
  <mo>,</mo>
</mrow>
</math>
<mathematica-file id="MATHEMATICA-00001" file="US20030005068A1-20030102-M00001.NB"/>
<image id="EMI-M00001" wi="216.027" he="17.03835" file="US20030005068A1-20030102-M00001.TIF" imf="TIFF" ti="MF"/>
</math-cwu>
</paragraph>
<paragraph id="P-0046" lvl="7"><number>&lsqb;0046&rsqb;</number> where I is the load previously assigned to member computer <highlight><bold>120</bold></highlight>, F is the load previously assigned to computer <highlight><bold>116</bold></highlight>, G is the load previously assigned to computer <highlight><bold>118</bold></highlight>, and T is the total load assigned to remaining member computers prior to redistribution of the load. Another way of assigning the loads to each computer is to make the loads assigned proportional to the relative speeds of the computers. In any event, the sum of the incremental loads for each system is equal to the load formerly assigned to the dropped computers. That is, F&plus;G &equals;A<highlight><subscript>1</subscript></highlight>&plus;B<highlight><subscript>1</subscript></highlight>&plus;C<highlight><subscript>1</subscript></highlight>&plus;D<highlight><subscript>1</subscript></highlight>&plus;E<highlight><subscript>1</subscript></highlight>&plus;H<highlight><subscript>1</subscript></highlight>&plus;I<highlight><subscript>1</subscript></highlight>. </paragraph>
<paragraph id="P-0047" lvl="0"><number>&lsqb;0047&rsqb;</number> Just as the virtual supercomputer of the present invention reconfigures itself when a computer drops out, the virtual supercomputer of the present invention reconfigures itself whenever it detects the availability of a new computer on the network. <cross-reference target="DRAWINGS">FIGS. 2A and 2B</cross-reference> are schematic diagrams showing how the virtual supercomputer of the present invention reconfigures itself when a new member computer joins the virtual supercomputer. In <cross-reference target="DRAWINGS">FIG. 2</cross-reference>A, network <highlight><bold>200</bold></highlight> comprises a plurality of multipurpose computers in communication with one another. Some of the computers are members of a virtual supercomputer, as indicated by the shading in <cross-reference target="DRAWINGS">FIG. 2A</cross-reference>. The processing load for each computer in network <highlight><bold>200</bold></highlight> associated with the virtual supercomputer is as shown. For example, member computer <highlight><bold>202</bold></highlight> has processing load B associated with computations assigned by master computer <highlight><bold>204</bold></highlight>. In contrast, computer <highlight><bold>206</bold></highlight> has a processing load of zero because it is not a member of the virtual supercomputer. In <cross-reference target="DRAWINGS">FIG. 2</cross-reference>B, computer <highlight><bold>206</bold></highlight> has just joined the virtual supercomputer of the present invention. When this occurs, master computer <highlight><bold>204</bold></highlight> redistributes the load among the member computers as shown in <cross-reference target="DRAWINGS">FIG. 2B</cross-reference>. </paragraph>
<paragraph id="P-0048" lvl="0"><number>&lsqb;0048&rsqb;</number> The new load on each member computer may be reduced according to some percentage depending on the processing capabilities of the new member computer. For example the new member computer <highlight><bold>206</bold></highlight> may receive an initial load of F, which in turn represents an incremental decrease in the load assigned to each member computer, as shown in <cross-reference target="DRAWINGS">FIG. 2B</cross-reference>. That is, F&equals;A<highlight><subscript>1</subscript></highlight>&plus;B<highlight><subscript>1</subscript></highlight>&plus;C<highlight><subscript>1</subscript></highlight>&plus;D<highlight><subscript>1</subscript></highlight>&plus;E<highlight><subscript>1</subscript></highlight>&plus;G<highlight><subscript>1</subscript></highlight>&plus;I<highlight><subscript>1</subscript></highlight>. The incremental decrease may be proportional to the processing capabilities of each member computer in the virtual supercomputer, or the incremental decrease could be determined according to some other algorithm for load distribution. For example, B<highlight><subscript>1</subscript></highlight>, the incremental decreased load on member computer <highlight><bold>202</bold></highlight>, may be given by:  
<math-cwu id="MATH-US-00002">
<number>2</number>
<math>
<mrow>
  <mrow>
    <msub>
      <mi>B</mi>
      <mn>1</mn>
    </msub>
    <mo>=</mo>
    <mrow>
      <mfrac>
        <mi>B</mi>
        <mi>T</mi>
      </mfrac>
      <mo>&times;</mo>
      <mi>F</mi>
    </mrow>
  </mrow>
  <mo>,</mo>
</mrow>
</math>
<mathematica-file id="MATHEMATICA-00002" file="US20030005068A1-20030102-M00002.NB"/>
<image id="EMI-M00002" wi="216.027" he="17.03835" file="US20030005068A1-20030102-M00002.TIF" imf="TIFF" ti="MF"/>
</math-cwu>
</paragraph>
<paragraph id="P-0049" lvl="7"><number>&lsqb;0049&rsqb;</number> where B is the load previously assigned to member computer <highlight><bold>202</bold></highlight>, F is the load to be assigned to new member computer <highlight><bold>206</bold></highlight>, and T is the total load assigned to member computers before assigning load F to member computer <highlight><bold>116</bold></highlight>. </paragraph>
<paragraph id="P-0050" lvl="0"><number>&lsqb;0050&rsqb;</number> Regardless of the precise mechanism used to redistribute the load among remaining member computers, master computer <highlight><bold>120</bold></highlight> may take into account the following items when redistributing the load. Master computer <highlight><bold>102</bold></highlight> may ensure that all fractions of the load are accounted for. That is, although the incremental load for each remaining member computer may be expressed as a percentage or fraction of the load previously assigned to the dropped computer, computer <highlight><bold>116</bold></highlight>, it may not be possible to divide the tasks in exact precentages because some tasks can only be reduced to a minimum defined quanta as discussed in more detail in the section on initial load balancing below. Moreover, master computer <highlight><bold>102</bold></highlight> may use the most recent processing statistics for each remaining member computer to recompute the distribution according to the current procesing capabilities of the computers. </paragraph>
<paragraph id="P-0051" lvl="0"><number>&lsqb;0051&rsqb;</number> The flow chart shown in <cross-reference target="DRAWINGS">FIG. 3</cross-reference> illustrates the steps used in one embodiment of the present invention to build and utilize a virtual supercomputer for solving a computationally intensive problem. As would be apparent to one of ordinary skill in the art, the steps presented below may be automated such that whenever the master computer is restarted, a virtual supercomputer is established and computations are assigned to member computers as necessary to solve a given problem. As shown in <cross-reference target="DRAWINGS">FIG. 3</cross-reference>, the steps may be grouped into three phases: Startup Phase <highlight><bold>300</bold></highlight>, Computations Phase <highlight><bold>310</bold></highlight> and Shutdown Phase <highlight><bold>320</bold></highlight>. Each of these phases is described in more detail in the sections below. </paragraph>
<paragraph id="P-0052" lvl="0"><number>&lsqb;0052&rsqb;</number> 1. Startup Phase </paragraph>
<paragraph id="P-0053" lvl="0"><number>&lsqb;0053&rsqb;</number> In step <highlight><bold>301</bold></highlight> the master computer is powered on and in step <highlight><bold>302</bold></highlight>, PVM daemon software is started on the master computer. Master application software and slave application software are started in steps <highlight><bold>303</bold></highlight> and <highlight><bold>304</bold></highlight>, respectively. At the completion of step <highlight><bold>304</bold></highlight>, a virtual supercomputer having a size of one member computer has been established. Although such a virtual supercomputer could be used to solve a computationally intensive problem according to the present invention, a larger virtual supercomputer is preferable because of its increased processing capabilities. </paragraph>
<paragraph id="P-0054" lvl="0"><number>&lsqb;0054&rsqb;</number> In step <highlight><bold>305</bold></highlight> the master computer (via the master application process) builds the virtual supercomputer according to steps such as shown in <cross-reference target="DRAWINGS">FIG. 4</cross-reference>. Once the virtual supercomputer is built, the master computer distributes data sets to each slave computer. As used herein, the terms &ldquo;slave computer&rdquo;, &ldquo;member computer&rdquo; and &ldquo;member of the virtual supercomputer&rdquo; refer to any computer in the virtual supercomputer actively running the slave software previously described. For example, master computer <highlight><bold>102</bold></highlight> and multipurpose computer <highlight><bold>120</bold></highlight>, shown in <cross-reference target="DRAWINGS">FIG. 1</cross-reference>A, are both &ldquo;slave computers.&rdquo; In step <highlight><bold>306</bold></highlight> the master computer distributes whatever data sets and information are required for subsequent computation to each slave computer in the virtual supercomputer of the present invention. In one embodiment, the data sets may include data together with calculation instructions to be applied to the data. In this embodiment, the slave application implemented on the slave computers may be a very simple application designed to receive instructions and data from the master and to execute the instructions as directed. Alternatively, the slave application could be more complex comprising the computational instructions needed to solve the problem under consideration. In another alternative embodiment, the slave application may comprise a combination of computational instructions embedded in the software as well as the capability to receive additional computational instructions from the master computer. </paragraph>
<paragraph id="P-0055" lvl="0"><number>&lsqb;0055&rsqb;</number> In step <highlight><bold>306</bold></highlight> master computer prepares each slave to perform computations. This may include providing data for computations and any other information needed by the slave to perform any tasks that may be assigned to the slave or needed by the slave to perform any calculations required for performance testing and load balancing. </paragraph>
<paragraph id="P-0056" lvl="0"><number>&lsqb;0056&rsqb;</number> In step <highlight><bold>307</bold></highlight> master computer <highlight><bold>102</bold></highlight> performs an initial load balancing for the virtual supercomputer. In this step the master computer can use information received from each member computer to estimate the individual and collective processing capacity for computer and the virtual supercomputer as a whole. Using such information, the master computer can determine the appropriate load distribution for the virtual supercomputer of the present invention. Additionally, as noted above, the master computer is programmed to divide the computationally intensive problem into discrete task sets. Each task set is herein defined as the task quanta. Accordingly, the entire computationally intensive problem can be thought of as the sum of all task quantum. Because the task quanta are discrete task sets, the master computer can keep track of which slave computers have been assigned which quanta. Moreover, in later steps, the slave computers provide the results for each task quanta on an on-going basis, thereby allowing the master computer to update data sets and task quanta as needed to solve the computationally intensive problem. </paragraph>
<paragraph id="P-0057" lvl="0"><number>&lsqb;0057&rsqb;</number> 2. Computational Phase </paragraph>
<paragraph id="P-0058" lvl="0"><number>&lsqb;0058&rsqb;</number> Each slave computer commences calculations as pre-programmed within the slave application, as directed by master computer <highlight><bold>102</bold></highlight>, or according to a combination of pre-programmed computations and dynamic computational instructions received from the master computer as assigned in step <highlight><bold>312</bold></highlight>. Tasks to be performed may be queued either on the master or on the slaves as appropriate In step <highlight><bold>313</bold></highlight>, master computer <highlight><bold>102</bold></highlight> monitors the status of the computational efforts of the virtual supercomputer. If all of the assigned computational tasks have been completed master computer <highlight><bold>102</bold></highlight> moves on to step <highlight><bold>321</bold></highlight>. Otherwise, in step <highlight><bold>314</bold></highlight> master computer <highlight><bold>102</bold></highlight> monitors the network for computational results transmitted by individual slave computers. As results are received in step <highlight><bold>314</bold></highlight>, master computer <highlight><bold>102</bold></highlight> may distribute them to one or more of the other slave computers in the virtual supercomputer. </paragraph>
<paragraph id="P-0059" lvl="0"><number>&lsqb;0059&rsqb;</number> As discussed above, each slave computer in the virtual supercomputer of the present invention provides periodic performance reports to master computer <highlight><bold>102</bold></highlight> (step <highlight><bold>315</bold></highlight>). The periodic performance reports may comprise results of special benchmark computations assigned to the slave computer or may comprise other performance metrics that can be used to indicate the load on the slave computer. Because the slave computers of the present invention comprise a plurality of multipurpose computer systems, it is important that the master computer monitor the performance and loading on the slave computers so that adjustments can be made in task assignments as necessary to provide maximum throughput for the virtual supercomputer&apos;s computations. </paragraph>
<paragraph id="P-0060" lvl="0"><number>&lsqb;0060&rsqb;</number> In step <highlight><bold>316</bold></highlight>, master computer <highlight><bold>102</bold></highlight> uses the performance data received from the slave computers to determine how to best balance the load on the virtual supercomputer. In step <highlight><bold>317</bold></highlight>, master computer <highlight><bold>102</bold></highlight> retrieves and reassigns uncompleted computational tasks from slave computers according to the load balancing scheme developed in step <highlight><bold>316</bold></highlight>. In step <highlight><bold>318</bold></highlight>, master computer <highlight><bold>102</bold></highlight> monitors the network to detect and process events that affect the size of the virtual supercomputer. Such events include the failure of a slave process on a slave computer or the failure of a slave computer itself, in which case, the virtual supercomputer may decrease in size. Similarly, the event may be the addition of new slave computers thereby increasing the size and computing capacity of the virtual supercomputer. In either case, master computer <highlight><bold>102</bold></highlight> may process the events as shown in <cross-reference target="DRAWINGS">FIG. 5</cross-reference>. </paragraph>
<paragraph id="P-0061" lvl="0"><number>&lsqb;0061&rsqb;</number> 3. Shutdown Phase </paragraph>
<paragraph id="P-0062" lvl="0"><number>&lsqb;0062&rsqb;</number> As described above, once all of the computational tasks for a given computationally intensive problem have been completed, the process moves on to Shutdown Phase <highlight><bold>320</bold></highlight>. In step <highlight><bold>321</bold></highlight> master computer <highlight><bold>102</bold></highlight> collects final performance statistics from each slave computer. These performance statistics can be used, for example, to establish a baseline for subsequent virtual supercomputers built to solve other computationally intensive problems. Among other things, the statistics could be used to show the increased utilization of computer assets within an organization. </paragraph>
<paragraph id="P-0063" lvl="0"><number>&lsqb;0063&rsqb;</number> Finally, in steps <highlight><bold>322</bold></highlight>-<highlight><bold>324</bold></highlight>, the virtual supercomputer of the present invention is torn down and general housekeeping procedures are invoked. That is, in step <highlight><bold>322</bold></highlight> the slave application on each slave computer is terminated and in step <highlight><bold>323</bold></highlight>, the PVM daemon is terminated. In step <highlight><bold>324</bold></highlight>, the results of the virtual supercomputer&apos;s computations are available for reporting to the user. </paragraph>
<paragraph id="P-0064" lvl="7"><number>&lsqb;0064&rsqb;</number> Building a PVM </paragraph>
<paragraph id="P-0065" lvl="0"><number>&lsqb;0065&rsqb;</number> As noted above, once the master computer comes online, i.e., the master computer has been powered on, the PVM daemon has been invoked and the master and slave applications have been started, the master computer attempts to increase the size of the virtual supercomputer in step <highlight><bold>305</bold></highlight>. <cross-reference target="DRAWINGS">FIG. 4</cross-reference> shows a flow of steps that may be used in an embodiment of the present invention to build the virtual supercomputer. In step <highlight><bold>400</bold></highlight>, master computer <highlight><bold>102</bold></highlight> determines if any potential host computers have been identified. This step may be carried out in several different ways, including for example, by polling known potential host computers to see if they are available. In another embodiment, each potential host computer periodically broadcasts its availability to the network, or directly to master computer <highlight><bold>102</bold></highlight>. In yet another embodiment, some combination of polling by the master and announcing by the potential hosts can be used to identify potential host computers. </paragraph>
<paragraph id="P-0066" lvl="0"><number>&lsqb;0066&rsqb;</number> Once a potential host computer has been identified, master computer <highlight><bold>102</bold></highlight> determines whether or not the communications path between itself and the potential host is adequate for supporting the virtual supercomputer in step <highlight><bold>402</bold></highlight>. This may be accomplished using standard network tools such as &ldquo;ping&rdquo; for a TCP/IP network, or using proprietary data communications tools for determining the network bandwidth or stability as needed. If the communications path is not good, master computer <highlight><bold>102</bold></highlight> returns to step <highlight><bold>400</bold></highlight> to see if any other potential hosts are identified. Otherwise, if the communications path is good, master computer <highlight><bold>102</bold></highlight> moves on to step <highlight><bold>404</bold></highlight> where PVM daemon software is downloaded from the master computer to the potential host computer. Step <highlight><bold>404</bold></highlight> (and/or step <highlight><bold>414</bold></highlight>, described below) could be accomplished using a &ldquo;push&rdquo; method wherein the master computer sends the daemon to the potential host with instructions to start the daemon. Alternatively, steps <highlight><bold>404</bold></highlight> and/or <highlight><bold>414</bold></highlight> could be accomplished using a &ldquo;pull&rdquo; method wherein the potential host actively retrieves the software from the master. In either case, it is to be understood that the software may also be downloaded from some other computer system as long as the master and potential slave know where the software is located for download. Alternatively, each potential host computer could have a local copy of the software on a local storage device. This latter embodiment would not be desirable in most environments because of the added administration costs due to configuration management and software changes which would require updates on each system individually. However, there may be some cases where the software configuration of the PVM daemon and/or the slave application are sufficiently stable that local distribution may be desirable. </paragraph>
<paragraph id="P-0067" lvl="0"><number>&lsqb;0067&rsqb;</number> In step <highlight><bold>406</bold></highlight>, the PVM daemon on the potential host computer is started and in step <highlight><bold>408</bold></highlight>, PVM to PVM communications are established between the PVM daemon process on the master computer and the PVM daemon process on the potential host computer. In step <highlight><bold>410</bold></highlight>, if the PVM to PVM communications could not be successfully established, the process moves on the step <highlight><bold>412</bold></highlight> where the PVM daemon is terminated on the potential host. Then, master computer <highlight><bold>102</bold></highlight> returns to step <highlight><bold>400</bold></highlight> to look for additional potential host computers. Otherwise, if the PVM to PVM communications is successfully established, the process moves on to step <highlight><bold>414</bold></highlight> where the slave application is downloaded to the potential computer. </paragraph>
<paragraph id="P-0068" lvl="0"><number>&lsqb;0068&rsqb;</number> In step <highlight><bold>416</bold></highlight> the slave application is started on the potential host computer and in step <highlight><bold>418</bold></highlight> the slave application is tested to ensure correct computations are made. The testing in step <highlight><bold>418</bold></highlight> may be accomplished, for example, by sending a computational instruction and a pre-determined data set to the slave. The slave then performs the required calculations and returns a result to the master computer. In step <highlight><bold>420</bold></highlight>, the master computer then compares the reported result with the known correct result to determine whether or not the slave application on the potential host is working properly. If the slave is not working properly, the process moves on to step <highlight><bold>424</bold></highlight> and the slave application is terminated on the potential host computer. Next, the process continues cleanup procedures in step <highlight><bold>412</bold></highlight> and returns to step <highlight><bold>400</bold></highlight> to look for the next potential host to join the virtual supercomputer. If, in step <highlight><bold>420</bold></highlight>, the slave application returns valid results, the &ldquo;potential host&rdquo; has become a &ldquo;member&rdquo; of the virtual supercomputer. The process moves on to step <highlight><bold>426</bold></highlight> where the performance or workload capacity for the new member computer is estimated. This estimation can be based on the speed of the test computations or on some other metric for gauging the expected performance for the computer. The estimated performance is used to perform the initial load balancing discussed in conjunction with step <highlight><bold>307</bold></highlight> in <cross-reference target="DRAWINGS">FIG. 3</cross-reference>. Finally, the process returns to step <highlight><bold>400</bold></highlight> to see if any additional potential hosts can be identified. If not, the process of building the PVM has been completed, i.e., step <highlight><bold>307</bold></highlight> in <cross-reference target="DRAWINGS">FIG. 3</cross-reference>, has been performed. </paragraph>
<paragraph id="P-0069" lvl="0"><number>&lsqb;0069&rsqb;</number> 1. Dynamic Reconfiguration&mdash;Master Computer </paragraph>
<paragraph id="P-0070" lvl="0"><number>&lsqb;0070&rsqb;</number> As described above, the virtual supercomputer of the present invention dynamically reconfigures itself as slave computers become available for joining the virtual supercomputer or as slave computers leave the virtual supercomputer. Events affecting the size of the virtual supercomputer are detected in step <highlight><bold>318</bold></highlight> in <cross-reference target="DRAWINGS">FIG. 3</cross-reference>. <cross-reference target="DRAWINGS">FIG. 5</cross-reference> provides a more detailed flow of steps that can be used to reconfigure the virtual supercomputer in response to such events. In step <highlight><bold>500</bold></highlight>, when an event is detected, the process determines whether the event is a new computer host becoming available to join the virtual supercomputer, or is a failure of one of the existing slave systems. In the former case, the process moves on to step <highlight><bold>501</bold></highlight> and in the latter case, the process moves on to step <highlight><bold>502</bold></highlight>. Events can be detected or indicated on master computer <highlight><bold>102</bold></highlight> using any suitable detection mechanism. For example, a slave failure may be indicated if master computer <highlight><bold>102</bold></highlight> sends a message to a particular slave computer but receives no response within some pre-determined interval of time. In another situation, master computer <highlight><bold>102</bold></highlight> may detect a slave failure if it does not receive periodic performance statistics or computational results within a pre-determined interval of time. Also, master computer <highlight><bold>102</bold></highlight> may detect a slave failure if a member computer provides erroneous results for a known set of baseline data. Similarly, detecting the availability of a potential host computer to join the virtual supercomputer can be accomplished in several ways. For example, the newly available potential computer may broadcast its availability to the network or provide direct notice to the master computer. Alternatively, the master computer could periodically poll the network for systems known to be potential members of the virtual supercomputer. </paragraph>
<paragraph id="P-0071" lvl="0"><number>&lsqb;0071&rsqb;</number> Looking first at the case of a failed slave system, the process moves on to step <highlight><bold>502</bold></highlight> where master computer <highlight><bold>102</bold></highlight> determines whether or not the host is reachable via PVM-to-PVM communications. If the host is reachable, i.e., slave computer is still part of the virtual supercomputer, the process moves on to step <highlight><bold>503</bold></highlight>. Otherwise, if the PVM daemon is not functioning properly on the slave computer, the process moves on to step <highlight><bold>504</bold></highlight> and master computer <highlight><bold>102</bold></highlight> instructs the host computer to restart its PVM daemon. In step <highlight><bold>505</bold></highlight>, if the PVM daemon can is successfully restarted on the host computer, the process moves on to step <highlight><bold>503</bold></highlight>. If the daemon could not be restarted, the process moves on to <highlight><bold>506</bold></highlight> and master computer <highlight><bold>102</bold></highlight> drops the slave computer from the virtual supercomputer. </paragraph>
<paragraph id="P-0072" lvl="0"><number>&lsqb;0072&rsqb;</number> The steps presented in <cross-reference target="DRAWINGS">FIG. 5</cross-reference> are from the perspective of the master computer. That is, master computer <highlight><bold>102</bold></highlight> may still attempt to perform steps <highlight><bold>504</bold></highlight> and <highlight><bold>505</bold></highlight> even if the network communications path between the master computer and the slave computer has failed. Also, it is possible that the slave application and/or PVM daemon continue to run on the failed slave computer even after the master has dropped it from the virtual supercomputer. In this case, the slave application and/or PVM daemon can be programmed to terminate on the slave computer if they do not receive feedback from the master within a pre-determined interval of time. </paragraph>
<paragraph id="P-0073" lvl="0"><number>&lsqb;0073&rsqb;</number> If PVM-to-PVM communications are running between the two computers, master computer <highlight><bold>102</bold></highlight> sends a message to the failed host instructing it to terminate the failed slave application on the host in step <highlight><bold>503</bold></highlight>, if it is still running on the machine. In step <highlight><bold>507</bold></highlight>, master computer <highlight><bold>102</bold></highlight> instructs the failed slave computer to start (or restart) the slave application. In step <highlight><bold>508</bold></highlight>, master computer <highlight><bold>102</bold></highlight> determines whether or not the slave application has been successfully started on the slave computer. If the slave application could not be started, the process moves on to step <highlight><bold>506</bold></highlight> where the failed host is dropped from the virtual supercomputer. As noted above, certain house keeping operations may be automatically or manually performed on the slave computer if it loses effective contact with the virtual supercomputer. </paragraph>
<paragraph id="P-0074" lvl="0"><number>&lsqb;0074&rsqb;</number> If the new slave application was successfully started on the host computer, the process moves on to step <highlight><bold>510</bold></highlight> where the slave application is initialized on the host computer. In step <highlight><bold>512</bold></highlight>, the slave application is tested to ensure proper results are computed for a known problem or data set. In step <highlight><bold>514</bold></highlight> if the slave application returns erroneous results, the process moves on to step <highlight><bold>516</bold></highlight>. In step <highlight><bold>516</bold></highlight>, master computer <highlight><bold>102</bold></highlight> instructs the slave computer to terminate the slave process. After the slave process is terminated, master computer <highlight><bold>102</bold></highlight> removes the slave computer from the virtual supercomputer in step <highlight><bold>506</bold></highlight>. As discussed above, step <highlight><bold>506</bold></highlight> may also comprise termination of the PVM daemon on the slave computer. </paragraph>
<paragraph id="P-0075" lvl="0"><number>&lsqb;0075&rsqb;</number> Back in step <highlight><bold>514</bold></highlight>, if the slave application produces the expected results the process moves on the step <highlight><bold>518</bold></highlight>. In step <highlight><bold>518</bold></highlight>, master computer redistributes the load on the virtual supercomputer as needed to achieve maximum efficiency and utilization of available computing resources. As shown in <cross-reference target="DRAWINGS">FIG. 5</cross-reference>, step <highlight><bold>518</bold></highlight> is performed whenever a slave drops out of the virtual supercomputer and whenever a slave joins the virtual supercomputer. In step <highlight><bold>520</bold></highlight> the process is complete and the virtual supercomputer continues solving the problem presented to it. </paragraph>
<paragraph id="P-0076" lvl="0"><number>&lsqb;0076&rsqb;</number> If the event detected in step <highlight><bold>500</bold></highlight> is the availability of a new host computer, many of the same steps as described above can be performed as shown in <cross-reference target="DRAWINGS">FIG. 5</cross-reference>. First, in step <highlight><bold>501</bold></highlight> the new host is added to the virtual supercomputer. Again, this may comprise an instruction from master computer <highlight><bold>102</bold></highlight> directing the new host to download and start a PVM daemon. In step <highlight><bold>522</bold></highlight>, if the new host cannot be joined into the virtual supercomputer, if, for example, PVM-to-PVM communications are not successfully established between the master computer and the new host, the process moves on to step <highlight><bold>520</bold></highlight>. In step <highlight><bold>520</bold></highlight> the virtual supercomputer continues in its existing configuration. On the other hand, if PVM-to-PVM communications are successfully established, the process moves on to step <highlight><bold>507</bold></highlight> where the new host is instructed to start slave application as described above. </paragraph>
<paragraph id="P-0077" lvl="0"><number>&lsqb;0077&rsqb;</number> 2. Dynamic Reconfiguration&mdash;Host Computer </paragraph>
<paragraph id="P-0078" lvl="0"><number>&lsqb;0078&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 6</cross-reference> shows steps the that can be performed on a new host computer when the host becomes available to join the virtual supercomputer in one embodiment of the present invention. The steps in <cross-reference target="DRAWINGS">FIG. 6</cross-reference> are presented as they would be performed on the new host computer. Starting in step <highlight><bold>600</bold></highlight>, the host computer is powered on or otherwise joins the computer network supporting the virtual supercomputer. In step <highlight><bold>602</bold></highlight>, the new host computer reports its availability to join the virtual supercomputer. As described above, this step may be initiated by the new host computer or may be in response to a query from master computer <highlight><bold>102</bold></highlight>. </paragraph>
<paragraph id="P-0079" lvl="0"><number>&lsqb;0079&rsqb;</number> In step <highlight><bold>604</bold></highlight> the new host computer joins the virtual supercomputer. As described above, this step may be performed by the new host computer after it downloads a PVM daemon application from the master computer or from some other location on the network. Alternatively, the new host computer may run the PVM daemon software from a copy stored locally on its hard disk. After the PVM daemon is running, PVM-to-PVM communications are established between the new host computer and the virtual supercomputer&apos;s master computer. </paragraph>
<paragraph id="P-0080" lvl="0"><number>&lsqb;0080&rsqb;</number> In step <highlight><bold>606</bold></highlight> the new host computer starts a slave application process so that it can participate in solving the problem presented. The slave application process may be downloaded from the master computer or from some other location on the network, or may be stored locally on the new host computer. In step <highlight><bold>608</bold></highlight> the new host computer performs a series of self-tests on the slave application. The self-tests may comprise computing results for a known data set and comparing the computed results to the known correct results. As shown in <cross-reference target="DRAWINGS">FIG. 6</cross-reference>, in step <highlight><bold>610</bold></highlight> if the self-test is not successful the process moves on to step <highlight><bold>612</bold></highlight> where the failure is reported to the master computer. Next, in step <highlight><bold>614</bold></highlight> the PVM daemon on the new host computer is terminated. Finally, in step <highlight><bold>616</bold></highlight> the slave application process on the new host computer is terminated. </paragraph>
<paragraph id="P-0081" lvl="0"><number>&lsqb;0081&rsqb;</number> In step <highlight><bold>610</bold></highlight>, if the self-test was successful, the process moves on to step <highlight><bold>617</bold></highlight> where the new host computer sends performance statistics to the master computer. The master computer uses these performance statistics to estimate the processing capabilities of the new host computer. In alternative embodiments, the new host computer may report processor speed and percent utilization, memory capacity and other such statistics that may affect its processing capacity. Alternatively, the master computer can base its estimates on past history for the particular new host computer or on other information such as the type or location of the computer, or the identity of the new host computer&apos;s primary user, if one has been identified. </paragraph>
<paragraph id="P-0082" lvl="0"><number>&lsqb;0082&rsqb;</number> In step <highlight><bold>618</bold></highlight> the new host computer receives one or more data sets from the master computer. As described above, the data sets may comprise data and/or computing instructions, depending on the complexity of the slave application implemented in the new host computer. In step <highlight><bold>620</bold></highlight> the new host computer receives one or more task quantum from the master computer, depending on the new host computer&apos;s estimated processing capabilities. In step <highlight><bold>622</bold></highlight>, the new host computer determines whether or not the task quantum received includes an instruction to terminate processing. If so, the process moves on to step <highlight><bold>614</bold></highlight> to cleanup before ending processing. If there has been no termination task assigned, the new host computer performs the assigned tasks in step <highlight><bold>624</bold></highlight>. In step <highlight><bold>626</bold></highlight>, the new host computer collects performance statistics and in step <highlight><bold>628</bold></highlight> these statistics are periodically sent to the master computer for use in load balancing operations. The new host computer continues processing the tasks as assigned and performing periodical self-tests until a termination task is received or until the an error is detected causing the slave application to self-terminate. </paragraph>
<paragraph id="P-0083" lvl="0"><number>&lsqb;0083&rsqb;</number> The architecture and steps described above can be used to build and operate a parallel virtual supercomputer comprising a single master computer and one or more slave computers. In the embodiments thus described, the master computer assigns all tasks and coordinates dissemination of all data among slave computers. In an alternative embodiment, one ore more slave computers can be configured to act as sub-master computers, as shown in <cross-reference target="DRAWINGS">FIG. 7</cross-reference>. </paragraph>
<paragraph id="P-0084" lvl="0"><number>&lsqb;0084&rsqb;</number> Network <highlight><bold>700</bold></highlight> in <cross-reference target="DRAWINGS">FIG. 7</cross-reference> comprises a plurality of multipurpose computers in communication with one another. Master computer <highlight><bold>710</bold></highlight> is the master of the virtual supercomputer shown in <cross-reference target="DRAWINGS">FIG. 7</cross-reference>. Additionally, multipurpose computers <highlight><bold>720</bold></highlight> and <highlight><bold>730</bold></highlight> act as sub-master computers. In this embodiment, master computer <highlight><bold>710</bold></highlight> still oversees the formation and operation of the virtual supercomputer as described above. For example, master computer <highlight><bold>710</bold></highlight> performs load balancing and assigns data sets and tasks to slave computers in the virtual supercomputer. Sub-master computers <highlight><bold>720</bold></highlight> and <highlight><bold>730</bold></highlight> can also assign data sets and tasks to other slave computers. This embodiment can be advantageously implemented to solve problems for which incremental or partial solutions to sub-problems are needed to solve the overall computationally intensive problem. For example, consider branch-and-bound optimization problems. The problem is divided into two subproblems. If no solution is found by examining each subproblem, each subproblem is in turn divided into two subproblems. In this manner, the original problem lies at the root of a binary tree. The tree grows and shrinks as each subproblem is investigated (or fathomed) to obtain a solution or is subdivided into two more sub-subproblems if no solution is found in the current subproblem. In this embodiment, a slave assigned a subproblem becomes a sub-master when it is necessary to split its subproblem in two by assigning the resulting subproblems to other slaves of the virtual supercomputer. In this manner, the work required to search the tree can be spread across the slaves in the virtual supercomputer. </paragraph>
<paragraph id="P-0085" lvl="0"><number>&lsqb;0085&rsqb;</number> In another alternative embodiment, a single network of multipurpose computers may be used to build more than one virtual supercomputer. In this embodiment, multiple master computers are each configured to solve one or more particular problems. <cross-reference target="DRAWINGS">FIG. 8</cross-reference> shows a schematic diagram of a network supporting multiple virtual supercomputers according to this embodiment of the present invention. Network <highlight><bold>800</bold></highlight> is a network comprising two master computers <highlight><bold>801</bold></highlight> and <highlight><bold>802</bold></highlight> and a plurality of multipurpose computers <highlight><bold>811</bold></highlight>-<highlight><bold>839</bold></highlight>. Because each virtual supercomputer, by design, consumes maximum available computing resources on its member computers, a prioritized resource allocation system may be implemented. For example, a system administrator may assign a higher priority to master computer <highlight><bold>801</bold></highlight> than is assigned to master computer <highlight><bold>802</bold></highlight>. The system administrator may further dictate that whenever master computer <highlight><bold>801</bold></highlight> is up and running it has complete priority over master computer <highlight><bold>802</bold></highlight>. In this case, every available multipurpose computers will attempt to join master computer <highlight><bold>801</bold></highlight>&apos;s virtual supercomputer. Only if master computer <highlight><bold>801</bold></highlight> is not available will the multipurpose computers join master computer <highlight><bold>802</bold></highlight>&apos;s virtual supercomputer. </paragraph>
<paragraph id="P-0086" lvl="0"><number>&lsqb;0086&rsqb;</number> Alternatively, the system administrator could allocate a predefined percentage of available multipurpose computers to the virtual supercomputer of each master computer. That is, for example, master computer <highlight><bold>801</bold></highlight>&apos;s virtual supercomputer may be entitled to 65% of the multipurpose computers available at any given time, while master computer <highlight><bold>802</bold></highlight>&apos;s virtual supercomputer only gets the remaining 35% of the computers. As member computers drop out of the network or as potential member computers become available, the two master computers coordinate to ensure the system administrator&apos;s allocation scheme is satisfied. As known in the art, the communications between the two master computers could be accomplished via a network channel separate from PVM communications. Alternatively, modifications could be made to the PVM daemon to facilitate PVM-to-PVM communications between multiple parallel virtual machines. </paragraph>
<paragraph id="P-0087" lvl="0"><number>&lsqb;0087&rsqb;</number> Alternatively, each potential member computer on the network may be assigned to a primary master computer. In this embodiment, whenever the assigned primary master computer is up and running, the assigned multipurpose computer will first attempt to join that master&apos;s supercomputer. If the primary master is not processing any problems when the potential member computer becomes available, the multipurpose computer can attempt to join one of its secondary master computers to assist in solving a different problem. </paragraph>
<paragraph id="P-0088" lvl="0"><number>&lsqb;0088&rsqb;</number> In another embodiment of the present invention, multiple virtual supercomputers may coexist on a network wherein the virtual supercomputers share some or all of the same member computers. That is, in this embodiment, a single multipurpose computer may be a member of more than one virtual supercomputer. This embodiment could be implemented in situations wherein the problems being solved by the virtual supercomputer do not consume all of the available resources on member computers all of the time. That is, some problems such as the branch-and-bound optimization solver mentioned previously may involve intermittent periods of idle processor time while data is retrieved or results are being transferred among member computers. During this idle processing time, the member computer can work on problems assigned to it by its other master computer. </paragraph>
<paragraph id="P-0089" lvl="0"><number>&lsqb;0089&rsqb;</number> In another alternative embodiment, multiple virtual supercomputers can be implemented on the same network wherein the master computers of each virtual supercomputer can &ldquo;negotiate&rdquo; with other master computers on the network. In this embodiment, a virtual supercomputer may be established to solve a given problem within a pre-determined timeframe. Based on the computing resources it has available, the master computer may determine that its deadline cannot be met without additional resources. In this case, the master computer can request additional resources from other master computers on the network. The request may include information such as the requesting virtual supercomputer&apos;s priority, the number of additional processors required, the amount of time the processors will be used by the requester and any other information needed to determine whether or not the request should be satisfied by one of the virtual supercomputers on the network. </paragraph>
<paragraph id="P-0090" lvl="0"><number>&lsqb;0090&rsqb;</number> In another alternative embodiment, a central database of potential member computers is maintained on the network. Whenever a master computer boots up and starts building a new virtual supercomputer, the master consults the list and &ldquo;checks out&rdquo; a block of potential member computers. The master computer attempts to join each checked out member computer into its virtual supercomputer. If one of the member computers fails to properly join the virtual supercomputer, the master computer can report this information to the central database and check out a replacement computer. Similarly, when a new master computer boots up and starts building its own virtual supercomputer, it consults the central database to check out available member computers. Whenever a member computer drops out of its previously assigned virtual supercomputer, the central database is updated to reflect that computer&apos;s availability or its unavailability if the drop out was due to a failure. The central database can be periodically updated by polling the network for new computers available to join a virtual supercomputer. Alternatively, the central database can be updated whenever a change in a multipurpose workstation&apos;s status is detected. </paragraph>
<paragraph id="P-0091" lvl="7"><number>&lsqb;0091&rsqb;</number> An Example of a Specific Implementation of One Embodiment of the Present Invention </paragraph>
<paragraph id="P-0092" lvl="0"><number>&lsqb;0092&rsqb;</number> A specific implementation of one embodiment of the invention is described to illustrate how certain factors may be taken into consideration when configuring both the virtual supercomputer and master and slave applications for solving a particular computationally intensive problem. The computationally intensive problem to be solved in this example is known as the Multi-Indenture, Multi-Echelon Readiness-Based Sparing (MIMERBS) system for solving large, non-linear integer optimization problems that arise in determining the retail and wholesale sparing policies that support the aircraft operating from a deployed aircraft carrier. MIMERBS determines the minimum cost mix of spare parts that meets required levels of expected aircraft availability. The size (thousands of variables) and the non-linear relationship between spare parts and aircraft availability make this problem hard. MIMERBS is a non-linear integer optimization methodology developed to run across a virtual supercomputer of existing Windows NT computers networked together by an ordinary office LAN. A more detailed description of this specific embodiment can be found in Nickel, R. H., Mikolic-Torreira, I., and Tolle, J. W., 2000, Implementing a Large Non-Linear Integer Optimization on a Distributed Collection of Office Computers, in <highlight><italic>Proceedings, </italic></highlight>2000 ASME International Mechanical Engineering Congress &amp; Exposition, which is herein incorporated by reference in its entirety. </paragraph>
<paragraph id="P-0093" lvl="0"><number>&lsqb;0093&rsqb;</number> This example describes the configuration of the virtual supercomputer and how the MIMERBS problem was parallelized to work efficiently in view of the high communications costs of this particular embodiment of a virtual supercomputer. Additionally, this example describes how the MIMERBS software was made highly fault-tolerant and dynamically configurable to accommodate handling the loss of individual computers, automatic on-the-fly addition of new computers, and dynamic load-balancing. Experience with this specific embodiment has shows that performance of several gigaFLOPS is possible with just a few dozen ordinary computers on an office LAN. </paragraph>
<paragraph id="P-0094" lvl="7"><number>&lsqb;0094&rsqb;</number> Introduction to MIMERBS </paragraph>
<paragraph id="P-0095" lvl="0"><number>&lsqb;0095&rsqb;</number> As noted above, MIMERBS is used for solving large, non-linear integer optimization problems that arise in determining the mix of spare parts that an aircraft carrier should carry to keep its aircraft available to fly missions. Because MIMERBS is a faithful mathematical model of real-world sparing and repair processes, the model presents a computationally intensive problem. The computational demands are so great that MIMERBS requires the power of a supercomputer to generate optimal sparing policies in a reasonable amount of time. In lieu of an actual supercomputer system, a virtual supercomputer constructed out of a collection of ordinary office computers according to the present invention was used to do the MIMERBS computations in a timely manner </paragraph>
<paragraph id="P-0096" lvl="7"><number>&lsqb;0096&rsqb;</number> Establishing A Virtual Supercomputer for this Specific Implementation </paragraph>
<paragraph id="P-0097" lvl="0"><number>&lsqb;0097&rsqb;</number> As previously described, any suitable software for establishing a virtual supercomputer may be used by the present invention. In this specific implementation the Parallel Virtual Machine (PVM) software package developed by Oak Ridge National Laboratory was used to establish a virtual supercomputer. While PVM was originally designed to work in a UNIX environment, ports of PVM to a Windows environment are available. For this example, a new port of the software was implemented to overcome problems experienced with existing ports. Two primary problems with existing ports were: (1) that they rely on the global variable ERRNO to determine error states even though it is not set consistently by functions in the Windows API; and, (2) their conversion to using registry entries instead of environment variables is incomplete or inconsistent. The ported version of the software used in this example corrected both of these problems and resulted in a highly stable implementation. In this specific implementation the virtual supercomputer has been routinely up and running for weeks at a time, even when participating computers are repeatedly dropped and added, and a series of application runs are made on the virtual supercomputer. </paragraph>
<paragraph id="P-0098" lvl="0"><number>&lsqb;0098&rsqb;</number> In this specific implementation, computers using the Windows NT v. 4.0 with Service Pack <highlight><bold>6</bold></highlight>A loaded were configured to participate in the virtual supercomputer as follows: </paragraph>
<paragraph id="P-0099" lvl="2"><number>&lsqb;0099&rsqb;</number> 1. One computer is configured as a Windows NT server. This computer contains all PVM and application executables. The directories containing these executables are published as shared folders that can be accessed by other participating computers, i.e., member computers. </paragraph>
<paragraph id="P-0100" lvl="2"><number>&lsqb;0100&rsqb;</number> 2. A separate shared directory is created on the server corresponding to each computer that may participate in the virtual supercomputer. These working directories are where each participating computer will store its corresponding PVM-related files. </paragraph>
<paragraph id="P-0101" lvl="2"><number>&lsqb;0101&rsqb;</number> 3. The other participating Windows NT computers are configured with the standard PVM-related registry entries, except that the environment variable for the PVM root directory, PVM_ROOT, is set to the shared PVM root directory on the server and the environment variable for the temporary directory, PVM_TMP, is set to a shared directory on the server that corresponds to this particular machine. </paragraph>
<paragraph id="P-0102" lvl="2"><number>&lsqb;0102&rsqb;</number> 4. No PVM or application software is installed on the other computers. All participating computers access the executables located on the server. </paragraph>
<paragraph id="P-0103" lvl="2"><number>&lsqb;0103&rsqb;</number> 5. PVM computer-to-computer logins were supported via a licensed Windows NT implementation of a remote shell software, rsh, available from Fischer, M., 1999, RSHD Services for Microsoft WIN32, Web site, http://www.markus-fischer.de/getservice.htm. This software was installed on all the computers used in this specific implementation of the virtual supercomputer of the present invention. Additionally, each computer was configured with a dedicated and consistent usemame and password to support PVM computer-to-computer logins. </paragraph>
<paragraph id="P-0104" lvl="0"><number>&lsqb;0104&rsqb;</number> The participating computers are connected by an ordinary office LAN (a mixture of 10BaseT and 100BaseT). Both the LAN and most computers are used primarily for ordinary office work (email, file-sharing, web-access, word-processing, etc.). Participation in virtual computing is a secondary task. This configuration has several advantages. First, there is no need to distribute executables or synchronize versions across computers because all executables reside on a single computer. Second, elimination of orphaned PVM-related files (necessary for a computer to join the virtual supercomputer) is easy because they all reside on one computer. Third, debugging is simplified because error logs-which by default appear either in the corresponding PVM-related files or in files in the same directory as the PVM-related files-are all in located on the same computer that is initiating the computations and performing the debugging. </paragraph>
<paragraph id="P-0105" lvl="0"><number>&lsqb;0105&rsqb;</number> In this example, <highlight><bold>45</bold></highlight> computers were configured to participate in the virtual supercomputer, but many of these machines are not available during business hours due to other processing requirements resulting in numerous computers not being connected to the LAN. Accordingly, the virtual supercomputer in this example generally comprised 20 to 30 computers during business hours, with the number rising to 40 to 45 computers at night and during weekends (and then dropping again on weekday mornings). As would be apparent to one of ordinary skill in the art, a virtual supercomputer built on office computers connected by an ordinary LAN has communications delays that constrain the algorithms that can be efficiently implemented. Such constraints would be even more prominent if the virtual supercomputer is operated on a slower network such as for example, the Internet. In this example, using a combination of 10BaseT or 100BaseT Ethernet, the virtual supercomputer experienced point-to-point communications rates of 4.5 to 6.5 bytes/&mgr;sec, with a minimum communication time of about 15 msec for small messages. </paragraph>
<paragraph id="P-0106" lvl="0"><number>&lsqb;0106&rsqb;</number> Many common parallel processing techniques (e.g., decomposing matrix multiplication into concurrent inner products) could not be efficiently utilized given these communications delays. Accordingly, the virtual supercomputer in this specific implementation is not suitable for applications that require extensive and rapid communications between processors (e.g., finite-element solutions of systems of partial differential equations). Efficient use of this implementation of the virtual supercomputer was accomplished by decomposing the problem to be solved so that the duration of compute tasks assigned to processors was large relative to the communications time the tasks require. A wide variety of problems lend themselves to such decomposition, including, e.g., Monte-Carlo simulations, network problems, and divide-and-conquer algorithms. </paragraph>
<paragraph id="P-0107" lvl="7"><number>&lsqb;0107&rsqb;</number> Implementing The MIMERBS Algorithm </paragraph>
<paragraph id="P-0108" lvl="0"><number>&lsqb;0108&rsqb;</number> The MIMERBS optimization algorithm is an interior point algorithm. Our implementation of the interior point algorithm consists of repeated iterations of the following three steps: (1) a centering step that moves to the center of the feasible region, (2) an objective step that moves toward the continuous otpimal solution, and (3) a rounding step produces a candidate integer optimal solution. The first and third steps require intense numerical computations. We perform these two steps using an asynchronous parallel pattern search (APPS) algorithm. Because APPS is asynchronous and the messages required to support it are relatively short, it is very well suited for solving via a virtual supercomputer according to the present invention. However, significant changes to the original APPS algorithm were implemented to achieve reasonable performance in this example. </paragraph>
<paragraph id="P-0109" lvl="0"><number>&lsqb;0109&rsqb;</number> The original APPS algorithm, described in Hough, P. D., Kolda, T. G., and Torczon, V. J., 2000<highlight><italic>, Asynchronous Parallel Search for Nonlinear Optimization</italic></highlight>, SAND 2000-8213, Sandia National Laboratory, can be described as follows: </paragraph>
<paragraph id="P-0110" lvl="0"><number>&lsqb;0110&rsqb;</number> APPS Algorithm 1. </paragraph>
<paragraph id="P-0111" lvl="0"><number>&lsqb;0111&rsqb;</number> The goal is to minimize f(x) where x is contained in R<highlight><superscript>n</superscript></highlight>. Each processor is assigned a search direction {right arrow over (d)} and maintains a current best point x<highlight><subscript>best</subscript></highlight>, a corresponding best value f<highlight><subscript>best</subscript></highlight>&equals;f(x<highlight><subscript>best</subscript></highlight>) and a current step size &Dgr;. Each processor performs the following steps: </paragraph>
<paragraph id="P-0112" lvl="2"><number>&lsqb;0112&rsqb;</number> 1. Check for potentially better points reported by another processor; if a better point is received, move there. Specifically, consider each incoming triplet &lcub;x<highlight><subscript>in</subscript></highlight>, f<highlight><subscript>in</subscript></highlight>, &Dgr;<highlight><subscript>in</subscript></highlight>&rcub; received from another processor. If f<highlight><subscript>in</subscript></highlight>&lt;f<highlight><subscript>best </subscript></highlight>then update &lcub;x<highlight><subscript>best</subscript></highlight>, f<highlight><subscript>best</subscript></highlight>, &Dgr;&rcub;&larr;&lcub;x<highlight><subscript>fin</subscript></highlight>, f<highlight><subscript>in</subscript></highlight>, &Dgr;<highlight><subscript>in</subscript></highlight>&rcub;. </paragraph>
<paragraph id="P-0113" lvl="2"><number>&lsqb;0113&rsqb;</number> 2. Take a step of the current size in the assigned direction. Specifically, compute x<highlight><subscript>trial</subscript></highlight>&larr;x<highlight><subscript>best</subscript></highlight>&plus;{right arrow over (d)} and evaluate f<highlight><subscript>trial</subscript></highlight>&equals;f(x<highlight><subscript>trial</subscript></highlight>). </paragraph>
<paragraph id="P-0114" lvl="2"><number>&lsqb;0114&rsqb;</number> 3. If the trial point is better, move there; otherwise reduce the step size. Specifically, if f<highlight><subscript>trial</subscript></highlight>&lt;f<highlight><subscript>best </subscript></highlight>then update &lcub;x<highlight><subscript>best</subscript></highlight>, f<highlight><subscript>best</subscript></highlight>&rcub;&larr;&lcub;x<highlight><subscript>trial</subscript></highlight>, f<highlight><subscript>trial</subscript></highlight>&rcub; and broadcast the new minimum triplet &lcub;x<highlight><subscript>best</subscript></highlight>, f<highlight><subscript>best</subscript></highlight>, &Dgr;&rcub; to all other processors. Otherwise &Dgr;&larr;&frac12;&Dgr;. </paragraph>
<paragraph id="P-0115" lvl="2"><number>&lsqb;0115&rsqb;</number> 4. If the step size isn&apos;t too small, repeat the process; otherwise report that the currently assigned search has been completed. Specifically, if &Dgr;&gt;&Dgr;<highlight><subscript>stop</subscript></highlight>, goto Step 1; else report local completion at &lcub;x<highlight><subscript>best</subscript></highlight>, f<highlight><subscript>best</subscript></highlight>&rcub;. </paragraph>
<paragraph id="P-0116" lvl="2"><number>&lsqb;0116&rsqb;</number> 5. Wait until either (a) all processors have locally completed at this point or (b) a better point is received from another processor. In case (a), halt and exit. In case (b), goto Step 1. </paragraph>
<paragraph id="P-0117" lvl="0"><number>&lsqb;0117&rsqb;</number> The set of all search directions must form a positive spanning set for R<highlight><superscript>n</superscript></highlight>. This requires at least n&plus;1 direction vectors, although it is common to use 2n direction vectors in the form of the compass set &lcub;e<highlight><subscript>1 </subscript></highlight>. . . , e<highlight><subscript>n </subscript></highlight>&minus;e<highlight><subscript>1 </subscript></highlight>. . . , &minus;e<highlight><subscript>n</subscript></highlight>&rcub; where e<highlight><subscript>j </subscript></highlight>is the jth unit vector. </paragraph>
<paragraph id="P-0118" lvl="0"><number>&lsqb;0118&rsqb;</number> The first problem with Algorithm 1 is that it implicitly assumes either that there are at least as many processors as search directions, or that many concurrent but independent search processes (each searching a single direction) run on each processor. Because the MIMERBS application involves thousands of variables (n&gt;&gt;1000) implementation of this APPS algorithm would require either thousands of processor or hundreds (even thousands) of concurrent processes on each processor. Neither option proved practical for the present implementation. Accordingly, Algorithm 1 was modified so that each processor searches a set of directions, as opposed to a single direction. </paragraph>
<paragraph id="P-0119" lvl="0"><number>&lsqb;0119&rsqb;</number> The second problem with Algorithm 1 was that it performed very inefficiently on the virtual supercomputer primarily because it propagates many &ldquo;better&rdquo; solutions that are not significant improvements on the solution. On small applications (under 100 variables or so), this would not to be a significant problem. But on applications with hundreds or thousands of variables, execution time and computing behavior can be dominated by the transmission and computational response to an overwhelming number of &ldquo;better&rdquo; solutions whose improvement was often indistinguishable from accumulated rounding error. To solve this problem, a more stringent criteria for declaring a solution to be &ldquo;better&rdquo; was implemented. Using the new criteria, a trial solution not only has to be better, it had to be significantly better before it is propagated to other processors. This resulted in the following algorithm: </paragraph>
<paragraph id="P-0120" lvl="0"><number>&lsqb;0120&rsqb;</number> APPS Algorithm 2. </paragraph>
<paragraph id="P-0121" lvl="0"><number>&lsqb;0121&rsqb;</number> The goal is to minimize f(x) where x is contained in R<highlight><superscript>n</superscript></highlight>. Each processor is assigned a set of search directions &lcub;{right arrow over (d)}<highlight><subscript>1</subscript></highlight>, . . . ,{right arrow over (d)}<highlight><subscript>m</subscript></highlight>&rcub; and maintains a current best point x<highlight><subscript>best</subscript></highlight>, a corresponding best value f<highlight><subscript>best</subscript></highlight>&equals;f(x<highlight><subscript>best</subscript></highlight>), a current step size &Dgr;, and an index k representing the current direction {right arrow over (d)}<highlight><subscript>k</subscript></highlight>. Each host performs the following steps: </paragraph>
<paragraph id="P-0122" lvl="2"><number>&lsqb;0122&rsqb;</number> 1. Check for potentially better points reported by another host; if a better point is received, move there. Specifically, consider each incoming triplet &lcub;x<highlight><subscript>in</subscript></highlight>, f<highlight><subscript>in</subscript></highlight>, &Dgr;<highlight><subscript>in</subscript></highlight>&rcub; received from another host. If f<highlight><subscript>in</subscript></highlight>&lt;f<highlight><subscript>best</subscript></highlight>&minus;&egr;(1&plus;&verbar;f<highlight><subscript>best</subscript></highlight>&verbar;) then update &lcub;x<highlight><subscript>best</subscript></highlight>, f<highlight><subscript>best</subscript></highlight>, &Dgr;&rcub;&larr;&lcub;x<highlight><subscript>in</subscript></highlight>, f<highlight><subscript>in</subscript></highlight>, &Dgr;<highlight><subscript>in</subscript></highlight>&rcub;. </paragraph>
<paragraph id="P-0123" lvl="2"><number>&lsqb;0123&rsqb;</number> 2. Determine the next direction to look in. Specifically, the index corresponding to the next direction is k&larr;(k mod m)&plus;1. </paragraph>
<paragraph id="P-0124" lvl="2"><number>&lsqb;0124&rsqb;</number> 3. If all m directions have been searched without success, reduce the step size, setting &Dgr;&larr;&frac12;&Dgr;. If &Dgr;&gt;&Dgr;<highlight><subscript>stop</subscript></highlight>, report local completion at &lcub;x<highlight><subscript>best</subscript></highlight>, f<highlight><subscript>best</subscript></highlight>&rcub; and goto Step 7. </paragraph>
<paragraph id="P-0125" lvl="2"><number>&lsqb;0125&rsqb;</number> 4. Take a step of the current size in the current direction. Specifically, compute x<highlight><subscript>trial</subscript></highlight>&larr;x<highlight><subscript>best</subscript></highlight>&plus;{right arrow over (d)}<highlight><subscript>k </subscript></highlight>and evaluate f<highlight><subscript>trail</subscript></highlight>&equals;f(x<highlight><subscript>trial</subscript></highlight>). </paragraph>
<paragraph id="P-0126" lvl="2"><number>&lsqb;0126&rsqb;</number> 5. If the trial point is better, move there. Specifically, if f<highlight><subscript>trail</subscript></highlight>&lt;f<highlight><subscript>best</subscript></highlight>&minus;&egr;(1&plus;&verbar;f<highlight><subscript>best</subscript></highlight>&verbar;) then update &lcub;x<highlight><subscript>best</subscript></highlight>, f<highlight><subscript>best</subscript></highlight>&rcub;&larr;&lcub;x<highlight><subscript>trial</subscript></highlight>, f<highlight><subscript>trial</subscript></highlight>&rcub; and broadcast the new minimum triplet &lcub;x<highlight><subscript>best</subscript></highlight>, f<highlight><subscript>best</subscript></highlight>, &Dgr;&rcub; to all other hosts. </paragraph>
<paragraph id="P-0127" lvl="2"><number>&lsqb;0127&rsqb;</number> 6. Goto Step 1 to repeat the process. </paragraph>
<paragraph id="P-0128" lvl="2"><number>&lsqb;0128&rsqb;</number> 7. Wait until either (a) all hosts have locally completed at this point or (b) a better point is received from another processor. In case (a), halt and exit. In case (b), goto Step 1. </paragraph>
<paragraph id="P-0129" lvl="0"><number>&lsqb;0129&rsqb;</number> Algorithm 2 is used in two places in MIMERBS&apos;s interior point algorithm: in the centering step and in the rounding step. Its use in the centering step is straightforward. To use Algorithm 2 in the rounding step, a rounding technique, described in Nickel, R. H., Goodwyn, S. C., Nunn, W., Tolle, J. W., and Mikolic-Torreira, I., 1999<highlight><italic>, A Multi</italic></highlight>-<highlight><italic>Indenture, Multi</italic></highlight>-<highlight><italic>Echelon Readiness</italic></highlight>-<highlight><italic>Based</italic></highlight>-<highlight><italic>Sparing </italic></highlight>(<highlight><italic>MIMERBS</italic></highlight>) <highlight><italic>Model</italic></highlight>, Research Memorandum 99-19, Center for Naval Analyses, is used to obtain an integer solution from the result of the objective step for each iteration of the interior point algorithm. This integer solution is then used as the starting point for Algorithm 2, and &Dgr;<highlight><subscript>initial </subscript></highlight>and &Dgr;<highlight><subscript>stop </subscript></highlight>are selected so that the algorithm always takes integer steps and thus remain on an integer lattice (&Dgr;<highlight><subscript>initial</subscript></highlight>&equals;4 and &Dgr;<highlight><subscript>stop</subscript></highlight>&equals;1 work well). In all cases, the current example uses the compass set for direction vectors and the objective function is set to the appropriate combination of cost and penalty functions. </paragraph>
<paragraph id="P-0130" lvl="7"><number>&lsqb;0130&rsqb;</number> Implementation Architecture </paragraph>
<paragraph id="P-0131" lvl="0"><number>&lsqb;0131&rsqb;</number> In the present example, MIMERBS was implemented using a master-slave architecture. This approach provides a clear division of responsibility between components that are executing in parallel. It also allows a very clean programming model that has a simple slave focused almost exclusively on actually carrying out APPS Algorithm 2 and a master that orchestrates the overall execution of the algorithm. This section describes in more detail exactly what the master and slaves do and how they coordinate their operations. The slave is the simpler of the two; its fundamental job is to execute APPS Algorithm 2 when directed by the master using the set of search directions assigned by the master. Although this is its primary task, the slave must also be able to respond to various demands from the master. Specifically, slaves: </paragraph>
<paragraph id="P-0132" lvl="2"><number>&lsqb;0132&rsqb;</number> 1. Load data and setup information provided by the master. </paragraph>
<paragraph id="P-0133" lvl="2"><number>&lsqb;0133&rsqb;</number> 2. Accept search directions as assigned by the master (including changes while APPS is in progress). </paragraph>
<paragraph id="P-0134" lvl="2"><number>&lsqb;0134&rsqb;</number> 3. Start APPS using the initial values and objective function specified by the master. </paragraph>
<paragraph id="P-0135" lvl="2"><number>&lsqb;0135&rsqb;</number> 4. Report new &ldquo;best&rdquo; solutions to the master. </paragraph>
<paragraph id="P-0136" lvl="2"><number>&lsqb;0136&rsqb;</number> 5. Receive new &ldquo;best&rdquo; solutions from the master. </paragraph>
<paragraph id="P-0137" lvl="2"><number>&lsqb;0137&rsqb;</number> 6. Report local search completion to the master. </paragraph>
<paragraph id="P-0138" lvl="2"><number>&lsqb;0138&rsqb;</number> 7. Terminate APPS at the direction of the master. </paragraph>
<paragraph id="P-0139" lvl="2"><number>&lsqb;0139&rsqb;</number> 8. Report performance statistics to the master. </paragraph>
<paragraph id="P-0140" lvl="2"><number>&lsqb;0140&rsqb;</number> 9. Report problems of any kind to the master. </paragraph>
<paragraph id="P-0141" lvl="0"><number>&lsqb;0141&rsqb;</number> The setup information sent to each slave includes all the data necessary to compute any version of the objective function at any point. The objective function is specified by passing appropriate parameters and selectors to the slaves when initiating APPS. In this specific example, slaves never communicate directly with each other. Slaves only report to, and receive updates from, the master. This greatly simplifies slaves because they do not need to keep track of other slaves-in fact, slaves are not even aware that other slaves exist. As previously described, however, alternative embodiments of the present invention may include communications between and among slave computers. In the present specific embodiment, it is up to the master to broadcast solutions received from one slave to all other slaves. The master is more complex than the slave. It is responsible for performing data input and output, for directing the overall execution of the interior point algorithm, and for managing the slaves. To do this, the master performs the following functions: </paragraph>
<paragraph id="P-0142" lvl="2"><number>&lsqb;0142&rsqb;</number> 1. Reads the input data. </paragraph>
<paragraph id="P-0143" lvl="2"><number>&lsqb;0143&rsqb;</number> 2. Creates slaves on all hosts in the virtual supercomputer. </paragraph>
<paragraph id="P-0144" lvl="2"><number>&lsqb;0144&rsqb;</number> 3. Sets up the optimization. </paragraph>
<paragraph id="P-0145" lvl="2"><number>&lsqb;0145&rsqb;</number> 4. Propagates data and setup information to the slaves. </paragraph>
<paragraph id="P-0146" lvl="2"><number>&lsqb;0146&rsqb;</number> 5. Carries out the iterations of the interior point algorithm: </paragraph>
<paragraph id="P-0147" lvl="3"><number>&lsqb;0147&rsqb;</number> (a) centering step: initiate and manage APPS by the slaves; </paragraph>
<paragraph id="P-0148" lvl="3"><number>&lsqb;0148&rsqb;</number> (b) objective step: performed entirely by master; and </paragraph>
<paragraph id="P-0149" lvl="3"><number>&lsqb;0149&rsqb;</number> (c) rounding step: the master rounds the continuous solution to get an initial integer solution, and then uses that solution to initiate and manage an integer search using APPS by the slaves. </paragraph>
<paragraph id="P-0150" lvl="2"><number>&lsqb;0150&rsqb;</number> 6. Shuts down the slaves. </paragraph>
<paragraph id="P-0151" lvl="2"><number>&lsqb;0151&rsqb;</number> 7. Generates appropriate reports and other output data. </paragraph>
<paragraph id="P-0152" lvl="0"><number>&lsqb;0152&rsqb;</number> From an implementation point of view, the most complex task the master does in this embodiment is to &ldquo;initiate and manage APPS by the slaves.&rdquo; This task is central to the parallel implementation and consists of the following subtasks: 1. start APPS by sending initial values and appropriate choice of objective function to all slaves; 2. receive &ldquo;best&rdquo; solution reports from individual slaves and rebroadcast them to all slaves; 3. track each slave&apos;s search status; 4. collect performance statistics provided by the slaves and adjust load allocations as needed; 5. terminate the search when all slaves have completed; and 6. deal with problems in the slave and changes in the virtual supercomputer. As noted previously, the master rebroadcasts received solutions to all other slaves. Although this may appear to be inefficient, it actually allows the present implementation to significantly speed up the convergence of APPS. This is accomplished by processing incoming solution reports in large blocks and re-broadcasting the best solution in the block instead of individually re-broadcasting every incoming solution. Because incoming solutions accumulate in a message queue while the master performs housekeeping chores, the master can process a large block of incoming solution reports without incurring any wait states. The net result is a reduction in network traffic due to re-broadcasts by about two orders of magnitude, without any significant degradation in how quickly slaves receive solution updates. This modification reduces the number of function evaluations needed for APPS to converge by 15 to 30 percent. Passing all communications through the master also gives an efficient means of dealing with the well-known asynchronous stopping problem, which arises when local slave termination is not a permanent state. That is, even after a slave has completed its search locally, it may restart if it receives an update containing a better solution than the one at which the slave stopped. Simply polling the slaves&apos; status to see if all slaves are stopped is not sufficient because an update message may be in transit while the slaves are polled. Because all updates are broadcast through the master in the present implementation, all outgoing update messages can be tracked. Furthermore, slaves acknowledge every update message (they do so in blocks sent at local search termination to minimize the communications burden). This provides sufficient information to implement a reliable stopping rule: the master declares the search finished when all slaves have completed locally and all slaves have acknowledged all updates. </paragraph>
<paragraph id="P-0153" lvl="7"><number>&lsqb;0153&rsqb;</number> Fault Tolerance and Robustness </paragraph>
<paragraph id="P-0154" lvl="0"><number>&lsqb;0154&rsqb;</number> The virtual supercomputer implemented in this specific example is highly fault tolerant and allows individual hosts making up the virtual supercomputer to drop out (either due to failures or because their owners reboot them). Although the PVM software used to establish the virtual supercomputer allows the underlying set of host computers to change over time, it is up to the application running on the virtual supercomputer, in this case MIMERBS, to make sure that computations will proceed correctly when such changes occur. There are two issues related to robustness. The first is dynamically incorporating PVM-capable processors whenever they come on line. The second is dynamic load balancing in response to changing loads on processors. </paragraph>
<paragraph id="P-0155" lvl="0"><number>&lsqb;0155&rsqb;</number> The MIMERBS application implemented in this example has been designed to be extremely robust in its handling of faults that may occur during operations. The fundamental fault that MIMERBS must deal with is the loss of a slave. A slave can be lost for a variety of reasons: the processor that slave is running on has left the virtual supercomputer (e.g., rebooted or shutdown), the slave program itself has failed (e.g., runtime error or resource exhaustion), the PVM daemon on that processor has had a fatal runtime error, or the communications path to that slave has been broken. In this example, fault tolerance depends upon two functions performed by the master application: determining that a slave is &ldquo;lost&rdquo; and reacting to that situation. </paragraph>
<paragraph id="P-0156" lvl="0"><number>&lsqb;0156&rsqb;</number> The master determines that a slave is lost in several ways. First, PVM&apos;s pvm notify service may be used to receive notification (via PVM message) that a slave process has terminated or that a processor has left the virtual machine. This detects slaves that abort for any reason; processors that crash, reboot, or shutdown; remote PVM daemons that fail; and communications paths that break. Second, if a slave reports any runtime errors or potential problems (e.g., low memory or inconsistent internal state) to the master, the master will terminate the slave and treat it as lost. Finally, if the master doesn&apos;t receive any messages from a slave for too long, it will declare the slave lost and treat it as such (this detects communications paths that break and slaves that &ldquo;hang&rdquo;). The master reacts to the loss of a slave essentially by reassigning the workload of that slave to other slaves. To do this, the master keeps track of which search directions are currently assigned to each slave. When a slave is lost, the master first checks whether the processor on which that slave was running is still part of the virtual supercomputer. If so, the master attempts to start a new slave on that processor. If that succeeds, the old slave&apos;s search directions are reassigned to the new slave on that processor. If the lost slave&apos;s processor is no longer part of the virtual supercomputer or if the master cannot start a new slave on it, the master instead redistributes the lost slave&apos;s directions to the remaining slaves. This ensures that all directions are maintained in the active search set and allows APPS to proceed without interruption even when slaves are lost. The net result is that MIMERBS is not affected by faults of any slaves, of any remote PVM daemons, or of any remote processors. In this specific implementation, only three types of faults will cause MIMERBS to fail: </paragraph>
<paragraph id="P-0157" lvl="2"><number>&lsqb;0157&rsqb;</number> 1. loss of the master processor; </paragraph>
<paragraph id="P-0158" lvl="2"><number>&lsqb;0158&rsqb;</number> 2. fatal runtime errors in the PVM daemon running on the master computer; or </paragraph>
<paragraph id="P-0159" lvl="2"><number>&lsqb;0159&rsqb;</number> 3. fatal runtime errors in the MIMERBS master program. </paragraph>
<paragraph id="P-0160" lvl="0"><number>&lsqb;0160&rsqb;</number> In each of these three cases a manual restart will resume computations from the last completed iteration of the interior point algorithm. The first two failure modes are an unavoidable consequence of using PVM: the virtual supercomputer will collapse if the processor that initiated the formation of the virtual supercomputer fails or if the PVM daemon running on that processor (the &ldquo;master&rdquo; PVM daemon) has a runtime error. Using a different software application to establish the virtual supercomputer can prevent this situation if an even more robust virtual supercomputer is desired. The third failure mode, fatal runtime errors in the MIMERBS master program, could be avoided by taking the approach (Hough et al., 2000) used in their implementation of APPS Algorithm 1. In that implementation there is no master computer: every slave is capable of functioning as &ldquo;master&rdquo; if and when required. Alternatively, the risk of fatal failure on the master computer can be reduced by initiating the formation of the virtual supercomputer and running the master program from the same processor. Moreover, if this processor is a dedicated computer that is isolated from other users and has an uninterruptible power supply, the risk can be further minimized. </paragraph>
<paragraph id="P-0161" lvl="7"><number>&lsqb;0161&rsqb;</number> Adding Hosts Dynamically </paragraph>
<paragraph id="P-0162" lvl="0"><number>&lsqb;0162&rsqb;</number> As previously noted, although PVM allows a processor to join the virtual supercomputer at any time, it is up to the master application, in this case MIMERBS, to initiate computations on that new processor. The MIMERBS master program uses PVM&apos;s pvm notify service to receive notification (via PVM message) whenever a new processor is added to the virtual supercomputer. The master responds to this notification by: </paragraph>
<paragraph id="P-0163" lvl="2"><number>&lsqb;0163&rsqb;</number> 1. starting a slave on the new host; </paragraph>
<paragraph id="P-0164" lvl="2"><number>&lsqb;0164&rsqb;</number> 2. sending all the necessary setup data to that slave; </paragraph>
<paragraph id="P-0165" lvl="2"><number>&lsqb;0165&rsqb;</number> 3. conducting a computation test to verify that the new slave/host combination performs satisfactorily (if the new slave/host does not perform satisfactorily, the slave is terminated and the host dropped from the virtual supercomputer); </paragraph>
<paragraph id="P-0166" lvl="2"><number>&lsqb;0166&rsqb;</number> 4. redistributing search direction assignments so that the new slave has its corresponding share; and </paragraph>
<paragraph id="P-0167" lvl="2"><number>&lsqb;0167&rsqb;</number> 5. if an APPS search is in progress, also initiate APPS on the new slave by passing the appropriate parameters and the current best solution. </paragraph>
<paragraph id="P-0168" lvl="0"><number>&lsqb;0168&rsqb;</number> The master keeps track of what setup data is needed by maintaining an ordered list of references to all messages broadcast to slaves as part of the normal MIMERBS startup process. When a new slave is started, the master simply resends the contents of the list in a first-in, first-out order. </paragraph>
<paragraph id="P-0169" lvl="0"><number>&lsqb;0169&rsqb;</number> The above only addresses what happens once a new processor has actually joined the virtual supercomputer. The problem remains of detecting that a potential processor has come online and actually adding it to the virtual supercomputer. This feature is needed to automatically exploit the capabilities of whatever computers may become available in the course of a run. PVM does not provide this service directly, so the present implementation included a routine on the master computer that periodically checks to see if PVM-configured computers have come online and then adds any it finds to the virtual supercomputer. </paragraph>
<paragraph id="P-0170" lvl="7"><number>&lsqb;0170&rsqb;</number> Load Balancing </paragraph>
<paragraph id="P-0171" lvl="0"><number>&lsqb;0171&rsqb;</number> In addition to hosts joining and leaving the virtual supercomputer, the master application must adapt to ever changing computational loads on the processors that make up the virtual supercomputer. Because the present example uses ordinary office computers that are being used concurrently for daily operations, processor loading can change dramatically depending on what the user is doing. MIMERBS needs to respond to this change in load to ensure efficient computation. </paragraph>
<paragraph id="P-0172" lvl="0"><number>&lsqb;0172&rsqb;</number> Because APPS is asynchronous, the definition of a balanced load is not self-evident. Loosely speaking all search directions should be searched &ldquo;equally rapidly.&rdquo; Accordingly, in the present example, load balance is determined by the number of directions assigned to each processor. For example, if there are two processors in the virtual supercomputer, one twice as fast as the other, the faster processor is assigned twice as many directions as the slower one. In this example, a balanced load is established when, for all processors i, N<highlight><subscript>i</subscript></highlight>/R<highlight><subscript>i</subscript></highlight>&equals;constant where N<highlight><subscript>i </subscript></highlight>is the number of assigned search directions on the ith processor and R<highlight><subscript>i </subscript></highlight>is the rate at which the objective function is evaluated on the ith processor. </paragraph>
<paragraph id="P-0173" lvl="0"><number>&lsqb;0173&rsqb;</number> To support dynamic load balancing, all slaves record performance data and regularly send it to the master. The master uses this data to detect changes in function evaluation rate and adjusts the load distribution whenever it becomes too unbalanced. The load is also rebalanced whenever a slave is lost or added. The actual rebalancing is accomplished by shifting assigned search directions from one slave to another. </paragraph>
<paragraph id="P-0174" lvl="0"><number>&lsqb;0174&rsqb;</number> In addition to performing dynamic load balancing, the present specific implementation also uses the Window NT scheduling scheme to regulate the relative priority of slave tasks so that slave tasks yield to user tasks, especially foreground user tasks. Specifically, slaves run as background processes with Process Priority Class set to NORMAL_PRIORITY_CLASS and Thread Priority Level set to THREAD_PRIORITY_BELOW_NORMAL. This ensures that desktop computers participating in the virtual supercomputer are highly responsive to their users. This is an important part of making the virtual supercomputer nearly invisible to ordinary users. As would be apparent to one of ordinary skill in the art, similar process prioritization schemes can be implemented on computers using different operating systems, including for example, UNIX, Linux, and the like. </paragraph>
<paragraph id="P-0175" lvl="7"><number>&lsqb;0175&rsqb;</number> Other Alternative Embodiments </paragraph>
<paragraph id="P-0176" lvl="0"><number>&lsqb;0176&rsqb;</number> The present invention also relates to applications and uses for the system and method of the present invention. In the currently preferred embodiment, the present invention can be used behind the firewall of an organization that has a number of under-utilized personal computers or workstations. Since many office environments use computers with powerful processors for tasks such as word processing and e-mail, there is ordinarily a substantial amount of under-utilized computer capacity in most medium and large sized offices. Thus, using the present invention, it is possible to take advantage of this under-utilized capacity to create a virtual supercomputer at a cost that makes the supercomputing capacity affordable. With the availability of low-cost supercomputing, a wide variety of applications that heretofore had not been practical to solve become solvable. Applications of the present invention include: </paragraph>
<paragraph id="P-0177" lvl="7"><number>&lsqb;0177&rsqb;</number> General Applications </paragraph>
<paragraph id="P-0178" lvl="0"><number>&lsqb;0178&rsqb;</number> Financial Applications. </paragraph>
<paragraph id="P-0179" lvl="0"><number>&lsqb;0179&rsqb;</number> Smaller financial institutions may desire to optimize loan or stock portfolios using the tools of modern portfolio theory. These tools often require the power of a supercomputer. The virtual supercomputer of the present invention would provide an option for acquiring this capability without making the financial commitment that a supercomputer would require. </paragraph>
<paragraph id="P-0180" lvl="0"><number>&lsqb;0180&rsqb;</number> Stock and Inventory Optimizations. </paragraph>
<paragraph id="P-0181" lvl="0"><number>&lsqb;0181&rsqb;</number> Determining optimal stock levels on a by-item, by-store basis, together with the corresponding trans-shipment plan (what to move to where) on a daily basis is a key competitive advantage of large distributors such as Wal-Mart. Up to now, solving such a large problem quickly enough to make daily updates possible has required dedicated high-end computer hardware. This problem can be solved by our virtual supercomputer on existing networks of office computers, allowing many smaller companies to take advantage of these techniques without any additional investment in computer hardware. </paragraph>
<paragraph id="P-0182" lvl="0"><number>&lsqb;0182&rsqb;</number> Branch-And-Bound Problems. </paragraph>
<paragraph id="P-0183" lvl="0"><number>&lsqb;0183&rsqb;</number> Optimization problems that use a branch-and-bound approach, e.g., integer programming problems, to find an optimal solution could be structured to use the virtual supercomputer. </paragraph>
<paragraph id="P-0184" lvl="0"><number>&lsqb;0184&rsqb;</number> Simulation. </paragraph>
<paragraph id="P-0185" lvl="0"><number>&lsqb;0185&rsqb;</number> Large-scale simulations could be run on the virtual supercomputer. With n computers in the virtual supercomputer, total run time could be reduced by a factor of n. </paragraph>
<paragraph id="P-0186" lvl="0"><number>&lsqb;0186&rsqb;</number> Routing and Scheduling. </paragraph>
<paragraph id="P-0187" lvl="0"><number>&lsqb;0187&rsqb;</number> Network optimization problems such as vehicle routing and scheduling could be subdivided and solved on the virtual supercomputer. </paragraph>
<paragraph id="P-0188" lvl="0"><number>&lsqb;0188&rsqb;</number> Database Analyses. </paragraph>
<paragraph id="P-0189" lvl="0"><number>&lsqb;0189&rsqb;</number> Database searching or mining could be done across a network of workstations using the virtual supercomputer. Each workstation could be assigned a portion of the database to search or analyze. </paragraph>
<paragraph id="P-0190" lvl="0"><number>&lsqb;0190&rsqb;</number> Image Analyses. </paragraph>
<paragraph id="P-0191" lvl="0"><number>&lsqb;0191&rsqb;</number> Suppose you need to find a tank (or other target), and you want to automatically search existing imagery. This takes much too long for a single personal computer, but it can be easily distributed across a network of computers. </paragraph>
<paragraph id="P-0192" lvl="7"><number>&lsqb;0192&rsqb;</number> Military Applications </paragraph>
<paragraph id="P-0193" lvl="0"><number>&lsqb;0193&rsqb;</number> Sonar Predictions. </paragraph>
<paragraph id="P-0194" lvl="0"><number>&lsqb;0194&rsqb;</number> In shallow water, you need to make 360-degree sonar predictions. These are generally done on one radial at a time. On an ordinary personal computer each radial takes about 15 seconds, so doing a full 360 degree analysis takes 90 minutes. The same computation running on a virtual supercomputer, comprising, e.g., 50 personal computers can be accomplished in about 2 minutes. Moreover, the individual results from the computers can be used to more readily formulate the sonar picture because each node can communicate with the other to share results. </paragraph>
<paragraph id="P-0195" lvl="0"><number>&lsqb;0195&rsqb;</number> Mission Planning. </paragraph>
<paragraph id="P-0196" lvl="0"><number>&lsqb;0196&rsqb;</number> Military mission planning systems, e.g., Tactical Aircraft Mission Planning System (TAMPS), are software tools used to plan air strikes and other operations. However most mission planning systems do not provide estimates of mission success or attrition because such estimates require extensive Monte Carlo modeling that is too time consuming (several hours) on the computer systems available at an operational level. As noted above, such simulation exercises can be easily accommodated on the cluster supercomputer of the present invention using computer systems readily available to the mission planners. </paragraph>
<paragraph id="P-0197" lvl="0"><number>&lsqb;0197&rsqb;</number> Finally, although the system is described in the currently preferred context of a secure (behind the firewall) office environment, the invention can be used with computers connected over the Internet or other such public network. In one embodiment, a process on the master computer for the cluster supercomputer logs into the slave computers to initiate the computing tasks on the slaves. Accordingly, a secure environment is preferable solely to alleviate concerns related to outside entities from logging into the computers in the network. A cluster supercomputer according to the present invention may operate on unsecured networks provided the cluster members are configured to allow access to the master computer. Also, from a practical standpoint, cluster supercomputers over the Internet would need higher speed connections between the linked computers to maximize the present invention&apos;s capabilities to share data between the nodes on a near real-time basis for use in subsequent calculations. </paragraph>
<paragraph id="P-0198" lvl="0"><number>&lsqb;0198&rsqb;</number> Further, in describing representative embodiments of the present invention, the specification may have presented the method and/or process of the present invention as a particular sequence of steps. However, to the extent that the method or process does not rely on the particular order of steps set forth herein, the method or process should not be limited to the particular sequence of steps described. As one of ordinary skill in the art would appreciate, other sequences of steps may be possible. Therefore, the particular order of the steps set forth in the specification should not be construed as limitations on the claims. In addition, the claims directed to the method and/or process of the present invention should not be limited to the performance of their steps in the order written, and one skilled in the art can readily appreciate that the sequences may be varied and still remain within the spirit and scope of the present invention. </paragraph>
<paragraph id="P-0199" lvl="0"><number>&lsqb;0199&rsqb;</number> The foregoing disclosure of the preferred embodiments of the present invention has been presented for purposes of illustration and description. It is not intended to be exhaustive or to limit the invention to the precise forms disclosed. Many variations and modifications of the embodiments described herein will be obvious to one of ordinary skill in the art in light of the above disclosure. The scope of the invention is to be defined only by the claims appended hereto, and by their equivalents. </paragraph>
</section>
</detailed-description>
</subdoc-description>
<subdoc-claims>
<heading lvl="1">What is claimed is: </heading>
<claim id="CLM-00001">
<claim-text><highlight><bold>1</bold></highlight>. A method for solving a computationally intensive problem using a plurality of multipurpose computer workstations, said method comprising the steps of: 
<claim-text>(a) building a virtual supercomputer comprising a master computer and at least one slave computer, wherein the at least one slave computer is selected from the plurality of multipurpose computer workstations; </claim-text>
<claim-text>(b) starting a master application on the master computer; </claim-text>
<claim-text>(c) starting a slave application on the at least one slave computer; </claim-text>
<claim-text>(d) dividing the computationally intensive problem into a plurality of task quantum; </claim-text>
<claim-text>(e) assigning to the at least one slave computer at least one task quanta selected from the plurality of task quantum; </claim-text>
<claim-text>(f) completing on the at least one slave computer the at least one task quanta; </claim-text>
<claim-text>(g) receiving on the master computer a result provided by the at least one slave computer; and </claim-text>
<claim-text>(h) repeating steps (e) through (g) until the computationally intensive task is solved. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00002">
<claim-text><highlight><bold>2</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein the master computer and the at least one slave computer are the same computer. </claim-text>
</claim>
<claim id="CLM-00003">
<claim-text><highlight><bold>3</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, further comprising the step of balancing a load on the virtual supercomputer. </claim-text>
</claim>
<claim id="CLM-00004">
<claim-text><highlight><bold>4</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, further comprising the steps of: 
<claim-text>(a) detecting an event affecting a size of the virtual supercomputer; and </claim-text>
<claim-text>(b) balancing a load on the virtual supercomputer in response to the event. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00005">
<claim-text><highlight><bold>5</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00004">claim 4</dependent-claim-reference>, wherein the event comprises an availability of a second slave computer selected from the plurality of computer workstations. </claim-text>
</claim>
<claim id="CLM-00006">
<claim-text><highlight><bold>6</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00005">claim 5</dependent-claim-reference>, further comprising the step adding the second slave computer to the virtual supercomputer, and wherein the step of balancing a load comprises the steps of: 
<claim-text>(a) retrieving the at least one task quanta assigned to the at least one slave computer; </claim-text>
<claim-text>(b) determining a processing capability for the at least one slave computer and the second slave computer; and </claim-text>
<claim-text>(c) distributing task quantum to the at least one slave computer and the second slave computer according to the determined processing capability. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00007">
<claim-text><highlight><bold>7</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00004">claim 4</dependent-claim-reference>, wherein the event comprises a failure of the slave application on the at least one slave computer. </claim-text>
</claim>
<claim id="CLM-00008">
<claim-text><highlight><bold>8</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00007">claim 7</dependent-claim-reference>, further comprising the steps of: 
<claim-text>(a) restarting the slave application on the at least one slave computer; and </claim-text>
<claim-text>(b) testing the restarted slave application on the at least one slave computer. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00009">
<claim-text><highlight><bold>9</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00004">claim 4</dependent-claim-reference>, wherein the event comprises a loss of communications between the master computer and the at least one slave computer. </claim-text>
</claim>
<claim id="CLM-00010">
<claim-text><highlight><bold>10</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00009">claim 9</dependent-claim-reference>, further comprising the steps of: 
<claim-text>(a) removing the at least one slave computer from the virtual supercomputer; </claim-text>
<claim-text>(b) determining which task quantum selected from the plurality of task quantum have not been completed; </claim-text>
<claim-text>(c) determining a processing capability for any remaining slave computers in the virtual supercomputer; and </claim-text>
<claim-text>(d) distributing at least one uncompleted task quantum to the remaining slave computers according to the determined processing capability. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00011">
<claim-text><highlight><bold>11</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, further comprising the steps of: 
<claim-text>(a) determining a processing capability for the at least one slave computer; and </claim-text>
<claim-text>(b) distributing the plurality of task quantum to the at least one slave computer according to the determined processing capability. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00012">
<claim-text><highlight><bold>12</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, further comprising the step of collecting on the master computer a plurality of final performance statistics from the at least one slave computer. </claim-text>
</claim>
<claim id="CLM-00013">
<claim-text><highlight><bold>13</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, further comprising the step of tearing down the virtual supercomputer. </claim-text>
</claim>
<claim id="CLM-00014">
<claim-text><highlight><bold>14</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00011">claim 13</dependent-claim-reference>, wherein the step of tearing down the virtual supercomputer comprises the steps of: 
<claim-text>(a) terminating the slave application on the at least one slave computer; </claim-text>
<claim-text>(b) terminating the master application on the master computer; and </claim-text>
<claim-text>(c) terminating the virtual supercomputer daemon application on the at least one slave computer and on the master computer. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00015">
<claim-text><highlight><bold>15</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, further comprising the step of providing a result to the computationally intensive problem. </claim-text>
</claim>
<claim id="CLM-00016">
<claim-text><highlight><bold>16</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein the step of building a virtual supercomputer comprises the steps of: 
<claim-text>(a) starting a first virtual supercomputer daemon application on the master computer; </claim-text>
<claim-text>(b) identifying a potential slave computer selected from the plurality of multipurpose workstations; </claim-text>
<claim-text>(c) downloading a second virtual supercomputer daemon application from the master computer to the potential slave computer; </claim-text>
<claim-text>(d) starting the second virtual supercomputer daemon application on the potential slave computer; and </claim-text>
<claim-text>(e) establishing a communications session between the first virtual supercomputer daemon and the second virtual supercomputer daemon. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00017">
<claim-text><highlight><bold>17</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00011">claim 15</dependent-claim-reference>, further comprising the steps of: 
<claim-text>(a) downloading a slave application from the master computer to the potential slave computer; </claim-text>
<claim-text>(b) starting the slave application on the potential slave computer; </claim-text>
<claim-text>(c) testing the slave application on the potential slave computer; and </claim-text>
<claim-text>(d) making the potential slave computer the at least one slave computer in the virtual supercomputer if the testing of the slave application is successful. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00018">
<claim-text><highlight><bold>18</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00011">claim 16</dependent-claim-reference>, further comprising the step determining a performance capability of the at least one slave computer. </claim-text>
</claim>
<claim id="CLM-00019">
<claim-text><highlight><bold>19</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00011">claim 16</dependent-claim-reference>, further comprising the step of repeating steps 16(b) through 16(e) until there are no more potential slave computers available among the plurality of multipurpose workstations. </claim-text>
</claim>
<claim id="CLM-00020">
<claim-text><highlight><bold>20</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00011">claim 19</dependent-claim-reference>, further comprising the steps of: 
<claim-text>(a) downloading a slave application from the master computer to the potential slave computer; </claim-text>
<claim-text>(b) starting the slave application on the potential slave computer; </claim-text>
<claim-text>(c) testing the slave application on the potential slave computer; </claim-text>
<claim-text>(d) making the potential slave computer the at least one slave computer in the virtual supercomputer if the testing of the slave application is successful; </claim-text>
<claim-text>(e) calculating a performance capability of the at least one slave computer; and </claim-text>
<claim-text>(f) repeating steps (a) through (e) until there are no more potential slave computers available among the plurality of multipurpose workstations. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00021">
<claim-text><highlight><bold>21</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein the computationally intensive problem comprises optimization of a financial portfolio. </claim-text>
</claim>
<claim id="CLM-00022">
<claim-text><highlight><bold>22</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein the computationally intensive problem comprises optimization of supply chain logistics. </claim-text>
</claim>
<claim id="CLM-00023">
<claim-text><highlight><bold>23</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00022">claim 22</dependent-claim-reference>, further comprises determining optimal stock levels on a by-item, by-store basis, together with a corresponding trans-shipment plan on a daily basis. </claim-text>
</claim>
<claim id="CLM-00024">
<claim-text><highlight><bold>24</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein the computationally intensive problem is used in conjunction with a branch-and-bound approach to optimization. </claim-text>
</claim>
<claim id="CLM-00025">
<claim-text><highlight><bold>25</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein the computationally intensive problem is used in connection with large-scale simulations to reduce total run time. </claim-text>
</claim>
<claim id="CLM-00026">
<claim-text><highlight><bold>26</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein the computationally intensive problem comprises a network optimization problem. </claim-text>
</claim>
<claim id="CLM-00027">
<claim-text><highlight><bold>27</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00022">claim 26</dependent-claim-reference>, wherein the network optimization problem comprises a vehicle routing and scheduling system. </claim-text>
</claim>
<claim id="CLM-00028">
<claim-text><highlight><bold>28</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein the computationally intensive problem comprises database searching across a network of workstations. </claim-text>
</claim>
<claim id="CLM-00029">
<claim-text><highlight><bold>29</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein the computationally intensive problem comprises database mining across a network of workstations. </claim-text>
</claim>
<claim id="CLM-00030">
<claim-text><highlight><bold>30</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein the computationally intensive problem comprises an image analysis program. </claim-text>
</claim>
<claim id="CLM-00031">
<claim-text><highlight><bold>31</bold></highlight>. A method of building a virtual supercomputer comprising a master computer and at least one slave computer selected from a plurality of multipurpose workstations, said method comprising the steps of: 
<claim-text>(a) starting a first virtual supercomputer daemon on the master computer; </claim-text>
<claim-text>(b) identifying a potential slave computer selected from the plurality of multipurpose workstations; </claim-text>
<claim-text>(c) starting a second virtual supercomputer daemon on the potential slave computer; </claim-text>
<claim-text>(d) establishing a communications session between the first virtual supercomputer daemon and the second virtual supercomputer daemon; </claim-text>
<claim-text>(e) starting the slave application on the potential slave computer; </claim-text>
<claim-text>(f) testing the slave application on the potential slave computer; and </claim-text>
<claim-text>(g) making the potential slave computer the at least one slave computer in the virtual supercomputer if the testing of the slave application is successful. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00032">
<claim-text><highlight><bold>32</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00033">claim 31</dependent-claim-reference>, further comprising the step of calculating a performance capability of the at least one slave computer. </claim-text>
</claim>
<claim id="CLM-00033">
<claim-text><highlight><bold>33</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00033">claim 31</dependent-claim-reference>, further comprising the step of repeating steps 31(b) through 31(g) until there are no more potential slave computers available among the plurality of multipurpose workstations. </claim-text>
</claim>
<claim id="CLM-00034">
<claim-text><highlight><bold>34</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00033">claim 31</dependent-claim-reference>, further comprising the step of downloading a slave virtual supercomputer software from the master computer to the potential slave computer. </claim-text>
</claim>
<claim id="CLM-00035">
<claim-text><highlight><bold>35</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00033">claim 31</dependent-claim-reference>, further comprising the step of downloading a slave application from the master computer to the potential slave computer. </claim-text>
</claim>
<claim id="CLM-00036">
<claim-text><highlight><bold>36</bold></highlight>. A method for dynamically reconfiguring a virtual supercomputer while the virtual supercomputer is solving a computationally intensive problem, said method comprising the steps of: 
<claim-text>(a) detecting an event affecting a size of the virtual supercomputer; </claim-text>
<claim-text>(b) reassigning a task quantum from a failed slave computer to at least one slave computer in the virtual supercomputer if the event is a slave failure event; and </claim-text>
<claim-text>(c) retrieving a task quantum from at least one slave computer in the virtual supercomputer and assigning the retrieved task quantum to a new slave computer if the event is a newly available slave event. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00037">
<claim-text><highlight><bold>37</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00033">claim 36</dependent-claim-reference>, further comprising the steps of: 
<claim-text>(a) terminating a slave application on the failed slave computer if the event is a slave failure event; and </claim-text>
<claim-text>(b) restarting the slave application on the failed slave computer if the event is a slave failure event, wherein steps 37(a) and 37(b) are performed before step 36(b) is performed. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00038">
<claim-text><highlight><bold>38</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00033">claim 37</dependent-claim-reference>, further comprising the steps of: 
<claim-text>(a) initializing the restarted slave application if the event is a slave failure event; </claim-text>
<claim-text>(b) testing the restarted slave application if the event is a slave failure event; and </claim-text>
<claim-text>(c) terminating the restarted slave application if the testing step is not successful, wherein steps 38(a) through 38(c) are performed before step 36(b) is performed. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00039">
<claim-text><highlight><bold>39</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00033">claim 38</dependent-claim-reference>, further comprising the step of removing the failed slave computer from the virtual supercomputer if the testing step is not successful. </claim-text>
</claim>
<claim id="CLM-00040">
<claim-text><highlight><bold>40</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00033">claim 36</dependent-claim-reference>, wherein the step of adding an additional slave computer to the virtual supercomputer if a workstation selected from the plurality of multipurpose workstations becomes newly available comprises the steps of: 
<claim-text>(a) starting a virtual supercomputer daemon on the available workstation; and </claim-text>
<claim-text>(b) starting a slave application the newly available workstation. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00041">
<claim-text><highlight><bold>41</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00044">claim 40</dependent-claim-reference>, further comprising the steps of: 
<claim-text>(a) initializing the slave application on the newly available workstation; and </claim-text>
<claim-text>(b) testing the slave application on the newly available workstation. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00042">
<claim-text><highlight><bold>42</bold></highlight>. A method for solving a computationally intensive problem using a plurality of multipurpose computer workstations, said method comprising the steps of: 
<claim-text>(a) building a virtual supercomputer comprising a master computer and at least one slave computer, wherein the slave computer is selected from the plurality of multipurpose computer workstations; </claim-text>
<claim-text>(b) distributing a plurality of data sets to the at least one slave computer; </claim-text>
<claim-text>(c) balancing an initial load distribution on the virtual supercomputer; </claim-text>
<claim-text>(d) assigning initial computational tasks to the at least one slave computer; </claim-text>
<claim-text>(e) initiating computations on the at least one slave computer; </claim-text>
<claim-text>(f) receiving a first set of results from the at least one slave computer; </claim-text>
<claim-text>(g) adding an additional slave computer to the virtual supercomputer if a workstation selected from the plurality of multipurpose workstations becomes available; </claim-text>
<claim-text>(h) removing a slave computer from the virtual supercomputer if the slave computer becomes unavailable; and </claim-text>
<claim-text>(i) distributing the first set of results and additional data sets to the at least one slave computer until the computationally intensive problem has been solved. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00043">
<claim-text><highlight><bold>43</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00044">claim 42</dependent-claim-reference>, further comprising the steps of: 
<claim-text>(a) collecting a plurality of final performance statistics from each slave computer; and </claim-text>
<claim-text>(b) tearing down the virtual supercomputer. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00044">
<claim-text><highlight><bold>44</bold></highlight>. A method for incorporating a new member computer into a virtual supercomputer solving a computationally intensive problem, said method comprising the steps of: 
<claim-text>(a) sending a first messages from the new member computer to a master computer in the virtual supercomputer, said first message comprising an availability status for the new member computer; </claim-text>
<claim-text>(b) starting a virtual supercomputer daemon on the new member computer; </claim-text>
<claim-text>(c) starting a slave application on the new member computer; </claim-text>
<claim-text>(d) performing a self-test of the slave application on the new computer; </claim-text>
<claim-text>(e) sending a second message from the new member computer to the master computer, said second message comprising a performance statistic report; </claim-text>
<claim-text>(f) receiving a data set on the new member computer; </claim-text>
<claim-text>(g) receiving a task on the new member computer, said task comprising computational instructions for solving a portion of the computationally intensive problem; and </claim-text>
<claim-text>(h) completing the task on the new member computer. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00045">
<claim-text><highlight><bold>45</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00044">claim 44</dependent-claim-reference>, further comprising the steps of: 
<claim-text>(a) sending a third message from the new member computer to the master computer, said third message comprising a failure report if the self-test is not successful; </claim-text>
<claim-text>(b) terminating the virtual supercomputer daemon on the new member computer if the self-test is not successful; and </claim-text>
<claim-text>(c) terminating the slave application on the new member computer, if the self-test is not successful. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00046">
<claim-text><highlight><bold>46</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00044">claim 44</dependent-claim-reference>, further comprising the steps of: 
<claim-text>(a) collecting a plurality of performance statistics on the new member computer; and </claim-text>
<claim-text>(b) sending a third message from the new member computer to the master computer, said third message comprising the plurality of performance statistics. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00047">
<claim-text><highlight><bold>47</bold></highlight>. A method for dynamically adding a new member computer to a virtual supercomputer while the virtual supercomputer is solving a computationally intensive problem, said method comprising the steps of: 
<claim-text>(a) detecting the availability of the new member computer; </claim-text>
<claim-text>(b) providing a virtual supercomputer daemon to the new member computer; </claim-text>
<claim-text>(c) instructing the new member computer to start the virtual supercomputer daemon; </claim-text>
<claim-text>(d) providing a slave application to the new member computer; </claim-text>
<claim-text>(e) instructing the new member computer to start the slave application; </claim-text>
<claim-text>(f) redistributing a computational load on the virtual supercomputer; and </claim-text>
<claim-text>(g) performing computational tasks on the virtual supercomputer. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00048">
<claim-text><highlight><bold>48</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00044">claim 47</dependent-claim-reference> further comprising the steps of: 
<claim-text>(a) providing an initial data set to the new member computer; and </claim-text>
<claim-text>(b) instructing the new member computer to perform a self-test using the initial data set. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00049">
<claim-text><highlight><bold>49</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00044">claim 48</dependent-claim-reference> further comprising the steps of: 
<claim-text>(a) receiving a self-test result from the new member computer; </claim-text>
<claim-text>(b) instructing the new member computer to terminate the slave application if the self-test is not successful; and </claim-text>
<claim-text>(c) removing the new member computer from the virtual supercomputer if the self-test is not successful. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00050">
<claim-text><highlight><bold>50</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00044">claim 47</dependent-claim-reference>, wherein the step of redistributing a computational load on the virtual supercomputer comprises the steps of: 
<claim-text>(a) retrieving a plurality of task quantum from each member computer in the virtual supercomputer; </claim-text>
<claim-text>(b) determining a computational capability for each member computer in the virtual supercomputer; and </claim-text>
<claim-text>(c) assigning a subset of the plurality of task quantum to each member computer in the virtual supercomputer. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00051">
<claim-text><highlight><bold>51</bold></highlight>. A method for dynamically processing a slave application failure on a member computer in a virtual supercomputer while the virtual supercomputer is solving a computationally intensive problem, said method comprising the steps of: 
<claim-text>(a) detecting the slave application failure; </claim-text>
<claim-text>(b) instructing the member computer to restart the slave application; </claim-text>
<claim-text>(c) providing an initial data set to the member computer; </claim-text>
<claim-text>(d) instructing the member computer to perform a self-test using the initial data set; </claim-text>
<claim-text>(e) instructing the member computer to continue performing computational tasks for a previously assigned task quanta. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00052">
<claim-text><highlight><bold>52</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00055">claim 51</dependent-claim-reference> further comprising the steps of: 
<claim-text>(a) receiving a self-test result from the member computer; </claim-text>
<claim-text>(b) instructing the member computer to terminate the slave application if the self-test is not successful; </claim-text>
<claim-text>(c) redistributing a computational load on the virtual supercomputer; and </claim-text>
<claim-text>(d) removing the member computer from the virtual supercomputer if the self-test is not successful. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00053">
<claim-text><highlight><bold>53</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00055">claim 52</dependent-claim-reference>, wherein the step of redistributing a computational load on the virtual supercomputer comprises the steps of: 
<claim-text>(a) determining which of a plurality of task quantum was previously assigned to the member computer; </claim-text>
<claim-text>(b) determining a computational capability for each member computer in the virtual supercomputer; and </claim-text>
<claim-text>(c) assigning a subset of the plurality of task quantum to each member computer in the virtual supercomputer. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00054">
<claim-text><highlight><bold>54</bold></highlight>. A method for dynamically processing a termination of a virtual supercomputer daemon on a member computer in a virtual supercomputer while the virtual supercomputer is solving a computationally intensive problem, said method comprising the steps of: 
<claim-text>(a) detecting the termination of the virtual supercomputer daemon; </claim-text>
<claim-text>(b) instructing the member computer to restart the virtual supercomputer daemon; </claim-text>
<claim-text>(c) instructing the member computer to restart the slave application; </claim-text>
<claim-text>(d) providing an initial data set to the member computer; </claim-text>
<claim-text>(e) instructing the member computer to perform a self-test using the initial data set; </claim-text>
<claim-text>(f) instructing the member computer to continue performing computational tasks for a previously assigned task quanta. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00055">
<claim-text><highlight><bold>55</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00055">claim 54</dependent-claim-reference> further comprising the steps of: 
<claim-text>(a) receiving a self-test result from the member computer; </claim-text>
<claim-text>(b) instructing the member computer to terminate the slave application if the self-test is not successful; </claim-text>
<claim-text>(c) redistributing a computational load on the virtual supercomputer; and </claim-text>
<claim-text>(d) removing the member computer from the virtual supercomputer if the self-test is not successful. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00056">
<claim-text><highlight><bold>56</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00055">claim 55</dependent-claim-reference>, wherein the step of redistributing a computational load on the virtual supercomputer comprises the steps of: 
<claim-text>(a) determining which of a plurality of task quantum was previously assigned to the member computer; </claim-text>
<claim-text>(b) determining a computational capability for each member computer in the virtual supercomputer; and </claim-text>
<claim-text>(c) assigning a subset of the plurality of task quantum to each member computer in the virtual supercomputer. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00057">
<claim-text><highlight><bold>57</bold></highlight>. A system for solving a computationally intensive problem using a virtual supercomputer, said system comprising: 
<claim-text>(a) a virtual supercomputer comprising a master computer in communication with at least one slave computer; </claim-text>
<claim-text>(b) a master application running on the master computer; and </claim-text>
<claim-text>(c) a slave application running on the at least one slave computer; </claim-text>
<claim-text>wherein the master application divides the computationally intensive task into a plurality of task quantum and distributes a subset of the plurality of task quantum to the at least one slave computer, and wherein the slave application performs computations on the subset as directed by the master computer. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00058">
<claim-text><highlight><bold>58</bold></highlight>. The system of <dependent-claim-reference depends_on="CLM-00055">claim 57</dependent-claim-reference>, wherein the communication between the master computer and the at least one slave computer comprises a local area network. </claim-text>
</claim>
<claim id="CLM-00059">
<claim-text><highlight><bold>59</bold></highlight>. The system of <dependent-claim-reference depends_on="CLM-00055">claim 57</dependent-claim-reference>, wherein the communication between the master computer and the at least one slave computer comprises a wide area network. </claim-text>
</claim>
<claim id="CLM-00060">
<claim-text><highlight><bold>60</bold></highlight>. The system of <dependent-claim-reference depends_on="CLM-00055">claim 57</dependent-claim-reference>, wherein the communication between the master computer and the at least one slave computer comprises the Internet. </claim-text>
</claim>
<claim id="CLM-00061">
<claim-text><highlight><bold>61</bold></highlight>. The system of <dependent-claim-reference depends_on="CLM-00055">claim 57</dependent-claim-reference>, wherein the master application is adapted to receive periodic messages from the slave application, and wherein in response to the periodic messages, the master application performs a load balancing procedure on the virtual supercomputer. </claim-text>
</claim>
<claim id="CLM-00062">
<claim-text><highlight><bold>62</bold></highlight>. The system of <dependent-claim-reference depends_on="CLM-00055">claim 57</dependent-claim-reference>, wherein the master application detects the availability of a new slave computer and dynamically reconfigures the virtual supercomputer to incorporate the new slave computer in solving the computationally intensive problem. </claim-text>
</claim>
<claim id="CLM-00063">
<claim-text><highlight><bold>63</bold></highlight>. The system of <dependent-claim-reference depends_on="CLM-00055">claim 57</dependent-claim-reference>, wherein the computationally intensive problem comprises optimization of a financial portfolio. </claim-text>
</claim>
<claim id="CLM-00064">
<claim-text><highlight><bold>64</bold></highlight>. The system of <dependent-claim-reference depends_on="CLM-00055">claim 57</dependent-claim-reference>, wherein the computationally intensive problem comprises optimization of supply chain logistics. </claim-text>
</claim>
<claim id="CLM-00065">
<claim-text><highlight><bold>65</bold></highlight>. The system of <dependent-claim-reference depends_on="CLM-00066">claim 64</dependent-claim-reference>, wherein the computationally intensive problem further comprises determination of optimal stock levels on a by-item, by-store basis, together with a corresponding trans-shipment plan on a daily basis. </claim-text>
</claim>
<claim id="CLM-00066">
<claim-text><highlight><bold>66</bold></highlight>. The system of <dependent-claim-reference depends_on="CLM-00055">claim 57</dependent-claim-reference>, wherein the computationally intensive problem is used in conjunction with a branch-and-bound approach to optimization. </claim-text>
</claim>
<claim id="CLM-00067">
<claim-text><highlight><bold>67</bold></highlight>. The system of <dependent-claim-reference depends_on="CLM-00055">claim 57</dependent-claim-reference>, wherein the computationally intensive problem is used in connection with large-scale simulations to reduce total run time. </claim-text>
</claim>
<claim id="CLM-00068">
<claim-text><highlight><bold>68</bold></highlight>. The system of <dependent-claim-reference depends_on="CLM-00055">claim 57</dependent-claim-reference>, wherein the computationally intensive problem comprises network optimization problem. </claim-text>
</claim>
<claim id="CLM-00069">
<claim-text><highlight><bold>69</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00066">claim 68</dependent-claim-reference>, wherein the network optimization problem comprises a vehicle routing and scheduling system. </claim-text>
</claim>
<claim id="CLM-00070">
<claim-text><highlight><bold>70</bold></highlight>. The system of <dependent-claim-reference depends_on="CLM-00055">claim 57</dependent-claim-reference>, wherein the computationally intensive problem comprises database searching across a network of workstations. </claim-text>
</claim>
<claim id="CLM-00071">
<claim-text><highlight><bold>71</bold></highlight>. The system of <dependent-claim-reference depends_on="CLM-00055">claim 57</dependent-claim-reference>, wherein the computationally intensive problem comprises database mining across a network of workstations. </claim-text>
</claim>
<claim id="CLM-00072">
<claim-text><highlight><bold>72</bold></highlight>. The system of <dependent-claim-reference depends_on="CLM-00055">claim 57</dependent-claim-reference>, wherein the computationally intensive problem comprises an image analysis program.</claim-text>
</claim>
</subdoc-claims>
<subdoc-drawings id="DRAWINGS">
<heading lvl="0" align="CENTER">Drawings</heading>
<representative-figure>3</representative-figure>
<figure id="figure-D00000">
<image id="EMI-D00000" file="US20030005068A1-20030102-D00000.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00001">
<image id="EMI-D00001" file="US20030005068A1-20030102-D00001.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00002">
<image id="EMI-D00002" file="US20030005068A1-20030102-D00002.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00003">
<image id="EMI-D00003" file="US20030005068A1-20030102-D00003.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00004">
<image id="EMI-D00004" file="US20030005068A1-20030102-D00004.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00005">
<image id="EMI-D00005" file="US20030005068A1-20030102-D00005.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00006">
<image id="EMI-D00006" file="US20030005068A1-20030102-D00006.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00007">
<image id="EMI-D00007" file="US20030005068A1-20030102-D00007.TIF" imf="TIFF" ti="DR"/>
</figure>
</subdoc-drawings>
</patent-application-publication>
