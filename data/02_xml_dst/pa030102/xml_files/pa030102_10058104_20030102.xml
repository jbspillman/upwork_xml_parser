<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE patent-application-publication SYSTEM "pap-v16-2002-01-01.dtd" [
<!ENTITY US20030004723A1-20030102-M00001.NB SYSTEM "US20030004723A1-20030102-M00001.NB" NDATA NB>
<!ENTITY US20030004723A1-20030102-M00001.TIF SYSTEM "US20030004723A1-20030102-M00001.TIF" NDATA TIF>
<!ENTITY US20030004723A1-20030102-D00000.TIF SYSTEM "US20030004723A1-20030102-D00000.TIF" NDATA TIF>
<!ENTITY US20030004723A1-20030102-D00001.TIF SYSTEM "US20030004723A1-20030102-D00001.TIF" NDATA TIF>
<!ENTITY US20030004723A1-20030102-D00002.TIF SYSTEM "US20030004723A1-20030102-D00002.TIF" NDATA TIF>
<!ENTITY US20030004723A1-20030102-D00003.TIF SYSTEM "US20030004723A1-20030102-D00003.TIF" NDATA TIF>
<!ENTITY US20030004723A1-20030102-D00004.TIF SYSTEM "US20030004723A1-20030102-D00004.TIF" NDATA TIF>
<!ENTITY US20030004723A1-20030102-D00005.TIF SYSTEM "US20030004723A1-20030102-D00005.TIF" NDATA TIF>
<!ENTITY US20030004723A1-20030102-D00006.TIF SYSTEM "US20030004723A1-20030102-D00006.TIF" NDATA TIF>
<!ENTITY US20030004723A1-20030102-D00007.TIF SYSTEM "US20030004723A1-20030102-D00007.TIF" NDATA TIF>
<!ENTITY US20030004723A1-20030102-D00008.TIF SYSTEM "US20030004723A1-20030102-D00008.TIF" NDATA TIF>
<!ENTITY US20030004723A1-20030102-D00009.TIF SYSTEM "US20030004723A1-20030102-D00009.TIF" NDATA TIF>
<!ENTITY US20030004723A1-20030102-D00010.TIF SYSTEM "US20030004723A1-20030102-D00010.TIF" NDATA TIF>
<!ENTITY US20030004723A1-20030102-D00011.TIF SYSTEM "US20030004723A1-20030102-D00011.TIF" NDATA TIF>
<!ENTITY US20030004723A1-20030102-D00012.TIF SYSTEM "US20030004723A1-20030102-D00012.TIF" NDATA TIF>
<!ENTITY US20030004723A1-20030102-D00013.TIF SYSTEM "US20030004723A1-20030102-D00013.TIF" NDATA TIF>
<!ENTITY US20030004723A1-20030102-D00014.TIF SYSTEM "US20030004723A1-20030102-D00014.TIF" NDATA TIF>
<!ENTITY US20030004723A1-20030102-D00015.TIF SYSTEM "US20030004723A1-20030102-D00015.TIF" NDATA TIF>
<!ENTITY US20030004723A1-20030102-D00016.TIF SYSTEM "US20030004723A1-20030102-D00016.TIF" NDATA TIF>
<!ENTITY US20030004723A1-20030102-D00017.TIF SYSTEM "US20030004723A1-20030102-D00017.TIF" NDATA TIF>
<!ENTITY US20030004723A1-20030102-D00018.TIF SYSTEM "US20030004723A1-20030102-D00018.TIF" NDATA TIF>
<!ENTITY US20030004723A1-20030102-D00019.TIF SYSTEM "US20030004723A1-20030102-D00019.TIF" NDATA TIF>
<!ENTITY US20030004723A1-20030102-D00020.TIF SYSTEM "US20030004723A1-20030102-D00020.TIF" NDATA TIF>
<!ENTITY US20030004723A1-20030102-D00021.TIF SYSTEM "US20030004723A1-20030102-D00021.TIF" NDATA TIF>
]>
<patent-application-publication>
<subdoc-bibliographic-information>
<document-id>
<doc-number>20030004723</doc-number>
<kind-code>A1</kind-code>
<document-date>20030102</document-date>
</document-id>
<publication-filing-type>new</publication-filing-type>
<domestic-filing-data>
<application-number>
<doc-number>10058104</doc-number>
</application-number>
<application-number-series-code>10</application-number-series-code>
<filing-date>20020129</filing-date>
</domestic-filing-data>
<foreign-priority-data>
<priority-application-number>
<doc-number>2001-192778</doc-number>
</priority-application-number>
<filing-date>20010626</filing-date>
<country-code>JP</country-code>
</foreign-priority-data>
<technical-information>
<classification-ipc>
<classification-ipc-primary>
<ipc>G10L013/08</ipc>
</classification-ipc-primary>
<classification-ipc-secondary>
<ipc>G10L013/00</ipc>
</classification-ipc-secondary>
<classification-ipc-edition>07</classification-ipc-edition>
</classification-ipc>
<classification-us>
<classification-us-primary>
<uspc>
<class>704</class>
<subclass>260000</subclass>
</uspc>
</classification-us-primary>
</classification-us>
<title-of-invention>Method of controlling high-speed reading in a text-to-speech conversion system</title-of-invention>
</technical-information>
<inventors>
<first-named-inventor>
<name>
<given-name>Keiichi</given-name>
<family-name>Chihara</family-name>
</name>
<residence>
<residence-non-us>
<city>Tokyo</city>
<country-code>JP</country-code>
</residence-non-us>
</residence>
<authority-applicant>INV</authority-applicant>
</first-named-inventor>
</inventors>
<correspondence-address>
<name-1>KANESAKA &amp; TAKEUCHI</name-1>
<name-2></name-2>
<address>
<address-1>1423 Powhatan Street</address-1>
<city>Alexandria</city>
<state>VA</state>
<postalcode>22314</postalcode>
<country>
<country-code>US</country-code>
</country>
</address>
</correspondence-address>
</subdoc-bibliographic-information>
<subdoc-abstract>
<paragraph id="A-0001" lvl="0">A method of high-speed reading in a text-to-speech conversion system including a text analysis module (<highlight><bold>101</bold></highlight>) for generating a phoneme and prosody character string from an input text; a prosody generation module (<highlight><bold>102</bold></highlight>) for generating a synthesis parameter of at least a voice segment, a phoneme duration, and a fundamental frequency for the phoneme and prosody character string; and a speech generation module (<highlight><bold>103</bold></highlight>) for generating a synthetic waveform by waveform superimposition by referring to a voice segment dictionary (<highlight><bold>105</bold></highlight>). The prosody generation module is provided with both a duration rule table containing empirically found phoneme durations and a duration prediction table containing phoneme durations predicted by statistical analysis and, when the user-designated utterance speed exceeds a threshold, uses the duration rule table and, when the threshold is not exceeded, uses the duration prediction table to determined the phoneme duration. </paragraph>
</subdoc-abstract>
<subdoc-description>
<summary-of-invention>
<section>
<heading lvl="1">BACKGROUND OF THE INVENTION </heading>
<paragraph id="P-0001" lvl="0"><number>&lsqb;0001&rsqb;</number> 1. Field of the Invention </paragraph>
<paragraph id="P-0002" lvl="0"><number>&lsqb;0002&rsqb;</number> The present invention relates to text-to-speech conversion technologies for outputting a speech for a text that is composed of Japanese Kanji and Kana characters and, particularly, to a prosody control in high-speed reading. </paragraph>
<paragraph id="P-0003" lvl="0"><number>&lsqb;0003&rsqb;</number> 2. Description of the Related Art </paragraph>
<paragraph id="P-0004" lvl="0"><number>&lsqb;0004&rsqb;</number> A text-to-speech conversion system, which receives a text composed of Japanese Kanji and Kana characters and coverts it to a speech for outputting, is limitless in the output vocabularies and is expected to replace the record/playback speech synthesis technology in a variety of application fields. </paragraph>
<paragraph id="P-0005" lvl="0"><number>&lsqb;0005&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 15</cross-reference> shows a typical text-to-speech conversion system. When a text of sentences composed of Japanese Kanji and Kana characters (hereinafter &ldquo;text&rdquo;) is inputted, a text analysis module <highlight><bold>101</bold></highlight> generates a phoneme and prosody character string or sequence from the character information. The &ldquo;phoneme and prosody character string or sequence&rdquo; herein used means a sequence of characters representing the reading of an input sentence and the prosodic information such as accent and intonation (hereinafter &ldquo;intermediate language&rdquo;). A word dictionary <highlight><bold>104</bold></highlight> is a pronunciation dictionary in which the reading, accent, etc. of each word are registered. The text analysis module <highlight><bold>101</bold></highlight> performs a linguistic process, such as morphemic analysis and syntax analysis, by referring to the pronunciation dictionary to generate an intermediate language. </paragraph>
<paragraph id="P-0006" lvl="0"><number>&lsqb;0006&rsqb;</number> Based on the intermediate language generated by the text analysis module <highlight><bold>101</bold></highlight>, a prosody generation module <highlight><bold>102</bold></highlight> determines a composite or synthesis parameter composed of a voice segment (kind of a sound), a sound quality conversion coefficient (tone of a sound), a phoneme duration (length of a sound), a phoneme power (intensity of a sound), and a fundamental frequency (loudness of a sound, hereinafter &ldquo;pitch&rdquo;) and transmits it to a speech generation module <highlight><bold>103</bold></highlight>. </paragraph>
<paragraph id="P-0007" lvl="0"><number>&lsqb;0007&rsqb;</number> The &ldquo;voice segments&rdquo; herein used mean units of voice connected to produce a composite or synthetic waveform (speech) and vary with the kind of sound. Generally, the voice segment is composed of a string of phonemes such as CV, VV, VCV, or CVC wherein C and V represent a consonant and a vowel, respectively. </paragraph>
<paragraph id="P-0008" lvl="0"><number>&lsqb;0008&rsqb;</number> Based on the respective parameters generated by the prosody generation module <highlight><bold>102</bold></highlight>, the speech generation module <highlight><bold>103</bold></highlight> generates a composite or synthetic waveform (speech) by referring to a voice segment dictionary <highlight><bold>105</bold></highlight> that is composed of a read-only memory (ROM), etc., in which voice segments are stored, and outputs the synthetic speech through a speaker. The synthetic speech can be made by, for example, putting a pitch mark (as a reference point) on the voice waveform and, upon synthesis, superimposing it by shifting the position of the pitch mark according to the synthesis pitch cycle. The foregoing is a brief description of the text-to-speech conversion process. </paragraph>
<paragraph id="P-0009" lvl="0"><number>&lsqb;0009&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 16</cross-reference> shows the conventional prosody generation module <highlight><bold>102</bold></highlight>. The intermediate language inputted to the prosody generation module <highlight><bold>102</bold></highlight> is a phoneme character sequence containing prosodic information such as an accent position and a pause position. Based on this information, the module <highlight><bold>102</bold></highlight> determines a parameter for generating waveforms (hereinafter &ldquo;synthesis parameter&rdquo;) such as temporal changes of the pitch (hereinafter &ldquo;pitch contour&rdquo;), the voice power, the phoneme duration, and the voice segment addresses stored in a voice segment dictionary. In addition, the user may input a control parameter for designating at least one utterance property such as a utterance speed, pitch, intonation, intensity, speaker, and sound quality. </paragraph>
<paragraph id="P-0010" lvl="0"><number>&lsqb;0010&rsqb;</number> An intermediate language analysis unit <highlight><bold>201</bold></highlight> analyzes a character sequence for the input intermediate language to determined a word boundary from the breath group and word end symbols put on the intermediate language and the mora (syllable) position of an accent nuclear from the accent symbol. The &ldquo;breath group&rdquo; means a unit of utterance made in a breath. The &ldquo;accent nuclear&rdquo; means the position at which the accent falls. A word with the accent nuclear at the first mora is called &ldquo;accent type one word&rdquo;, a word with the accent nuclear at the n-th mora is called &ldquo;accent type n word&rdquo; and, generally, it is called &ldquo;accent type uneven word&rdquo;. Conversely, a word with no accent nuclear, such as &ldquo;shinbun&rdquo; or &ldquo;pasocon&rdquo;, is called &ldquo;accent type 0&rdquo; or &ldquo;accent type flat&rdquo; word. The information about such prosody is transmitted to a pitch contour determination unit <highlight><bold>202</bold></highlight>, a phoneme duration determination unit <highlight><bold>203</bold></highlight>, a phoneme power determination unit <highlight><bold>204</bold></highlight>, a voice segment determination unit <highlight><bold>205</bold></highlight>, and a sound quality coefficient determination unit <highlight><bold>206</bold></highlight>, respectively. </paragraph>
<paragraph id="P-0011" lvl="0"><number>&lsqb;0011&rsqb;</number> The pitch contour determination unit <highlight><bold>202</bold></highlight> calculates pitch frequency changes in an accent or phrase unit from the prosody information on the intermediate language. The pitch control mechanism model specified by critically damped second-order linear systems, which is called &ldquo;Fujisaki model&rdquo;, has been used. According to the pitch control mechanism model, the fundamental frequency, which determines the pitch, is generated as follows. The frequency of a glottal oscillation or fundamental frequency is controlled by an impulse command issued every time a phrase is switched and a step command issued whenever the accent goes up or down. The impulse command becomes a gently falling curve from the head to the tail of a sentence (phrase component) because of a delay in the physiological mechanism. The step command becomes a locally very uneven curve (accent component). These components are made models as responses to the critically damped second-order linear systems. The logarithmic fundamental frequency changes are expressed as the sum of these components (hereinafter &ldquo;intonation component&rdquo;). </paragraph>
<paragraph id="P-0012" lvl="0"><number>&lsqb;0012&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 17</cross-reference> shows the pitch control mechanism model. The log-fundamental frequency, lnFo(t), wherein t is the time, is formulated as follows.  
<math-cwu id="MATH-US-00001">
<number>1</number>
<math>
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mi>ln</mi>
          <mo>&it;</mo>
          <mstyle>
            <mtext>&emsp;</mtext>
          </mstyle>
          <mo>&it;</mo>
          <mrow>
            <msub>
              <mi>F</mi>
              <mi>o</mi>
            </msub>
            <mo>&af;</mo>
            <mrow>
              <mo>(</mo>
              <mi>t</mi>
              <mo>)</mo>
            </mrow>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mrow>
          <mrow>
            <mi>ln</mi>
            <mo>&it;</mo>
            <mstyle>
              <mtext>&emsp;</mtext>
            </mstyle>
            <mo>&it;</mo>
            <msub>
              <mi>F</mi>
              <mi>min</mi>
            </msub>
          </mrow>
          <mo>+</mo>
          <mrow>
            <munderover>
              <mo>&Sum;</mo>
              <mrow>
                <mi>i</mi>
                <mo>=</mo>
                <mn>1</mn>
              </mrow>
              <mi>I</mi>
            </munderover>
            <mo>&it;</mo>
            <mrow>
              <msub>
                <mi>A</mi>
                <mi>pi</mi>
              </msub>
              <mo>&it;</mo>
              <mrow>
                <msub>
                  <mi>G</mi>
                  <mi>pi</mi>
                </msub>
                <mo>&af;</mo>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <mi>t</mi>
                    <mo>-</mo>
                    <msub>
                      <mi>T</mi>
                      <mi>oi</mi>
                    </msub>
                  </mrow>
                  <mo>)</mo>
                </mrow>
              </mrow>
            </mrow>
          </mrow>
          <mo>+</mo>
          <mrow>
            <munderover>
              <mo>&Sum;</mo>
              <mrow>
                <mi>j</mi>
                <mo>=</mo>
                <mn>1</mn>
              </mrow>
              <mi>J</mi>
            </munderover>
            <mo>&it;</mo>
            <mrow>
              <msub>
                <mi>A</mi>
                <mi>aj</mi>
              </msub>
              <mo>&it;</mo>
              <mrow>
                <mo>{</mo>
                <mrow>
                  <mrow>
                    <msub>
                      <mi>G</mi>
                      <mi>aj</mi>
                    </msub>
                    <mo>&af;</mo>
                    <mrow>
                      <mo>(</mo>
                      <mrow>
                        <mi>t</mi>
                        <mo>-</mo>
                        <msub>
                          <mi>T</mi>
                          <mi>ij</mi>
                        </msub>
                      </mrow>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                  <mo>-</mo>
                  <mrow>
                    <msub>
                      <mi>G</mi>
                      <mi>aj</mi>
                    </msub>
                    <mo>&af;</mo>
                    <mrow>
                      <mo>(</mo>
                      <mrow>
                        <mi>t</mi>
                        <mo>-</mo>
                        <msub>
                          <mi>T</mi>
                          <mrow>
                            <mn>2</mn>
                            <mo>&it;</mo>
                            <mi>j</mi>
                          </mrow>
                        </msub>
                      </mrow>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                </mrow>
                <mo>}</mo>
              </mrow>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>1</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
<mathematica-file id="MATHEMATICA-00001" file="US20030004723A1-20030102-M00001.NB"/>
<image id="EMI-M00001" wi="216.027" he="39.123" file="US20030004723A1-20030102-M00001.TIF" imf="TIFF" ti="MF"/>
</math-cwu>
</paragraph>
<paragraph id="P-0013" lvl="7"><number>&lsqb;0013&rsqb;</number> wherein Fmin is the minimum frequency (hereinafter &ldquo;base pitch&rdquo;), I is the number of phrase commands in the sentence, Api is the amplitude of the i-th phrase command, Toi is the start time of the i-th phrase command, J is the number of accent commands in the sentence, Aaj is the amplitude of the j-th accent command, and T1j and T2j are the start and end times of the j-th accent command, respectively. Gpi(t) and Gaj(t) are the impulse response function of the phrase control mechanism and the step response function of the accent control mechanism, respectively, and given by the following equations.</paragraph>
<paragraph lvl="0"><in-line-formula><highlight><italic>G</italic></highlight><highlight><subscript>pi</subscript></highlight>(<highlight><italic>t</italic></highlight>)&equals;&agr;<highlight><subscript>i</subscript></highlight><highlight><superscript>2</superscript></highlight><highlight><italic>t </italic></highlight>exp(&minus;&agr;<highlight><subscript>i</subscript></highlight><highlight><italic>t</italic></highlight>)&emsp;&emsp;(2)</in-line-formula></paragraph>
<paragraph lvl="0"><in-line-formula><highlight><italic>G</italic></highlight><highlight><subscript>aj</subscript></highlight>(<highlight><italic>t</italic></highlight>)&equals;min&lsqb;1&minus;(1&plus;&bgr;<highlight><subscript>j</subscript></highlight><highlight><italic>t</italic></highlight>)exp(&minus;&bgr;<highlight><subscript>j</subscript></highlight><highlight><italic>t</italic></highlight>), &thgr;&rsqb;&emsp;&emsp;(3)</in-line-formula></paragraph>
<paragraph id="P-0014" lvl="7"><number>&lsqb;0014&rsqb;</number> The above equations are the response functions at t&gE;0. If t&lt;0, then Gpi(t)&equals;Gaj(t) . </paragraph>
<paragraph id="P-0015" lvl="0"><number>&lsqb;0015&rsqb;</number> In Equation (3), the symbol min&lsqb;x, y&rsqb; means that the smaller of x and y is taken, which corresponds to the fact that the accent component of a voice reaches the upper limit in a finite time. &agr;i is the natural angular frequency of the phrase control mechanism for the i-th phrase command and, for example, set at 3.0. &bgr;j is the natural angular frequency of the accent control mechanism for the j-th accent command and, for example, set at 20.0. &thgr; is the upper limit of the accent component and, for example, set at 0.9. </paragraph>
<paragraph id="P-0016" lvl="0"><number>&lsqb;0016&rsqb;</number> The units of the fundamental frequency and pitch control parameters, Api, Aaj, Toi, T1j, T2j, &agr;i, &bgr;j, and Fmin, are defined as follows. The unit of Fo(t) and Fmin is Hz, the unit of Toi, T1j, and T2j is sec, and the unit of &agr;i and &bgr;j is rad/sec. The unit of Api and Aaj is derived from the above units of the fundamental frequency and pitch control parameters. </paragraph>
<paragraph id="P-0017" lvl="0"><number>&lsqb;0017&rsqb;</number> The pitch contour determination unit <highlight><bold>202</bold></highlight> determines the pitch control parameter from the intermediate language. For example, the start time of a phrase command, Toi, is set at the position of a punctuation on the intermediate language, the start time of an accent command, T1j, is set immediately after the word boundary symbol, and the end time of the accent command, T2j, is set at either the position of the accent symbol or immediately before the word boundary symbol for an accent type flat word with no accent symbol. The amplitudes of phrase and accent commands, Api and Aaj, are determined in most cases by statistical analysis such as Quantification theory (type one), which is well known and its description will be omitted. </paragraph>
<paragraph id="P-0018" lvl="0"><number>&lsqb;0018&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 18</cross-reference> shows the pitch contour generation process. The analysis result generated by the intermediate language analysis unit <highlight><bold>201</bold></highlight> is sent to a control factor setting section <highlight><bold>501</bold></highlight>, where control factors required to predict the amplitudes of phrase and accent components are set. The information necessary for phrase component prediction, such as the number of moras in the phrase, the position within the sentence, and the accent type of the leading word, is sent to a phrase component estimation section <highlight><bold>503</bold></highlight>. The information necessary for accent component prediction, such as the accent type of the accented phrase, the number of moras, the part of speech, and the position in the phrase, is sent to an accent component estimation section <highlight><bold>502</bold></highlight>. The prediction of respective component values uses a prediction table <highlight><bold>506</bold></highlight> that has been trained by using statistical analysis, such as Quantification theory (type one), based on the natural utterance data. </paragraph>
<paragraph id="P-0019" lvl="0"><number>&lsqb;0019&rsqb;</number> The predicted results are sent to a pitch contour correction section <highlight><bold>504</bold></highlight>, in which the estimated values Api and Aaj are corrected when the user designates the intonation. This control function is used to emphasize or suppress the word in the sentence. Usually, the intonation is controlled at three to five levels by multiplying each level with a predetermined constant. Where there is no intonation designation, no correction is made. </paragraph>
<paragraph id="P-0020" lvl="0"><number>&lsqb;0020&rsqb;</number> After both the phrase and accent component values are corrected, they are sent to a base pitch addition section <highlight><bold>505</bold></highlight> to generate a sequence of data according to Equation (1). Based on user&apos;s pitch designation, data for the designated level is retrieved as a base pitch from a base pitch table <highlight><bold>507</bold></highlight> for making addition. The logarithmic base pitch, lnFmin, represents the minimum pitch of a synthetic voice and is used to control the pitch of a voice. Usually, lnFmin is quantized at five to 10 levels and stored in the table. It is increased where the user desires overall loud voices. Conversely, it is lowered when soft voices are desired. </paragraph>
<paragraph id="P-0021" lvl="0"><number>&lsqb;0021&rsqb;</number> The base pitch table <highlight><bold>507</bold></highlight> is divided into two sections; one for men&apos;s voice and the other for women&apos;s voice. Based on user&apos;s speaker designation, the base pitch is selected for retrieval. Usually, men&apos;s voice is quantized at pitch levels between 3.0 and 4.0 while women&apos;s voice is at pitch levels between 4.0 and 5.0. </paragraph>
<paragraph id="P-0022" lvl="0"><number>&lsqb;0022&rsqb;</number> The phoneme duration control will be described. The phoneme duration determination unit <highlight><bold>203</bold></highlight> determines the phoneme length and the pause length from the phoneme character string and the prosodic symbol. The &ldquo;pause length&rdquo; means the length between phrases or sentences. The phoneme length determines the length of consonant and/or vowel which constitute a syllable and the silent length between closed sections that occurs immediately before a plosive phoneme such as p, t, or k. The phoneme duration and pause lengths are called generally &ldquo;duration length&rdquo;. The phoneme duration is determined by statistical analysis, such as Quantification theory (type one), based on the kind of phonemes adjacent to the target phoneme or the syllable position in the word or breath group. The pause length is determined by statistical analysis, such as Quantification theory (type one), based on the number of moras in adjacent phrases. Where the user designates the utterance speed, the phoneme duration is adjusted accordingly. Usually, the utterance speed is controlled at five to 10 levels by multiplying each level by a predetermined constant. When slow utterance is desired, the phoneme duration is lengthened while the phoneme duration is shortened for high utterance speed. The phoneme duration control is the subject matter of this application and will be described later. </paragraph>
<paragraph id="P-0023" lvl="0"><number>&lsqb;0023&rsqb;</number> The phoneme power determination unit <highlight><bold>204</bold></highlight> calculates the waveform amplitudes of individual phonemes from a phoneme character string. The waveform amplitudes are determined empirically from the kind of a phoneme, such as a, i, u, e, or o, and the syllable position in the breath group. The power transition within the syllable is also determined from the rising period when the amplitude gradually increases to the falling period when the amplitude decreases through the stationary-state period. The power control is made by using the coefficient table. When the user designates the intensity, the amplitude is adjusted accordingly. The intensity is controlled usually at 10 levels by multiplying each level by a predetermined constant. </paragraph>
<paragraph id="P-0024" lvl="0"><number>&lsqb;0024&rsqb;</number> The voice segment determination unit <highlight><bold>205</bold></highlight> determines the addresses, within the voice segment dictionary <highlight><bold>105</bold></highlight>, of voice segments required to express a phoneme character string. The voice dictionary <highlight><bold>105</bold></highlight> contains voice segments of a plurality of speakers including both men and women and determines the address of a voice segment according to user&apos;s speaker designation. The voice segment data in the dictionary <highlight><bold>105</bold></highlight> is composed of various units corresponding to the adjacent phoneme environment, such as CV or VCV, so that the optimum synthesis unit is selected from the phoneme character string of an input text. </paragraph>
<paragraph id="P-0025" lvl="0"><number>&lsqb;0025&rsqb;</number> The sound quality determination unit <highlight><bold>206</bold></highlight> determines the conversion parameter when the user makes a sound quality conversion designation. The &ldquo;sound quality conversion&rdquo; means the process of signals for the voice segment data stored in the dictionary <highlight><bold>105</bold></highlight> so that the voice segment data is treated as the voice segment data of another speaker. Generally, it is achieved by linearly expanding or compressing the voice segment data. The expansion process is made by oversampling the voice segment data, resulting in the deep voice. Conversely, the compression process is made by downsampling the voice segment data, resulting in the thin voice. The sound quality conversion is controlled usually at five to 10 levels, each of which has been assigned with a re-sampling rate. </paragraph>
<paragraph id="P-0026" lvl="0"><number>&lsqb;0026&rsqb;</number> The pitch contour, phoneme power, phoneme duration, voice segment address, and expansion/compression parameters are sent to the synthesis parameter generation unit <highlight><bold>207</bold></highlight> to provide a synthesis parameter. The synthesis parameter is used to generate a waveform in a frame unit of 8 ms, for example, and sent to the waveform (speech) generation module <highlight><bold>103</bold></highlight>. </paragraph>
<paragraph id="P-0027" lvl="0"><number>&lsqb;0027&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 19</cross-reference> shows the speech generation process. A voice segment decoder <highlight><bold>301</bold></highlight> loads voice segment data from the voice segment dictionary <highlight><bold>105</bold></highlight> with a voice segment address of the synthesis parameter as a reference pointer and, if necessary, processes the signal. If a compression process has been applied to the dictionary <highlight><bold>105</bold></highlight>, which contains voice segment data for voice synthesis, a decoding process is applied to the dictionary <highlight><bold>105</bold></highlight>. The decoded voice segment data is multiplied by an amplitude coefficient in an amplitude controller <highlight><bold>302</bold></highlight> for making power control. The expansion/compression process of a voice segment is made in a voice segment processor <highlight><bold>303</bold></highlight> for making voice conversion. When a deep voice is desired, the voice segment is expanded and, when a thin voice is desired, the voice segment is compressed. In a superimposition controller <highlight><bold>304</bold></highlight>, superimposition of the segment data is controlled according to the information such as the pitch contour and phoneme duration to generate a synthetic waveform. The superimposed data is written sequentially into a digital/analog (D/A) ring buffer <highlight><bold>305</bold></highlight> and transferred to a D/A converter with an output sampling cycle for output from a speaker. </paragraph>
<paragraph id="P-0028" lvl="0"><number>&lsqb;0028&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 20</cross-reference> shows the phoneme duration determination process. The intermediate language analysis unit <highlight><bold>201</bold></highlight> feeds the analysis result into a control factor setting section <highlight><bold>601</bold></highlight>, where the control factors required to predict the duration length of each phoneme or word are set. The prediction uses pieces of information such as the phoneme, the kind of adjacent phonemes, the number of moras in the phrase, and the position in the sentence, which are sent to a duration estimation section <highlight><bold>602</bold></highlight>. The prediction of each of the accent and phrase component values uses a duration prediction table <highlight><bold>604</bold></highlight> that has been trained by using statistical analysis, such as Quantification theory (type one), based on the natural utterance data. The predicted result is sent to a duration correcting section <highlight><bold>603</bold></highlight> to correct the predicted value where the user designates the utterance speed. The utterance speed designation is controlled at five to 10 levels by multiplying each level by a predetermined constant. When a low utterance speed is desired, the phoneme duration is increased and, when a high utterance speed is desired, the phoneme duration is decreased. Suppose that there are five utterance speed levels and that Level 0 to Level 4 may be designated. A constant Tn for Level n is set as follows:</paragraph>
<paragraph lvl="0"><in-line-formula>To&equals;<highlight><bold>2</bold></highlight>.<highlight><bold>0</bold></highlight>, T<highlight><bold>1</bold></highlight>&equals;<highlight><bold>1</bold></highlight>.<highlight><bold>5</bold></highlight>, T<highlight><bold>2</bold></highlight>&equals;<highlight><bold>1</bold></highlight>.<highlight><bold>0</bold></highlight>, T<highlight><bold>3</bold></highlight>&equals;<highlight><bold>0</bold></highlight>.<highlight><bold>75</bold></highlight>, and T<highlight><bold>4</bold></highlight>&equals;<highlight><bold>0</bold></highlight>.<highlight><bold>5</bold></highlight></in-line-formula></paragraph>
<paragraph id="P-0029" lvl="7"><number>&lsqb;0029&rsqb;</number> Among the predicted phoneme durations, the vowel and pause lengths are multiplied by the constant Tn for the level n that is designated by the user. For Level 0, they are multiplied by 2.0 so that the generated waveform is lengthened while the utterance speed is shortened. For Level 4, they are multiplied by 0.5 so that the generated waveform is shortened and the utterance speed is raised. In the above example, Level 2 is made the normal utterance speed (default). </paragraph>
<paragraph id="P-0030" lvl="0"><number>&lsqb;0030&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 21</cross-reference> shows synthetic waveforms to which the utterance speed control has been applied. The utterance speed control of a phoneme duration is made only for the vowel. The length between closed sections or of a consonant is considered almost constant regardless of the utterance speed. In Graph (a) at a high utterance speed, only the vowel is multiplied by 0.5 and the number of superimposed voice segments is subtracted to make the waveform. Conversely, in Graph (c) at a low utterance speed, only the vowel is multiplied by 1.5 and the number of superimposed voice segment is repeated for making the waveform. Regarding the pause length, the constant for the designated level is multiplied so that the lower the utterance speed, the longer the pause length while the higher the utterance speed, the shorter the pause length. </paragraph>
<paragraph id="P-0031" lvl="0"><number>&lsqb;0031&rsqb;</number> Let consider the case of a high utterance speed, which corresponds to Level 4 in the above example. In the text-to-speech conversion system, the maximum utterance speed means &ldquo;Fast Reading Function (FRF)&rdquo;. In the text, there are both important and not-so important portions for the user so that the not-so important portion is read at a high utterance speed and the important portion is read at the normal utterance speed for synthetic speech. Most of all latest model has such an FRF button. When this button is held down, the utterance speed is set at the maximum level for synthesizing a speech at the highest utterance speed and, when the button is released, the utterance speed is returned to the previous level. </paragraph>
<paragraph id="P-0032" lvl="0"><number>&lsqb;0032&rsqb;</number> The above technology, however, has the following disadvantages. </paragraph>
<paragraph id="P-0033" lvl="0"><number>&lsqb;0033&rsqb;</number> (A) When FRF is turned on, merely the phoneme duration is decreased. In other words, the length of a generated waveform is reduced so that an additional load is applied to the speech generation module. In the speech generation module, the speech data generated upon waveform superimposition is written sequentially into the D/A ring buffer. Consequently, if the waveform length is small, the time for waveform generation becomes short. When the waveform data length becomes a half, the process time must be made a half. If the phoneme duration length becomes a half, the calculation amount does not necessarily becomes a half so that the &ldquo;voice interruption&rdquo; phenomenon, in which the synthetic voice stops before completion, can take place where the waveform generation cannot keep up with the transfer to the D/A converter. </paragraph>
<paragraph id="P-0034" lvl="0"><number>&lsqb;0034&rsqb;</number> (B) Also, the pitch contour is compressed linearly. That is, the intonation changes at shorter cycles and the synthetic voice is so unnatural that it is hard to understand. FRF is used not to skip the text but read it fast so that it is not suitable for the synthetic voice that has a very uneven intonation. The intonation of a speech synthesized with FRF changes so violently that the speech is difficult to understand. </paragraph>
<paragraph id="P-0035" lvl="0"><number>&lsqb;0035&rsqb;</number> (C) In addition, the pause between sentences is compressed with the same rate as the rate for the phoneme duration so that the boundary between sentences becomes too vague to distinguish. Synthetic speeches are outputted rapidly one after another so that the speeches synthesized with FRF are not suitable for understanding the text contents. </paragraph>
<paragraph id="P-0036" lvl="0"><number>&lsqb;0036&rsqb;</number> (D) Moreover, the utterance speed becomes high over the entire text so that it is difficult to time releasing FRF. The ordinary FRF reads the not-so important portion at high speeds and synthesizes a speech at the normal speed for the important portion of a text. When the user releases the FRF button, a considerable part of the desired portion has been read already. This makes it necessary to reset the reading section before starting speech synthesis at the normal utterance speed. In order to turn on or off FRF, the user must make great efforts in sorting out the necessary portion from the unnecessary one by listening to the unclear speech. </paragraph>
<paragraph id="P-0037" lvl="0"><number>&lsqb;0037&rsqb;</number> Accordingly, it is an object of the invention to provide a method of controlling the fast reading function (FRF) in a text-to-speech conversion system capable of solving the above problems (A) through (D). </paragraph>
<paragraph id="P-0038" lvl="0"><number>&lsqb;0038&rsqb;</number> In order to solve the problem (A), according to an aspect of the invention, when the utterance speed is designated at the maximum speed or FRF is turned on, the phoneme duration and the pitch contour are determined in the phoneme duration and pitch contour determination units, respectively, of the prosody generation module by replacing the duration prediction table predicted by statistical analysis with the duration rule table that has been found from experience and such a sound quality conversion coefficient as to keep the sound quality is selected in the sound quality determination unit. </paragraph>
<paragraph id="P-0039" lvl="0"><number>&lsqb;0039&rsqb;</number> In order to solve the problem (B), according to another aspect of the invention, when the utterance speed is designated at the maximum speed, neither calculation of the accent and phrase components nor change of the base pitch are made. </paragraph>
<paragraph id="P-0040" lvl="0"><number>&lsqb;0040&rsqb;</number> In order to solve the problem (C), according to still another aspect of the invention, when the utterance speed is designated at the maximum speed, a signal sound is inserted between sentences. </paragraph>
<paragraph id="P-0041" lvl="0"><number>&lsqb;0041&rsqb;</number> In order to solve the problem (D), according to yet another aspect of the invention, when the utterance speed is designated at the maximum speed, at least the leading word of a sentence is read at the normal utterance speed.</paragraph>
</section>
</summary-of-invention>
<brief-description-of-drawings>
<section>
<heading lvl="1">BRIEF DESCRIPTION OF THE DRAWINGS </heading>
<paragraph id="P-0042" lvl="0"><number>&lsqb;0042&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 1</cross-reference> is a block diagram of a prosody generation module according to the first embodiment of the invention; </paragraph>
<paragraph id="P-0043" lvl="0"><number>&lsqb;0043&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 2</cross-reference> is a block diagram of a pitch contour determination unit for the prosody generation module; </paragraph>
<paragraph id="P-0044" lvl="0"><number>&lsqb;0044&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 3</cross-reference> is a block diagram of a phoneme duration determination unit for the prosody generation module; </paragraph>
<paragraph id="P-0045" lvl="0"><number>&lsqb;0045&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 4</cross-reference> is a block diagram of a sound quality coefficient determination unit for the prosody generation module; </paragraph>
<paragraph id="P-0046" lvl="0"><number>&lsqb;0046&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 5</cross-reference> is a diagram of data re-sampling cycles for the sound quality conversion; </paragraph>
<paragraph id="P-0047" lvl="0"><number>&lsqb;0047&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 6</cross-reference> is a block diagram of a prosody generation module according to the second embodiment of the invention; </paragraph>
<paragraph id="P-0048" lvl="0"><number>&lsqb;0048&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 7</cross-reference> is a pitch contour determination unit according to the second embodiment of the invention; </paragraph>
<paragraph id="P-0049" lvl="0"><number>&lsqb;0049&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 8</cross-reference> is a flowchart of the pitch contour generation according to the second embodiment; </paragraph>
<paragraph id="P-0050" lvl="0"><number>&lsqb;0050&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 9</cross-reference> is a graph of pitch contours at different utterance speeds; </paragraph>
<paragraph id="P-0051" lvl="0"><number>&lsqb;0051&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 10</cross-reference> is a block diagram of a prosody generation module according to the third embodiment of the invention; </paragraph>
<paragraph id="P-0052" lvl="0"><number>&lsqb;0052&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 11</cross-reference> is a block diagram of a signal sound determination unit according to the third embodiment; </paragraph>
<paragraph id="P-0053" lvl="0"><number>&lsqb;0053&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 12</cross-reference> is a block diagram of a speech generation module according to the third embodiment; </paragraph>
<paragraph id="P-0054" lvl="0"><number>&lsqb;0054&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 13</cross-reference> is a block diagram of a phoneme duration determination unit according to the fourth embodiment; </paragraph>
<paragraph id="P-0055" lvl="0"><number>&lsqb;0055&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 14</cross-reference> is a flowchart of the phoneme duration determination according to the fourth embodiment; </paragraph>
<paragraph id="P-0056" lvl="0"><number>&lsqb;0056&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 15</cross-reference> is a block diagram of a common text-to-speech conversion system; </paragraph>
<paragraph id="P-0057" lvl="0"><number>&lsqb;0057&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 16</cross-reference> is a block diagram of a conventional prosody generation module; </paragraph>
<paragraph id="P-0058" lvl="0"><number>&lsqb;0058&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 17</cross-reference> is a diagram of a pitch contour generation model; </paragraph>
<paragraph id="P-0059" lvl="0"><number>&lsqb;0059&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 18</cross-reference> is a block diagram of a conventional pitch contour determination unit; </paragraph>
<paragraph id="P-0060" lvl="0"><number>&lsqb;0060&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 19</cross-reference> is a block diagram of a conventional speech generation module; </paragraph>
<paragraph id="P-0061" lvl="0"><number>&lsqb;0061&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 20</cross-reference> is a block diagram of a conventional phoneme duration determination unit; and </paragraph>
<paragraph id="P-0062" lvl="0"><number>&lsqb;0062&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 21</cross-reference> is a graph of waveforms at different utterance speeds.</paragraph>
</section>
</brief-description-of-drawings>
<detailed-description>
<section>
<heading lvl="1">DESCRIPTION OF THE PREFERRED EMBODIMENTS </heading>
<paragraph id="P-0063" lvl="7"><number>&lsqb;0063&rsqb;</number> First Embodiment </paragraph>
<paragraph id="P-0064" lvl="0"><number>&lsqb;0064&rsqb;</number> The first embodiment is different from the conventional system in that when the utterance speed is set at the maximum level or Fast Reading Function (FRF) is turned on, part of the inside process is simplified or omitted to reduce the load. </paragraph>
<paragraph id="P-0065" lvl="0"><number>&lsqb;0065&rsqb;</number> In <cross-reference target="DRAWINGS">FIG. 1, a</cross-reference> prosody generation module <highlight><bold>102</bold></highlight> receives the intermediate language from the text analysis module <highlight><bold>101</bold></highlight> identical with the conventional one and the prosody control parameters designated by the user. An intermediate language analysis unit <highlight><bold>801</bold></highlight> receives the intermediate language sentence by sentence and outputs the analysis results, such as the phoneme string, phrase, and accent information, to a pitch contour determination unit <highlight><bold>802</bold></highlight>, a phoneme duration determination unit <highlight><bold>803</bold></highlight>, a phoneme power determination unit <highlight><bold>804</bold></highlight>, a voice segment determination unit <highlight><bold>805</bold></highlight>, and a sound quality coefficient determination unit <highlight><bold>806</bold></highlight>, respectively. </paragraph>
<paragraph id="P-0066" lvl="0"><number>&lsqb;0066&rsqb;</number> In addition to the analysis results, the pitch contour determination unit <highlight><bold>802</bold></highlight> receives each of the intonation, pitch, speed, and speaker designated by the user and outputs a pitch contour a synthesis parameter (prosody) generation unit <highlight><bold>807</bold></highlight>. The &ldquo;pitch contour&rdquo; herein used means temporal changes of the fundamental frequency. </paragraph>
<paragraph id="P-0067" lvl="0"><number>&lsqb;0067&rsqb;</number> In addition to the analysis results, the phoneme duration determination unit <highlight><bold>803</bold></highlight> receives the utterance speed parameter designated by the user and outputs the phoneme duration and pause length data to the synthesis parameter generation unit <highlight><bold>807</bold></highlight>. </paragraph>
<paragraph id="P-0068" lvl="0"><number>&lsqb;0068&rsqb;</number> In addition to the analysis results, the phoneme power determination unit <highlight><bold>804</bold></highlight> receives the voice intensity parameter designated by the user and outputs the phoneme amplitude coefficient to the synthesis parameter generation unit <highlight><bold>807</bold></highlight>. </paragraph>
<paragraph id="P-0069" lvl="0"><number>&lsqb;0069&rsqb;</number> In addition to the analysis results, the voice segment determination unit <highlight><bold>805</bold></highlight> receives the speaker parameter designated by the user and outputs the voice segment address required for waveform superimposition to the synthesis parameter generation unit <highlight><bold>807</bold></highlight>. </paragraph>
<paragraph id="P-0070" lvl="0"><number>&lsqb;0070&rsqb;</number> In addition to the analysis results, the sound quality coefficient determination unit <highlight><bold>806</bold></highlight> receives each of the sound quality and utterance speed parameters designated by the user and outputs the sound quality conversion parameter to the synthesis parameter generation unit <highlight><bold>807</bold></highlight>. </paragraph>
<paragraph id="P-0071" lvl="0"><number>&lsqb;0071&rsqb;</number> Based on the input prosodic parameters, such as the pitch contour, phoneme duration, pause length, phoneme amplitude coefficient, voice segment address, and sound quality conversion coefficient, the synthesis parameter generation unit <highlight><bold>807</bold></highlight> generates and outputs a waveform generating parameter in a frame unit of, for example, 8 ms to the speech generation module <highlight><bold>103</bold></highlight>. </paragraph>
<paragraph id="P-0072" lvl="0"><number>&lsqb;0072&rsqb;</number> The prosody generation module <highlight><bold>102</bold></highlight> is different from the convention not only in that the utterance speed designating parameter is inputted to the pitch contour determination unit <highlight><bold>802</bold></highlight> and the sound quality coefficient determination unit <highlight><bold>806</bold></highlight> as well as the phoneme duration determination unit <highlight><bold>803</bold></highlight> but also in terms of the inside process of each of the pitch contour determination unit <highlight><bold>802</bold></highlight>, the phoneme duration determination <highlight><bold>803</bold></highlight>, and the sound quality coefficient determination unit <highlight><bold>806</bold></highlight>. The text analysis module <highlight><bold>101</bold></highlight> and the speech generation module <highlight><bold>103</bold></highlight> are the same as the conventions and, therefore, the description of their structure will be omitted. </paragraph>
<paragraph id="P-0073" lvl="0"><number>&lsqb;0073&rsqb;</number> In <cross-reference target="DRAWINGS">FIG. 2</cross-reference>, the accent and phrase components are determined by either statistical analysis, such as Quantification theory (type one), or rule. The control by rule uses a rule table <highlight><bold>910</bold></highlight> that has been made empirically while the control by statistical analysis uses a prediction table <highlight><bold>909</bold></highlight> that has been trained by using statistical analysis, such as Quantification theory (type one), based on the natural utterance data. The data output of the prediction table <highlight><bold>909</bold></highlight> is connected to a terminal (a) of a switch <highlight><bold>907</bold></highlight> while the data output of the rule table <highlight><bold>910</bold></highlight> is connected to a terminal (b) of the switch <highlight><bold>907</bold></highlight>. The output of a selector <highlight><bold>906</bold></highlight> determines which terminal (a) or (b) is used. </paragraph>
<paragraph id="P-0074" lvl="0"><number>&lsqb;0074&rsqb;</number> The utterance speed level designated by the user is inputted to the selector <highlight><bold>906</bold></highlight>, and the output is connected to the switch <highlight><bold>907</bold></highlight> for controlling the switch <highlight><bold>907</bold></highlight>. When the utterance speed is at the highest level, the output signal is connected to the terminal (b) while, otherwise, it is connected to the terminal (a). The output of the switch <highlight><bold>907</bold></highlight> is connected to the accent component determination section <highlight><bold>902</bold></highlight> and the phrase component determination section <highlight><bold>903</bold></highlight>. </paragraph>
<paragraph id="P-0075" lvl="0"><number>&lsqb;0075&rsqb;</number> The output of the intermediate language analysis section <highlight><bold>801</bold></highlight> is inputted to a control factor setting section <highlight><bold>901</bold></highlight> to analyze the factor parameters for the accent and phrase component determination, and the output is connected to the accent component determination section <highlight><bold>902</bold></highlight> and the phrase component determination section <highlight><bold>903</bold></highlight>. </paragraph>
<paragraph id="P-0076" lvl="0"><number>&lsqb;0076&rsqb;</number> The accent and phrase component determination sections <highlight><bold>902</bold></highlight> and <highlight><bold>903</bold></highlight> receive the output of the switch <highlight><bold>907</bold></highlight> and use the prediction or rule table <highlight><bold>909</bold></highlight> or <highlight><bold>910</bold></highlight> to determine and output respective component values to a pitch contour correction section <highlight><bold>904</bold></highlight>. In the pitch contour correction section <highlight><bold>904</bold></highlight> to which the intonation level designated by the user has been inputted, they are multiplied by a constant predetermined according to the level, and the results are inputted to a base pitch adding section <highlight><bold>905</bold></highlight>. </paragraph>
<paragraph id="P-0077" lvl="0"><number>&lsqb;0077&rsqb;</number> Also, the pitch level designated by the user, the speaker designation, and a base pitch table <highlight><bold>908</bold></highlight> are connected to the base pitch addition section <highlight><bold>905</bold></highlight>. The addition section <highlight><bold>905</bold></highlight> adds to the input from the pitch contour correction section <highlight><bold>904</bold></highlight> the constant value predetermined according to the user-designated pitch level and the sex and stored in the base pitch table <highlight><bold>908</bold></highlight> and outputs a pitch contour sequence data to a synthesis parameter generation unit <highlight><bold>807</bold></highlight>. </paragraph>
<paragraph id="P-0078" lvl="0"><number>&lsqb;0078&rsqb;</number> In <cross-reference target="DRAWINGS">FIG. 3</cross-reference>, the phoneme duration is determined by either statistical analysis, such as Quantification theory (type one), or rule. The control by rule uses a duration rule table <highlight><bold>1007</bold></highlight> that has been made empirically. The control by statistical analysis uses a duration prediction table <highlight><bold>1006</bold></highlight> that has been trained by statistical analysis, such as Quantification theory (type one), based on natural utterance data. The data output of the duration prediction table <highlight><bold>1006</bold></highlight> is connected to the terminal (a) of a switch <highlight><bold>1005</bold></highlight> while the output data of the duration rule table <highlight><bold>1007</bold></highlight> is connected to the terminal (b). The output of a selector <highlight><bold>1004</bold></highlight> determines which terminal is used. </paragraph>
<paragraph id="P-0079" lvl="0"><number>&lsqb;0079&rsqb;</number> The selector <highlight><bold>1004</bold></highlight> receives the utterance speed designated by the user and feeds the switch <highlight><bold>1005</bold></highlight> with a signal for controlling the switch <highlight><bold>1005</bold></highlight>. When the utterance speed is at the highest level, the switch <highlight><bold>1005</bold></highlight> selects the terminal (b) and, otherwise, the terminal (a). The output of the switch <highlight><bold>1005</bold></highlight> is connected to a duration determination section <highlight><bold>1002</bold></highlight>. </paragraph>
<paragraph id="P-0080" lvl="0"><number>&lsqb;0080&rsqb;</number> The control factor setting section <highlight><bold>1001</bold></highlight> receives the output of the intermediate language analysis unit <highlight><bold>801</bold></highlight>, analyzes the factor parameters for phoneme duration determination, and feeds its output to the duration determination section <highlight><bold>1002</bold></highlight>. </paragraph>
<paragraph id="P-0081" lvl="0"><number>&lsqb;0081&rsqb;</number> The duration determination section <highlight><bold>1002</bold></highlight> receives the output of the switch <highlight><bold>1005</bold></highlight>, determines the phoneme duration length using the duration prediction table <highlight><bold>1006</bold></highlight> or duration rule table <highlight><bold>1007</bold></highlight>, and feeds it to a duration correction section <highlight><bold>1003</bold></highlight>. The duration correction section <highlight><bold>1003</bold></highlight> also receives the utterance speed level designated by the user, multiplies the phoneme duration length by a constant predetermined according to the level for making correction, and feeds the result to the synthesis parameter generation unit <highlight><bold>807</bold></highlight>. </paragraph>
<paragraph id="P-0082" lvl="0"><number>&lsqb;0082&rsqb;</number> In <cross-reference target="DRAWINGS">FIG. 4</cross-reference>, the sound quality conversion is designated at five levels. A selector <highlight><bold>1102</bold></highlight> receives the utterance speed and sound quality levels designated by the user and feeds a switch <highlight><bold>1103</bold></highlight> with a signal for controlling the switch <highlight><bold>1103</bold></highlight>. The control signal turns on a terminal (c) unconditionally where the utterance speed is at the highest level and, otherwise, the terminal corresponding to the designated sound quality level. That is, the terminals (a), (b), (c), (d), or (e) is connected at the sound quality Level 0, 1, 2, 3, or 4, respectively. The respective terminals (a)-(e) are connected to a sound quality conversion coefficient table <highlight><bold>1104</bold></highlight> so that a corresponding sound quality coefficient data is outputted to a sound quality coefficient selection section <highlight><bold>1101</bold></highlight>. The sound quality coefficient selection section <highlight><bold>1101</bold></highlight> feeds the sound quality conversion coefficient to the synthesis parameter generation unit <highlight><bold>807</bold></highlight>. </paragraph>
<paragraph id="P-0083" lvl="0"><number>&lsqb;0083&rsqb;</number> In operation, only the parameter (prosody) generation process is different from the convention and, therefore, description of the other processes will be omitted. </paragraph>
<paragraph id="P-0084" lvl="0"><number>&lsqb;0084&rsqb;</number> The intermediate language generated by the text analysis module <highlight><bold>101</bold></highlight> is sent to the intermediate language analysis unit <highlight><bold>801</bold></highlight> of the prosody generation module <highlight><bold>102</bold></highlight>. The intermediate language analysis unit <highlight><bold>801</bold></highlight> extracts the data required for prosody generation from the phrase end symbol, word end symbol, accent symbol indicative of the accent nuclear, and the phoneme character string and sends it to the pitch contour determination unit <highlight><bold>802</bold></highlight>, phoneme duration determination unit <highlight><bold>803</bold></highlight>, phoneme power determination unit <highlight><bold>804</bold></highlight>, voice segment determination unit <highlight><bold>805</bold></highlight>, and sound quality coefficient determination unit <highlight><bold>806</bold></highlight>, respectively. </paragraph>
<paragraph id="P-0085" lvl="0"><number>&lsqb;0085&rsqb;</number> The pitch contour determination unit <highlight><bold>802</bold></highlight> generates an intonation indicating pitch changes, the phoneme duration determination unit <highlight><bold>803</bold></highlight> determines the pause length inserted between phrases or sentences as well as the phoneme duration. The phoneme power determination unit <highlight><bold>804</bold></highlight> generates a phoneme power indicating changes in the amplitude of a voice waveform. The voice segment determination unit <highlight><bold>805</bold></highlight> determines the address, in the voice segment dictionary <highlight><bold>105</bold></highlight>, of a voice segment required for a synthetic waveform generation. The sound quality coefficient determination unit <highlight><bold>806</bold></highlight> determines a parameter for processing the signal of voice segment data. Of the prosody control designations made by the user, the intonation and pitch designations are sent to the pitch contour determination unit <highlight><bold>802</bold></highlight>. The utterance speed designation is sent to the pitch contour, phoneme duration, and sound quality coefficient determination units <highlight><bold>802</bold></highlight>, <highlight><bold>803</bold></highlight>, and <highlight><bold>806</bold></highlight>, respectively. The intensity designation is sent to the voice power determination unit <highlight><bold>804</bold></highlight>, and the speaker designation is sent to the pitch contour and voice segment determination units <highlight><bold>802</bold></highlight> and <highlight><bold>805</bold></highlight>, respectively, and the sound quality designation is sent to the sound quality coefficient determination unit <highlight><bold>806</bold></highlight>. </paragraph>
<paragraph id="P-0086" lvl="0"><number>&lsqb;0086&rsqb;</number> Referring back to <cross-reference target="DRAWINGS">FIG. 2</cross-reference>, the operation of the pitch contour determination unit <highlight><bold>802</bold></highlight> will be described. The analysis result of the intermediate language analysis unit <highlight><bold>201</bold></highlight> is inputted to the control factor setting section <highlight><bold>901</bold></highlight>. The setting section <highlight><bold>901</bold></highlight> sets control factors required for determining the amplitudes of phrase and accent components. The data required for determining the amplitude of a phrase component is such information as the number of moras of a phrase, relative position in the sentence, and accent type of the leading word. The data required for determining the amplitude of an accent component is such information as the accent type of an accent phrase, the number of total moras, part of the speech, and relative position in the phrase. The value of such a component is determined by using the prediction table <highlight><bold>909</bold></highlight> or rule table <highlight><bold>910</bold></highlight>. The prediction table <highlight><bold>909</bold></highlight> has been trained by using statistical analysis, such as Quantification theory (type one), based on natural utterance data while the rule table <highlight><bold>910</bold></highlight> contains component values found from preparatory experiments. Quantification theory (type one) is will known and, therefore, its description will be omitted. When the output of the switch <highlight><bold>907</bold></highlight> is connected to the terminal (a), the prediction table <highlight><bold>909</bold></highlight> is selected while, when the output of the switch <highlight><bold>909</bold></highlight> is connected to the terminal (b), the rule table <highlight><bold>910</bold></highlight> is selected. </paragraph>
<paragraph id="P-0087" lvl="0"><number>&lsqb;0087&rsqb;</number> The utterance speed level designated by the user is inputted to the pitch contour determination unit <highlight><bold>802</bold></highlight> to actuate the switch <highlight><bold>907</bold></highlight> via the selector <highlight><bold>906</bold></highlight>. When the input utterance speed is at the highest level, the selector <highlight><bold>906</bold></highlight> feeds the switch <highlight><bold>907</bold></highlight> with a control signal for selecting the terminal (b). Conversely, if the input utterance speed is not at the highest level, it feeds the switch <highlight><bold>907</bold></highlight> with a control signal for selecting the terminal (a). For example, where the utterance speed is able to set at five levels from Level 0 to Level 4 wherein the larger the number, the higher the utterance speed, only when the input utterance speed is set at Level 4, the selector <highlight><bold>906</bold></highlight> feeds the switch <highlight><bold>907</bold></highlight> with a control signal for selecting the terminal (b) and, otherwise, selecting the terminal (a). That is, when the utterance speed is set at the highest level, the rule table <highlight><bold>910</bold></highlight> is selected and, otherwise, the prediction table <highlight><bold>909</bold></highlight> is selected. </paragraph>
<paragraph id="P-0088" lvl="0"><number>&lsqb;0088&rsqb;</number> The accent and phrase component determination sections <highlight><bold>902</bold></highlight> and <highlight><bold>903</bold></highlight> calculate the respective component vales using the selected table. When the prediction table <highlight><bold>909</bold></highlight> is selected, the amplitudes of both the accent and phrase components are determined by statistical analysis. Where the rule table <highlight><bold>910</bold></highlight> is selected, the amplitudes of the accent and phrase components are determined according to the predetermined rule. For example, the phrase component amplitude is determined by the position in the sentence. The leading, tailing, and intermediate phrase components of a sentence are assigned with respective values 0.3, 0.1, and 0.2, respectively. The accent component amplitude is assigned with a component value for each of such conditions whether the accent type is type one or not and whether the word is at the leading position in the phrase or not. This makes it possible to determine both the phrase and accent component values merely by looking up the table. The subject matter of the present application is to provide the contour determination unit with a mode that requires a smaller process amount and a shorter process time than those of the statistical analysis so that the rule making procedure is not limited to the above technique. </paragraph>
<paragraph id="P-0089" lvl="0"><number>&lsqb;0089&rsqb;</number> The intonation of the accent and phrase components is controlled in the pitch contour correction unit <highlight><bold>904</bold></highlight>, and the pitch control is made in the base pitch addition unit <highlight><bold>905</bold></highlight>. In the pitch contour correction unit <highlight><bold>904</bold></highlight>, the coefficient at the intonation level designated by the user is multiplied. The intonation control designation is made at three levels, for example. That is, the intonation is multiplied by 1.5 at Level 1, 1.0 at Level 2, and 0.5 at Level 3. </paragraph>
<paragraph id="P-0090" lvl="0"><number>&lsqb;0090&rsqb;</number> In the base pitch addition unit <highlight><bold>905</bold></highlight>, the constant according to the pitch or speaker (sex) designated by the user is added to the accent and phrase components, respectively, to output pitch contour sequence data to the synthesis parameter generation unit <highlight><bold>807</bold></highlight>. For example, in the system where the voice pitch is able to set at five levels from Level 0 to Level 4, wherein usual numbers are 3.0, 3,2, 3,4, 3,6, and 3.8 for the male voice and 4.0, 4.2, 4.4, 4.6, and 4.8 for the female voice. </paragraph>
<paragraph id="P-0091" lvl="0"><number>&lsqb;0091&rsqb;</number> In <cross-reference target="DRAWINGS">FIG. 3</cross-reference>, the analysis result is inputted from the intermediate language analysis module <highlight><bold>201</bold></highlight> to the control factor setting unit <highlight><bold>1001</bold></highlight>, where the control factors required to determine the phoneme duration (consonant, vowel, and closed section) and pause lengths. The data required to determine the phoneme duration include the type of the phoneme or phonemes adjacent the phrase, or the syllable position in the word or breath group. The data required for determining the pause length is the number of moras in adjacent phrases. The duration prediction or rule table <highlight><bold>1006</bold></highlight> or <highlight><bold>1007</bold></highlight> is used to determine these duration lengths. The duration prediction table <highlight><bold>1006</bold></highlight> has been trained by statistical analysis, such as Quantification theory (type one), based on natural utterance data. The duration rule table <highlight><bold>1007</bold></highlight> stores component values learned from preparatory experiments. The use of these tables is controlled by the switch <highlight><bold>1005</bold></highlight>. When the terminal (a) is connected to the output of the switch <highlight><bold>1005</bold></highlight>, the duration prediction table <highlight><bold>1006</bold></highlight> is selected while the terminal (b) is connected, the duration rule table <highlight><bold>1007</bold></highlight> is selected. </paragraph>
<paragraph id="P-0092" lvl="0"><number>&lsqb;0092&rsqb;</number> The user-designated utterance speed level, which has been inputted to the phoneme duration determination unit <highlight><bold>803</bold></highlight>, actuates the switch <highlight><bold>1005</bold></highlight> via the selector <highlight><bold>1004</bold></highlight>. When the input utterance speed level is at the maximum speed, a control signal for connecting the terminal (b) is outputted from the selector <highlight><bold>1004</bold></highlight>. Conversely, when the input utterance speed is not at the maximum level, a control signal for connecting the terminal (a) is outputted. </paragraph>
<paragraph id="P-0093" lvl="0"><number>&lsqb;0093&rsqb;</number> The selected table is used in the duration determination unit <highlight><bold>1002</bold></highlight> to calculate the phoneme duration and pause lengths. When the duration prediction table <highlight><bold>1006</bold></highlight> is selected, statistical analysis is employed. When the duration rule table <highlight><bold>1007</bold></highlight> is selected, determination is made by the predetermined rule. For the phoneme duration rule, for example, a fundamental length is assigned according to the type of phoneme or the position in the sentence. The average value of a large amount of natural utterance data for each phoneme may be made the fundamental length. The pause length is either set at 300 ms or made so as to be determined only by referring to the table. The subject matter of the present application is to provide the phoneme duration determination unit with such a mode as to make the process amount and time less than those of statistical analysis so that the rule making procedure is not limited to the above technique. </paragraph>
<paragraph id="P-0094" lvl="0"><number>&lsqb;0094&rsqb;</number> The thus determined duration is sent to the duration correction section <highlight><bold>1003</bold></highlight>, to which the user-designated utterance speed level has been inputted, and the phoneme duration is expanded or compressed according to the level. Usually, the utterance speed designation is controlled at five to 10 levels by multiplying the vowel or pause duration by the constant that has been assigned to each level. When a low utterance speed is desired, the phoneme duration is lengthened while, when a high utterance speed is desired, the phoneme duration is shortened. </paragraph>
<paragraph id="P-0095" lvl="0"><number>&lsqb;0095&rsqb;</number> In <cross-reference target="DRAWINGS">FIG. 4</cross-reference>, the user-designated sound quality conversion and utterance speed levels are inputted to the sound quality coefficient determination unit <highlight><bold>806</bold></highlight>. These prosodic parameters are used to control the switch <highlight><bold>1103</bold></highlight> via the selector <highlight><bold>1102</bold></highlight>, where the utterance speed level is determined. When the utterance speed is at the maximum speed level, the terminal (c) is connected to the output of the switch <highlight><bold>1103</bold></highlight> and, otherwise, the sound quality conversion level is determined by controlling the switch <highlight><bold>1103</bold></highlight> so that the terminal corresponding to the sound quality level is connected. When the sound quality designation is Level 0, 1, 2, 3, or 4, the terminal (a), (b), (c), (d), or (e) is connected. That is, the respective terminals (a)-(b) are connected to the sound quality conversion coefficient table <highlight><bold>1104</bold></highlight> to retrieve the corresponding sound quality conversion coefficient data. </paragraph>
<paragraph id="P-0096" lvl="0"><number>&lsqb;0096&rsqb;</number> The expansion/compression coefficients of voice segments are stored in the sound quality conversion coefficient table <highlight><bold>1104</bold></highlight>. For example, the expansion/compression coefficient Kn corresponding to the sound quality level n is determined as follows.</paragraph>
<paragraph lvl="0"><in-line-formula>Ko&equals;<highlight><bold>2</bold></highlight>.<highlight><bold>0</bold></highlight>, K<highlight><bold>1</bold></highlight>&equals;<highlight><bold>1</bold></highlight>.<highlight><bold>5</bold></highlight>, K<highlight><bold>2</bold></highlight>&equals;<highlight><bold>1</bold></highlight>.<highlight><bold>0</bold></highlight>, K<highlight><bold>3</bold></highlight>&equals;<highlight><bold>0</bold></highlight>.<highlight><bold>8</bold></highlight>, K<highlight><bold>4</bold></highlight>&equals;<highlight><bold>0</bold></highlight>.<highlight><bold>5</bold></highlight></in-line-formula></paragraph>
<paragraph id="P-0097" lvl="7"><number>&lsqb;0097&rsqb;</number> The voice segment length is multiplied by Kn and the waveform is superimposed to generate a synthetic voice. At Level 2, the coefficient is 1.0 so that no sound quality conversion is made. When the terminal (a) is connected, the coefficient Ko is selected and sent to the sound quality selection section <highlight><bold>1101</bold></highlight>. When the terminal (b) is connected, the coefficient K<highlight><bold>1</bold></highlight> is selected and sent to the sound quality selection section <highlight><bold>1101</bold></highlight> and so on. </paragraph>
<paragraph id="P-0098" lvl="0"><number>&lsqb;0098&rsqb;</number> In <cross-reference target="DRAWINGS">FIG. 5</cross-reference>, if Xnm is defined as the m-th sample of voice segment data at a sound quality conversion level n, the data sequence after sound quality conversion is calculated as follows: </paragraph>
<paragraph id="P-0099" lvl="0"><number>&lsqb;0099&rsqb;</number> At Level 0,</paragraph>
<paragraph lvl="0"><in-line-formula><highlight><italic>X</italic></highlight><highlight><subscript>00</subscript></highlight><highlight><italic>&equals;X</italic></highlight><highlight><subscript>20</subscript></highlight></in-line-formula></paragraph>
<paragraph lvl="0"><in-line-formula><highlight><italic>X</italic></highlight><highlight><subscript>01</subscript></highlight><highlight><italic>&equals;X</italic></highlight><highlight><subscript>20</subscript></highlight>&times;&frac12;<highlight><italic>&plus;X</italic></highlight><highlight><subscript>21</subscript></highlight>&times;&frac12;</in-line-formula></paragraph>
<paragraph lvl="0"><in-line-formula><highlight><italic>X</italic></highlight><highlight><subscript>02</subscript></highlight><highlight><italic>&equals;X</italic></highlight><highlight><subscript>21</subscript></highlight></in-line-formula></paragraph>
<paragraph id="P-0100" lvl="0"><number>&lsqb;0100&rsqb;</number> At Level 1,</paragraph>
<paragraph lvl="0"><in-line-formula><highlight><italic>X</italic></highlight><highlight><subscript>10</subscript></highlight><highlight><italic>&equals;X</italic></highlight><highlight><subscript>20</subscript></highlight></in-line-formula></paragraph>
<paragraph lvl="0"><in-line-formula><highlight><italic>X</italic></highlight><highlight><subscript>11</subscript></highlight><highlight><italic>&equals;X</italic></highlight><highlight><subscript>20</subscript></highlight>&times;&frac13;<highlight><italic>&plus;X</italic></highlight><highlight><subscript>21</subscript></highlight>&times;&frac23;</in-line-formula></paragraph>
<paragraph lvl="0"><in-line-formula><highlight><italic>X</italic></highlight><highlight><subscript>12</subscript></highlight><highlight><italic>&equals;X</italic></highlight><highlight><subscript>21</subscript></highlight>&times;&frac23;<highlight><italic>&plus;X</italic></highlight><highlight><subscript>22</subscript></highlight>&times;&frac13;</in-line-formula></paragraph>
<paragraph lvl="0"><in-line-formula><highlight><italic>X</italic></highlight><highlight><subscript>13</subscript></highlight><highlight><italic>&equals;X</italic></highlight><highlight><subscript>22</subscript></highlight></in-line-formula></paragraph>
<paragraph id="P-0101" lvl="0"><number>&lsqb;0101&rsqb;</number> At Level 3,</paragraph>
<paragraph lvl="0"><in-line-formula><highlight><italic>X</italic></highlight><highlight><subscript>30</subscript></highlight><highlight><italic>&equals;X</italic></highlight><highlight><subscript>20</subscript></highlight></in-line-formula></paragraph>
<paragraph lvl="0"><in-line-formula><highlight><italic>X</italic></highlight><highlight><subscript>31</subscript></highlight><highlight><italic>&equals;X</italic></highlight><highlight><subscript>21</subscript></highlight>&times;&frac34;<highlight><italic>&plus;X</italic></highlight><highlight><subscript>22</subscript></highlight>&times;&frac14;</in-line-formula></paragraph>
<paragraph lvl="0"><in-line-formula><highlight><italic>X</italic></highlight><highlight><subscript>32</subscript></highlight><highlight><italic>&equals;X</italic></highlight><highlight><subscript>22</subscript></highlight>&times;&frac12;<highlight><italic>&plus;X</italic></highlight><highlight><subscript>23</subscript></highlight>&times;&frac12;</in-line-formula></paragraph>
<paragraph lvl="0"><in-line-formula><highlight><italic>X</italic></highlight><highlight><subscript>33</subscript></highlight><highlight><italic>&equals;X</italic></highlight><highlight><subscript>23</subscript></highlight>&times;&frac14;<highlight><italic>&plus;X</italic></highlight><highlight><subscript>24</subscript></highlight>&times;&frac34;</in-line-formula></paragraph>
<paragraph lvl="0"><in-line-formula><highlight><italic>X</italic></highlight><highlight><subscript>34</subscript></highlight><highlight><italic>&equals;X</italic></highlight><highlight><subscript>25</subscript></highlight></in-line-formula></paragraph>
<paragraph id="P-0102" lvl="0"><number>&lsqb;0102&rsqb;</number> At Level 4,</paragraph>
<paragraph lvl="0"><in-line-formula><highlight><italic>X</italic></highlight><highlight><subscript>40</subscript></highlight><highlight><italic>&equals;X</italic></highlight><highlight><subscript>20</subscript></highlight></in-line-formula></paragraph>
<paragraph lvl="0"><in-line-formula><highlight><italic>X</italic></highlight><highlight><subscript>41</subscript></highlight><highlight><italic>&equals;X</italic></highlight><highlight><subscript>22</subscript></highlight></in-line-formula></paragraph>
<paragraph id="P-0103" lvl="7"><number>&lsqb;0103&rsqb;</number> wherein X2n is the data sequence before conversion. It should be noted that the foregoing is mere an example for the sound quality conversion. According to the first embodiment of the invention, the sound quality coefficient determination unit has such a function that when the utterance speed is at the maximum speed level, the sound quality conversion designation is made invalid to reduce the process time. </paragraph>
<paragraph id="P-0104" lvl="0"><number>&lsqb;0104&rsqb;</number> As has been described above, according to the first embodiment of the invention, when the utterance speed is set at the maximum level, the text-to-speech conversion system simplifies or invalidates the function block having a heavy process load so that the sound interruption due to the heavy load is minimized to generate an easy-to-understand synthetic speech. </paragraph>
<paragraph id="P-0105" lvl="0"><number>&lsqb;0105&rsqb;</number> The prosody properties, such as the pitch and duration, are slightly different from those of the synthetic voice at utterance speeds other than the maximum speed, and the sound quality conversion function is made invalid in this embodiment, but the synthetic speech output at the maximum utterance speed is used generally for &ldquo;FRF&rdquo; in which it is important only to understand the contents of a text so that these drawbacks are more tolerable than the sound interruption. </paragraph>
<paragraph id="P-0106" lvl="7"><number>&lsqb;0106&rsqb;</number> Second Embodiment </paragraph>
<paragraph id="P-0107" lvl="0"><number>&lsqb;0107&rsqb;</number> This embodiment is different from the convention in that when the utterance speed is set at the maximum level or FRF is turned on, the pitch contour generation process is changed. Accordingly, only the prosody generation module and the pitch contour determination unit that are different from the convention will be described. </paragraph>
<paragraph id="P-0108" lvl="0"><number>&lsqb;0108&rsqb;</number> In <cross-reference target="DRAWINGS">FIG. 6</cross-reference>, the prosody generation module <highlight><bold>102</bold></highlight> receives the intermediate language from the text analysis module <highlight><bold>101</bold></highlight> and the prosodic parameters designated by the user. An intermediate language analysis unit <highlight><bold>1301</bold></highlight> receives the intermediate language sentence by sentence and outputs the intermediate language analysis results, such as a phoneme string, phrase information, and accent information, that are required for subsequent prosody generation process to a pitch contour determination unit <highlight><bold>1302</bold></highlight>, a phoneme duration determination unit <highlight><bold>1303</bold></highlight>, a phoneme power determination unit <highlight><bold>1304</bold></highlight>, a voice segment determination unit <highlight><bold>1305</bold></highlight>, and a sound quality coefficient determination unit <highlight><bold>1306</bold></highlight>, respectively. </paragraph>
<paragraph id="P-0109" lvl="0"><number>&lsqb;0109&rsqb;</number> The pitch contour determination unit <highlight><bold>1302</bold></highlight> receives the intermediate language analysis results and each of the user-designated intonation, pitch, utterance speed, and speaker parameters and outputs a pitch contour to a synthetic parameter generation unit <highlight><bold>1307</bold></highlight>. </paragraph>
<paragraph id="P-0110" lvl="0"><number>&lsqb;0110&rsqb;</number> The phoneme duration determination unit <highlight><bold>1303</bold></highlight> receives the intermediate analysis results and the user-designated utterance speed parameter and outputs data, such as respective phoneme duration and pause lengths, to the synthetic parameter generation unit <highlight><bold>1307</bold></highlight>. </paragraph>
<paragraph id="P-0111" lvl="0"><number>&lsqb;0111&rsqb;</number> The phoneme power determination unit <highlight><bold>1304</bold></highlight> receives the intermediate language analysis results and the user-designated intensity parameter and outputs respective phoneme amplitude coefficients to the synthetic parameter generation unit <highlight><bold>1307</bold></highlight>. </paragraph>
<paragraph id="P-0112" lvl="0"><number>&lsqb;0112&rsqb;</number> The voice segment determination unit <highlight><bold>1305</bold></highlight> receives the intermediate language analysis results and the user-designated speaker parameter and outputs a phoneme segment address necessary for waveform superimposition to the synthetic parameter generation unit <highlight><bold>1307</bold></highlight>. </paragraph>
<paragraph id="P-0113" lvl="0"><number>&lsqb;0113&rsqb;</number> The sound quality coefficient determination unit <highlight><bold>1306</bold></highlight> receives the intermediate language analysis results and the user-designated sound quality and utterance speed parameters and outputs a sound quality conversion coefficient to the synthetic parameter generation unit <highlight><bold>1307</bold></highlight>. </paragraph>
<paragraph id="P-0114" lvl="0"><number>&lsqb;0114&rsqb;</number> The synthetic parameter generation unit <highlight><bold>1307</bold></highlight> converts the input prosodic parameters (pitch contour, phoneme duration, pause length, phoneme amplitude coefficient, voice segment address, and sound conversion coefficient) into a waveform generation parameter in a frame of approximately 8 ms and outputs it to the waveform or speech generation module <highlight><bold>103</bold></highlight>. </paragraph>
<paragraph id="P-0115" lvl="0"><number>&lsqb;0115&rsqb;</number> The prosody generation module <highlight><bold>102</bold></highlight> is different from the convention in that the utterance speed parameter is inputted to both the phoneme duration determination unit <highlight><bold>1303</bold></highlight> and the pitch contour determination unit <highlight><bold>1302</bold></highlight>, and in the process inside the pitch contour determination unit <highlight><bold>1302</bold></highlight>. The structures of the text analysis and speech generation modules <highlight><bold>101</bold></highlight> and <highlight><bold>103</bold></highlight> are identical with the conventions and, therefore, their description will be omitted. Also, the structure of the prosody generation module <highlight><bold>102</bold></highlight> is identical with the convention except for the pitch contour determination unit <highlight><bold>1302</bold></highlight> and, therefore, its description will be omitted. </paragraph>
<paragraph id="P-0116" lvl="0"><number>&lsqb;0116&rsqb;</number> In <cross-reference target="DRAWINGS">FIG. 7, a</cross-reference> control factor setting section <highlight><bold>1401</bold></highlight> receives the output from the intermediate language analysis unit <highlight><bold>1301</bold></highlight>, and analyzes and outputs a factor parameter for determination of both accent and phrase components to access and phrase component determination sections <highlight><bold>1402</bold></highlight> and <highlight><bold>1403</bold></highlight>, respectively. </paragraph>
<paragraph id="P-0117" lvl="0"><number>&lsqb;0117&rsqb;</number> The accent and phrase determination sections <highlight><bold>1402</bold></highlight> and <highlight><bold>1403</bold></highlight> are connected to a prediction table <highlight><bold>1408</bold></highlight> and predict the amplitudes of the respective components by using statistical analysis such as Quantification theory (type one). The predicted accent and phrase component values are inputted to a pitch contour correction section <highlight><bold>1404</bold></highlight>. </paragraph>
<paragraph id="P-0118" lvl="0"><number>&lsqb;0118&rsqb;</number> The pitch contour correction section <highlight><bold>104</bold></highlight> receives the intonation level designated by the user, multiplies the accent and phrase components by the constant predetermined according to the level, and outputs the result to the terminal (a) of a switch <highlight><bold>1405</bold></highlight>. The switch <highlight><bold>1405</bold></highlight> includes a terminal (b), and a selector <highlight><bold>1406</bold></highlight> outputs a control signal for selecting either the terminal (a) or (b). </paragraph>
<paragraph id="P-0119" lvl="0"><number>&lsqb;0119&rsqb;</number> The selector <highlight><bold>1406</bold></highlight> receives the utterance speed level designated by the user and outputs a control signal for selecting the terminal (b) when the utterance speed is at the maximum level and, otherwise, the terminal (a) of the switch <highlight><bold>1405</bold></highlight>. The terminal (b) is grounded so that when the terminal (a) is selected or valid, the switch <highlight><bold>1405</bold></highlight> outputs the output of the pitch contour correction section <highlight><bold>1404</bold></highlight> and, when the terminal (b) is valid, it outputs 0 to a base pitch addition section <highlight><bold>1407</bold></highlight>. </paragraph>
<paragraph id="P-0120" lvl="0"><number>&lsqb;0120&rsqb;</number> The base pitch addition section <highlight><bold>1407</bold></highlight> receives the pitch level and speaker designated by the user, and data from a base pitch table <highlight><bold>1409</bold></highlight>. The base pitch table <highlight><bold>1409</bold></highlight> stores constants predetermined according to the pitch level and the sex of the speaker. The base pitch addition section <highlight><bold>1407</bold></highlight> adds a constant from the table <highlight><bold>1409</bold></highlight> to the input from the switch <highlight><bold>1405</bold></highlight> and outputs a pitch contour sequential data to the synthesis parameter generation unit <highlight><bold>1307</bold></highlight>. </paragraph>
<paragraph id="P-0121" lvl="0"><number>&lsqb;0121&rsqb;</number> In operation, the intermediate language generated by the text analysis module <highlight><bold>101</bold></highlight> is sent to the intermediate language analysis unit <highlight><bold>1301</bold></highlight> of the prosody generation module <highlight><bold>102</bold></highlight>. In the intermediate language analysis unit <highlight><bold>1301</bold></highlight>, the data necessary for prosody generation is extracted from the phrase end symbol, word end symbol, accent symbol indicative of the accent nuclear, and phoneme character string and sent to each of the pitch contour, phoneme duration, phoneme power, voice segment, and sound quality coefficient determination units <highlight><bold>1302</bold></highlight>, <highlight><bold>1303</bold></highlight>, <highlight><bold>1304</bold></highlight>, <highlight><bold>1305</bold></highlight>, and <highlight><bold>1306</bold></highlight>, respectively. </paragraph>
<paragraph id="P-0122" lvl="0"><number>&lsqb;0122&rsqb;</number> In the pitch contour determination unit <highlight><bold>1302</bold></highlight>, the intonation or transition of the pitch is generated and, in the phoneme duration determination unit <highlight><bold>1303</bold></highlight>, the duration of each phoneme and the pause length between phrases or sentences are determined. In the phoneme power determination unit <highlight><bold>1304</bold></highlight>, the phoneme power or transition of the voice waveform amplitude is generated and, in the voice segment determination unit <highlight><bold>1305</bold></highlight>, the address, in the voice segment dictionary <highlight><bold>105</bold></highlight>, of a voice segment necessary for synthetic waveform generation is determined. In the sound quality coefficient determination unit <highlight><bold>1306</bold></highlight>, the parameter for processing the voice segment data by signal process is determined. </paragraph>
<paragraph id="P-0123" lvl="0"><number>&lsqb;0123&rsqb;</number> Among the various prosody control designations, the intonation and pitch designations are sent to the pitch contour determination unit <highlight><bold>1302</bold></highlight>, the utterance speed designation is sent to the pitch contour determination unit <highlight><bold>1302</bold></highlight>, the intensity designation is sent to the phoneme power determination unit <highlight><bold>1304</bold></highlight>, the speaker designation is sent to the pitch contour and voice segment determination units <highlight><bold>1302</bold></highlight> and <highlight><bold>1305</bold></highlight>, and the sound quality designation is sent to the sound quality coefficient determination unit <highlight><bold>1306</bold></highlight>. </paragraph>
<paragraph id="P-0124" lvl="0"><number>&lsqb;0124&rsqb;</number> In <cross-reference target="DRAWINGS">FIG. 7</cross-reference>, only the process for pitch contour generation is different from the conventional one and, therefore, the description of the other process will be omitted. The analysis results are inputted from the intermediate language analysis module <highlight><bold>201</bold></highlight> to the control factor setting section <highlight><bold>1401</bold></highlight>, wherein the control factors necessary for predicting the amplitudes of phrase and accent components are set. The data necessary for prediction of the amplitude of a phrase component include the number of malas that constitute the phrase, the relative position in the sentence, and the accent type of the leading word. The data necessary for prediction of the amplitude of an accent component include the accent type of the accent phrase, the number of moras, part of the speech, and relative position in the phrase. These component values are determined by using the prediction table <highlight><bold>1408</bold></highlight> that has been trained by using statistical analysis, such as Quantification theory (type one), based on the natural utterance data. Quantification theory (type one) is well known and, therefore, its description will be omitted. </paragraph>
<paragraph id="P-0125" lvl="0"><number>&lsqb;0125&rsqb;</number> The prediction control factors analyzed in the control factor setting section <highlight><bold>1401</bold></highlight> are sent to the accent and phrase component determination sections <highlight><bold>1402</bold></highlight> and <highlight><bold>1403</bold></highlight>, respectively, wherein the amplitude of each of the accent and phrase components is predicted by using the prediction table <highlight><bold>1408</bold></highlight>. As in the first embodiment, each component value may be determined by rule. The calculated accent and phrase components are sent to the pitch contour correction section <highlight><bold>1404</bold></highlight>, wherein they are multiplied by the coefficient corresponding to the intonation level designated by the user. </paragraph>
<paragraph id="P-0126" lvl="0"><number>&lsqb;0126&rsqb;</number> The user-designated intonation is set at three levels, for example, from Level 1 to Level 3, and it is multiplied by 1.5 at Level 1, 1.0 at Level 2, and 0.5 at Level 3. </paragraph>
<paragraph id="P-0127" lvl="0"><number>&lsqb;0127&rsqb;</number> The corrected accent and phrase components are sent to the terminal (a) of the switch <highlight><bold>1405</bold></highlight>. The terminal (a) or (b) of the switch <highlight><bold>1405</bold></highlight> is connected responsive to the control signal from the selector <highlight><bold>1406</bold></highlight>. Always, 0 is inputted to the terminal (b). </paragraph>
<paragraph id="P-0128" lvl="0"><number>&lsqb;0128&rsqb;</number> The user inputs the utterance speed level to the selector <highlight><bold>1406</bold></highlight> for output control. When the input utterance speed is at the maximum level, the selector <highlight><bold>1406</bold></highlight> issues a control signal for connecting the terminal (b). Conversely, when the input utterance speed is not at the maximum level, it issues a control signal for connecting the terminal (a). If the utterance speed may vary at five levels from Level 0 to Level 4, wherein the higher the level, the higher the utterance speed, it issues a control signal for connecting the terminal (b) only when the input utterance speed is at Level 4 and, otherwise, a control signal for connecting the terminal (a). That is, when the utterance speed is at the highest level, 0 is selected and, otherwise, the corrected accent and phrase component values from the pitch contour correction section <highlight><bold>1404</bold></highlight> are selected. </paragraph>
<paragraph id="P-0129" lvl="0"><number>&lsqb;0129&rsqb;</number> The selected data is sent to the base pitch addition section <highlight><bold>1407</bold></highlight>. The base pitch addition section <highlight><bold>1407</bold></highlight>, into which the pitch designation level is inputted by the user, retrieves the base pitch data corresponding to the level from the base pitch table <highlight><bold>1409</bold></highlight>, adds it to the output value from the switch <highlight><bold>1405</bold></highlight>, and outputs a pitch contour sequential data to the synthesis parameter generation unit <highlight><bold>1307</bold></highlight>. </paragraph>
<paragraph id="P-0130" lvl="0"><number>&lsqb;0130&rsqb;</number> In the system wherein the pitch can be set at five levels from Level 0 to Level 4, for example, the usual data stored in the base pitch table <highlight><bold>1409</bold></highlight> are numbers such as 3.0, 3.2, 3.4, 3.6, and 3.8 for the male voice and 4.0, 4.2, 4.4, 4.6, and 4.8 for the female voice. </paragraph>
<paragraph id="P-0131" lvl="0"><number>&lsqb;0131&rsqb;</number> When the utterance speed designation is at the highest level, the process from the control factor setting section <highlight><bold>1401</bold></highlight> to the pitch contour correction section <highlight><bold>1404</bold></highlight> is not necessary. </paragraph>
<paragraph id="P-0132" lvl="0"><number>&lsqb;0132&rsqb;</number> In <cross-reference target="DRAWINGS">FIG. 8, I</cross-reference> is the number of phrases in the input sentence, J is the number of words, Api is the amplitude of an i-th phrase component, Aaj is the amplitude of a j-th accent component, and Ej is the intonation control coefficient designated for the j-th accent phrase. </paragraph>
<paragraph id="P-0133" lvl="0"><number>&lsqb;0133&rsqb;</number> The amplitude of a phrase component, Api, is calculated from Step ST<highlight><bold>101</bold></highlight> to ST<highlight><bold>106</bold></highlight>. In ST<highlight><bold>101</bold></highlight>, the phrase counter i is initialized. In ST<highlight><bold>102</bold></highlight>, the utterance speed level is determined and, when the utterance speed is at the highest level, the process goes to ST<highlight><bold>104</bold></highlight> and, otherwise, to ST<highlight><bold>103</bold></highlight>. In ST<highlight><bold>104</bold></highlight>, the amplitude of the i-th phrase, Api, is set at 0 and the process goes to ST<highlight><bold>105</bold></highlight>. In ST<highlight><bold>103</bold></highlight>, the amplitude of the i-th phrase component, Api, is predicted by using statistical analysis, such as Quantification theory (type one), and the process goes to ST<highlight><bold>105</bold></highlight>. In ST<highlight><bold>105</bold></highlight>, the phrase counter i is incremented by one. In ST<highlight><bold>106</bold></highlight>, it is compared with the number of phrases, I, in the input sentence. When it exceeds the number of phrases, I, or the process for all the phrases is completed, the phrase component generation process is terminated and the process goes to ST<highlight><bold>107</bold></highlight>. Otherwise, the process returns to ST<highlight><bold>102</bold></highlight> to repeat the above process for the next phrase. </paragraph>
<paragraph id="P-0134" lvl="0"><number>&lsqb;0134&rsqb;</number> The amplitude of an accent component, Aaj, is calculated in steps from ST<highlight><bold>107</bold></highlight> to ST<highlight><bold>113</bold></highlight>. In ST<highlight><bold>107</bold></highlight>, the word counter j is initialized to 0. In ST<highlight><bold>108</bold></highlight>, the utterance speed level is determined. When the utterance speed is at the highest level, the process goes to ST<highlight><bold>111</bold></highlight> and, otherwise, goes to ST<highlight><bold>109</bold></highlight>. In ST<highlight><bold>111</bold></highlight>, the amplitude of the j-th accent component, Aaj, is set at 0 and the process goes to ST<highlight><bold>112</bold></highlight>. In ST<highlight><bold>109</bold></highlight>, the amplitude of the j-th accent component, Aaj, is predicted by using statistical analysis, such as Quantification theory (type one), and the process goes to ST<highlight><bold>110</bold></highlight>. In ST<highlight><bold>110</bold></highlight>, the intonation correction to the j-th accent phrase is made by the following equation</paragraph>
<paragraph lvl="0"><in-line-formula><highlight><italic>A</italic></highlight><highlight><subscript>aj</subscript></highlight><highlight><italic>&equals;A</italic></highlight><highlight><subscript>aj</subscript></highlight><highlight><italic>&times;E</italic></highlight><highlight><subscript>j</subscript></highlight>&emsp;&emsp;(4)</in-line-formula></paragraph>
<paragraph id="P-0135" lvl="7"><number>&lsqb;0135&rsqb;</number> wherein Ej is the intonation control coefficient predetermined corresponding to the intonation control level designated by the user. For example, if it is provided at three levels, wherein the intonation is multiplied by 1.5 at Level 0, 1.0 at Level 1, and 0.5 at Level 3, Ej is given as follows.</paragraph>
<paragraph lvl="0"><in-line-formula>Level 0 (Intonation&times;1.5) <highlight><italic>Ej&equals;</italic></highlight>1.5</in-line-formula></paragraph>
<paragraph lvl="0"><in-line-formula>Level 1 (Intonation&times;1.0) <highlight><italic>Ej&equals;</italic></highlight>1.0</in-line-formula></paragraph>
<paragraph lvl="0"><in-line-formula>Level 2 (Intonation&times;0.5) <highlight><italic>Ej&equals;</italic></highlight>0.5</in-line-formula></paragraph>
<paragraph id="P-0136" lvl="7"><number>&lsqb;0136&rsqb;</number> After the intonation correction is completed, the process goes to ST<highlight><bold>112</bold></highlight>. In ST<highlight><bold>112</bold></highlight>, the word counter j is incremented by one. In ST<highlight><bold>113</bold></highlight>, it is compared with the number of words, J, in the input sentence. When the word counter j exceeds the number or words, J, or the process for all the words is completed, the accent component generation process is terminated and the process goes to ST<highlight><bold>114</bold></highlight>. Otherwise, the process returns to ST<highlight><bold>108</bold></highlight> to repeat the above process for the next accent phrase. </paragraph>
<paragraph id="P-0137" lvl="0"><number>&lsqb;0137&rsqb;</number> In ST<highlight><bold>114</bold></highlight>, a pitch contour is generated from the phrase component amplitude, Api, the accent component amplitude, Aaj, and the base pitch, ln Fmin, which is obtained by referring to the base pitch table <highlight><bold>1409</bold></highlight>, by using Equation (1). </paragraph>
<paragraph id="P-0138" lvl="0"><number>&lsqb;0138&rsqb;</number> As has been described above, according to the second embodiment of the invention, when the utterance speed is set at the highest level, the intonation component of the pitch contour is made 0 for pitch contour generation so that the intonation does not change at short cycles, thus avoiding the generation of a hard-to-listen synthetic voice. </paragraph>
<paragraph id="P-0139" lvl="0"><number>&lsqb;0139&rsqb;</number> In <cross-reference target="DRAWINGS">FIG. 9</cross-reference>, Graph (a) shows the pitch contour at the normal utterance speed and Graph (b) shows the pitch contour at the highest utterance speed. The dotted line represents the phrase component and the solid line represents the accent component. If the highest speed is twice the normal speed, the generated waveform is approximately one half of the normal one. T<highlight><bold>2</bold></highlight>&equals;T&frac12;. Since the pitch contour changes faster in proportion to the utterance speed, the intonation of the synthetic voice changes at very short cycles. Actually, however, the phrase or accent phrase boundary can disappear owing to the phrase or accent linkage phenomenon so that the pitch contour (b) is not produced. As the utterance speed becomes higher, the pitch contour changes in a relatively gentle fashion. </paragraph>
<paragraph id="P-0140" lvl="0"><number>&lsqb;0140&rsqb;</number> In <cross-reference target="DRAWINGS">FIG. 9</cross-reference>, there are two phrases that can be linked together but, according to the second embodiment of the invention, it is possible to generate an easy-to-listen synthetic speech by making the intonation component 0. By making the intonation 0, the generated voice sounds as a robotics voice having a flat intonation. However, the voice synthesis at the highest speed is used for FRF and, therefore, it is sufficient to grasp the contents of a text and the flat synthetic voice is usable. </paragraph>
<paragraph id="P-0141" lvl="7"><number>&lsqb;0141&rsqb;</number> Third Embodiment </paragraph>
<paragraph id="P-0142" lvl="0"><number>&lsqb;0142&rsqb;</number> The third embodiment is different from the conventional one in that a signal sound is inserted between sentences to clarify the boundary between them. </paragraph>
<paragraph id="P-0143" lvl="0"><number>&lsqb;0143&rsqb;</number> In <cross-reference target="DRAWINGS">FIG. 10</cross-reference>, the prosody generation module <highlight><bold>102</bold></highlight> receives the intermediate language from the text analysis module <highlight><bold>1</bold></highlight> and the prosody control parameters designated by the user. The signal sound designation, which designates the kind of a sound inserted between sentences, is a new parameter that is included in neither the conventional one nor the first and second embodiments. </paragraph>
<paragraph id="P-0144" lvl="0"><number>&lsqb;0144&rsqb;</number> The intermediate language analysis unit <highlight><bold>1701</bold></highlight> receives the intermediate language sentence by sentence and outputs the intermediate language analysis results, such as the phoneme string, phrase information, and accent information, necessary for subsequent prosody generation process to each of pitch contour, phoneme duration, phoneme power, voice segment, and sound quality coefficient determination units <highlight><bold>1702</bold></highlight>, <highlight><bold>1703</bold></highlight>, <highlight><bold>1704</bold></highlight>, <highlight><bold>1705</bold></highlight>, and <highlight><bold>1706</bold></highlight>. </paragraph>
<paragraph id="P-0145" lvl="0"><number>&lsqb;0145&rsqb;</number> The pitch contour determination unit <highlight><bold>1702</bold></highlight> receives the intermediate language analysis results and each of the intonation, pitch, utterance speed, and speaker parameters designated by the user and outputs a pitch contour to a synthesis parameter generation unit <highlight><bold>1708</bold></highlight>. </paragraph>
<paragraph id="P-0146" lvl="0"><number>&lsqb;0146&rsqb;</number> The phoneme duration determination unit <highlight><bold>1703</bold></highlight> receives the intermediate language analysis results and the utterance speed parameter designated by the user and outputs data, such as the phoneme duration and pause length, to the synthesis parameter generation unit <highlight><bold>1708</bold></highlight>. </paragraph>
<paragraph id="P-0147" lvl="0"><number>&lsqb;0147&rsqb;</number> The phoneme power determination unit <highlight><bold>1704</bold></highlight> receives the intermediate language analysis results and the sound intensity designated by the user and outputs respective phoneme amplitude coefficients to the synthesis parameter generation unit <highlight><bold>1708</bold></highlight>. </paragraph>
<paragraph id="P-0148" lvl="0"><number>&lsqb;0148&rsqb;</number> The voice segment determination unit <highlight><bold>1705</bold></highlight> receives the intermediate language analysis results and the speaker parameter designated by the user and outputs the voice segment address necessary for waveform superimposition to the synthesis parameter generation unit <highlight><bold>1708</bold></highlight>. </paragraph>
<paragraph id="P-0149" lvl="0"><number>&lsqb;0149&rsqb;</number> The sound quality coefficient determination unit <highlight><bold>1706</bold></highlight> receives the intermediate language analysis results and the sound quality parameter designated by the user and outputs a sound quality conversion parameter to the synthesis parameter generation unit <highlight><bold>1708</bold></highlight>. </paragraph>
<paragraph id="P-0150" lvl="0"><number>&lsqb;0150&rsqb;</number> The signal sound determination unit <highlight><bold>1707</bold></highlight> receives the utterance speed and signal sound parameters designated by the user and outputs a signal sound control signal for the kind and control of a signal sound to the speech generation module <highlight><bold>103</bold></highlight>. </paragraph>
<paragraph id="P-0151" lvl="0"><number>&lsqb;0151&rsqb;</number> The synthesis parameter generation unit <highlight><bold>1708</bold></highlight> converts the input prosody parameters (pitch contour, phoneme duration, pause length, phoneme amplitude coefficient, voice segment address, and sound quality conversion coefficient) into a waveform (speech) generation parameter in the frame of about 8 ms and outputs it to the speech generation module <highlight><bold>103</bold></highlight>. </paragraph>
<paragraph id="P-0152" lvl="0"><number>&lsqb;0152&rsqb;</number> The prosody generation module <highlight><bold>102</bold></highlight> is different from the conventional one in that the signal sound determination unit <highlight><bold>1707</bold></highlight> is provided and that the signal sound parameter is designated by the user, and in the inside structure of the speech generation module <highlight><bold>103</bold></highlight>. The text analysis module <highlight><bold>101</bold></highlight> is identical with the conventional one and, therefore, the description of its structure will be omitted. </paragraph>
<paragraph id="P-0153" lvl="0"><number>&lsqb;0153&rsqb;</number> In <cross-reference target="DRAWINGS">FIG. 11</cross-reference>, the signal sound determination unit <highlight><bold>1707</bold></highlight> is merely a switch. The utterance speed level designated by the user is connected to the terminal (a) of a switch <highlight><bold>1801</bold></highlight> while the terminal (b) always is grounded. The switch <highlight><bold>1801</bold></highlight> is made such that either of the terminals (a) and (b) is selected according to the utterance speed level. That is, when the utterance speed is at the highest level, the terminal (a) is selected and, otherwise, the terminal (b) is selected. Consequently, when the utterance speed is at the highest level, the signal sound code is outputted and, otherwise, 0 is outputted. The signal sound control signal from the switch <highlight><bold>1801</bold></highlight> is inputted to the speech generation module <highlight><bold>103</bold></highlight>. </paragraph>
<paragraph id="P-0154" lvl="0"><number>&lsqb;0154&rsqb;</number> In <cross-reference target="DRAWINGS">FIG. 12</cross-reference>, the speech generation module <highlight><bold>103</bold></highlight> according to the third embodiment comprises a voice segment decoding unit <highlight><bold>1901</bold></highlight>, an amplitude control unit <highlight><bold>1902</bold></highlight>, a voice segment processing unit <highlight><bold>1903</bold></highlight>, a superimposition control unit <highlight><bold>1904</bold></highlight>, a signal sound control unit <highlight><bold>1905</bold></highlight>, a D/A ring buffer <highlight><bold>1906</bold></highlight>, and a signal sound dictionary <highlight><bold>1907</bold></highlight>. </paragraph>
<paragraph id="P-0155" lvl="0"><number>&lsqb;0155&rsqb;</number> The prosody generation module <highlight><bold>102</bold></highlight> outputs a synthesis parameter to the voice segment decoding unit <highlight><bold>1901</bold></highlight>. The voice segment decoding unit <highlight><bold>1901</bold></highlight>, to which the voice segment dictionary <highlight><bold>105</bold></highlight> is connected, loads voice segment data from the dictionary <highlight><bold>105</bold></highlight> with the voice segment address as a reference pointer, performs a decoding process, if necessary, and outputs the decoded voice segment data to the amplitude control unit <highlight><bold>1902</bold></highlight>. The voice segment dictionary <highlight><bold>105</bold></highlight> stores voice segment data for voice synthesis. Where some kind of compression has been applied for saving the storage capacity, the decoding process is effected and, otherwise, mere reading is made. </paragraph>
<paragraph id="P-0156" lvl="0"><number>&lsqb;0156&rsqb;</number> The amplitude control unit <highlight><bold>1902</bold></highlight> receives the decoded voice segment data and the synthesis parameter and controls the power of the voice segment data with the phoneme amplitude coefficient of the synthesis parameter, and outputs it to the voice segment process unit <highlight><bold>1903</bold></highlight>. </paragraph>
<paragraph id="P-0157" lvl="0"><number>&lsqb;0157&rsqb;</number> The voice segment process unit <highlight><bold>1903</bold></highlight> receives the amplitude-controlled voice segment data and the synthesis parameter and performs an expansion/compression process of the voice segment data with the sound quality conversion coefficient of the synthesis parameter, and outputs it to the superimposition control unit <highlight><bold>1904</bold></highlight>. </paragraph>
<paragraph id="P-0158" lvl="0"><number>&lsqb;0158&rsqb;</number> The superimposition control unit <highlight><bold>1904</bold></highlight> receives the expansion/compression-processed voice date and the synthesis parameter, performs waveform superimposition of the voice segment data with the pitch contour, phoneme duration, and pause length parameters of the synthesis parameter, and outputs the generated waveform sequentially to the D/A ring buffer <highlight><bold>1906</bold></highlight> for writing. The D/A ring buffer <highlight><bold>1906</bold></highlight> sends the written data to a D/A converter (not shown) at an output sampling cycle set in the text-to-speech conversion system for outputting a synthetic voice from a speaker. </paragraph>
<paragraph id="P-0159" lvl="0"><number>&lsqb;0159&rsqb;</number> The signal sound control unit <highlight><bold>1905</bold></highlight> of the speech generation module <highlight><bold>103</bold></highlight> receives the signal sound control signal from the prosody generation module <highlight><bold>102</bold></highlight>. It is connected to the signal sound dictionary <highlight><bold>1907</bold></highlight> so that it processes the stored data as need arises and outputs it to the D/A ring buffer <highlight><bold>1906</bold></highlight>. The writing is made after the superimposition control unit <highlight><bold>1904</bold></highlight> has outputted a sentence of synthetic waveform (speech) or before the synthetic waveform (speech) is written. </paragraph>
<paragraph id="P-0160" lvl="0"><number>&lsqb;0160&rsqb;</number> The signal sound dictionary <highlight><bold>1907</bold></highlight> may store either pulse code modulation (PCM) or standard sine wave data of various kinds of effective sound. In the case of PCM data, the signal sound control unit <highlight><bold>1905</bold></highlight> reads data from the signal sound dictionary <highlight><bold>1907</bold></highlight> and outputs it as it is to the D/A ring buffer <highlight><bold>1906</bold></highlight>. In the case of sine wave data, it reads data from the signal sound dictionary <highlight><bold>1907</bold></highlight> and connects it repeatedly for output. Where the signal sound control signal is 0, no process is made for output to the D/A ring buffer <highlight><bold>1906</bold></highlight>. </paragraph>
<paragraph id="P-0161" lvl="0"><number>&lsqb;0161&rsqb;</number> In operation, only the differences from the convention are the pitch contour and waveform (speech) generation processes and, therefore, the description of the other processes will be omitted. </paragraph>
<paragraph id="P-0162" lvl="0"><number>&lsqb;0162&rsqb;</number> The intermediate language generated in the text analysis module <highlight><bold>101</bold></highlight> is sent to the intermediate language analysis unit <highlight><bold>1701</bold></highlight> of the prosodic parameter generation module <highlight><bold>102</bold></highlight>. In the intermediate language analysis unit <highlight><bold>1701</bold></highlight>, the data necessary for prosody generation is extracted from the phrase end code, word end code, accent code indicative of the accent nuclear, and phoneme code string and sends it to the pitch contour, phoneme duration, phoneme power, voice segment, and sound quality coefficient determination units <highlight><bold>1702</bold></highlight>, <highlight><bold>1703</bold></highlight>, <highlight><bold>1704</bold></highlight>, <highlight><bold>1705</bold></highlight>, and <highlight><bold>1706</bold></highlight>, respectively. </paragraph>
<paragraph id="P-0163" lvl="0"><number>&lsqb;0163&rsqb;</number> In the pitch contour determination unit <highlight><bold>1702</bold></highlight>, the intonation indicative of transition of the pitch is generated and, in the phoneme duration determination unit <highlight><bold>1703</bold></highlight>, the duration of each phoneme and the pause length inserted in phrases or sentences are determined. In the phoneme power determination unit <highlight><bold>1704</bold></highlight>, the phoneme power indicative of changes in the amplitude of a voice waveform is generated and, in the voice segment termination unit <highlight><bold>1705</bold></highlight>, the address, in the voice segment dictionary <highlight><bold>105</bold></highlight>, of a phoneme segment necessary for synthetic waveform generation. In the sound quality coefficient determination unit <highlight><bold>1706</bold></highlight>, the parameter for processing signals of the voice segment data is determined. Of the prosody control designations, the intonation and pitch designations are sent to the pitch contour determination unit <highlight><bold>1702</bold></highlight>, the utterance speed designation is sent to the phoneme duration and signal sound determination units <highlight><bold>1703</bold></highlight> and <highlight><bold>1707</bold></highlight>, respectively, the intensity designation is sent to the phoneme power determination unit <highlight><bold>1704</bold></highlight>, the speaker designation is sent to the pitch contour and voice segment determination units <highlight><bold>1702</bold></highlight> and <highlight><bold>705</bold></highlight>, respectively, the sound quality designation is sent to the sound quality coefficient determination unit <highlight><bold>1706</bold></highlight>, and the signal sound designation is sent to the signal sound determination unit <highlight><bold>1707</bold></highlight>. </paragraph>
<paragraph id="P-0164" lvl="0"><number>&lsqb;0164&rsqb;</number> The pitch contour, phoneme duration, phoneme power, voice segment, and sound quality coefficient determination units <highlight><bold>1702</bold></highlight>, <highlight><bold>1703</bold></highlight>, <highlight><bold>1704</bold></highlight>, <highlight><bold>1705</bold></highlight>, and <highlight><bold>1706</bold></highlight> are identical with the convention and, therefore, their description will be omitted. </paragraph>
<paragraph id="P-0165" lvl="0"><number>&lsqb;0165&rsqb;</number> The prosody generation module <highlight><bold>102</bold></highlight> according to the third embodiment is different from the convention in that the signal sound determination unit <highlight><bold>1707</bold></highlight> is added so that its operation will be described with reference to <cross-reference target="DRAWINGS">FIG. 11</cross-reference>. The signal sound determination unit <highlight><bold>1707</bold></highlight> comprises a switch <highlight><bold>1801</bold></highlight> that is made such that it is controlled by the utterance speed designated by the user to connect either terminal (a) or (b). When the utterance speed level is at the highest speed, the terminal (a) is connected and, otherwise, the terminal (b) is connected to the output. The signal sound code designated by the user is inputted to the terminal (a) while the ground level or 0 is inputted to the terminal (b). That is, the switch <highlight><bold>1801</bold></highlight> outputs the signal sound code at the highest utterance speed and 0 at the other utterance speeds. The signal sound control signal outputted from the switch <highlight><bold>1801</bold></highlight> is sent to the waveform (speech) generation module <highlight><bold>103</bold></highlight>. </paragraph>
<paragraph id="P-0166" lvl="0"><number>&lsqb;0166&rsqb;</number> In <cross-reference target="DRAWINGS">FIG. 12</cross-reference>, the synthesis parameter generated in the synthesis parameter generation unit <highlight><bold>1708</bold></highlight> of the prosody generation module <highlight><bold>102</bold></highlight> is sent to the voice segment decoder, amplitude control, voice segment process, and superimposition control units <highlight><bold>1901</bold></highlight>, <highlight><bold>1902</bold></highlight>, <highlight><bold>1903</bold></highlight>, and <highlight><bold>1904</bold></highlight>, respectively, of the speech generation module <highlight><bold>103</bold></highlight>. </paragraph>
<paragraph id="P-0167" lvl="0"><number>&lsqb;0167&rsqb;</number> In the voice segment decoder unit <highlight><bold>1901</bold></highlight>, the voice segment data is loaded from the voice segment dictionary <highlight><bold>105</bold></highlight> with the voice address as a reference pointer, decoded, if necessary, and sends the decoded voice segment data to the amplitude control unit <highlight><bold>1902</bold></highlight>. The voice segments, a source of speech synthesis, stored in the voice segment dictionary <highlight><bold>105</bold></highlight> are superimposed at the cycle specified by the pitch contour to generate a voice waveform. </paragraph>
<paragraph id="P-0168" lvl="0"><number>&lsqb;0168&rsqb;</number> The voice segments herein used mean units of voice that are connected to generate a synthetic waveform (speech) and vary with the kind of sound. Generally, they are composed of a phoneme string such as CV, VV, VCV, and CVC, wherein C and V represent consonant and vowel, respectively. The voice segments of the same phoneme can be composed of various units according to adjacent phoneme environments so that the data capacity becomes huge. For this reason, it is frequent to apply a compression technique such as adaptive differential PCM or composition by pairing a frequency parameter and a driving sound source data. In some cases, it is composed as PCM data without compression. The voice segment data decoded in the voice segment decoder unit <highlight><bold>1901</bold></highlight> is sent to the amplitude control unit <highlight><bold>1902</bold></highlight> for power control. </paragraph>
<paragraph id="P-0169" lvl="0"><number>&lsqb;0169&rsqb;</number> In the amplitude control unit <highlight><bold>1902</bold></highlight>, the voice segment data is multiplied by the amplitude coefficient for making amplitude control. The amplitude coefficient is determined empirically from information such as the intensity level designated by the user, the kind of a phoneme, the position of a phoneme in the breath group, and the position in the phoneme (rising, stationary, and falling sections). The amplitude-controlled voice segment is sent to the voice segment process unit <highlight><bold>1903</bold></highlight>. </paragraph>
<paragraph id="P-0170" lvl="0"><number>&lsqb;0170&rsqb;</number> In the voice segment process unit <highlight><bold>1903</bold></highlight>, the expansion/compression (re-sampling) of the voice segment is effected according to the sound quality conversion level designated by the user. The sound quality conversion is a function of processing signals of the voice segments registered in the voice segment dictionary <highlight><bold>105</bold></highlight> so that the voice segments sound as those of other speakers. Generally, it is achieved by linearly expanding or compressing the voice segment data. The expansion is made by over-sampling the voice segment data, providing deep voice. Conversely, the compression is made by down-sampling the voice segment data, providing thin voice. This is a function for providing other speakers with the same data and is not limited to the above techniques. Where there is no sound quality conversion designated by the user, no process is made in the voice segment process unit <highlight><bold>1903</bold></highlight>. </paragraph>
<paragraph id="P-0171" lvl="0"><number>&lsqb;0171&rsqb;</number> The generated voice segments undergo waveform superimposition in the superimposition control unit <highlight><bold>1904</bold></highlight>. The common technique is to superimpose the voice segment data while shifting them with the pitch cycle specified by the pitch contour. </paragraph>
<paragraph id="P-0172" lvl="0"><number>&lsqb;0172&rsqb;</number> The thus generated synthetic waveform is written sequentially in the D/A ring buffer <highlight><bold>1906</bold></highlight> and sent to a D/A converter (not shown) with the output sampling cycle set in the text-to-speech conversion system for outputting a synthetic voice or speech from a speaker. </paragraph>
<paragraph id="P-0173" lvl="0"><number>&lsqb;0173&rsqb;</number> The signal sound control signal is inputted to the speech generation module <highlight><bold>103</bold></highlight> from the signal sound determination unit <highlight><bold>1707</bold></highlight>. It is a signal for writing in the D/A ring buffer <highlight><bold>1906</bold></highlight> the data registered in the signal sound dictionary <highlight><bold>1907</bold></highlight> via the signal sound control unit <highlight><bold>1905</bold></highlight>. When the signal sound control signal is 0 or the user-designated utterance speed is not at the highest speed level, no process is made in the signal sound control unit <highlight><bold>1905</bold></highlight>. When the user-designated utterance speed is at the highest speed level, the signal sound control signal is considered as a kind of signal sound to load data from the signal sound dictionary <highlight><bold>1907</bold></highlight>. </paragraph>
<paragraph id="P-0174" lvl="0"><number>&lsqb;0174&rsqb;</number> Suppose that there are three kinds of signal sound; that is, one cycle of each of sine wave data at 500 Hz, 1 k Hz, and 2 k Hz is stored in the signal sound dictionary <highlight><bold>1907</bold></highlight> and that a synthetic sound &ldquo;pit&rdquo; is generated by connecting them repeatedly for a plurality of times. The signal sound control signal can take four values; i.e., 0, 1, 2, and 3. At 0, no process is effected and, at 1, the sine wave data of 500 Hz is read from the signal sound dictionary <highlight><bold>1907</bold></highlight>, connected for a predetermined times, and written in the D/A ring buffer <highlight><bold>1906</bold></highlight>. At 2, the sine wave data of 2 k Hz is read from the signal sound dictionary <highlight><bold>1907</bold></highlight>, connected for a predetermined times, and written in the D/A ring buffer <highlight><bold>1906</bold></highlight>. The writing is made after the superimposition control unit <highlight><bold>1904</bold></highlight> has outputted a sentence of synthetic waveform (speech) or before the synthetic waveform is written. Consequently, the signal sound is outputted between sentences. The appropriate cycles of the output sine wave data range between 100 and 200 ms. </paragraph>
<paragraph id="P-0175" lvl="0"><number>&lsqb;0175&rsqb;</number> The signal sounds to be outputted may be stored as PCM data in the signal sound dictionary <highlight><bold>1907</bold></highlight>. In this case, the data read from the signal sound dictionary <highlight><bold>1907</bold></highlight> is output as it is to the D/A ring buffer <highlight><bold>1906</bold></highlight>. </paragraph>
<paragraph id="P-0176" lvl="0"><number>&lsqb;0176&rsqb;</number> As been described above, according to the third embodiment, when the utterance speed is set at the highest level, the function for inserting a signal sound between sentences resolves the problem that the boundaries between sentences are so vague that the contents of the read text are difficult to understand. Suppose that the following sentences are synthesized into a text. </paragraph>
<paragraph id="P-0177" lvl="1"><number>&lsqb;0177&rsqb;</number> &ldquo;Planned Attendants: Development Division Chief Yamada. Planning Division Chief Saito. Sales Division No. 1 Chief Watanabe.&rdquo;</paragraph>
<paragraph id="P-0178" lvl="1"><number>&lsqb;0178&rsqb;</number> If the process unit or distinction between sentences is made by the period &ldquo;.&rdquo;, the above composition is composed of the following three sentences. </paragraph>
<paragraph id="P-0179" lvl="1"><number>&lsqb;0179&rsqb;</number> (1) &ldquo;Planned attendants: Development Division Chief Yamada.&rdquo;</paragraph>
<paragraph id="P-0180" lvl="1"><number>&lsqb;0180&rsqb;</number> (2) &ldquo;Planning Division Chief Saito.&rdquo;</paragraph>
<paragraph id="P-0181" lvl="1"><number>&lsqb;0181&rsqb;</number> (3) &ldquo;Sales Division No. 1 Chief Watanabe.&rdquo;</paragraph>
<paragraph id="P-0182" lvl="7"><number>&lsqb;0182&rsqb;</number> According to the convention, as the utterance speed becomes higher, the pause length at the end of a sentence becomes smaller so that the synthetic voice of &ldquo;Yamada&rdquo; at the tail of the sentence (1) and the synthetic voice &ldquo;Planning Division&rdquo; at the head of the sentence (2) are outputted almost continuously so that such misunderstanding as &ldquo;Yamada&rdquo;&equals;&ldquo;Planning Division&rdquo; can take place. </paragraph>
<paragraph id="P-0183" lvl="0"><number>&lsqb;0183&rsqb;</number> According to the third embodiment, however, the signal sound, such as &ldquo;pit&rdquo;, is inserted between the synthetic voices &ldquo;Yamada&rdquo; and &ldquo;Planning Division&rdquo; so that such misunderstanding is avoided. </paragraph>
<paragraph id="P-0184" lvl="7"><number>&lsqb;0184&rsqb;</number> Fourth Embodiment </paragraph>
<paragraph id="P-0185" lvl="0"><number>&lsqb;0185&rsqb;</number> In <cross-reference target="DRAWINGS">FIG. 13</cross-reference>, the fourth embodiment is different from the convention in that, it determines whether the text under process is the leading word or phrase in the sentence to determine the expansion/compression rate of the phoneme duration for FRF. Accordingly, the description will be made centered on the phoneme duration determination unit. </paragraph>
<paragraph id="P-0186" lvl="0"><number>&lsqb;0186&rsqb;</number> The phoneme duration determination unit <highlight><bold>203</bold></highlight> receives the analysis results containing the phoneme and prosody information from the intermediate language analysis unit <highlight><bold>201</bold></highlight> and the utterance speed level designated by the user. The intermediate language analysis results of a sentence are outputted to a control factor setting unit <highlight><bold>2001</bold></highlight> and a word counter <highlight><bold>2005</bold></highlight>. The control factor setting unit <highlight><bold>2001</bold></highlight> analyzes the control factor parameter necessary for phoneme duration determination and outputs the result to a duration estimation unit <highlight><bold>2002</bold></highlight>. The duration is determined by statistical analysis, such as Quantification theory (type one). Usually, the phoneme duration estimation is based on the kinds of phonemes adjacent the target phoneme or the syllable position in the word and breath group. The pause length is estimated from the information such as the number of moras in adjacent phrases. The control factor setting unit <highlight><bold>2001</bold></highlight> extracts the information necessary for these predictions. </paragraph>
<paragraph id="P-0187" lvl="0"><number>&lsqb;0187&rsqb;</number> The duration estimation unit <highlight><bold>2002</bold></highlight> is connected to a duration prediction table <highlight><bold>2004</bold></highlight> for making duration predication and outputs it to a duration correction unit <highlight><bold>2003</bold></highlight>. The duration prediction table <highlight><bold>2004</bold></highlight> contains the data that has been trained by using statistical analysis, such as Quantification theory (type one), based on a large amount of natural utterance data. </paragraph>
<paragraph id="P-0188" lvl="0"><number>&lsqb;0188&rsqb;</number> The word counter <highlight><bold>2005</bold></highlight> determines whether the phoneme under analysis is contained in the leading word or phrase in the sentence and outputs the result to an expansion/compression coefficient determination unit <highlight><bold>2006</bold></highlight>. </paragraph>
<paragraph id="P-0189" lvl="0"><number>&lsqb;0189&rsqb;</number> The expansion/compression coefficient determination unit <highlight><bold>2006</bold></highlight> also receives the utterance speed level designated by the user and determines the correction coefficient of a phoneme duration for the phoneme under process and outputs it to the duration correction unit <highlight><bold>2003</bold></highlight>. </paragraph>
<paragraph id="P-0190" lvl="0"><number>&lsqb;0190&rsqb;</number> The duration correction unit <highlight><bold>2003</bold></highlight> multiplies the phoneme duration predicted in the duration estimation unit <highlight><bold>2002</bold></highlight> by the expansion/compression coefficient determined in the expansion/compression coefficient determination unit <highlight><bold>2006</bold></highlight> for making phoneme correction and outputs it to the synthesis parameter (prosody) generation module. </paragraph>
<paragraph id="P-0191" lvl="0"><number>&lsqb;0191&rsqb;</number> In operation, the phoneme duration determination process will be described with reference to <cross-reference target="DRAWINGS">FIGS. 13 and 14</cross-reference>. </paragraph>
<paragraph id="P-0192" lvl="0"><number>&lsqb;0192&rsqb;</number> The analysis results of a sentence are inputted from the intermediate language analysis unit <highlight><bold>201</bold></highlight> to the control factor setting unit <highlight><bold>2001</bold></highlight> and the word counter <highlight><bold>2005</bold></highlight>, respectively. In the control factor setting unit <highlight><bold>2001</bold></highlight>, the control factors necessary for determining the phoneme duration (consonant, vowel, and closed section) and the pause length. The data necessary for phoneme duration determination includes the kind of the target phoneme, kinds of phonemes adjacent the target syllable, or the syllable position in the word or breath group. The data necessary for pause length determination is information such as the number of moras in adjacent phrases. The determination of these durations employs the duration prediction table <highlight><bold>2004</bold></highlight>. </paragraph>
<paragraph id="P-0193" lvl="0"><number>&lsqb;0193&rsqb;</number> The duration prediction table <highlight><bold>2004</bold></highlight> is a table that has been trained based on the natural utterance data by statistical analysis such as Quantification theory (type one). The duration estimation unit <highlight><bold>2002</bold></highlight> looks up this table to predict the phoneme duration and pause length. The respective phoneme duration lengths calculated in the duration estimation unit <highlight><bold>2002</bold></highlight> are for the normal utterance speed. They have been are corrected in the duration correction unit <highlight><bold>2003</bold></highlight> according to the utterance speed designated by the user. Usually, the utterance speed designation is controlled at five to 10 steps by multiplication of a constant predetermined for each level. Where a low utterance speed is desired, the phoneme duration is lengthened while, where a high utterance speed is desired, the phoneme duration is shortened. </paragraph>
<paragraph id="P-0194" lvl="0"><number>&lsqb;0194&rsqb;</number> Also, the word counter <highlight><bold>2005</bold></highlight>, into which the analysis results of a sentence has been inputted from the intermediate language analysis unit <highlight><bold>201</bold></highlight>, determines whether the phoneme under analysis is contained in the leading word or phrase in the sentence. The result outputted from the word counter <highlight><bold>2005</bold></highlight> is either TRUE where the phoneme is contained in the leading word or FALSE in the other case. The result from the word counter <highlight><bold>2005</bold></highlight> is sent to the expansion/compression coefficient determination unit <highlight><bold>2006</bold></highlight>. </paragraph>
<paragraph id="P-0195" lvl="0"><number>&lsqb;0195&rsqb;</number> The result from the word counter <highlight><bold>2005</bold></highlight> and the utterance speed level designated by the user is inputted to the expansion/compression coefficient determination unit <highlight><bold>2006</bold></highlight> to calculate the expansion/compression coefficient of the phoneme. If the utterance speed is controlled at five steps: Levels 0, 1, 2, 3, and 4, and the constant Tn for each level n is defined as follows.</paragraph>
<paragraph lvl="0"><in-line-formula>To&equals;2.0, T<highlight><bold>1</bold></highlight>&equals;1.5, T<highlight><bold>2</bold></highlight>&equals;1.0, T<highlight><bold>3</bold></highlight> 0.75, and T<highlight><bold>4</bold></highlight>&equals;0.5.</in-line-formula></paragraph>
<paragraph id="P-0196" lvl="7"><number>&lsqb;0196&rsqb;</number> The normal utterance speed is set at Level 2, and the utterance speed for FRF is set at Level 4. When the signal from the word counter <highlight><bold>2005</bold></highlight> is TRUE, Tn is outputted Lo the duration correction unit <highlight><bold>2003</bold></highlight> as it is if the utterance speed is at Level 0 to 3. If the utterance speed is at Level 4, the normal utterance value, T<highlight><bold>2</bold></highlight>, is outputted. If the signal from the word counter <highlight><bold>2005</bold></highlight> is FALSE, Tn is outputted to the duration correction unit <highlight><bold>2003</bold></highlight> as it is regardless of the utterance speed level. </paragraph>
<paragraph id="P-0197" lvl="0"><number>&lsqb;0197&rsqb;</number> In the duration correction unit <highlight><bold>2003</bold></highlight>, the phoneme duration from the duration estimation unit <highlight><bold>2002</bold></highlight> is multiplied by the expansion/compression coefficient from the expansion/compression coefficient determination unit <highlight><bold>2006</bold></highlight>. Usually, only the vowel length is corrected. The phoneme duration corrected according to the utterance speed level is sent to the synthesis parameter generation unit. </paragraph>
<paragraph id="P-0198" lvl="0"><number>&lsqb;0198&rsqb;</number> In <cross-reference target="DRAWINGS">FIG. 14, I</cross-reference> is the number of words in the input sentence, Tci is the duration correction coefficient for the phoneme in the i-th word, lev is the utterance speed level designated by the user, T(n) is the expansion/compression coefficient at the utterance speed level n, Tij is the length of a j-th vowel in a i-th word, and J is the number of syllables which constitute a word. </paragraph>
<paragraph id="P-0199" lvl="0"><number>&lsqb;0199&rsqb;</number> In step ST<highlight><bold>201</bold></highlight>, the word counter i is initialized to 0. In ST<highlight><bold>202</bold></highlight>, the word number and the utterance speed level are determined. When the count of a word under process is 0 and the utterance speed level is 4, or the syllable under process belongs to the leading word in the sentence and the utterance speed is at the highest level, the process goes to ST<highlight><bold>204</bold></highlight> and, otherwise, ST<highlight><bold>203</bold></highlight>. In ST<highlight><bold>204</bold></highlight>, the value at the utterance speed level 2 is selected as the correction coefficient and the process goes to ST<highlight><bold>205</bold></highlight>.</paragraph>
<paragraph lvl="0"><in-line-formula><highlight><italic>TC</italic></highlight><highlight><subscript>i</subscript></highlight><highlight><italic>&equals;T</italic></highlight>(2)&emsp;&emsp;(5)</in-line-formula></paragraph>
<paragraph id="P-0200" lvl="7"><number>&lsqb;0200&rsqb;</number> In ST<highlight><bold>203</bold></highlight>, the correction coefficient at the level designated by the user is selected and the process goes to ST<highlight><bold>205</bold></highlight>.</paragraph>
<paragraph lvl="0"><in-line-formula><highlight><italic>TC</italic></highlight><highlight><subscript>i</subscript></highlight><highlight><italic>&equals;T</italic></highlight>(lev)&emsp;&emsp;(6)</in-line-formula></paragraph>
<paragraph id="P-0201" lvl="7"><number>&lsqb;0201&rsqb;</number> In ST<highlight><bold>205</bold></highlight>, the syllable counter j is initialized to 0 and the process goes to ST<highlight><bold>206</bold></highlight>, in which the duration time, Tij, of the j-th vowel in the i-th word is determined by the following equation.</paragraph>
<paragraph lvl="0"><in-line-formula><highlight><italic>T</italic></highlight><highlight><subscript>ij</subscript></highlight><highlight><italic>&equals;T</italic></highlight><highlight><subscript>ij</subscript></highlight><highlight><italic>&times;TC</italic></highlight><highlight><subscript>i</subscript></highlight>&emsp;&emsp;(7)</in-line-formula></paragraph>
<paragraph id="P-0202" lvl="7"><number>&lsqb;0202&rsqb;</number> In ST<highlight><bold>207</bold></highlight>, the syllable counter j is incremented by one and the process goes to ST<highlight><bold>208</bold></highlight>, in which the syllable counter j is compared with the number of syllables J in the word. When the syllable counter j exceeds the number of syllables J, or all of the syllables in the word have been processed, the process goes to ST<highlight><bold>209</bold></highlight>. Otherwise, the process returns to ST<highlight><bold>206</bold></highlight> to repeat the above process for syllable. </paragraph>
<paragraph id="P-0203" lvl="0"><number>&lsqb;0203&rsqb;</number> In ST<highlight><bold>209</bold></highlight>, the word counter i is incremented by one and the process goes to ST<highlight><bold>2</bold></highlight>l<highlight><bold>0</bold></highlight>, in which the word counter i is compared with the number of words I. When the word counter i exceeds the number of words I, or all of the words in the input sentence have been processed, the process is terminated and, otherwise, the process goes back to ST<highlight><bold>202</bold></highlight> to repeat the above process for the next word. </paragraph>
<paragraph id="P-0204" lvl="0"><number>&lsqb;0204&rsqb;</number> By the above process, even if the utterance speed designated by the user is at the highest level, the leading ward in the sentence always is read at the normal utterance speed to generate a synthetic voice. </paragraph>
<paragraph id="P-0205" lvl="0"><number>&lsqb;0205&rsqb;</number> As has been described above, according to the fourth embodiment of the invention, when the utterance speed level is set at the maximum speed, the leading word of a sentence is process at the normal utterance speed so that it is easy to release FRF timely. In user&apos;s manuals or software specifications, for example, such a heading number as &ldquo;Chapter 3&rdquo; or &ldquo;4.1.3.&rdquo; is used. Where it is desired to read such a manual from Chapter 3 or 4.1.3, it has been necessary for the convention to distinguish such key words as &ldquo;chapter three&rdquo; or &ldquo;four period one period three&rdquo; among the synthetic voices outputted at high speeds to release FRF. According to the fourth embodiment, it is easy to turn on or off FRF. </paragraph>
<paragraph id="P-0206" lvl="0"><number>&lsqb;0206&rsqb;</number> The invention is not limited to the above illustrated embodiments, and a variety of modifications may be made without departing from the sprit and scope of the invention. </paragraph>
<paragraph id="P-0207" lvl="0"><number>&lsqb;0207&rsqb;</number> In the first embodiment, for example, the simplification or termination of the function unit on which a large load is applied during the text-to-speech conversion process when the utterance speed is set at the maximum level may not be limited to the maximum utterance speed. That is, the above process may be modified for application only when the utterance speed exceeds a certain threshold. The heavy load processes are not limited to the phoneme parameter prediction by Quantification theory (type one) and the voice segment data process for sound quality conversion. Where there is another heavy load processing capability, such as an audio process of echoes or high pitch emphasis, it is preferred to simplify or invalidate such function. In the sound quality conversion process, the waveform may be expanded or compressed non-linearly or changed through the specified conversion function for the frequency parameter. As far as the calculation amount and process time are minimized, the rule making procedures are not limited to the phoneme duration and pitch contour determination rules. If the prosodic parameter prediction at the normal utterance speed by using statistic analysis involves more calculation load than the prediction by rule, the prediction may not be limited to the above process. The control factors described for the prediction are illustrative only. </paragraph>
<paragraph id="P-0208" lvl="0"><number>&lsqb;0208&rsqb;</number> In the second embodiment, the process by which the intonation component of a pitch contour is made 0 for pitch contour generation when the utterance speed is set at the maximum level, but such process may not be limited to the maximum utterance speed. That is, the process may be applied when the utterance speed exceeds a certain threshold. The intonation component may be made lower than the normal one. For example, when the utterance speed is set at the maximum level, the intonation designation level is forced to set at the lowest level to minimize the intonation component in the pitch contour correction unit. However, the intonation designation level at this point must be sufficient to provide an easy-to-listen intonation at the time of high-speed synthesis The accent and phrase components of a pitch contour may be determined by rule. The control factors described for making prediction are illustrative only. </paragraph>
<paragraph id="P-0209" lvl="0"><number>&lsqb;0209&rsqb;</number> In the third embodiment, the insertion of a signal sound between sentences may be made at utterance speeds other than the maximum speed. That is, the insertion may be made when the utterance speed exceeds a certain threshold. The signal sound may be generated by any technique as far as it attracts user&apos;s attention. The recorded sound effects may be output as they are. The signal sound dictionary may be replaced by an internal circuitry or program for generating them. The insertion of a signal sound may be made immediately before the synthetic waveform as far as the sentence boundary is clear at the maximum utterance speed. The kind of a signal sound inputted to the parameter generation unit may be omitted owing to the hardware or software limitation. However, it is preferred that the signal sound be changeable according to the user&apos;s preference. </paragraph>
<paragraph id="P-0210" lvl="0"><number>&lsqb;0210&rsqb;</number> In the fourth embodiment, the process of the phoneme duration control of the leading word at the normal (default) utterance speed may be made at other utterance speeds. That is, the above process may be made when the utterance speed exceeds a certain threashold. The unit process at the normal utterance speed may be the two leading words or phrases. Also, it may be made at a level one lower than the normal utterance speed. </paragraph>
<paragraph id="P-0211" lvl="0"><number>&lsqb;0211&rsqb;</number> As has been described above, according to an aspect of the invention, there is provided a method of controlling high-speed reading in a text-to-speech conversion system including a text analysis module for generating a phoneme and prosody character string from an input text; a prosody generation module for generating a synthesis parameter of at least a voice segment, a phoneme duration, and a fundamental frequency for the phoneme and prosody character string; a voice segment dictionary in which voice segments as a source of voice are registered; and a speech generation module for generating a synthetic waveform by waveform superimposition by referring to the voice segment dictionary, the method comprising the step of providing the prosody generation module with </paragraph>
<paragraph id="P-0212" lvl="2"><number>&lsqb;0212&rsqb;</number> (1) a phoneme duration determination unit that includes both a duration rule table containing empirically found phoneme durations and a duration prediction table containing phoneme durations predicted by statistical analysis and determines a phoneme duration by using, when a user-designated utterance speed exceeds a threshold, the duration rule table and, when the threshold is not exceeded, the duration prediction table, </paragraph>
<paragraph id="P-0213" lvl="2"><number>&lsqb;0213&rsqb;</number> (2) a pitch contour determination unit that has both an empirically found rule table and a prediction table predicted by statistical analysis and determines a pitch contour by determining both accent and phrase components with, when a user-designated utterance speed exceeds a threshold, the duration rule table and, when the threshold is not exceeded, the duration prediction table, or </paragraph>
<paragraph id="P-0214" lvl="2"><number>&lsqb;0214&rsqb;</number> (3) a sound quality coefficient determination unit that has a sound quality conversion coefficient table for changing the voice segment to switch sound quality and selects from the sound quality conversion coefficient table such a coefficient that sound quality does not change when a user-designated utterance speed exceeds a threshold, thus simplifying or invalidating the function with a heavy process load in the text-to-speech conversion process to minimize the voice interruption due to the heavy load and generate an easy-to-understand speech even if the utterance speed is set at the maximum level. </paragraph>
<paragraph id="P-0215" lvl="0"><number>&lsqb;0215&rsqb;</number> According to another aspect of the invention, there is provided a method of controlling high-speed reading in a text-to-speech conversion system, comprising the step of providing the prosody generation module with both a pitch contour correction unit for outputting a pitch contour corrected according to an intonation level designated by the user and a switch for determining whether a base pitch is added to the pitch contour corrected according to the user-designated utterance speed such that when the utterance speed exceeds a predetermined threshold, the base pitch is not changed. Consequently, when the utterance speed is set at the predetermined maximum level, the intonation component of the pitch contour is made 0 to generate the pitch contour so that the intonation does not change at short cycles, thus avoiding synthesis of unintelligible speech. </paragraph>
<paragraph id="P-0216" lvl="0"><number>&lsqb;0216&rsqb;</number> According to still another aspect of the invention there is provided a method of controlling high-speed reading in a text-to-speech conversion system, comprising the step of providing the speech generation module with signal sound generation means for inserting a signal sound between sentences to indicate an end of a sentence when a user-designated utterance speed exceeds a threshold so that when the utterance speed is set at the maximum level, a signal sound is inserted between sentences to clarify the sentence boundary, making it easy to understand the synthetic speech. </paragraph>
<paragraph id="P-0217" lvl="0"><number>&lsqb;0217&rsqb;</number> According to yet another aspect of the invention there is provided a method of controlling high-speed reading in a text-to-speech conversion system, comprising the step of providing the prosody generation module with a phoneme duration determination unit for performing a process in which when a user-designated utterance speed exceeds a threshold, an utterance speed of at least a leading word in a sentence is returned to a normal utterance speed so that the utterance speed is at the maximum level, the leading word is processed at the normal utterance speed, making it easy to timely release the FRF operation. </paragraph>
</section>
</detailed-description>
</subdoc-description>
<subdoc-claims>
<claim id="CLM-00001">
<claim-text><highlight><bold>1</bold></highlight>. A method of controlling high-speed reading in a text-to-speech conversion system including a text analysis module for generating a phoneme and prosody character string from an input text; a prosody generation module for generating a synthesis parameter of at least a voice segment, a phoneme duration, and a fundamental frequency for said phoneme and prosody character string; a voice segment dictionary in which voice segments as a source of voice are registered; and a speech generation module for generating a synthetic waveform by waveform superimposition by referring to said voice segment dictionary, 
<claim-text>said method comprising the step of providing said prosody generation module with a phoneme duration determination unit that includes both a duration rule table containing empirically found phoneme durations and a duration prediction table containing phoneme durations predicted by statistical analysis and determines a phoneme duration by using, when a user-designated utterance speed exceeds a threshold, said duration rule table and, when said threshold is not exceeded, said duration prediction table. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00002">
<claim-text><highlight><bold>2</bold></highlight>. The method according to <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein said threshold is a predetermined maximum utterance speed. </claim-text>
</claim>
<claim id="CLM-00003">
<claim-text><highlight><bold>3</bold></highlight>. A method of controlling high-speed reading in a text-to-speech conversion system including a text analysis module for generating a phoneme and prosody character string from an input text; a prosody generation module for generating a synthesis parameter of at least a voice segment, a phoneme duration, and a fundamental frequency for the phoneme and prosody character string; a voice segment dictionary in which voice segments as a source of voice are registered; and a speech generation module for generating a synthetic waveform by waveform superimposition while referring to said voice segment dictionary, 
<claim-text>said method comprising the step of providing said prosody generation module with a pitch contour determination unit that has both an empirically found rule table and a prediction table predicted by statistical analysis and determines a pitch contour by determining both accent and phrase components with, when a user-designated utterance speed exceeds a threshold, said duration rule table and, when said threshold is not exceeded, said duration prediction table. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00004">
<claim-text><highlight><bold>4</bold></highlight>. The method according to <dependent-claim-reference depends_on="CLM-00003">claim 3</dependent-claim-reference>, wherein said threshold is a predetermined maximum utterance speed. </claim-text>
</claim>
<claim id="CLM-00005">
<claim-text><highlight><bold>5</bold></highlight>. A method of controlling high-speed reading in a text-to-speech conversion system including a text analysis module for generating a phoneme and prosody character string from an input text; a prosody generation module for generating a synthesis parameter of at least a voice segment, a phoneme duration, and a fundamental frequency for the phoneme and prosody character string; a voice segment dictionary in which voice segments as a source of voice are registered; and a speech generation module for generating a synthetic waveform by waveform superimposition by referring to said voice segment dictionary, 
<claim-text>said method comprising the step of providing said prosody generation module with a sound quality coefficient determination unit that has a sound quality conversion coefficient table for changing said voice segment to switch sound quality and selects from said sound quality conversion coefficient table such a coefficient that sound quality does not change when a user-designated utterance speed exceeds a threshold. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00006">
<claim-text><highlight><bold>6</bold></highlight>. The method according to <dependent-claim-reference depends_on="CLM-00005">claim 5</dependent-claim-reference>, wherein said threshold is a predetermined maximum utterance speed. </claim-text>
</claim>
<claim id="CLM-00007">
<claim-text><highlight><bold>7</bold></highlight>. A method of controlling high-speed reading in a text-to-speech conversion system including a text analysis module for generating a phoneme and prosody character string from an input text; a prosody generation module for generating a synthesis parameter of at least a voice segment, phoneme duration, and fundamental frequency for the phoneme and prosody character string; a voice segment dictionary in which voice segments as a source of voice are registered; and a speech generation module for generating a synthetic waveform by waveform superimposition by referring to said voice segment dictionary, 
<claim-text>said method comprising the step of providing said prosody generation module with both a pitch contour correction unit for outputting a pitch contour corrected according to an intonation level designated by the user and a switch for determining whether a base pitch is added to said pitch contour corrected according to said user-designated utterance speed. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00008">
<claim-text><highlight><bold>8</bold></highlight>. The method according to <dependent-claim-reference depends_on="CLM-00007">claim 7</dependent-claim-reference>, wherein said threshold is a predetermined maximum utterance speed. </claim-text>
</claim>
<claim id="CLM-00009">
<claim-text><highlight><bold>9</bold></highlight>. The method according to <dependent-claim-reference depends_on="CLM-00007">claim 7</dependent-claim-reference>, wherein said pitch contour correction unit performs a pitch contour generation process that includes a phrase component calculation process in which all phrases of an input sentence are processed by calculating a phrase component by statistical analysis according to said user-designated utterance speed or making said phrase component zero and a process in which all words in said input sentence are processed by calculating an accent component by statistical analysis according to said user-designated utterance speed and either correcting said accent component according to said user-designated intonation level or making said accent component zero. </claim-text>
</claim>
<claim id="CLM-00010">
<claim-text><highlight><bold>10</bold></highlight>. A method of controlling high-speed reading in a text-to-speech conversion system including a text analysis module for generating a phoneme and prosody character string from an input text; a prosody generation module for generating a synthesis parameter of at least a voice segment, a phoneme duration, and a fundamental frequency for said phoneme and prosody character string; a voice segment dictionary in which voice segments as a source of voice are registered; and a speech generation module for generating a synthetic waveform by waveform superimposition while referring to said voice segment dictionary, 
<claim-text>said method comprising the step of providing said speech generation module with signal sound generation means for inserting a signal sound between sentences to indicate an end of a sentence when a user-designated utterance speed exceeds a threshold. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00011">
<claim-text><highlight><bold>11</bold></highlight>. The method according to <dependent-claim-reference depends_on="CLM-00011">claim 10</dependent-claim-reference>, wherein said threshold is a predetermined maximum utterance speed. </claim-text>
</claim>
<claim id="CLM-00012">
<claim-text><highlight><bold>12</bold></highlight>. A method of controlling high-speed reading in a text-to-speech conversion system including a text analysis module for generating a phoneme and prosody character string from an input text; a prosody generation module for generating a synthesis parameter of at least a voice segment, a phoneme duration, and a fundamental frequency for the phoneme and prosody character string; a voice segment dictionary in which voice segments as a source of voice are registered; and a speech generation module for generating a synthetic waveform by waveform superimposition by referring to said voice segment dictionary, 
<claim-text>said method comprising the step of providing said prosody generation module with a phoneme duration determination unit for performing a process in which when a user-designated utterance speed exceeds a threshold, an utterance speed of at least a leading word in a sentence is returned to a normal utterance speed. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00013">
<claim-text><highlight><bold>13</bold></highlight>. The method according to <dependent-claim-reference depends_on="CLM-00011">claim 12</dependent-claim-reference>, wherein said threshold is a predetermined maximum utterance speed. </claim-text>
</claim>
<claim id="CLM-00014">
<claim-text><highlight><bold>14</bold></highlight>. The method according to <dependent-claim-reference depends_on="CLM-00011">claim 12</dependent-claim-reference>, wherein said phoneme duration determination unit performs a process in which when a word under process is a leading word in a sentence and said user-designated utterance speed exceeds said threshold, a phoneme duration is not corrected and, when said word under process is not a leading word of a sentence or said user-designated utterance speed does not exceed said threshold, a first process by which a phoneme duration correction coefficient is changed according to said user-designated utterance speed and a second process in which all syllables of said word are processed by correcting a length of a vowel or vowels of said word, and carrying out said first and second processes for all words contained in the sentence.</claim-text>
</claim>
</subdoc-claims>
<subdoc-drawings id="DRAWINGS">
<heading lvl="0" align="CENTER">Drawings</heading>
<representative-figure>1</representative-figure>
<figure id="figure-D00000">
<image id="EMI-D00000" file="US20030004723A1-20030102-D00000.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00001">
<image id="EMI-D00001" file="US20030004723A1-20030102-D00001.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00002">
<image id="EMI-D00002" file="US20030004723A1-20030102-D00002.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00003">
<image id="EMI-D00003" file="US20030004723A1-20030102-D00003.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00004">
<image id="EMI-D00004" file="US20030004723A1-20030102-D00004.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00005">
<image id="EMI-D00005" file="US20030004723A1-20030102-D00005.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00006">
<image id="EMI-D00006" file="US20030004723A1-20030102-D00006.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00007">
<image id="EMI-D00007" file="US20030004723A1-20030102-D00007.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00008">
<image id="EMI-D00008" file="US20030004723A1-20030102-D00008.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00009">
<image id="EMI-D00009" file="US20030004723A1-20030102-D00009.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00010">
<image id="EMI-D00010" file="US20030004723A1-20030102-D00010.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00011">
<image id="EMI-D00011" file="US20030004723A1-20030102-D00011.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00012">
<image id="EMI-D00012" file="US20030004723A1-20030102-D00012.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00013">
<image id="EMI-D00013" file="US20030004723A1-20030102-D00013.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00014">
<image id="EMI-D00014" file="US20030004723A1-20030102-D00014.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00015">
<image id="EMI-D00015" file="US20030004723A1-20030102-D00015.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00016">
<image id="EMI-D00016" file="US20030004723A1-20030102-D00016.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00017">
<image id="EMI-D00017" file="US20030004723A1-20030102-D00017.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00018">
<image id="EMI-D00018" file="US20030004723A1-20030102-D00018.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00019">
<image id="EMI-D00019" file="US20030004723A1-20030102-D00019.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00020">
<image id="EMI-D00020" file="US20030004723A1-20030102-D00020.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00021">
<image id="EMI-D00021" file="US20030004723A1-20030102-D00021.TIF" imf="TIFF" ti="DR"/>
</figure>
</subdoc-drawings>
</patent-application-publication>
