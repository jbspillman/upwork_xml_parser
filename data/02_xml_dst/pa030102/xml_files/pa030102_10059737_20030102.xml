<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE patent-application-publication SYSTEM "pap-v16-2002-01-01.dtd" [
<!ENTITY US20030004720A1-20030102-M00001.NB SYSTEM "US20030004720A1-20030102-M00001.NB" NDATA NB>
<!ENTITY US20030004720A1-20030102-M00001.TIF SYSTEM "US20030004720A1-20030102-M00001.TIF" NDATA TIF>
<!ENTITY US20030004720A1-20030102-M00002.NB SYSTEM "US20030004720A1-20030102-M00002.NB" NDATA NB>
<!ENTITY US20030004720A1-20030102-M00002.TIF SYSTEM "US20030004720A1-20030102-M00002.TIF" NDATA TIF>
<!ENTITY US20030004720A1-20030102-M00003.NB SYSTEM "US20030004720A1-20030102-M00003.NB" NDATA NB>
<!ENTITY US20030004720A1-20030102-M00003.TIF SYSTEM "US20030004720A1-20030102-M00003.TIF" NDATA TIF>
<!ENTITY US20030004720A1-20030102-M00004.NB SYSTEM "US20030004720A1-20030102-M00004.NB" NDATA NB>
<!ENTITY US20030004720A1-20030102-M00004.TIF SYSTEM "US20030004720A1-20030102-M00004.TIF" NDATA TIF>
<!ENTITY US20030004720A1-20030102-M00005.NB SYSTEM "US20030004720A1-20030102-M00005.NB" NDATA NB>
<!ENTITY US20030004720A1-20030102-M00005.TIF SYSTEM "US20030004720A1-20030102-M00005.TIF" NDATA TIF>
<!ENTITY US20030004720A1-20030102-M00006.NB SYSTEM "US20030004720A1-20030102-M00006.NB" NDATA NB>
<!ENTITY US20030004720A1-20030102-M00006.TIF SYSTEM "US20030004720A1-20030102-M00006.TIF" NDATA TIF>
<!ENTITY US20030004720A1-20030102-M00007.NB SYSTEM "US20030004720A1-20030102-M00007.NB" NDATA NB>
<!ENTITY US20030004720A1-20030102-M00007.TIF SYSTEM "US20030004720A1-20030102-M00007.TIF" NDATA TIF>
<!ENTITY US20030004720A1-20030102-M00008.NB SYSTEM "US20030004720A1-20030102-M00008.NB" NDATA NB>
<!ENTITY US20030004720A1-20030102-M00008.TIF SYSTEM "US20030004720A1-20030102-M00008.TIF" NDATA TIF>
<!ENTITY US20030004720A1-20030102-M00009.NB SYSTEM "US20030004720A1-20030102-M00009.NB" NDATA NB>
<!ENTITY US20030004720A1-20030102-M00009.TIF SYSTEM "US20030004720A1-20030102-M00009.TIF" NDATA TIF>
<!ENTITY US20030004720A1-20030102-M00010.NB SYSTEM "US20030004720A1-20030102-M00010.NB" NDATA NB>
<!ENTITY US20030004720A1-20030102-M00010.TIF SYSTEM "US20030004720A1-20030102-M00010.TIF" NDATA TIF>
<!ENTITY US20030004720A1-20030102-M00011.NB SYSTEM "US20030004720A1-20030102-M00011.NB" NDATA NB>
<!ENTITY US20030004720A1-20030102-M00011.TIF SYSTEM "US20030004720A1-20030102-M00011.TIF" NDATA TIF>
<!ENTITY US20030004720A1-20030102-D00000.TIF SYSTEM "US20030004720A1-20030102-D00000.TIF" NDATA TIF>
<!ENTITY US20030004720A1-20030102-D00001.TIF SYSTEM "US20030004720A1-20030102-D00001.TIF" NDATA TIF>
<!ENTITY US20030004720A1-20030102-D00002.TIF SYSTEM "US20030004720A1-20030102-D00002.TIF" NDATA TIF>
<!ENTITY US20030004720A1-20030102-D00003.TIF SYSTEM "US20030004720A1-20030102-D00003.TIF" NDATA TIF>
<!ENTITY US20030004720A1-20030102-D00004.TIF SYSTEM "US20030004720A1-20030102-D00004.TIF" NDATA TIF>
<!ENTITY US20030004720A1-20030102-D00005.TIF SYSTEM "US20030004720A1-20030102-D00005.TIF" NDATA TIF>
<!ENTITY US20030004720A1-20030102-D00006.TIF SYSTEM "US20030004720A1-20030102-D00006.TIF" NDATA TIF>
<!ENTITY US20030004720A1-20030102-D00007.TIF SYSTEM "US20030004720A1-20030102-D00007.TIF" NDATA TIF>
<!ENTITY US20030004720A1-20030102-D00008.TIF SYSTEM "US20030004720A1-20030102-D00008.TIF" NDATA TIF>
<!ENTITY US20030004720A1-20030102-D00009.TIF SYSTEM "US20030004720A1-20030102-D00009.TIF" NDATA TIF>
<!ENTITY US20030004720A1-20030102-D00010.TIF SYSTEM "US20030004720A1-20030102-D00010.TIF" NDATA TIF>
]>
<patent-application-publication>
<subdoc-bibliographic-information>
<document-id>
<doc-number>20030004720</doc-number>
<kind-code>A1</kind-code>
<document-date>20030102</document-date>
</document-id>
<publication-filing-type>new</publication-filing-type>
<domestic-filing-data>
<application-number>
<doc-number>10059737</doc-number>
</application-number>
<application-number-series-code>10</application-number-series-code>
<filing-date>20020128</filing-date>
</domestic-filing-data>
<technical-information>
<classification-ipc>
<classification-ipc-primary>
<ipc>G10L015/00</ipc>
</classification-ipc-primary>
<classification-ipc-edition>07</classification-ipc-edition>
</classification-ipc>
<classification-us>
<classification-us-primary>
<uspc>
<class>704</class>
<subclass>247000</subclass>
</uspc>
</classification-us-primary>
</classification-us>
<title-of-invention>System and method for computing and transmitting parameters in a distributed voice recognition system</title-of-invention>
</technical-information>
<continuity-data>
<non-provisional-of-provisional>
<document-id>
<doc-number>60265263</doc-number>
<document-date>20010130</document-date>
<country-code>US</country-code>
</document-id>
</non-provisional-of-provisional>
<non-provisional-of-provisional>
<document-id>
<doc-number>60265769</doc-number>
<document-date>20010131</document-date>
<country-code>US</country-code>
</document-id>
</non-provisional-of-provisional>
</continuity-data>
<inventors>
<first-named-inventor>
<name>
<given-name>Harinath</given-name>
<family-name>Garudadri</family-name>
</name>
<residence>
<residence-us>
<city>San Diego</city>
<state>CA</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</first-named-inventor>
<inventor>
<name>
<given-name>Hynek</given-name>
<family-name>Hermansky</family-name>
</name>
<residence>
<residence-us>
<city>Portland</city>
<state>OR</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
<inventor>
<name>
<given-name>Lukas</given-name>
<family-name>Burget</family-name>
</name>
<residence>
<residence-us>
<city>Hillsboro</city>
<state>OR</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
<inventor>
<name>
<given-name>Pratibha</given-name>
<family-name>Jain</family-name>
</name>
<residence>
<residence-us>
<city>Beaverton</city>
<state>OR</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
<inventor>
<name>
<given-name>Sachin</given-name>
<family-name>Kajarekar</family-name>
</name>
<residence>
<residence-us>
<city>Portland</city>
<state>OR</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
<inventor>
<name>
<given-name>Sunil</given-name>
<family-name>Sivadas</family-name>
</name>
<residence>
<residence-us>
<city>Portland</city>
<state>OR</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
<inventor>
<name>
<given-name>Stephane</given-name>
<middle-name>N.</middle-name>
<family-name>Dupont</family-name>
</name>
<residence>
<residence-non-us>
<city>Saint-Vaast</city>
<country-code>BE</country-code>
</residence-non-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
<inventor>
<name>
<given-name>Maria</given-name>
<middle-name>Carmen Benitez</middle-name>
<family-name>Ortuzar</family-name>
</name>
<residence>
<residence-non-us>
<city>Granada</city>
<country-code>ES</country-code>
</residence-non-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
<inventor>
<name>
<given-name>Nelson</given-name>
<middle-name>H.</middle-name>
<family-name>Morgan</family-name>
</name>
<residence>
<residence-us>
<city>Oakland</city>
<state>CA</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
</inventors>
<correspondence-address>
<name-1>Qualcomm Incorporated</name-1>
<name-2>Patents Department</name-2>
<address>
<address-1>5775 Morehouse Drive</address-1>
<city>San Diego</city>
<state>CA</state>
<postalcode>92121-1714</postalcode>
<country>
<country-code>US</country-code>
</country>
</address>
</correspondence-address>
</subdoc-bibliographic-information>
<subdoc-abstract>
<paragraph id="A-0001" lvl="0">A system and method for extracting acoustic features and speech activity on a device and transmitting them in a distributed voice recognition system. The distributed voice recognition system includes a local VR engine in a subscriber unit and a server VR engine on a server . The local VR engine comprises a feature extraction (FE) module that extracts features from a speech signal, and a voice activity detection module (VAD) that detects voice activity within a speech signal. The system includes filters, framing and windowing modules, power spectrum analyzers, a neural network, a nonlinear element, and other components to selectively provide an advanced front end vector including predetermined portions of the voice activity detection indication and extracted features from the subscriber unit to the server . The system also includes a module to generate additional feature vectors on the server from the received features using a feed-forward multilayer perceptron (MLP) and providing the same to the speech server. </paragraph>
</subdoc-abstract>
<subdoc-description>
<cross-reference-to-related-applications>
<heading lvl="1">CROSS REFERENCE </heading>
<paragraph id="P-0001" lvl="0"><number>&lsqb;0001&rsqb;</number> This application claims priority based on Provisional Application No. 60/265,769, filed Jan. 31, 2001, entitled &ldquo;Method for Extracting Terminal Features In A Distributed Voice Recognition System,&rdquo; and Provisional Application No. 60/265,263, filed Jan. 30, 2001, entitled &ldquo;Method for Extracting Front End Features In A Distributed Voice Recognition System,&rdquo; both currently assigned to the assignee of the present invention.</paragraph>
</cross-reference-to-related-applications>
<summary-of-invention>
<section>
<heading lvl="1">BACKGROUND </heading>
<paragraph id="P-0002" lvl="0"><number>&lsqb;0002&rsqb;</number> 1. Field </paragraph>
<paragraph id="P-0003" lvl="0"><number>&lsqb;0003&rsqb;</number> The present invention relates generally to the field of communications and more specifically to transmitting speech activity in a distributed voice recognition system. </paragraph>
<paragraph id="P-0004" lvl="0"><number>&lsqb;0004&rsqb;</number> 2. Background </paragraph>
<paragraph id="P-0005" lvl="0"><number>&lsqb;0005&rsqb;</number> Voice recognition (VR) represents an important technique enabling a machine with simulated intelligence to recognize user-voiced commands and to facilitate a human interface with the machine. VR also represents a key technique for human speech understanding. Systems employing techniques to recover a linguistic message from an acoustic speech signal are called voice recognizers. </paragraph>
<paragraph id="P-0006" lvl="0"><number>&lsqb;0006&rsqb;</number> VR, also known as speech recognition, provides certain safety benefits to the public. For example, VR may be employed to replace the manual task of pushing buttons on a wireless keypad, a particularly useful replacement when the operator is using a wireless handset while driving an automobile. When a user employs a wireless telephone without VR capability, the driver must remove his or her hand from the steering wheel and look at the telephone keypad while pushing buttons to dial the call. Such actions tend to increase the probability of an automobile accident. A speech-enabled automobile telephone, or telephone designed for speech recognition, enables the driver to place telephone calls while continuously monitoring the road. In addition, a hands-free automobile wireless telephone system allows the driver to hold both hands on the steering wheel while initiating a phone call. A sample vocabulary for a simple hands-free automobile wireless telephone kit might include the 10 digits, the keywords &ldquo;call,&rdquo; &ldquo;send,&rdquo; &ldquo;dial&rdquo; &ldquo;cancel,&rdquo; &ldquo;clear,&rdquo; &ldquo;add,&rdquo; &ldquo;delete,&rdquo; history,&rdquo; &ldquo;program,&rdquo; &ldquo;yes,&rdquo; and &ldquo;no,&rdquo; and the names of a predefined number of commonly called co-workers, friends, or family members. </paragraph>
<paragraph id="P-0007" lvl="0"><number>&lsqb;0007&rsqb;</number> A voice recognizer, or VR system, comprises an acoustic processor, also called the front end of a voice recognizer, and a word decoder, also called the back end of the voice recognizer. The acoustic processor performs feature extraction for the system by extracting a sequence of information bearing features, or vectors, necessary for performing voice recognition on the incoming raw speech. The word decoder subsequently decodes the sequence of features, or vectors, to provide a meaningful and desired output, such as the sequence of linguistic words corresponding to the received input utterance. </paragraph>
<paragraph id="P-0008" lvl="0"><number>&lsqb;0008&rsqb;</number> In a voice recognizer implementation using a distributed system architecture, it is often desirable to place the word decoding task on a subsystem having the ability to appropriately manage computational and memory load, such as a network server. The acoustic processor should physically reside as close to the speech source as possible to reduce adverse effects associated with vocoders. Vocoders compress speech prior to transmission, and can in certain circumstances introduce adverse characteristics due to signal processing and/or channel induced errors. These effects typically result from vocoding at the user device. The advantage to a Distributed Voice Recognition (DVR) system is that the acoustic processor resides in the user device and the word decoder resides remotely, such as on a network, thereby decreasing the risk of user device signal processing errors or channel errors. </paragraph>
<paragraph id="P-0009" lvl="0"><number>&lsqb;0009&rsqb;</number> DVR systems enable devices such as cell phones, personal communications devices, personal digital assistants (PDAs), and other devices to access information and services from a wireless network, such as the Internet, using spoken commands. These devices access voice recognition servers on the network and are much more versatile, robust and useful than systems recognizing only limited vocabulary sets. </paragraph>
<paragraph id="P-0010" lvl="0"><number>&lsqb;0010&rsqb;</number> In wireless applications, air interface methods degrade the overall accuracy of the voice recognition systems. This degradation can in certain circumstances be mitigated by extracting VR features from a user&apos;s spoken commands. Extraction occurs on a device, such as a subscriber unit, also called a subscriber station, mobile station, mobile, remote station, remote terminal, access terminal, or user equipment. The subscriber unit can transmit the VR features in data traffic, rather than transmitting spoken words in voice traffic. </paragraph>
<paragraph id="P-0011" lvl="0"><number>&lsqb;0011&rsqb;</number> Thus, in a DVR system, front end features are extracted at the device and are sent to the network. A device may be mobile or stationary, and may communicate with one or more base stations (BSes), also called cellular base stations, cell base stations, base transceiver system (BTSes), base station transceivers, central communication centers, access points, access nodes, Node Bs, and modem pool transceivers (MPTs). </paragraph>
<paragraph id="P-0012" lvl="0"><number>&lsqb;0012&rsqb;</number> Complex voice recognition tasks require significant computational resources. Such systems cannot practically reside on a subscriber unit having limited CPU, battery, and memory resources. Distributed systems leverage the computational resources available on the network. In a typical DVR system, the word decoder has significantly higher computational and memory requirements than the front end of the voice recognizer. Thus a server based voice recognition system within the network serves as the backend of the voice recognition system and performs word decoding. Using the server based VR system as the backend provides the benefit of performing complex VR tasks using network resources rather than user device resources. Examples of DVR systems are disclosed in U.S. Pat. No. 5,956,683, entitled &ldquo;Distributed Voice Recognition System,&rdquo; assigned to the assignee of the present invention and incorporated by reference herein. </paragraph>
<paragraph id="P-0013" lvl="0"><number>&lsqb;0013&rsqb;</number> The subscriber may perform simple VR tasks in addition to the feature extraction function. Performance of these functions at the user terminal frees the network of the need to engage in simple VR tasks, thereby reducing network traffic and the associated cost of providing speech enabled services. In certain circumstances, traffic congestion on the network can result in poor service for subscriber units from the server based VR system. A distributed VR system enables rich user interface features using complex VR tasks, with the downside of increased network traffic and occasional delay. If a local VR engine on the subscriber unit fails to recognize a user&apos;s spoken commands, the user&apos;s spoken commands must be transmitted to the server based VR engine after front end processing, thereby increasing network traffic and network congestion. Network congestion occurs when a significant quantity of network traffic is concurrently transmitted from subscriber units to the server based VR system. After the network based VR engine interprets the spoken commands, the results must be transmitted back to the subscriber unit, which can introduce system delays if network congestion is present. </paragraph>
<paragraph id="P-0014" lvl="0"><number>&lsqb;0014&rsqb;</number> In a DVR system, a need exists to extract robust acoustic features and transmit them with minimal delay over the network. </paragraph>
</section>
<section>
<heading lvl="1">SUMMARY </heading>
<paragraph id="P-0015" lvl="0"><number>&lsqb;0015&rsqb;</number> The aspects described herein are directed to a system and method for computing robust acoustic features and speech activity on a device and further transmitting these to a device on a network. A system and method for transmitting speech activity for voice recognition includes a Voice Activity Detection (VAD) module and a Feature Extraction (FE) module on the subscriber unit. </paragraph>
<paragraph id="P-0016" lvl="0"><number>&lsqb;0016&rsqb;</number> In one aspect, a system for processing and transmitting speech information comprises a feature extraction module configured to extract at least one feature from a speech signal, a voice activity detection module configured to detect voice activity within the speech signal and provide an indication of detected voice activity, and a transmitter configured to selectively transmit aspects associated with the indication of detected voice activity from the voice activity detection module and the at least one feature from the feature extraction module. </paragraph>
<paragraph id="P-0017" lvl="0"><number>&lsqb;0017&rsqb;</number> In another aspect, a system for processing speech comprises a terminal feature extraction submodule for extracting at least one feature from the speech, and a terminal compression module for distinguishing the presence of voice activity from silence in the speech to determine voice activity data, compressing the at least one feature, and selectively combining and transmitting the at least one feature with selected voice activity data. </paragraph>
<paragraph id="P-0018" lvl="0"><number>&lsqb;0018&rsqb;</number> In another aspect, a distributed voice recognition system for transmitting speech activity comprises a subscriber unit, comprising a processing/feature extraction element receiving speech activity and converting the speech activity into features, a voice activity detector for detecting voice activity within the speech and providing at least one voice activity indication, and a processor for selectively combining the features with the at least one voice activity indication into advanced front end features, and a transmitter for transmitting the advanced front end features to a remote device. </paragraph>
<paragraph id="P-0019" lvl="0"><number>&lsqb;0019&rsqb;</number> In still another aspect, a subscriber unit comprises a feature extraction module configured to extract a plurality of features of a speech signal, a voice activity detection module configured to detect voice activity within the speech signal and provides an indication of the detected voice activity, and a processor/transmitter coupled to the feature extraction module and the voice activity detection module and configured to selectively receive detected voice activity and the plurality of features and transmit a set of at least one advanced front end feature. </paragraph>
<paragraph id="P-0020" lvl="0"><number>&lsqb;0020&rsqb;</number> In yet another aspect, a subscriber unit comprises means for extracting a plurality of features of a speech signal, means for detecting voice activity with the speech signal and providing an indication of the detected voice activity, and a transmitter coupled to the feature extraction means and the voice activity detection means and configured to selectively transmit indication of detected voice activity in selective combination with the plurality of features to a remote device. </paragraph>
<paragraph id="P-0021" lvl="0"><number>&lsqb;0021&rsqb;</number> In another aspect, a method of transmitting speech activity comprises extracting a plurality of features of a speech signal, detecting voice activity within the speech signal and providing an indication of the detected voice activity, and selectively transmitting the indication of detected voice activity in selective combination with the plurality of features. </paragraph>
<paragraph id="P-0022" lvl="0"><number>&lsqb;0022&rsqb;</number> In another aspect, a method of transmitting speech activity comprises extracting a plurality of features of a speech signal, detecting voice activity with the speech signal and providing an indication of the detected voice activity, and selectively combining the plurality of features with an indication of the detected voice activity, thereby creating an advanced front end combined indication of detected voice activity and features. </paragraph>
<paragraph id="P-0023" lvl="0"><number>&lsqb;0023&rsqb;</number> In another aspect, a method of detecting voice activity comprises receiving nonlinearly transformed filtered spectral data, performing a discrete cosine transformation of the nonlinearly transformed filtered data, providing an estimate of a probability of a current frame being speech based on said discrete cosine transformation, applying a threshold to the estimate, and providing the option of combining the result of said applying to a feature extraction function. </paragraph>
<paragraph id="P-0024" lvl="0"><number>&lsqb;0024&rsqb;</number> In another aspect, a system for detecting speech activity comprises a processor for generating filtered spectral data, a voice activity detector receiving said filtered spectral data and generating an indication of detected voice activity, and a feature extraction module for extracting a plurality of features of a speech signal based on said filtered spectral data, and a transmitter, wherein the system employs at least one of the voice activity detector and feature extraction module to form an advanced front end feature vector and provide the advanced front end feature vector to the transmitter.</paragraph>
</section>
</summary-of-invention>
<brief-description-of-drawings>
<section>
<heading lvl="1">BRIEF DESCRIPTION OF THE DRAWINGS </heading>
<paragraph id="P-0025" lvl="0"><number>&lsqb;0025&rsqb;</number> The features, nature, and advantages of the present invention will become more apparent from the detailed description set forth below when taken in conjunction with the drawings in which like reference characters identify correspondingly throughout and wherein: </paragraph>
<paragraph id="P-0026" lvl="0"><number>&lsqb;0026&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 1</cross-reference> shows a voice recognition system including an Acoustic Processor and a Word Decoder in accordance with one aspect; </paragraph>
<paragraph id="P-0027" lvl="0"><number>&lsqb;0027&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 2</cross-reference> shows an exemplary aspect of a distributed voice recognition system; </paragraph>
<paragraph id="P-0028" lvl="0"><number>&lsqb;0028&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 3</cross-reference> illustrates delays in an exemplary aspect of a distributed voice recognition system; </paragraph>
<paragraph id="P-0029" lvl="0"><number>&lsqb;0029&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 4</cross-reference> shows a block diagram of a VAD module in accordance with one aspect; </paragraph>
<paragraph id="P-0030" lvl="0"><number>&lsqb;0030&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 5</cross-reference> shows a block diagram of a VAD submodule in accordance with one aspect; </paragraph>
<paragraph id="P-0031" lvl="0"><number>&lsqb;0031&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 6</cross-reference> shows a block diagram of a combined VAD submodule and FE module in accordance with one aspect; </paragraph>
<paragraph id="P-0032" lvl="0"><number>&lsqb;0032&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 7</cross-reference> shows a VAD module state diagram in accordance with one aspect; </paragraph>
<paragraph id="P-0033" lvl="0"><number>&lsqb;0033&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 8</cross-reference> shows parts of speech and VAD events on a timeline in accordance with one aspect; </paragraph>
<paragraph id="P-0034" lvl="0"><number>&lsqb;0034&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 9</cross-reference> an overall system block diagram including terminal and server components; </paragraph>
<paragraph id="P-0035" lvl="0"><number>&lsqb;0035&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 10</cross-reference> shows frame information for the mth frame; </paragraph>
<paragraph id="P-0036" lvl="0"><number>&lsqb;0036&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 11</cross-reference> is the CRC protected packet stream; and </paragraph>
<paragraph id="P-0037" lvl="0"><number>&lsqb;0037&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 12</cross-reference> shows server feature vector generation. </paragraph>
</section>
</brief-description-of-drawings>
<detailed-description>
<section>
<heading lvl="1">DETAILED DESCRIPTION </heading>
<paragraph id="P-0038" lvl="0"><number>&lsqb;0038&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 1</cross-reference> illustrates a voice recognition system <highlight><bold>2</bold></highlight> including an acoustic processor <highlight><bold>4</bold></highlight> and a word decoder <highlight><bold>6</bold></highlight> in accordance with one aspect of the current system. The word decoder <highlight><bold>6</bold></highlight> includes an acoustic pattern matching element <highlight><bold>8</bold></highlight> and a language modeling element <highlight><bold>10</bold></highlight>. The language modeling element <highlight><bold>10</bold></highlight> is also known by some in the art as a grammar specification element. The acoustic processor <highlight><bold>4</bold></highlight> is coupled to the acoustic matching element <highlight><bold>8</bold></highlight> of the word decoder <highlight><bold>6</bold></highlight>. The acoustic pattern matching element <highlight><bold>8</bold></highlight> is coupled to a language modeling element <highlight><bold>10</bold></highlight>. </paragraph>
<paragraph id="P-0039" lvl="0"><number>&lsqb;0039&rsqb;</number> The acoustic processor <highlight><bold>4</bold></highlight> extracts features from an input speech signal and provides those features to word decoder <highlight><bold>6</bold></highlight>. In general, the word decoder <highlight><bold>6</bold></highlight> translates the acoustic features received from the acoustic processor <highlight><bold>4</bold></highlight> into an estimate of the speaker&apos;s original word string. The estimate is created via acoustic pattern matching and language modeling. Language modeling may be omitted in certain situations, such as applications of isolated word recognition. The acoustic pattern matching element <highlight><bold>8</bold></highlight> detects and classifies possible acoustic patterns, such as phonemes, syllables, words, and so forth. The acoustic pattern matching element <highlight><bold>8</bold></highlight> provides candidate patterns to language modeling element <highlight><bold>10</bold></highlight>, which models syntactic constraint rules to determine gramatically well formed and meaningful word sequences. Syntactic information can be employed in voice recognition when acoustic information alone is ambiguous. The voice recognition system sequentially interprets acoustic feature matching results and provides the estimated word string based on language modeling. </paragraph>
<paragraph id="P-0040" lvl="0"><number>&lsqb;0040&rsqb;</number> Both the acoustic pattern matching and language modeling in the word decoder <highlight><bold>6</bold></highlight> require deterministic or stochastic modeling to describe the speaker&apos;s phonological and acoustic-phonetic variations. Speech recognition system performance is related to the quality of pattern matching and language modeling. Two commonly used models for acoustic pattern matching known by those skilled in the art are template-based dynamic time warping (DTW) and stochastic hidden Markov modeling (HMM). </paragraph>
<paragraph id="P-0041" lvl="0"><number>&lsqb;0041&rsqb;</number> The acoustic processor <highlight><bold>4</bold></highlight> represents a front end speech analysis subsystem of the voice recognizer <highlight><bold>2</bold></highlight>. In response to an input speech signal, the acoustic processor <highlight><bold>4</bold></highlight> provides an appropriate representation to characterize the time varying speech signal. The acoustic processor <highlight><bold>4</bold></highlight> may discard irrelevant information such as background noise, channel distortion, speaker characteristics, and manner of speaking. The acoustic feature may furnish voice recognizers with higher acoustic discrimination power. In this aspect of the invention, the short time spectral envelope is a highly useful characteristic. In characterizing the short time spectral envelope, a commonly used spectral analysis technique is filter-bank based spectral analysis. </paragraph>
<paragraph id="P-0042" lvl="0"><number>&lsqb;0042&rsqb;</number> Combining multiple VR systems, or VR engines, provides enhanced accuracy and uses a greater amount of information from the input speech signal than a single VR system. One system for combining VR engines is described in U.S. patent application Ser. No. 09/618,177, entitled &ldquo;Combined Engine System and Method for Voice Recognition,&rdquo; filed Jul. 18, 2000, and U.S. patent application Ser. No. 09/657,760, entitled &ldquo;System and Method for Automatic Voice Recognition Using Mapping,&rdquo; filed Sep. 8, 2000, assigned to the assignee of the present application and fully incorporated herein by reference. </paragraph>
<paragraph id="P-0043" lvl="0"><number>&lsqb;0043&rsqb;</number> In one aspect of the present system, multiple VR engines are combined into a distributed VR system. The multiple VR engines provide a VR engine at both the subscriber unit and the network server. The VR engine on the subscriber unit is called the local VR engine, while the VR engine on the server is called the network VR engine. The local VR engine comprises a processor for executing the local VR engine and a memory for storing speech information. The network VR engine comprises a processor for executing the network VR engine and a memory for storing speech information. </paragraph>
<paragraph id="P-0044" lvl="0"><number>&lsqb;0044&rsqb;</number> One example of a distributed VR system is disclosed in U.S. patent application Ser. No. 09/755,651, entitled &ldquo;System and Method for Improving Voice Recognition in a Distributed Voice Recognition System,&rdquo; filed Jan. 5, 2001, assigned to the assignee of the present invention and incorporated by reference herein. </paragraph>
<paragraph id="P-0045" lvl="0"><number>&lsqb;0045&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 2</cross-reference> shows one aspect of the present invention. In <cross-reference target="DRAWINGS">FIG. 2</cross-reference>, the environment is a wireless communication system comprising a subscriber unit <highlight><bold>40</bold></highlight> and a central communications center known as a cell base station <highlight><bold>42</bold></highlight>. In this aspect, the distributed VR includes an acoustic processor or feature extraction element <highlight><bold>22</bold></highlight> residing in a subscriber unit <highlight><bold>40</bold></highlight> and a word decoder <highlight><bold>48</bold></highlight> residing in the central communications center. Because of the high computation costs associated with voice recognition implemented solely on a subscriber unit, voice recognition in a non-distributed voice recognition system for even a medium size vocabulary would be highly infeasible. If VR resides solely at the base station or on a remote network, accuracy may be decreased dramatically due to degradation of speech signals associated with speech codec and channel effects. Advantages for a distributed system include reduction in cost of the subscriber unit resulting from the absence of word decoder hardware, and reduction of subscriber unit battery drain associated with local performance of the computationally intensive word decoder operation. A distributed system improves recognition accuracy in addition to providing flexibility and extensibility of the voice recognition functionality. </paragraph>
<paragraph id="P-0046" lvl="0"><number>&lsqb;0046&rsqb;</number> Speech is provided to microphone <highlight><bold>20</bold></highlight>, which converts the speech signal into electrical signals and provided to feature extraction element <highlight><bold>22</bold></highlight>. Signals from microphone <highlight><bold>20</bold></highlight> may be analog or digital. If analog, an AND converter (not shown) may be interposed between microphone <highlight><bold>20</bold></highlight> and feature extraction element <highlight><bold>22</bold></highlight>. Speech signals are provided to feature extraction element <highlight><bold>22</bold></highlight>, which extracts relevant characteristics of the input speech used to decode the linguistic interpretation of the input speech. One example of characteristics used to estimate speech is the frequency characteristics of an input speech frame. Input speech frame characteristics are frequently employed as linear predictive coding parameters of the input speech frame. The extracted speech features are then provided to transmitter <highlight><bold>24</bold></highlight> which codes, modulates, and amplifies the extracted feature signal and provides the features through duplexer <highlight><bold>26</bold></highlight> to antenna <highlight><bold>28</bold></highlight>, where the speech features are transmitted to cellular base station or central communications center <highlight><bold>42</bold></highlight>. Various types of digital coding, modulation, and transmission schemes known in the art may be employed by the transmitter <highlight><bold>24</bold></highlight>. </paragraph>
<paragraph id="P-0047" lvl="0"><number>&lsqb;0047&rsqb;</number> At central communications center <highlight><bold>42</bold></highlight>, the transmitted features are received at antenna <highlight><bold>44</bold></highlight> and provided to receiver <highlight><bold>46</bold></highlight>. Receiver <highlight><bold>46</bold></highlight> may perform the functions of demodulating and decoding received transmitted features, and receiver <highlight><bold>46</bold></highlight> provides these features to word decoder <highlight><bold>48</bold></highlight>. Word decoder <highlight><bold>48</bold></highlight> determines a linguistic estimate of the speech from the speech features and provides an action signal to transmitter <highlight><bold>50</bold></highlight>. Transmitter <highlight><bold>50</bold></highlight> amplifies, modulates, and codes the action signal, and provides the amplified signal to antenna <highlight><bold>52</bold></highlight>. Antenna <highlight><bold>52</bold></highlight> transmits the estimated words or a command signal to portable phone <highlight><bold>40</bold></highlight>. Transmitter <highlight><bold>50</bold></highlight> may also employ digital coding, modulation, or transmission techniques known in the art. </paragraph>
<paragraph id="P-0048" lvl="0"><number>&lsqb;0048&rsqb;</number> At subscriber unit <highlight><bold>40</bold></highlight>, the estimated words or command signals are received at antenna <highlight><bold>28</bold></highlight>, which provides the received signal through duplexer <highlight><bold>26</bold></highlight> to receiver <highlight><bold>30</bold></highlight> which demodulates and decodes the signal and provides command signal or estimated words to control element <highlight><bold>38</bold></highlight>. In response to the received command signal or estimated words, control element <highlight><bold>38</bold></highlight> provides the intended response, such as dialing a phone number, providing information to a display screen on the portable phone, and so forth. </paragraph>
<paragraph id="P-0049" lvl="0"><number>&lsqb;0049&rsqb;</number> In one aspect of the present invention, the information sent from central communications center <highlight><bold>42</bold></highlight> need not be an interpretation of the transmitted speech, but may instead be a response to the decoded message sent by the portable phone. For example, one may inquire about messages on a remote answering machine coupled via a communications network to central communications center <highlight><bold>42</bold></highlight>, in which case the signal transmitted from the central communications center <highlight><bold>42</bold></highlight> to subscriber unit <highlight><bold>40</bold></highlight> may be the messages from the answering machine. A second control element for controlling the data, such as the answering machine messages, may also be located in the central communications center. </paragraph>
<paragraph id="P-0050" lvl="0"><number>&lsqb;0050&rsqb;</number> A VR engine obtains speech data in the form of Pulse Code Modulation, or PCM, signals. The VR engine processes the signal until a valid recognition is made or the user has stopped speaking and all speech has been processed. In one aspect, the DVR architecture includes a local VR engine that obtains PCM data and transmits front end information. The front end information may include cepstral parameters, or may be any type of information or features that characterize the input speech signal. Any type of features known in the art could be used to characterize the input speech signal. </paragraph>
<paragraph id="P-0051" lvl="0"><number>&lsqb;0051&rsqb;</number> For a typical recognition task, the local VR engine obtains a set of trained templates from its memory. The local VR engine obtains a grammar specification from an application. An application is service logic that enables users to accomplish a task using the subscriber unit. This logic is executed by a processor on the subscriber unit. It is a component of a user interface module in the subscriber unit. </paragraph>
<paragraph id="P-0052" lvl="0"><number>&lsqb;0052&rsqb;</number> A system and method for improving storage of templates in a voice recognition system is described in U.S. patent application Ser. No. 09/760,076, entitled &ldquo;System And Method For Efficient Storage Of Voice Recognition Models&rdquo;, filed Jan. 12, 2001, which is assigned to the assignee of the present invention and fully incorporated herein by reference. A system and method for improving voice recognition in noisy environments and frequency mismatch conditions and improving storage of templates is described in U.S. patent application Ser. No. 09/703,191, entitled &ldquo;System and Method for Improving Voice Recognition In Noisy Environments and Frequency Mismatch Conditions&rdquo;, filed Oct. 30, 2000, which is assigned to the assignee of the present invention and fully incorporated herein by reference. </paragraph>
<paragraph id="P-0053" lvl="0"><number>&lsqb;0053&rsqb;</number> A &ldquo;grammar&rdquo; specifies the active vocabulary using sub-word models. Typical grammars include 7-digit phone numbers, dollar amounts, and a name of a city from a set of names. Typical grammar specifications include an &ldquo;Out of Vocabulary (OOV)&rdquo; condition to represent the situation where a confident recognition decision could not be made based on the input speech signal. </paragraph>
<paragraph id="P-0054" lvl="0"><number>&lsqb;0054&rsqb;</number> In one aspect, the local VR engine generates a recognition hypothesis locally if it can handle the VR task specified by the grammar. The local VR engine transmits front-end data to the VR server when the grammar specified is too complex to be processed by the local VR engine. </paragraph>
<paragraph id="P-0055" lvl="0"><number>&lsqb;0055&rsqb;</number> As used herein, a forward link refers to transmission from the network server to a subscriber unit and a reverse link refers to transmission from the subscriber unit to the network server. Transmission time is partitioned into time units. In one aspect of the present system, the transmission time may be partitioned into frames. In another aspect, the transmission time may be partitioned into time slots. In accordance with one aspect, the system partitions data into data packets and transmits each data packet over one or more time units. At each time unit, the base station can direct data transmission to any subscriber unit, which is in communication with the base station. In one aspect, frames may be further partitioned into a plurality of time slots. In yet another aspect, time slots may be further partitioned, such as into half-slots and quarter-slots. </paragraph>
<paragraph id="P-0056" lvl="0"><number>&lsqb;0056&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 3</cross-reference> illustrates delays in an exemplary aspect of a distributed voice recognition system <highlight><bold>100</bold></highlight>. The DVR system <highlight><bold>100</bold></highlight> comprises a subscriber unit <highlight><bold>102</bold></highlight>, a network <highlight><bold>150</bold></highlight>, and a speech recognition (SR) server <highlight><bold>160</bold></highlight>. The subscriber unit <highlight><bold>102</bold></highlight> is coupled to the network <highlight><bold>150</bold></highlight> and the network <highlight><bold>150</bold></highlight> is coupled to the SR server <highlight><bold>160</bold></highlight>. The front-end of the DVR system <highlight><bold>100</bold></highlight> is the subscriber unit <highlight><bold>102</bold></highlight>, which comprises a feature extraction (FE) module <highlight><bold>104</bold></highlight> and a voice activity detection (VAD) module <highlight><bold>106</bold></highlight>. The FE performs feature extraction from a speech signal and compression of resulting features. In one aspect, the VAD module <highlight><bold>106</bold></highlight> determines which frames will be transmitted from a subscriber unit to an SR server. The VAD module <highlight><bold>106</bold></highlight> divides the input speech into segments comprising frames where speech is detected and the adjacent frames before and after the frame with detected speech. In one aspect, an end of each segment (EOS) is marked in a payload by sending a null frame. </paragraph>
<paragraph id="P-0057" lvl="0"><number>&lsqb;0057&rsqb;</number> The VR front end performs front end processing in order to characterize a speech segment. Vector S is a speech signal and vector F and vector V are FE and VAD vectors, respectively. In one aspect, the VAD vector is one element long and the one element contains a binary value. In another aspect, the VAD vector is a binary value concatenated with additional features. In one aspect, the additional features are band energies enabling server fine end-pointing. End-pointing constitutes demarcation of a speech signal into silence and speech segments. Use of band energies to enable server fine end-pointing allows use of additional computational resources to arrive at a more reliable VAD decision. </paragraph>
<paragraph id="P-0058" lvl="0"><number>&lsqb;0058&rsqb;</number> Band energies correspond to bark amplitudes. The Bark scale is a warped frequency scale of critical bands corresponding to human perception of hearing. Bark amplitude calculation is known in the art and described in Lawrence Rabiner &amp; Biing-Hwang Juang, Fundamentals of Speech Recognition (1993), which is fully incorporated herein by reference. In one aspect, digitized PCM speech signals are converted to band energies. </paragraph>
<paragraph id="P-0059" lvl="0"><number>&lsqb;0059&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 3</cross-reference> illustrates delays in an exemplary aspect of a distributed voice recognition system. The delays in computing vectors F and V and transmitting them over the network are shown using Z transform notation. The algorithm latency introduced in computing vector F is k, and in one aspect, the range of k is from 100 to 300 msec. Similarly, the algorithm latency for computing VAD information is j and in one aspect, the range of j is from 10 to 100 msec. Thus, FE feature vectors are available with a delay of k units and VAD information is available with a delay of j units. The delay introduced in transmitting the information over the network is n units. The network delay is the same for both vectors F and V. </paragraph>
<paragraph id="P-0060" lvl="0"><number>&lsqb;0060&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 4</cross-reference> illustrates a block diagram of the VAD module <highlight><bold>400</bold></highlight>. The framing module <highlight><bold>402</bold></highlight> includes an analog-to-digital converter (not shown). In one aspect, the output speech sampling rate of the analog-to-digital converter is 8 kHz. It would be understood by those skilled in the art that other output sampling rates can be used. The speech samples are divided into overlapping frames. In one aspect, the frame length is 25 ms (200 samples) and the frame rate is 10 ms (80 samples). </paragraph>
<paragraph id="P-0061" lvl="0"><number>&lsqb;0061&rsqb;</number> In one aspect of the current system, each frame is windowed by a windowing module <highlight><bold>404</bold></highlight> using a Hamming window function.  
<math-cwu id="MATH-US-00001">
<number>1</number>
<math>
<mrow>
  <mrow>
    <mrow>
      <msub>
        <mi>s</mi>
        <mi>w</mi>
      </msub>
      <mo>&af;</mo>
      <mrow>
        <mo>(</mo>
        <mi>n</mi>
        <mo>)</mo>
      </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
      <mrow>
        <mo>{</mo>
        <mrow>
          <mn>0.54</mn>
          <mo>-</mo>
          <mrow>
            <mn>0.46</mn>
            <mo>.</mo>
            <mrow>
              <mi>cos</mi>
              <mo>&af;</mo>
              <mrow>
                <mo>(</mo>
                <mfrac>
                  <mrow>
                    <mn>2</mn>
                    <mo>&it;</mo>
                    <mrow>
                      <mi>&pi;</mi>
                      <mo>&af;</mo>
                      <mrow>
                        <mo>(</mo>
                        <mrow>
                          <mi>n</mi>
                          <mo>-</mo>
                          <mn>1</mn>
                        </mrow>
                        <mo>)</mo>
                      </mrow>
                    </mrow>
                  </mrow>
                  <mrow>
                    <mi>N</mi>
                    <mo>-</mo>
                    <mn>1</mn>
                  </mrow>
                </mfrac>
                <mo>)</mo>
              </mrow>
            </mrow>
          </mrow>
        </mrow>
        <mo>}</mo>
      </mrow>
      <mo>&CenterDot;</mo>
      <mrow>
        <mi>s</mi>
        <mo>&af;</mo>
        <mrow>
          <mo>(</mo>
          <mi>n</mi>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mrow>
  </mrow>
  <mo>,</mo>
  <mrow>
    <mn>1</mn>
    <mo>&leq;</mo>
    <mi>n</mi>
    <mo>&leq;</mo>
    <mi>N</mi>
  </mrow>
</mrow>
</math>
<mathematica-file id="MATHEMATICA-00001" file="US20030004720A1-20030102-M00001.NB"/>
<image id="EMI-M00001" wi="216.027" he="19.93005" file="US20030004720A1-20030102-M00001.TIF" imf="TIFF" ti="MF"/>
</math-cwu>
</paragraph>
<paragraph id="P-0062" lvl="7"><number>&lsqb;0062&rsqb;</number> where N is the frame length and s(n) and s<highlight><subscript>w</subscript></highlight>(n) are the input and output of the windowing block, respectively. </paragraph>
<paragraph id="P-0063" lvl="0"><number>&lsqb;0063&rsqb;</number> A fast Fourier transform (FFT) module <highlight><bold>406</bold></highlight> computes a magnitude spectrum for each windowed frame. In one aspect, the system uses a fast Fourier transform of length 256 to compute the magnitude spectrum for each windowed frame. The first 129 bins from the magnitude spectrum may be retained for further processing. Fast fourier transformation takes place according to the following equation:  
<math-cwu id="MATH-US-00002">
<number>2</number>
<math>
<mrow>
  <mrow>
    <msub>
      <mi>bin</mi>
      <mi>k</mi>
    </msub>
    <mo>=</mo>
    <mrow>
      <mo>|</mo>
      <mrow>
        <munderover>
          <mo>&Sum;</mo>
          <mrow>
            <mi>n</mi>
            <mo>=</mo>
            <mn>0</mn>
          </mrow>
          <mrow>
            <mrow>
              <mi>F</mi>
              <mo>&it;</mo>
              <mstyle>
                <mtext>&emsp;</mtext>
              </mstyle>
              <mo>&it;</mo>
              <mi>F</mi>
              <mo>&it;</mo>
              <mstyle>
                <mtext>&emsp;</mtext>
              </mstyle>
              <mo>&it;</mo>
              <mi>T</mi>
              <mo>&it;</mo>
              <mstyle>
                <mtext>&emsp;</mtext>
              </mstyle>
              <mo>&it;</mo>
              <mi>L</mi>
            </mrow>
            <mo>-</mo>
            <mn>1</mn>
          </mrow>
        </munderover>
        <mo>&it;</mo>
        <mrow>
          <mrow>
            <msub>
              <mi>s</mi>
              <mi>w</mi>
            </msub>
            <mo>&it;</mo>
            <mrow>
              <mo>(</mo>
              <mi>n</mi>
              <mo>)</mo>
            </mrow>
          </mrow>
          <mo>&it;</mo>
          <msup>
            <mi>&ee;</mi>
            <mrow>
              <mrow>
                <mo>-</mo>
                <mi>j</mi>
              </mrow>
              <mo>&it;</mo>
              <mstyle>
                <mtext>&emsp;</mtext>
              </mstyle>
              <mo>&it;</mo>
              <mi>n</mi>
              <mo>&it;</mo>
              <mstyle>
                <mtext>&emsp;</mtext>
              </mstyle>
              <mo>&it;</mo>
              <mi>k</mi>
              <mo>&it;</mo>
              <mfrac>
                <mrow>
                  <mn>2</mn>
                  <mo>&it;</mo>
                  <mi>&pi;</mi>
                </mrow>
                <mrow>
                  <mi>F</mi>
                  <mo>&it;</mo>
                  <mstyle>
                    <mtext>&emsp;</mtext>
                  </mstyle>
                  <mo>&it;</mo>
                  <mi>F</mi>
                  <mo>&it;</mo>
                  <mstyle>
                    <mtext>&emsp;</mtext>
                  </mstyle>
                  <mo>&it;</mo>
                  <mi>T</mi>
                  <mo>&it;</mo>
                  <mstyle>
                    <mtext>&emsp;</mtext>
                  </mstyle>
                  <mo>&it;</mo>
                  <mi>L</mi>
                </mrow>
              </mfrac>
            </mrow>
          </msup>
        </mrow>
      </mrow>
      <mo>|</mo>
    </mrow>
  </mrow>
  <mo>,</mo>
  <mrow>
    <mi>k</mi>
    <mo>=</mo>
    <mn>0</mn>
  </mrow>
  <mo>,</mo>
  <mi>&hellip;</mi>
  <mo>&it;</mo>
  <mstyle>
    <mtext>&emsp;</mtext>
  </mstyle>
  <mo>,</mo>
  <mrow>
    <mi>FFTL</mi>
    <mo>-</mo>
    <mn>1.</mn>
  </mrow>
</mrow>
</math>
<mathematica-file id="MATHEMATICA-00002" file="US20030004720A1-20030102-M00002.NB"/>
<image id="EMI-M00002" wi="216.027" he="24.97635" file="US20030004720A1-20030102-M00002.TIF" imf="TIFF" ti="MF"/>
</math-cwu>
</paragraph>
<paragraph id="P-0064" lvl="7"><number>&lsqb;0064&rsqb;</number> where s<highlight><subscript>w</subscript></highlight>(n) is the input to the FFT module <highlight><bold>406</bold></highlight>, FFTL is the block length (256), and bin<highlight><subscript>k </subscript></highlight>is the absolute value of the resulting complex vector. The power spectrum (PS) module <highlight><bold>408</bold></highlight> computes a power spectrum by taking the square of the magnitude spectrum. </paragraph>
<paragraph id="P-0065" lvl="0"><number>&lsqb;0065&rsqb;</number> In one aspect, a Mel-filtering module <highlight><bold>409</bold></highlight> computes a MEL-warped spectrum using a complete frequency range &lsqb;0-4000 Hz&rsqb;. This band is divided into 23 channels equidistant in MEL frequency scale, providing 23 energy values per frame. In this aspect, Mel-filtering corresponds to the following equations:  
<math-cwu id="MATH-US-00003">
<number>3</number>
<math>
  <mrow>
    <mrow>
      <mrow>
        <mi>Mel</mi>
        <mo>&it;</mo>
        <mrow>
          <mo>{</mo>
          <mi>x</mi>
          <mo>}</mo>
        </mrow>
      </mrow>
      <mo>=</mo>
      <mrow>
        <mn>2595</mn>
        <mo>*</mo>
        <mrow>
          <msub>
            <mi>log</mi>
            <mn>10</mn>
          </msub>
          <mo>&it;</mo>
          <mrow>
            <mo>(</mo>
            <mrow>
              <mn>1</mn>
              <mo>+</mo>
              <mfrac>
                <mi>x</mi>
                <mn>700</mn>
              </mfrac>
            </mrow>
            <mo>)</mo>
          </mrow>
        </mrow>
      </mrow>
    </mrow>
    <mo>,</mo>
    <mstyle>
      <mtext>&NewLine;</mtext>
    </mstyle>
    <mo>&it;</mo>
    <mrow>
      <msub>
        <mi>f</mi>
        <msub>
          <mi>c</mi>
          <mi>i</mi>
        </msub>
      </msub>
      <mo>=</mo>
      <mrow>
        <msup>
          <mi>Mel</mi>
          <mrow>
            <mo>-</mo>
            <mn>1</mn>
          </mrow>
        </msup>
        <mo>&it;</mo>
        <mrow>
          <mo>{</mo>
          <mrow>
            <mi>i</mi>
            <mo>*</mo>
            <mi>Mel</mi>
            <mo>&it;</mo>
            <mrow>
              <mo>{</mo>
              <mfrac>
                <mrow>
                  <msub>
                    <mi>f</mi>
                    <mi>s</mi>
                  </msub>
                  <mo>/</mo>
                  <mn>2</mn>
                </mrow>
                <mrow>
                  <mn>23</mn>
                  <mo>+</mo>
                  <mn>1</mn>
                </mrow>
              </mfrac>
              <mo>}</mo>
            </mrow>
          </mrow>
          <mo>}</mo>
        </mrow>
      </mrow>
    </mrow>
    <mo>,</mo>
    <mrow>
      <mi>i</mi>
      <mo>=</mo>
      <mn>1</mn>
    </mrow>
    <mo>,</mo>
    <mi>&hellip;</mi>
    <mo>&it;</mo>
    <mstyle>
      <mtext>&emsp;</mtext>
    </mstyle>
    <mo>,</mo>
    <mn>23</mn>
  </mrow>
</math>
<math>
  <mrow>
    <mi>cbin</mi>
    <mo>=</mo>
    <mrow>
      <mi>floor</mi>
      <mo>&it;</mo>
      <mrow>
        <mo>{</mo>
        <mrow>
          <mfrac>
            <msub>
              <mi>f</mi>
              <msub>
                <mi>c</mi>
                <mi>i</mi>
              </msub>
            </msub>
            <msub>
              <mi>f</mi>
              <mi>s</mi>
            </msub>
          </mfrac>
          <mo>*</mo>
          <mi>F</mi>
          <mo>&it;</mo>
          <mstyle>
            <mtext>&emsp;</mtext>
          </mstyle>
          <mo>&it;</mo>
          <mi>F</mi>
          <mo>&it;</mo>
          <mstyle>
            <mtext>&emsp;</mtext>
          </mstyle>
          <mo>&it;</mo>
          <mi>T</mi>
          <mo>&it;</mo>
          <mstyle>
            <mtext>&emsp;</mtext>
          </mstyle>
          <mo>&it;</mo>
          <mi>L</mi>
        </mrow>
        <mo>}</mo>
      </mrow>
    </mrow>
  </mrow>
</math>
<mathematica-file id="MATHEMATICA-00003" file="US20030004720A1-20030102-M00003.NB"/>
<image id="EMI-M00003" wi="216.027" he="60.01695" file="US20030004720A1-20030102-M00003.TIF" imf="TIFF" ti="MF"/>
</math-cwu>
</paragraph>
<paragraph id="P-0066" lvl="7"><number>&lsqb;0066&rsqb;</number> where floor(.) stands for rounding down to the nearest integer. The output of the MEL filter is the weighted sum of the FFT power spectrum values, bin<highlight><subscript>i </subscript></highlight>in each band. Triangular, half overlapped windowing may be employed according to the following equation:  
<math-cwu id="MATH-US-00004">
<number>4</number>
<math>
<mrow>
  <mrow>
    <msub>
      <mi>fbank</mi>
      <mi>k</mi>
    </msub>
    <mo>=</mo>
    <mrow>
      <mrow>
        <munderover>
          <mo>&Sum;</mo>
          <mrow>
            <mi>j</mi>
            <mo>=</mo>
            <msub>
              <mi>cbin</mi>
              <mrow>
                <mi>k</mi>
                <mo>-</mo>
                <mn>1</mn>
              </mrow>
            </msub>
          </mrow>
          <msub>
            <mi>cbin</mi>
            <mi>k</mi>
          </msub>
        </munderover>
        <mo>&it;</mo>
        <mrow>
          <mfrac>
            <mrow>
              <mi>j</mi>
              <mo>-</mo>
              <msub>
                <mi>cbin</mi>
                <mrow>
                  <mi>k</mi>
                  <mo>-</mo>
                  <mn>1</mn>
                </mrow>
              </msub>
            </mrow>
            <mrow>
              <msub>
                <mi>cbin</mi>
                <mi>k</mi>
              </msub>
              <mo>-</mo>
              <msub>
                <mi>cbin</mi>
                <mrow>
                  <mi>k</mi>
                  <mo>-</mo>
                  <mn>1</mn>
                </mrow>
              </msub>
            </mrow>
          </mfrac>
          <mo>&it;</mo>
          <msub>
            <mi>bin</mi>
            <mi>i</mi>
          </msub>
        </mrow>
      </mrow>
      <mo>+</mo>
      <mrow>
        <msubsup>
          <munder>
            <mo>&Sum;</mo>
            <mi>cbin</mi>
          </munder>
          <mi>i</mi>
          <msub>
            <mi>cbin</mi>
            <mrow>
              <mi>i</mi>
              <mo>+</mo>
              <mn>1</mn>
            </mrow>
          </msub>
        </msubsup>
        <mo>&it;</mo>
        <mfrac>
          <mrow>
            <msub>
              <mi>cbin</mi>
              <mrow>
                <mi>k</mi>
                <mo>+</mo>
                <mn>1</mn>
              </mrow>
            </msub>
            <mo>-</mo>
            <mi>j</mi>
          </mrow>
          <mrow>
            <msub>
              <mi>cbin</mi>
              <mrow>
                <mi>k</mi>
                <mo>+</mo>
                <mn>1</mn>
              </mrow>
            </msub>
            <mo>-</mo>
            <msub>
              <mi>cbin</mi>
              <mi>k</mi>
            </msub>
          </mrow>
        </mfrac>
      </mrow>
    </mrow>
  </mrow>
  <mo>,</mo>
</mrow>
</math>
<mathematica-file id="MATHEMATICA-00004" file="US20030004720A1-20030102-M00004.NB"/>
<image id="EMI-M00004" wi="216.027" he="27.13095" file="US20030004720A1-20030102-M00004.TIF" imf="TIFF" ti="MF"/>
</math-cwu>
</paragraph>
<paragraph id="P-0067" lvl="7"><number>&lsqb;0067&rsqb;</number> where k&equals;1, . . . , 23. cbin<highlight><subscript>0 </subscript></highlight>and cbin<highlight><subscript>24 </subscript></highlight>denote FFT bin indices corresponding to the starting frequency and half of the sampling frequency, respectively:  
<math-cwu id="MATH-US-00005">
<number>5</number>
<math>
  <mrow>
    <msub>
      <mi>cbin</mi>
      <mn>0</mn>
    </msub>
    <mo>=</mo>
    <mn>0</mn>
  </mrow>
</math>
<math>
  <mrow>
    <msub>
      <mi>cbin</mi>
      <mn>24</mn>
    </msub>
    <mo>=</mo>
    <mrow>
      <mrow>
        <mi>floor</mi>
        <mo>&it;</mo>
        <mrow>
          <mo>{</mo>
          <mrow>
            <mfrac>
              <mrow>
                <msub>
                  <mi>f</mi>
                  <mi>s</mi>
                </msub>
                <mo>/</mo>
                <mn>2</mn>
              </mrow>
              <msub>
                <mi>f</mi>
                <mi>s</mi>
              </msub>
            </mfrac>
            <mo>*</mo>
            <mi>FFTL</mi>
          </mrow>
          <mo>}</mo>
        </mrow>
      </mrow>
      <mo>=</mo>
      <mrow>
        <mi>FFTL</mi>
        <mo>/</mo>
        <mn>2</mn>
      </mrow>
    </mrow>
  </mrow>
</math>
<mathematica-file id="MATHEMATICA-00005" file="US20030004720A1-20030102-M00005.NB"/>
<image id="EMI-M00005" wi="216.027" he="31.9221" file="US20030004720A1-20030102-M00005.TIF" imf="TIFF" ti="MF"/>
</math-cwu>
</paragraph>
<paragraph id="P-0068" lvl="7"><number>&lsqb;0068&rsqb;</number> It would be understood by those skilled in the art that alternate MEL-filtering equations and parameters may be employed depending on the circumstances. Warping the frequency axis with a Bark Scale in place of a MEL scale is one such example. </paragraph>
<paragraph id="P-0069" lvl="0"><number>&lsqb;0069&rsqb;</number> The output of the Mel-filtering module <highlight><bold>409</bold></highlight> is the weighted sum of FFT power spectrum values in each band. The output of the Mel-filtering module <highlight><bold>409</bold></highlight> passes through a logarithm module <highlight><bold>410</bold></highlight> that performs non- linear transformation of the Mel-filtering output. In one aspect, the non-linear transformation is a natural logarithm. It would be understood by those skilled in the art that other non-linear transformations could be used. </paragraph>
<paragraph id="P-0070" lvl="0"><number>&lsqb;0070&rsqb;</number> A Voice Activity Detector (VAD) sub-module <highlight><bold>412</bold></highlight> takes as input the transformed output of the logarithm module <highlight><bold>409</bold></highlight> and discriminates between speech and non-speech frames. As shown in <cross-reference target="DRAWINGS">FIG. 4</cross-reference>, the transformed output of the logarithm module may be directly transmitted rather than passed to the VAD submodule <highlight><bold>412</bold></highlight>. Bypassing the VAD submodule <highlight><bold>412</bold></highlight> occurs when Voice Activity Detection is not required, such as when no frames of data are present. The VAD sub-module <highlight><bold>412</bold></highlight> detects the presence of voice activity within a frame. The VAD sub-module <highlight><bold>412</bold></highlight> determines whether a frame has voice activity or has no voice activity. In one aspect, the VAD sub-module <highlight><bold>412</bold></highlight> is a three layer Feed-Forward Neural Net. The Feed-Forward Neural Net may be trained to discriminate between speech and non-speech frames using Backpropagation algorithm. The system performs training offline using noisy databases such as the training part of Aurora2-TIDigits and SpeechDatCar-Italian, artificially corrupted TIMIT and Speech in Noise Environment (SPINE) databases. </paragraph>
<paragraph id="P-0071" lvl="0"><number>&lsqb;0071&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 5</cross-reference> shows a block diagram of a VAD sub-module <highlight><bold>500</bold></highlight>. In one aspect, a downsample module <highlight><bold>420</bold></highlight> downsamples the output of the logarithm module by a factor of two. </paragraph>
<paragraph id="P-0072" lvl="0"><number>&lsqb;0072&rsqb;</number> A Discrete Cosine Transform (DCT) module <highlight><bold>422</bold></highlight> calculates cepstral coefficients from the downsampled 23 logarithmic energies on the MEL scale. In one aspect, the DCT module <highlight><bold>422</bold></highlight> calculates 15 cepstral coefficients. </paragraph>
<paragraph id="P-0073" lvl="0"><number>&lsqb;0073&rsqb;</number> A neural net (NN) module <highlight><bold>424</bold></highlight> provides an estimate of the posterior probability of the current frame being speech or non-speech. A threshold module <highlight><bold>426</bold></highlight> applies a threshold to the estimate from the NN module <highlight><bold>424</bold></highlight> in order to convert the estimate to a binary feature. In one aspect, the system uses a threshold of 0.5. </paragraph>
<paragraph id="P-0074" lvl="0"><number>&lsqb;0074&rsqb;</number> A Median Filter module <highlight><bold>427</bold></highlight> smoothes the binary feature. In one aspect, the binary feature is smoothed using an 11-point median filter. In one aspect, the Median Filter module <highlight><bold>427</bold></highlight> removes any short pauses or short bursts of speech of duration less than 40 ms. In one aspect, the Median Filter module <highlight><bold>427</bold></highlight> also adds seven frames before and after the transition from silence to speech. In one aspect, the system sets a bit according to whether a frame is determined to be speech activity or silence. </paragraph>
<paragraph id="P-0075" lvl="0"><number>&lsqb;0075&rsqb;</number> The neural net module <highlight><bold>424</bold></highlight> and median filter module <highlight><bold>427</bold></highlight> may operate as follows. The Neural Net module <highlight><bold>424</bold></highlight> has six input units, fifteen hidden units and one output. Input to the Neural Net module <highlight><bold>424</bold></highlight> may consist of three frames, current frame and two adjacent frames, of two cepstral coefficients, C<highlight><bold>0</bold></highlight> and C<highlight><bold>1</bold></highlight>, derived from the log-Mel-filterbank energies. As the three frames used are after downsampling, they effectively represent five frames of information. During training, neural net module <highlight><bold>424</bold></highlight> has two outputs, one each for speech and non-speech targets. Output of the trained neural net module <highlight><bold>424</bold></highlight> may provide an estimate of the posterior probability of the current frame being speech or non-speech. During testing under normal conditions only the output corresponding to the posterior probability of non-speech is used. A threshold of 0.5 may be applied to the output to convert it to a binary feature. The binary feature may be smoothed using an eleven point median filter corresponding to median filter module <highlight><bold>427</bold></highlight>. Any short pauses or short bursts of speech of duration less than approximately 40 ms are removed by this filtering. The filtering also adds seven frames before and after the transition from silence to speech and speech to silence to detected respectively. Although the eleven point median filter, five frames in the past and five frames ahead, causes a delay of ten frames, or about 100 ms. This delay is the result of downsampling and is absorbed into the 200 ms delay caused by the subsequent LDA filtering. </paragraph>
<paragraph id="P-0076" lvl="0"><number>&lsqb;0076&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 6</cross-reference> shows a block diagram of the FE module <highlight><bold>600</bold></highlight>. A framing module <highlight><bold>602</bold></highlight>, windowing module <highlight><bold>604</bold></highlight>, FFT module <highlight><bold>606</bold></highlight>, PS module <highlight><bold>608</bold></highlight>, MF module <highlight><bold>609</bold></highlight>, and a logarithm module <highlight><bold>610</bold></highlight>, are also part of the FE and serve the same functions in the FE module <highlight><bold>600</bold></highlight> as they do in the VAD module <highlight><bold>400</bold></highlight>. In one aspect, these common modules are shared between the VAD module <highlight><bold>400</bold></highlight> and the FE module <highlight><bold>600</bold></highlight>. </paragraph>
<paragraph id="P-0077" lvl="0"><number>&lsqb;0077&rsqb;</number> A VAD sub-module <highlight><bold>612</bold></highlight> is coupled to the logarithm module <highlight><bold>610</bold></highlight>. A Linear Discriminant Analysis (LDA) module <highlight><bold>428</bold></highlight> is coupled to the VAD sub-module <highlight><bold>612</bold></highlight> and applies an anti-aliasing bandpass filter to the output of the VAD sub-module <highlight><bold>610</bold></highlight>. In one aspect, the bandpass filter is a RASTA filter. An exemplary bandpass filter that can be used in the VR front end is the RASTA filter described in U.S. Pat. No. 5,450,522 entitled, &ldquo;Auditory Model for Parametrization of Speech&rdquo; filed Sep. 12, 1995, which is incorporated by reference herein. As employed herein, the system may filter the time trajectory of log energies for each of the 23 channels using a 41-tap FIR filter. The filter coefficients may be those derived using the linear discriminant analysis (LDA) technique on the phonetically labeled OGI-Stories database known in the art. Two filters may be retained to reduce the memory requirement. These two filters may be further approximated using 41 tap symmetric FIR filters. The filter with 6 Hz cutoff is applied to Mel channels <highlight><bold>1</bold></highlight> and <highlight><bold>2</bold></highlight>, and the filter with 16 Hz cutoff is applied to channels <highlight><bold>3</bold></highlight> to <highlight><bold>23</bold></highlight>. The output of the filters is the weighted sum of the time trajectory centered around the current frame, the weighting being given by the filter coefficients. This temporal filtering assumes a look-ahead of approximately 20 frames, or approximately 200 ms. Again, those skilled in the art may use different computations and coefficients depending on circumstances and desired performance. One skilled in the art understands that the anti-aliasing filter can be omitted under certain circumstances, e.g., the signal from the preceding module is band limited, the alias is removed in later modules, and other circumstances known to one skilled in the art. </paragraph>
<paragraph id="P-0078" lvl="0"><number>&lsqb;0078&rsqb;</number> A downsample module <highlight><bold>430</bold></highlight> downsamples the output of the LDA module. In one aspect, a downsample module <highlight><bold>430</bold></highlight> downsamples the output of the LDA module by a factor of two. Time trajectories of the 23 Mel channels may be filtered only every second frame. </paragraph>
<paragraph id="P-0079" lvl="0"><number>&lsqb;0079&rsqb;</number> A Discrete Cosine Transform (DCT) module <highlight><bold>432</bold></highlight> calculates cepstral coefficients from the downsampled 23 logarithmic energies on the MEL scale. In one aspect, the DCT module <highlight><bold>432</bold></highlight> calculates 15 cepstral coefficients according to the following equation:  
<math-cwu id="MATH-US-00006">
<number>6</number>
<math>
<mrow>
  <mrow>
    <msub>
      <mi>C</mi>
      <mi>i</mi>
    </msub>
    <mo>=</mo>
    <mfrac>
      <mrow>
        <munderover>
          <mo>&Sum;</mo>
          <mrow>
            <mi>j</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mn>23</mn>
        </munderover>
        <mo>&it;</mo>
        <mrow>
          <msub>
            <mi>f</mi>
            <mi>i</mi>
          </msub>
          <mo>*</mo>
          <mrow>
            <mi>cos</mi>
            <mo>&it;</mo>
            <mrow>
              <mo>(</mo>
              <mrow>
                <mfrac>
                  <mrow>
                    <mi>&pi;</mi>
                    <mo>&CenterDot;</mo>
                    <mi>i</mi>
                  </mrow>
                  <mn>23</mn>
                </mfrac>
                <mo>&CenterDot;</mo>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <mi>j</mi>
                    <mo>-</mo>
                    <mn>0.5</mn>
                  </mrow>
                  <mo>)</mo>
                </mrow>
              </mrow>
              <mo>)</mo>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
      <msqrt>
        <mrow>
          <munderover>
            <mo>&Sum;</mo>
            <mrow>
              <mi>j</mi>
              <mo>=</mo>
              <mn>1</mn>
            </mrow>
            <mn>23</mn>
          </munderover>
          <mo>&it;</mo>
          <mrow>
            <mrow>
              <mi>cos</mi>
              <mo>&it;</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mfrac>
                    <mrow>
                      <mi>&pi;</mi>
                      <mo>&CenterDot;</mo>
                      <mi>i</mi>
                    </mrow>
                    <mn>23</mn>
                  </mfrac>
                  <mo>&CenterDot;</mo>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <mi>j</mi>
                      <mo>-</mo>
                      <mn>0.5</mn>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>*</mo>
            <mrow>
              <mi>cos</mi>
              <mo>&it;</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mfrac>
                    <mrow>
                      <mi>&pi;</mi>
                      <mo>&CenterDot;</mo>
                      <mi>i</mi>
                    </mrow>
                    <mn>23</mn>
                  </mfrac>
                  <mo>&CenterDot;</mo>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <mi>j</mi>
                      <mo>-</mo>
                      <mn>0.5</mn>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
          </mrow>
        </mrow>
      </msqrt>
    </mfrac>
  </mrow>
  <mo>,</mo>
  <mrow>
    <mn>0</mn>
    <mo>&leq;</mo>
    <mi>i</mi>
    <mo>&leq;</mo>
    <mn>14</mn>
  </mrow>
</mrow>
</math>
<mathematica-file id="MATHEMATICA-00006" file="US20030004720A1-20030102-M00006.NB"/>
<image id="EMI-M00006" wi="216.027" he="59.05305" file="US20030004720A1-20030102-M00006.TIF" imf="TIFF" ti="MF"/>
</math-cwu>
</paragraph>
<paragraph id="P-0080" lvl="0"><number>&lsqb;0080&rsqb;</number> In order to compensate for the noises, an online normalization (OLN) module <highlight><bold>434</bold></highlight> applies a mean and variance normalization to the cepstral coefficients from the DCT module <highlight><bold>432</bold></highlight>. The estimates of the local mean and variance are updated for each frame. In one aspect, an experimentally determined bias is added to the estimates of the variance before normalizing the features. The bias eliminates the effects of small noisy estimates of the variance in the long silence regions. Dynamic features are derived from the normalized static features. The bias not only saves computation required for normalization but also provides better recognition performance. Normalization may employ the following equations:  
<math-cwu id="MATH-US-00007">
<number>7</number>
<math>
  <mrow>
    <msub>
      <mi>m</mi>
      <mi>t</mi>
    </msub>
    <mo>=</mo>
    <mrow>
      <mrow>
        <msub>
          <mi>m</mi>
          <mrow>
            <mi>t</mi>
            <mo>-</mo>
            <mn>1</mn>
          </mrow>
        </msub>
        <mo>-</mo>
        <mrow>
          <mi>&alpha;</mi>
          <mo>&it;</mo>
          <mrow>
            <mrow>
              <mo>(</mo>
              <mrow>
                <msub>
                  <mi>x</mi>
                  <mi>t</mi>
                </msub>
                <mo>-</mo>
                <msub>
                  <mi>m</mi>
                  <mrow>
                    <mi>t</mi>
                    <mo>-</mo>
                    <mn>1</mn>
                  </mrow>
                </msub>
              </mrow>
              <mo>)</mo>
            </mrow>
            <mo>.</mo>
            <mstyle>
              <mtext>&NewLine;</mtext>
            </mstyle>
            <mo>&it;</mo>
            <msubsup>
              <mi>&sigma;</mi>
              <mi>t</mi>
              <mn>2</mn>
            </msubsup>
          </mrow>
        </mrow>
      </mrow>
      <mo>=</mo>
      <mrow>
        <msubsup>
          <mi>&sigma;</mi>
          <mrow>
            <mi>t</mi>
            <mo>-</mo>
            <mn>1</mn>
          </mrow>
          <mn>2</mn>
        </msubsup>
        <mo>-</mo>
        <mrow>
          <mi>&alpha;</mi>
          <mo>&it;</mo>
          <mrow>
            <mo>&LeftFloor;</mo>
            <mrow>
              <msup>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <msub>
                      <mi>x</mi>
                      <mi>t</mi>
                    </msub>
                    <mo>-</mo>
                    <msub>
                      <mi>m</mi>
                      <mi>t</mi>
                    </msub>
                  </mrow>
                  <mo>)</mo>
                </mrow>
                <mn>2</mn>
              </msup>
              <mo>-</mo>
              <msubsup>
                <mi>&sigma;</mi>
                <mrow>
                  <mi>t</mi>
                  <mo>-</mo>
                  <mn>1</mn>
                </mrow>
                <mn>2</mn>
              </msubsup>
            </mrow>
            <mo>&RightFloor;</mo>
          </mrow>
        </mrow>
      </mrow>
    </mrow>
  </mrow>
</math>
<math>
  <mrow>
    <msub>
      <mi>x</mi>
      <mi>t</mi>
    </msub>
    <mo>=</mo>
    <mfrac>
      <mrow>
        <mo>(</mo>
        <mrow>
          <msub>
            <mi>x</mi>
            <mi>t</mi>
          </msub>
          <mo>-</mo>
          <msub>
            <mi>m</mi>
            <mi>t</mi>
          </msub>
        </mrow>
        <mo>)</mo>
      </mrow>
      <mrow>
        <msub>
          <mi>&sigma;</mi>
          <mi>t</mi>
        </msub>
        <mo>+</mo>
        <mi>&theta;</mi>
      </mrow>
    </mfrac>
  </mrow>
</math>
<mathematica-file id="MATHEMATICA-00007" file="US20030004720A1-20030102-M00007.NB"/>
<image id="EMI-M00007" wi="216.027" he="40.0869" file="US20030004720A1-20030102-M00007.TIF" imf="TIFF" ti="MF"/>
</math-cwu>
</paragraph>
<paragraph id="P-0081" lvl="7"><number>&lsqb;0081&rsqb;</number> where x<highlight><subscript>t </subscript></highlight>is the cepstral coefficient at time t, m<highlight><subscript>t </subscript></highlight>and &sgr;<highlight><subscript>t</subscript></highlight><highlight><superscript>2 </superscript></highlight>are the mean and the variance of the cepstral coefficient estimated at time t, and x<highlight><subscript>t</subscript></highlight>&prime; is the normalized cepstral coefficient at time t. The value of &agr; may be less than one to provide positive estimate of the variance. The value of &agr; may be 0.1 and the bias, &thgr; may be fixed at 1.0. The final feature vector may include 15 cepstral coefficients, including C<highlight><bold>0</bold></highlight>. These 15 cepstral coefficients constitute the front end output. </paragraph>
<paragraph id="P-0082" lvl="0"><number>&lsqb;0082&rsqb;</number> A feature compression module <highlight><bold>436</bold></highlight> compresses the feature vectors. A bit stream formatting and framing module <highlight><bold>438</bold></highlight> performs bitstream formatting of the compressed feature vectors, thereby preparing them for transmission. In one aspect, the feature compression module <highlight><bold>436</bold></highlight> performs error protection of the formatted bit stream. </paragraph>
<paragraph id="P-0083" lvl="0"><number>&lsqb;0083&rsqb;</number> The FE module <highlight><bold>600</bold></highlight> concatenates vector F Z<highlight><superscript>&minus;k </superscript></highlight>and vector V Z<highlight><superscript>&minus;j</superscript></highlight>. Thus, each FE feature vector is comprised of a concatenation of vector F Z<highlight><superscript>&minus;k </superscript></highlight>and vector V Z<highlight><superscript>&minus;j</superscript></highlight>. </paragraph>
<paragraph id="P-0084" lvl="0"><number>&lsqb;0084&rsqb;</number> In the present invention, the system transmits VAD output ahead of a payload, which reduces a DVR system&apos;s overall latency since the front end processing of the VAD is shorter (j&lt;k) than the FE front end processing. In one aspect, an application running on the server can determine the end of a user&apos;s utterance when vector V indicates silence for more than an S<highlight><subscript>hangover </subscript></highlight>period of time. S<highlight><subscript>hangover </subscript></highlight>is the period of silence following active speech for utterance capture to be complete. S<highlight><subscript>hangover </subscript></highlight>is typically greater than an embedded silence allowed in an utterance. If S<highlight><subscript>hangover</subscript></highlight>&gt;k, FE algorithm latency will not increase the response time. FE features corresponding to time t-k and VAD features corresponding to time t-j may be combined to form extended FE features. The system transmits VAD output when available and does not depend on the availability of FE output for transmission. Both the VAD output and the FE output are synchronized with the transmission payload. Information corresponding to each segment of speech may be transmitted without frame dropping. </paragraph>
<paragraph id="P-0085" lvl="0"><number>&lsqb;0085&rsqb;</number> Channel bandwidth may be reduced during silence periods. Vector F is quantized with a lower bit rate when vector V indicates silence regions. This lower rate quantizing is similar to variable rate and multi-rate vocoders where a bit rate is changed based on voice activity detection. The system synchronizes both the VAD output and the FE output with the transmission payload. The system then transmits information corresponding to each segment of speech, thereby transmitting VAD output. The bit rate is reduced on frames with silence. </paragraph>
<paragraph id="P-0086" lvl="0"><number>&lsqb;0086&rsqb;</number> Alternately, only speech frames may be transmitted to the server. Frames with silence are dropped completely. When only speech frames are transmitted to the server, the server may attempt to conclude that the user has finished speaking. This speech completion occurs irrespective of the value of latencies k, j and n. Consider a multi-word like &ldquo;Portland &lt;PAUSE&gt; Maine&rdquo; or &ldquo;617-555- &lt;PAUSE&gt; 1212&rdquo;. The system employs a separate channel to transmit VAD information. FE features corresponding to the &lt;PAUSE&gt; region are dropped at the subscriber unit. As a result, the server would have no information to deduce that a user has finished speaking without a separate channel. This aspect has a separate channel for transmitting VAD information. </paragraph>
<paragraph id="P-0087" lvl="0"><number>&lsqb;0087&rsqb;</number> The status of a recognizer may be maintained even when there are long pauses in the user&apos;s speech as per the state diagram in <cross-reference target="DRAWINGS">FIG. 7</cross-reference> and the events and actions in Table 1. When the system detects speech activity, it transmits an average vector of the FE module <highlight><bold>600</bold></highlight> corresponding to the frames dropped and the total number of frames dropped prior to transmitting speech frames. In addition, when the terminal or mobile detects that S<highlight><subscript>hangover </subscript></highlight>frames of silence have been observed, this signifies an end of the user&apos;s utterance. In one aspect, the speech frames and the total number of frames dropped are transmitted to the server along with the average vector of the FE module <highlight><bold>600</bold></highlight> on the same channel. Thus, the payload includes both features and VAD output. In one aspect, the VAD output is sent last in the payload to indicate end of speech. </paragraph>
<paragraph id="P-0088" lvl="0"><number>&lsqb;0088&rsqb;</number> For a typical utterance, the VAD module <highlight><bold>400</bold></highlight> will begin in Idle state <highlight><bold>702</bold></highlight> and transition to Initial Silence state <highlight><bold>704</bold></highlight> as a result of event A. A few B events may occur, leaving the module in Initial Silence state. When the system detects speech, event C causes a transition to Active Speech state <highlight><bold>706</bold></highlight>. The module then toggles between Active Speech <highlight><bold>706</bold></highlight> and Embedded Silence states <highlight><bold>708</bold></highlight> with events D and E. When the embedded silence is longer than S<highlight><subscript>hangover</subscript></highlight>, this constitutes an end of utterance and event F causes a transition to Idle state <highlight><bold>702</bold></highlight>. Event Z represents a long initial silence in an utterance. This long initial silence facilitates a TIME OUT error condition when a user&apos;s speech is not detected. Event X aborts a given state and returns the module to the Idle state <highlight><bold>702</bold></highlight>. This can be a user or a system initiated event. </paragraph>
<paragraph id="P-0089" lvl="0"><number>&lsqb;0089&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 8</cross-reference> shows parts of speech and VAD events on a timeline. Referring to <cross-reference target="DRAWINGS">FIG. 8</cross-reference> and Table 2, the events causing state transitions are shown with respect to the VAD module <highlight><bold>400</bold></highlight>.  
<table-cwu id="TABLE-US-00001">
<number>1</number>
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="2">
<colspec colname="1" colwidth="21PT" align="center"/>
<colspec colname="2" colwidth="196PT" align="left"/>
<thead>
<row>
<entry namest="1" nameend="2" align="center">TABLE 1</entry>
</row>
<row>
<entry></entry>
</row>
<row><entry namest="1" nameend="2" align="center" rowsep="1"></entry>
</row>
<row>
<entry>Event</entry>
<entry>Action</entry>
</row>
<row><entry namest="1" nameend="2" align="center" rowsep="1"></entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry>A</entry>
<entry>User initiated utterance capture.</entry>
</row>
<row>
<entry>B</entry>
<entry>S<highlight><subscript>active </subscript></highlight>&lt; S<highlight><subscript>min</subscript></highlight>. Active Speech duration is less than minimum</entry>
</row>
<row>
<entry></entry>
<entry>utterance duration. Prevent false detection due to clicks and</entry>
</row>
<row>
<entry></entry>
<entry>other extraneous noises.</entry>
</row>
<row>
<entry>C</entry>
<entry>S<highlight><subscript>active </subscript></highlight>&gt; S<highlight><subscript>min</subscript></highlight>. Initial speech found. Send average FE feature vector,</entry>
</row>
<row>
<entry></entry>
<entry>FDcount, S<highlight><subscript>before </subscript></highlight>frames. Start sending FE feature vectors.</entry>
</row>
<row>
<entry>D</entry>
<entry>S<highlight><subscript>sil </subscript></highlight>&gt; S<highlight><subscript>after</subscript></highlight>. Send S<highlight><subscript>after </subscript></highlight>frames. Reset FDcount to zero.</entry>
</row>
<row>
<entry>E</entry>
<entry>S<highlight><subscript>active </subscript></highlight>&gt; S<highlight><subscript>min</subscript></highlight>. Active speech found after an embedded silence. Send</entry>
</row>
<row>
<entry></entry>
<entry>average FE feature vector, FDcount, S<highlight><subscript>before </subscript></highlight>frames. Start sending</entry>
</row>
<row>
<entry></entry>
<entry>FE feature vectors.</entry>
</row>
<row>
<entry>F</entry>
<entry>S<highlight><subscript>sil </subscript></highlight>&gt; S<highlight><subscript>hangover</subscript></highlight>. End of user&rsquo;s speech is detected. Send average FE</entry>
</row>
<row>
<entry></entry>
<entry>feature vector and FDcount.</entry>
</row>
<row>
<entry>X</entry>
<entry>User initiated abort. Can be user initiated from the keypad, server</entry>
</row>
<row>
<entry></entry>
<entry>initiated when recognition is complete or a higher priority</entry>
</row>
<row>
<entry></entry>
<entry>interrupt in the device.</entry>
</row>
<row>
<entry>Z</entry>
<entry>S<highlight><subscript>sil </subscript></highlight>&gt; MAXSILDURATION. MAXSILDURATION &lt; 2.5 seconds</entry>
</row>
<row>
<entry></entry>
<entry>for 8 bit FDCounter. Send average FE feature vector and FDcount.</entry>
</row>
<row>
<entry></entry>
<entry>Reset FDcount to zero.</entry>
</row>
<row><entry namest="1" nameend="2" align="center" rowsep="1"></entry>
</row>
</tbody>
</tgroup>
</table>
</table-cwu>
</paragraph>
<paragraph id="P-0090" lvl="0"><number>&lsqb;0090&rsqb;</number> In Table 1, S<highlight><subscript>before </subscript></highlight>and S<highlight><subscript>after </subscript></highlight>are the number of silence frames transmitted to the server before and after active speech. </paragraph>
<paragraph id="P-0091" lvl="0"><number>&lsqb;0091&rsqb;</number> From the state diagram and the table of events that show the corresponding actions on the mobile, certain thresholds are used in initiating state transitions. It is possible to use certain default values for these thresholds. However, it would be understood by those skilled in the art that other values for the thresholds shown in Table 1 may be used. </paragraph>
<paragraph id="P-0092" lvl="0"><number>&lsqb;0092&rsqb;</number> In addition, the server can modify the default values depending on the application. The default values are programmable as identified in Table 2.  
<table-cwu id="TABLE-US-00002">
<number>2</number>
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="3">
<colspec colname="1" colwidth="35PT" align="left"/>
<colspec colname="2" colwidth="35PT" align="left"/>
<colspec colname="3" colwidth="147PT" align="left"/>
<thead>
<row>
<entry namest="1" nameend="3" align="center">TABLE 2</entry>
</row>
<row>
<entry></entry>
</row>
<row><entry namest="1" nameend="3" align="center" rowsep="1"></entry>
</row>
<row>
<entry></entry>
<entry>Coordi-</entry>
<entry></entry>
</row>
<row>
<entry>Segment</entry>
<entry>nates</entry>
</row>
<row>
<entry>Name</entry>
<entry>in <cross-reference target="DRAWINGS">FIG. 8</cross-reference></entry>
<entry>Description</entry>
</row>
<row><entry namest="1" nameend="3" align="center" rowsep="1"></entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry>S<highlight><subscript>min</subscript></highlight></entry>
<entry>&gt; (b-a)</entry>
<entry>Minimum Utterance Duration in frames. Used to</entry>
</row>
<row>
<entry></entry>
<entry></entry>
<entry>prevent false detection of clicks and noises as</entry>
</row>
<row>
<entry></entry>
<entry></entry>
<entry>active speech.</entry>
</row>
<row>
<entry>S<highlight><subscript>active</subscript></highlight></entry>
<entry>(e-d) and</entry>
<entry>Duration of an active speech segment in frames,</entry>
</row>
<row>
<entry></entry>
<entry>(i-h)</entry>
<entry>as detected by the VAD module.</entry>
</row>
<row>
<entry>S<highlight><subscript>before</subscript></highlight></entry>
<entry>(d-c) and</entry>
<entry>Number of frames to be transmitted before active</entry>
</row>
<row>
<entry></entry>
<entry>(h-g)</entry>
<entry>speech, as detected by the VAD. Amount of</entry>
</row>
<row>
<entry></entry>
<entry></entry>
<entry>silence region to be transmitted preceding active</entry>
</row>
<row>
<entry></entry>
<entry></entry>
<entry>speech.</entry>
</row>
<row>
<entry>S<highlight><subscript>after</subscript></highlight></entry>
<entry>(f-e) and</entry>
<entry>Number of frames to be transmitted after active</entry>
</row>
<row>
<entry></entry>
<entry>(j-i)</entry>
<entry>speech, as detected by the VAD. Amount of</entry>
</row>
<row>
<entry></entry>
<entry></entry>
<entry>silence region to be transmitted following active</entry>
</row>
<row>
<entry></entry>
<entry></entry>
<entry>speech.</entry>
</row>
<row>
<entry>S<highlight><subscript>sil</subscript></highlight></entry>
<entry>(d-0),</entry>
<entry>Duration of current silence segment in frames, as</entry>
</row>
<row>
<entry></entry>
<entry>(h-e),</entry>
<entry>detected by VAD.</entry>
</row>
<row>
<entry></entry>
<entry>(k-i)</entry>
</row>
<row>
<entry>S<highlight><subscript>embedded</subscript></highlight></entry>
<entry>&gt; (h-e)</entry>
<entry>Duration of silence in frames (S<highlight><subscript>sil</subscript></highlight>) between two</entry>
</row>
<row>
<entry></entry>
<entry></entry>
<entry>active speech segments.</entry>
</row>
<row>
<entry>FDcount</entry>
<entry>&mdash;</entry>
<entry>Number of silence frames dropped prior to the</entry>
</row>
<row>
<entry></entry>
<entry></entry>
<entry>current active speech segment.</entry>
</row>
<row>
<entry>S<highlight><subscript>hangover</subscript></highlight></entry>
<entry>&lt; (k-i)</entry>
<entry>Duration of silence in frames (S<highlight><subscript>sil</subscript></highlight>) after the last</entry>
</row>
<row>
<entry></entry>
<entry>&gt; (h-e)</entry>
<entry>active speech segments for utterance capture to</entry>
</row>
<row>
<entry></entry>
<entry></entry>
<entry>be complete. S<highlight><subscript>hangover </subscript></highlight>&gt;&equals; S<highlight><subscript>embedded</subscript></highlight></entry>
</row>
<row>
<entry>S<highlight><subscript>maxsil</subscript></highlight></entry>
<entry></entry>
<entry>Maximum silence duration in which the mobile</entry>
</row>
<row>
<entry></entry>
<entry></entry>
<entry>drops frames. If the maximum silence duration is</entry>
</row>
<row>
<entry></entry>
<entry></entry>
<entry>exceeded, then the mobile sends an average FE</entry>
</row>
<row>
<entry></entry>
<entry></entry>
<entry>feature vector and resets the counter to zero.</entry>
</row>
<row>
<entry></entry>
<entry></entry>
<entry>This is useful for keeping the recognition state on</entry>
</row>
<row>
<entry></entry>
<entry></entry>
<entry>the server active.</entry>
</row>
<row>
<entry>S<highlight><subscript>minsil</subscript></highlight></entry>
<entry></entry>
<entry>Minimum silence duration expected before and</entry>
</row>
<row>
<entry></entry>
<entry></entry>
<entry>after active speech. If less than Sminsil is</entry>
</row>
<row>
<entry></entry>
<entry></entry>
<entry>observed prior to active speech, the server may</entry>
</row>
<row>
<entry></entry>
<entry></entry>
<entry>decide not to perform certain adaptation tasks</entry>
</row>
<row>
<entry></entry>
<entry></entry>
<entry>using the data. This is sometimes termed</entry>
</row>
<row>
<entry></entry>
<entry></entry>
<entry>Spoke_Too_Soon error. The server can deduce</entry>
</row>
<row>
<entry></entry>
<entry></entry>
<entry>this condition from the Fdcount value and a</entry>
</row>
<row>
<entry></entry>
<entry></entry>
<entry>separate variable may not be needed.</entry>
</row>
<row><entry namest="1" nameend="3" align="center" rowsep="1"></entry>
</row>
</tbody>
</tgroup>
</table>
</table-cwu>
</paragraph>
<paragraph id="P-0093" lvl="0"><number>&lsqb;0093&rsqb;</number> In one aspect, the minimum utterance duration S<highlight><subscript>min </subscript></highlight>is around 100 msec. In another aspect, the amount of silence region to be transmitted preceding active speech S<highlight><subscript>before </subscript></highlight>is around 200 msec. In another aspect, S<highlight><subscript>after</subscript></highlight>, the amount of silence to be transmitted following active speech is around 200 msec. In another aspect, the amount of silence duration following active speech for utterance capture to be complete, S<highlight><subscript>hangover</subscript></highlight>, is between 500 msec to 1500 msec., depending on the VR application. In still another aspect, an eight bit counter enables 2.5 seconds of S<highlight><subscript>maxsil </subscript></highlight>at 100 frames per second. In yet another aspect, minimum silence duration expected before and after active speech S<highlight><subscript>minsil </subscript></highlight>is around 200 msec. </paragraph>
<paragraph id="P-0094" lvl="0"><number>&lsqb;0094&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 9</cross-reference> shows the overall system design. Speech passes through the terminal feature extraction module <highlight><bold>901</bold></highlight>, which operates as illustrated in <cross-reference target="DRAWINGS">FIGS. 4, 5</cross-reference>, and <highlight><bold>6</bold></highlight>. Terminal compression module <highlight><bold>902</bold></highlight> is employed to compress the features extracted, and output from the terminal compression module passes over the channel to the server. Server decompression module <highlight><bold>911</bold></highlight> decompresses the data and passes it to server feature vector generation module <highlight><bold>912</bold></highlight>, which passes data to HTK module <highlight><bold>913</bold></highlight>. </paragraph>
<paragraph id="P-0095" lvl="0"><number>&lsqb;0095&rsqb;</number> Terminal compression module <highlight><bold>902</bold></highlight> employs vector quantization to quantize the features. The feature vector received from the front end is quantized at the terminal compression module <highlight><bold>902</bold></highlight> with a split vector quantizer. Received coefficients are grouped into pairs, except C<highlight><bold>0</bold></highlight>, and each pair is quantized using its own vector quantization codebook. The resulting set of index values is used to represent the speech frame. One aspect of coefficient pairings with corresponding codebook sizes are shown in Table 3. Those of skill in the art will appreciate that other pairings and codebook sizes may be employed while still within the scope of the present invention.  
<table-cwu id="TABLE-US-00003">
<number>3</number>
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="5">
<colspec colname="1" colwidth="56PT" align="center"/>
<colspec colname="2" colwidth="21PT" align="center"/>
<colspec colname="3" colwidth="70PT" align="center"/>
<colspec colname="4" colwidth="35PT" align="center"/>
<colspec colname="5" colwidth="35PT" align="center"/>
<thead>
<row>
<entry namest="1" nameend="5" align="center">TABLE 3</entry>
</row>
<row>
<entry></entry>
</row>
<row><entry namest="1" nameend="5" align="center" rowsep="1"></entry>
</row>
<row>
<entry>Codebook</entry>
<entry>Size</entry>
<entry>Weight Matrix</entry>
<entry>Elements</entry>
<entry>Bits</entry>
</row>
<row><entry namest="1" nameend="5" align="center" rowsep="1"></entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry>Q0-1</entry>
<entry>32</entry>
<entry>I</entry>
<entry>C13-14</entry>
<entry>5</entry>
</row>
<row>
<entry>Q2-3</entry>
<entry>32</entry>
<entry>I</entry>
<entry>C11, C12</entry>
<entry>5</entry>
</row>
<row>
<entry>Q4-5</entry>
<entry>32</entry>
<entry>I</entry>
<entry>C9, C10</entry>
<entry>5</entry>
</row>
<row>
<entry>Q6-7</entry>
<entry>32</entry>
<entry>I</entry>
<entry>C7, C8</entry>
<entry>5</entry>
</row>
<row>
<entry>Q8-9</entry>
<entry>32</entry>
<entry>I</entry>
<entry>C5, C6</entry>
<entry>5</entry>
</row>
<row>
<entry>Q10-11</entry>
<entry>64</entry>
<entry>I</entry>
<entry>C3, C4</entry>
<entry>6</entry>
</row>
<row>
<entry>Q12-13</entry>
<entry>128&ensp;</entry>
<entry>I</entry>
<entry>C1, C2</entry>
<entry>7</entry>
</row>
<row>
<entry>Q14</entry>
<entry>64</entry>
<entry>I</entry>
<entry>C0</entry>
<entry>6</entry>
</row>
<row><entry namest="1" nameend="5" align="center" rowsep="1"></entry>
</row>
</tbody>
</tgroup>
</table>
</table-cwu>
</paragraph>
<paragraph id="P-0096" lvl="0"><number>&lsqb;0096&rsqb;</number> To determine the index, the system may find the closest vector quantized (VQ) centroid using a Euclidean distance, with the weight matrix set to the identity matrix. The number of bits required for description of one frame after packing indices to the bit stream may be approximately 44. The LBG algorithm, known in the art, is used for training of the codebook. The system initializes the codebook with the mean value of all training data. In every step, the system splits each centroid into two and the two values are re-estimated. Splitting is performed in the positive and negative direction of standard deviation vector multiplied by 0.2 according to the following equations: </paragraph>
<paragraph lvl="0"><in-line-formula><highlight><italic>&mgr;</italic></highlight><highlight><subscript>i</subscript></highlight><highlight><superscript>&minus;</superscript></highlight>&equals;&mgr;<highlight><subscript>i</subscript></highlight>&minus;0.2.&sgr;<highlight><subscript>i</subscript></highlight></in-line-formula></paragraph>
<paragraph lvl="0"><in-line-formula><highlight><italic>&mgr;</italic></highlight><highlight><subscript>i</subscript></highlight><highlight><superscript>&plus;</superscript></highlight>&equals;&mgr;<highlight><subscript>i</subscript></highlight>&plus;0.2.&sgr;<highlight><subscript>i</subscript></highlight></in-line-formula></paragraph>
<paragraph id="P-0097" lvl="7"><number>&lsqb;0097&rsqb;</number> where &mgr;<highlight><subscript>i </subscript></highlight>and &sgr;<highlight><subscript>i </subscript></highlight>are the mean and standard deviation of the ith cluster respectively. </paragraph>
<paragraph id="P-0098" lvl="0"><number>&lsqb;0098&rsqb;</number> The bitstream employed to transmit the compressed feature vectors is as shown in <cross-reference target="DRAWINGS">FIG. 10</cross-reference>. The frame structure is well known in the art and the frame with a modified frame packet stream definition. One common example of frame structure is defined in ETSI ES 201 108 v1.1.2, &ldquo;Distributed Speech Recognition; Front-end Feature Extraction Algorithm; Compression Algorithm&rdquo;, April 2000 (&ldquo;the ETSI document&rdquo;), the entirety of which is incorporated herein by reference. The ETSI document discusses the multiframe format, the synchronization sequence, and the header field. Indices for a single frame are formatted as shown in <cross-reference target="DRAWINGS">FIG. 10</cross-reference>. Precise alignment with octet boundaries can vary from frame to frame. From <cross-reference target="DRAWINGS">FIG. 10</cross-reference>, two frames of indices or 88 bits are grouped together as pair. The features may be downsampled, and thus the same frame is repeated as shown in <cross-reference target="DRAWINGS">FIG. 11</cross-reference>. This frame repetition avoids delays in feature transmission. The system employs a four bit cyclic redundancy check (CRC) and combines the frame pair packets to fill the 138 octet feature stream commonly employed, such as in the ETSI document. The resulting format requires a data rate of 4800 bits/s. </paragraph>
<paragraph id="P-0099" lvl="0"><number>&lsqb;0099&rsqb;</number> On the server side, the server performs bitstream decoding and error mitigation as follows. An example of bitstream decoding, synchronization sequence detection, header decoding, and feature decompression may be found in the ETSI document. Error mitigation occurs in the present system by first detecting frames received with errors and subsequently substituting parameter values for frames received with errors. The system may use two methods to determine if a frame pair packet has been received with errors, CRC and Data Consistency. For the CRC method, an error exists when the CRC recomputed from the indices of the received frame pair packet data does not match the received CRC for the frame pair. For the Data Consistency method, the server compares parameters corresponding to each index, idx<highlight><superscript>i,i&plus;1 </superscript></highlight>of the two frames within a frame packet pair to determine if either of the indices are received with errors according to the following equation:  
<math-cwu id="MATH-US-00008">
<number>8</number>
<math>
<mrow>
  <msub>
    <mi>badindexflag</mi>
    <mi>i</mi>
  </msub>
  <mo>=</mo>
  <mrow>
    <mo>{</mo>
    <mtable>
      <mtr>
        <mtd>
          <mrow>
            <mrow>
              <mn>1</mn>
              <mo>&it;</mo>
              <mstyle>
                <mtext>&emsp;</mtext>
              </mstyle>
              <mo>&it;</mo>
              <mi>if</mi>
              <mo>&it;</mo>
              <mstyle>
                <mtext>&emsp;</mtext>
              </mstyle>
              <mo>&it;</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mrow>
                    <mrow>
                      <msub>
                        <mi>y</mi>
                        <mi>i</mi>
                      </msub>
                      <mo>&af;</mo>
                      <mrow>
                        <mo>(</mo>
                        <mrow>
                          <mi>m</mi>
                          <mo>+</mo>
                          <mn>1</mn>
                        </mrow>
                        <mo>)</mo>
                      </mrow>
                    </mrow>
                    <mo>-</mo>
                    <mrow>
                      <msub>
                        <mi>y</mi>
                        <mi>i</mi>
                      </msub>
                      <mo>&af;</mo>
                      <mrow>
                        <mo>(</mo>
                        <mi>m</mi>
                        <mo>)</mo>
                      </mrow>
                    </mrow>
                  </mrow>
                  <mo>&gt;</mo>
                  <mn>0</mn>
                </mrow>
                <mo>)</mo>
              </mrow>
              <mo>&it;</mo>
              <mstyle>
                <mtext>&emsp;</mtext>
              </mstyle>
              <mo>&it;</mo>
              <mrow>
                <mi>OR</mi>
                <mo>&it;</mo>
                <mrow>
                  <mstyle>
                    <mtext>&emsp;</mtext>
                  </mstyle>
                  <mo>&it;</mo>
                  <mstyle>
                    <mtext>&emsp;</mtext>
                  </mstyle>
                </mrow>
                <mo>(</mo>
                <mrow>
                  <mrow>
                    <mrow>
                      <msub>
                        <mi>y</mi>
                        <mrow>
                          <mi>i</mi>
                          <mo>+</mo>
                          <mn>1</mn>
                        </mrow>
                      </msub>
                      <mo>&af;</mo>
                      <mrow>
                        <mo>(</mo>
                        <mrow>
                          <mi>m</mi>
                          <mo>+</mo>
                          <mn>1</mn>
                        </mrow>
                        <mo>)</mo>
                      </mrow>
                    </mrow>
                    <mo>-</mo>
                    <mrow>
                      <msub>
                        <mi>y</mi>
                        <mrow>
                          <mi>i</mi>
                          <mo>+</mo>
                          <mn>1</mn>
                        </mrow>
                      </msub>
                      <mo>&af;</mo>
                      <mrow>
                        <mo>(</mo>
                        <mi>m</mi>
                        <mo>)</mo>
                      </mrow>
                    </mrow>
                  </mrow>
                  <mo>&gt;</mo>
                  <mn>0</mn>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>&it;</mo>
            <mstyle>
              <mtext>&emsp;</mtext>
            </mstyle>
          </mrow>
        </mtd>
      </mtr>
      <mtr>
        <mtd>
          <mtable>
            <mtr>
              <mtd>
                <mrow>
                  <mn>0</mn>
                  <mo>&it;</mo>
                  <mstyle>
                    <mtext>&emsp;</mtext>
                  </mstyle>
                </mrow>
              </mtd>
              <mtd>
                <mrow>
                  <mi>otherwise</mi>
                  <mo>&it;</mo>
                  <mstyle>
                    <mtext>&emsp;</mtext>
                  </mstyle>
                </mrow>
              </mtd>
              <mtd>
                <mrow>
                  <mstyle>
                    <mtext>&emsp;</mtext>
                  </mstyle>
                  <mo>&it;</mo>
                  <mrow>
                    <mrow>
                      <mi>i</mi>
                      <mo>=</mo>
                      <mn>0</mn>
                    </mrow>
                    <mo>,</mo>
                    <mn>2</mn>
                    <mo>,</mo>
                    <mi>&hellip;</mi>
                    <mo>&it;</mo>
                    <mstyle>
                      <mtext>&emsp;</mtext>
                    </mstyle>
                    <mo>,</mo>
                    <mn>13</mn>
                  </mrow>
                </mrow>
              </mtd>
            </mtr>
          </mtable>
        </mtd>
      </mtr>
    </mtable>
  </mrow>
</mrow>
</math>
<mathematica-file id="MATHEMATICA-00008" file="US20030004720A1-20030102-M00008.NB"/>
<image id="EMI-M00008" wi="216.027" he="34.0767" file="US20030004720A1-20030102-M00008.TIF" imf="TIFF" ti="MF"/>
</math-cwu>
</paragraph>
<paragraph id="P-0100" lvl="7"><number>&lsqb;0100&rsqb;</number> The frame pair packet is classified as received with error if:  
<math-cwu id="MATH-US-00009">
<number>9</number>
<math>
<mrow>
  <mrow>
    <munder>
      <mo>&Sum;</mo>
      <mrow>
        <mrow>
          <mi>i</mi>
          <mo>=</mo>
          <mn>0</mn>
        </mrow>
        <mo>,</mo>
        <mn>2</mn>
        <mo>,</mo>
        <mstyle>
          <mtext>&emsp;</mtext>
        </mstyle>
        <mo>&it;</mo>
        <mi>&hellip;</mi>
        <mo>&it;</mo>
        <mstyle>
          <mtext>&emsp;</mtext>
        </mstyle>
        <mo>,</mo>
        <mn>13</mn>
      </mrow>
    </munder>
    <mo>&it;</mo>
    <msub>
      <mi>badindexflag</mi>
      <mi>i</mi>
    </msub>
  </mrow>
  <mo>&GreaterEqual;</mo>
  <mn>2</mn>
</mrow>
</math>
<mathematica-file id="MATHEMATICA-00009" file="US20030004720A1-20030102-M00009.NB"/>
<image id="EMI-M00009" wi="216.027" he="19.93005" file="US20030004720A1-20030102-M00009.TIF" imf="TIFF" ti="MF"/>
</math-cwu>
</paragraph>
<paragraph id="P-0101" lvl="7"><number>&lsqb;0101&rsqb;</number> The system may apply the Data Consistency check for errored data when the server detects frame pair packets failing the CRC test. The server may apply the Data Consistency check to the frame pair packet received before the one failing the CRC test and subsequently to frames after one failing the CRC test until one is found that passes the Data Consistency test. </paragraph>
<paragraph id="P-0102" lvl="0"><number>&lsqb;0102&rsqb;</number> After the server has determined frames with errors, it substitutes parameter values for frames received with errors, such as in the manner presented in the ETSI document. </paragraph>
<paragraph id="P-0103" lvl="0"><number>&lsqb;0103&rsqb;</number> Server feature vector generation occurs according to <cross-reference target="DRAWINGS">FIG. 12</cross-reference>. From <cross-reference target="DRAWINGS">FIG. 12</cross-reference>, server decompression transmits 15 features in 20 milliseconds. Delta computation module <highlight><bold>1201</bold></highlight> computes time derivatives, or deltas. The system computes derivatives according to the following regression equation:  
<math-cwu id="MATH-US-00010">
<number>10</number>
<math>
<mrow>
  <msub>
    <mi>delta</mi>
    <mi>t</mi>
  </msub>
  <mo>=</mo>
  <mfrac>
    <mrow>
      <munderover>
        <mo>&Sum;</mo>
        <mrow>
          <mi>l</mi>
          <mo>=</mo>
          <mn>1</mn>
        </mrow>
        <mi>L</mi>
      </munderover>
      <mo>&it;</mo>
      <mrow>
        <mi>l</mi>
        <mo>*</mo>
        <mrow>
          <mo>(</mo>
          <mrow>
            <msub>
              <mi>x</mi>
              <mrow>
                <mi>t</mi>
                <mo>+</mo>
                <mi>l</mi>
              </mrow>
            </msub>
            <mo>-</mo>
            <msub>
              <mi>x</mi>
              <mrow>
                <mi>t</mi>
                <mo>-</mo>
                <mi>l</mi>
              </mrow>
            </msub>
          </mrow>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mrow>
    <mrow>
      <mn>2</mn>
      <mo>&it;</mo>
      <mrow>
        <munderover>
          <mo>&Sum;</mo>
          <mrow>
            <mi>l</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mi>L</mi>
        </munderover>
        <mo>&it;</mo>
        <msup>
          <mi>l</mi>
          <mn>2</mn>
        </msup>
      </mrow>
    </mrow>
  </mfrac>
</mrow>
</math>
<mathematica-file id="MATHEMATICA-00010" file="US20030004720A1-20030102-M00010.NB"/>
<image id="EMI-M00010" wi="216.027" he="45.1332" file="US20030004720A1-20030102-M00010.TIF" imf="TIFF" ti="MF"/>
</math-cwu>
</paragraph>
<paragraph id="P-0104" lvl="7"><number>&lsqb;0104&rsqb;</number> where x<highlight><subscript>t </subscript></highlight>is the t<highlight><subscript>th </subscript></highlight>frame of the feature vector </paragraph>
<paragraph id="P-0105" lvl="0"><number>&lsqb;0105&rsqb;</number> The system computes second order derivatives by applying this equation to already calculated deltas. The system then concatenates the original 15-dimensional features by the derivative and double derivative at concatenation block <highlight><bold>1202</bold></highlight>, yielding an augmented 45-dimensional feature vector. When calculating the first derivatives, the system may use an L of size 2, but may use an L of size 1 when calculating the double derivatives. Those of skill in the art will recognize that other parameters may be used while still within the scope of the present invention, and other calculations may be employed to compute the delta and derivatives. Use of low L sizes keeps latency relatively low, such as on the order of 40 ms, corresponding to two frames of future input. </paragraph>
<paragraph id="P-0106" lvl="0"><number>&lsqb;0106&rsqb;</number> KLT Block <highlight><bold>1203</bold></highlight> represents a Contextual Karhunen-Loeve Transformation (Principal Component Analysis), whereby three consecutive frames (one frame in the past plus current frame plus one frame in the future) of the 45-dimensional vector are stacked together to form a 1 by 135 vector. Prior to mean normalization, the server projects this vector using basis functions obtained through principal component analysis (PCA) on noisy training data. One example of PCA that may be employed uses a portion of the TIMIT database downsampled to 8 Khz and artificially corrupted by various types of noises at different signal to noise ratios. More precisely, the PCA takes 5040 utterances from the core training set of TIMIT and equally divides this set into 20 equal sized sets. The PCA may then add the four noises found in the Test A set of Aurora2&apos;s English digits, i.e., subway, babble, car, and exhibition, at signal to noise ratios of clean, 20, 15, 10, and 5 dB. The PCA keeps only the first 45 elements corresponding to the largest eigenvalues and employs a vector-matrix multiplication. </paragraph>
<paragraph id="P-0107" lvl="0"><number>&lsqb;0107&rsqb;</number> The server may apply a non-linear transformation to the augmented 45-dimensional feature vector, such as one using a feed-forward multilayer perceptron (MLP) in MLP module <highlight><bold>1204</bold></highlight>. One example of an MLP is that shown in Bourlard and Morgan, &ldquo;Connectionist Speech Recognition a Hybrid Approach,&rdquo; Kluwer Academic Publishers, 1994, the entirety of which is incorporated herein by reference. The server stacks five consecutive feature frames together to yield a 225 dimensional input vector to the MLP. This stacking can create a delay of two frames (40ms). The server then normalizes this 225 dimensional input vector by subtracting and dividing the global mean and the standard deviation calculated on features from a training corpus respectively. The MLP has two layers excluding the input layer; the hidden layer consists of 500 units equipped with sigmoid activation function, while the output layer consists of 56 output units equipped with softmax activation function. The MLP is trained on phonetic targets (typically 56 monophones for English) from a labeled database with added noise such as that outlined above with respect to the PCA transformation. During recognition, the server may not use the softmax function in the output units, so the output of this block corresponds to &ldquo;linear outputs&rdquo; of the MLP&apos;s hidden layer. The server also subtracts the average of the 56 &ldquo;linear outputs&rdquo; from each of the &ldquo;linear outputs&rdquo; according to the following equation:  
<math-cwu id="MATH-US-00011">
<number>11</number>
<math>
<mrow>
  <msubsup>
    <mi>LinOut</mi>
    <mi>i</mi>
    <mo>*</mo>
  </msubsup>
  <mo>=</mo>
  <mrow>
    <msub>
      <mi>LinOut</mi>
      <mi>i</mi>
    </msub>
    <mo>-</mo>
    <mrow>
      <mfrac>
        <mrow>
          <munderover>
            <mo>&Sum;</mo>
            <mrow>
              <mi>i</mi>
              <mo>=</mo>
              <mn>1</mn>
            </mrow>
            <mn>56</mn>
          </munderover>
          <mo>&it;</mo>
          <msub>
            <mi>LinOut</mi>
            <mi>i</mi>
          </msub>
        </mrow>
        <mn>56</mn>
      </mfrac>
      <mo>&it;</mo>
      <mstyle>
        <mtext>&emsp;</mtext>
      </mstyle>
      <mo>&it;</mo>
      <mtable>
        <mtr>
          <mtd>
            <mrow>
              <mrow>
                <mi>where</mi>
                <mo>&it;</mo>
                <mstyle>
                  <mtext>&emsp;</mtext>
                </mstyle>
                <mo>&it;</mo>
                <msub>
                  <mi>LinOut</mi>
                  <mi>i</mi>
                </msub>
                <mo>&it;</mo>
                <mstyle>
                  <mtext>&emsp;</mtext>
                </mstyle>
                <mo>&it;</mo>
                <mi>is</mi>
                <mo>&it;</mo>
                <mstyle>
                  <mtext>&emsp;</mtext>
                </mstyle>
                <mo>&it;</mo>
                <mi>the</mi>
                <mo>&it;</mo>
                <mstyle>
                  <mtext>&emsp;</mtext>
                </mstyle>
                <mo>&it;</mo>
                <mi>linear</mi>
                <mo>&it;</mo>
                <mstyle>
                  <mtext>&emsp;</mtext>
                </mstyle>
                <mo>&it;</mo>
                <mi>output</mi>
              </mrow>
              <mo>&it;</mo>
              <mstyle>
                <mtext>&emsp;</mtext>
              </mstyle>
            </mrow>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <mrow>
              <mi>of</mi>
              <mo>&it;</mo>
              <mstyle>
                <mtext>&emsp;</mtext>
              </mstyle>
              <mo>&it;</mo>
              <mi>the</mi>
              <mo>&it;</mo>
              <mstyle>
                <mtext>&emsp;</mtext>
              </mstyle>
              <mo>&it;</mo>
              <mi>ith</mi>
              <mo>&it;</mo>
              <mstyle>
                <mtext>&emsp;</mtext>
              </mstyle>
              <mo>&it;</mo>
              <mi>output</mi>
              <mo>&it;</mo>
              <mstyle>
                <mtext>&emsp;</mtext>
              </mstyle>
              <mo>&it;</mo>
              <mi>unit</mi>
            </mrow>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <mrow>
              <mrow>
                <mi>and</mi>
                <mo>&it;</mo>
                <mstyle>
                  <mtext>&emsp;</mtext>
                </mstyle>
                <mo>&it;</mo>
                <msubsup>
                  <mi>LinOut</mi>
                  <mi>i</mi>
                  <mo>*</mo>
                </msubsup>
                <mo>&it;</mo>
                <mstyle>
                  <mtext>&emsp;</mtext>
                </mstyle>
                <mo>&it;</mo>
                <mi>is</mi>
                <mo>&it;</mo>
                <mstyle>
                  <mtext>&emsp;</mtext>
                </mstyle>
                <mo>&it;</mo>
                <mi>the</mi>
                <mo>&it;</mo>
                <mstyle>
                  <mtext>&emsp;</mtext>
                </mstyle>
                <mo>&it;</mo>
                <mi>mean</mi>
              </mrow>
              <mo>&it;</mo>
              <mstyle>
                <mtext>&emsp;</mtext>
              </mstyle>
            </mrow>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <mrow>
              <mi>subtracted</mi>
              <mo>&it;</mo>
              <mstyle>
                <mtext>&emsp;</mtext>
              </mstyle>
              <mo>&it;</mo>
              <mi>linear</mi>
              <mo>&it;</mo>
              <mstyle>
                <mtext>&emsp;</mtext>
              </mstyle>
              <mo>&it;</mo>
              <mi>output</mi>
            </mrow>
          </mtd>
        </mtr>
      </mtable>
    </mrow>
  </mrow>
</mrow>
</math>
<mathematica-file id="MATHEMATICA-00011" file="US20030004720A1-20030102-M00011.NB"/>
<image id="EMI-M00011" wi="216.027" he="47.03265" file="US20030004720A1-20030102-M00011.TIF" imf="TIFF" ti="MF"/>
</math-cwu>
</paragraph>
<paragraph id="P-0108" lvl="0"><number>&lsqb;0108&rsqb;</number> The server can store each weight of the MLP in two byte words. One example of an MLP module <highlight><bold>1204</bold></highlight> has 225*500&equals;112500 input to hidden weights, 500*56&equals;28000 hidden to output weights, and 500&plus;56&equals;556 bias weights. The total amount of memory for this configuration required to store the weights is 141056 words. For each frame of output from the MLP module <highlight><bold>1204</bold></highlight>, the server may have each unit in the MLP perform a multiplication of its input by its weights, an accumulation, and for the hidden layers a look-up in the table for the sigmoid function evaluation. The look-up table may have a size of 4000 two byte words. Other MLP module configurations may be employed while still within the scope of the present invention. </paragraph>
<paragraph id="P-0109" lvl="0"><number>&lsqb;0109&rsqb;</number> The server performs Dimensionality Reduction and Decorrelation using PCA in PCA block <highlight><bold>1205</bold></highlight>. The server applies PCA to the 56-dimensional &ldquo;linear output&rdquo; of the MLP module <highlight><bold>1204</bold></highlight>. This PCA application projects the features onto a space with orthogonal bases. These bases are pre-computed using PCA on the same data that is used for training the MLP as discussed above. Of the 56 features, the server may select the 28 features corresponding to the largest eigenvalues. This computation involves multiplying a 1 by 56 vector with a 56 by 28 matrix. </paragraph>
<paragraph id="P-0110" lvl="0"><number>&lsqb;0110&rsqb;</number> Second concatenation block <highlight><bold>1206</bold></highlight> concatenates the vectors coming from the two paths for each frame to yield to a 73-dimensional feature vector. Up sample module <highlight><bold>1207</bold></highlight> up samples the feature stream by two. The server uses linear interpolation between successive frames to obtain the up sampled frames. 73 features are thereby transmitted to the HTK algorithm on the server. </paragraph>
<paragraph id="P-0111" lvl="0"><number>&lsqb;0111&rsqb;</number> Thus, a novel and improved method and apparatus for voice recognition has been described. Those of skill in the art will understand that the various illustrative logical blocks, modules, and mapping described in connection with the aspects disclosed herein may be implemented as electronic hardware, computer software, or combinations of both. The various illustrative components, blocks, modules, circuits, and steps have been described generally in terms of their functionality. Whether the functionality is implemented as hardware or software depends upon the particular application and design constraints imposed on the overall system. Skilled artisans recognize the interchangeability of hardware and software under these circumstances, and how best to implement the described functionality for each particular application. </paragraph>
<paragraph id="P-0112" lvl="0"><number>&lsqb;0112&rsqb;</number> As examples, the various illustrative logical blocks, modules, and mapping described in connection with the aspects disclosed herein may be implemented or performed with a processor executing a set of firmware instructions, an application specific integrated circuit (ASIC), a field programmable gate array (FPGA) or other programmable logic device, discrete gate or transistor logic, discrete hardware components such as, e.g., registers, any conventional programmable software module and a processor, or any combination thereof designed to perform the functions described herein. The VAD module <highlight><bold>400</bold></highlight> and the FE module <highlight><bold>600</bold></highlight> may advantageously be executed in a microprocessor, but in the alternative, the VAD module <highlight><bold>400</bold></highlight> and the FE module <highlight><bold>600</bold></highlight> may be executed in any conventional processor, controller, microcontroller, or state machine. The templates could reside in RAM memory, flash memory, ROM memory, EPROM memory, EEPROM memory, registers, hard disk, a removable disk, a CD-ROM, or any other form of storage medium known in the art. The memory (not shown) may be integral to any aforementioned processor (not shown). A processor (not shown) and memory (not shown) may reside in an ASIC (not shown). The ASIC may reside in a telephone. </paragraph>
<paragraph id="P-0113" lvl="0"><number>&lsqb;0113&rsqb;</number> The previous description of the embodiments of the invention is provided to enable any person skilled in the art to make or use the present invention. The various modifications to these embodiments will be readily apparent to those skilled in the art, and the generic principles defined herein may be applied to other embodiments without the use of the inventive faculty. Thus, the present invention is not intended to be limited to the embodiments shown herein but is to be accorded the widest scope consistent with the principles and novel features disclosed herein.</paragraph>
</section>
</detailed-description>
</subdoc-description>
<subdoc-claims>
<heading lvl="1">What is claimed is: </heading>
<claim id="CLM-00001">
<claim-text><highlight><bold>1</bold></highlight>. In a voice recognition system comprising a front end and a back end a feature extraction module, comprising: 
<claim-text>a processing sub-module; and </claim-text>
<claim-text>a feature extraction sub-module communicatively coupled to said processing sub-module; </claim-text>
<claim-text>wherein a digital signal provided from said processing sub-module is downsampled in a downsampling module. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00002">
<claim-text><highlight><bold>2</bold></highlight>. The voice recognition system as claimed in <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein said downsampling module is disposed in said feature extraction sub-module. </claim-text>
</claim>
<claim id="CLM-00003">
<claim-text><highlight><bold>3</bold></highlight>. The voice recognition system as claimed in <dependent-claim-reference depends_on="CLM-00002">claim 2</dependent-claim-reference> further comprising: 
<claim-text>a first filter module communicatively coupled to said processing sub-module and said downsampling module. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00004">
<claim-text><highlight><bold>4</bold></highlight>. The voice recognition system as claimed in <dependent-claim-reference depends_on="CLM-00003">claim 3</dependent-claim-reference> wherein said first filter module is configured to perform filtering in accordance with linear discriminant analysis. </claim-text>
</claim>
<claim id="CLM-00005">
<claim-text><highlight><bold>5</bold></highlight>. The voice recognition system as claimed in <dependent-claim-reference depends_on="CLM-00002">claim 2</dependent-claim-reference>, further comprising: 
<claim-text>a first transformation module communicatively coupled to said downsampling module; and </claim-text>
<claim-text>a normalization module communicatively coupled to said first transformation module. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00006">
<claim-text><highlight><bold>6</bold></highlight>. The voice recognition system as claimed in <dependent-claim-reference depends_on="CLM-00005">claim 5</dependent-claim-reference> wherein said first transformation module is configured to perform discrete cosine transform. </claim-text>
</claim>
<claim id="CLM-00007">
<claim-text><highlight><bold>7</bold></highlight>. The voice recognition system as claimed in <dependent-claim-reference depends_on="CLM-00005">claim 5</dependent-claim-reference>, further comprising: 
<claim-text>a bitstream processor communicatively coupled to said normalization module. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00008">
<claim-text><highlight><bold>8</bold></highlight>. The voice recognition system as claimed in <dependent-claim-reference depends_on="CLM-00007">claim 7</dependent-claim-reference>, further comprising: 
<claim-text>a compressor module communicatively coupled to said normalization module and said bitstream processor. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00009">
<claim-text><highlight><bold>9</bold></highlight>. The voice recognition system as claimed in <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein said processing sub-module comprises: 
<claim-text>a framing module; </claim-text>
<claim-text>a windowing module communicatively coupled to said framing module; </claim-text>
<claim-text>a second transformation module communicatively coupled to said windowing module; </claim-text>
<claim-text>a power spectrum module communicatively coupled to said transform module; </claim-text>
<claim-text>a second filter module communicatively coupled to said power spectrum module; and </claim-text>
<claim-text>a third transformation module communicatively coupled to said second filter module. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00010">
<claim-text><highlight><bold>10</bold></highlight>. The voice recognition system as claimed in <dependent-claim-reference depends_on="CLM-00009">claim 9</dependent-claim-reference>, wherein said framing module is configured to: 
<claim-text>accept speech signal; and </claim-text>
<claim-text>provide a frame of the speech signal. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00011">
<claim-text><highlight><bold>11</bold></highlight>. The voice recognition system as claimed in <dependent-claim-reference depends_on="CLM-00009">claim 9</dependent-claim-reference>, wherein said windowing module is configured to perform windowing by Hamming function. </claim-text>
</claim>
<claim id="CLM-00012">
<claim-text><highlight><bold>12</bold></highlight>. The voice recognition system as claimed in <dependent-claim-reference depends_on="CLM-00009">claim 9</dependent-claim-reference>, wherein said second transformation module is configured to perform a fourier transform. </claim-text>
</claim>
<claim id="CLM-00013">
<claim-text><highlight><bold>13</bold></highlight>. The voice recognition system as claimed in <dependent-claim-reference depends_on="CLM-00009">claim 9</dependent-claim-reference>, wherein said power spectrum module is configured to perform a power spectrum determination. </claim-text>
</claim>
<claim id="CLM-00014">
<claim-text><highlight><bold>14</bold></highlight>. The voice recognition system as claimed in <dependent-claim-reference depends_on="CLM-00009">claim 9</dependent-claim-reference>, wherein said second filter module is configured to perform a MEL filtering. </claim-text>
</claim>
<claim id="CLM-00015">
<claim-text><highlight><bold>15</bold></highlight>. The voice recognition system as claimed in <dependent-claim-reference depends_on="CLM-00009">claim 9</dependent-claim-reference>, wherein said third transformation module is configured to perform a non-linear transformation. </claim-text>
</claim>
<claim id="CLM-00016">
<claim-text><highlight><bold>16</bold></highlight>. The voice recognition system as claimed in <dependent-claim-reference depends_on="CLM-00011">claim 15</dependent-claim-reference>, wherein said non-linear transformation is logarithmic transformation. </claim-text>
</claim>
<claim id="CLM-00017">
<claim-text><highlight><bold>17</bold></highlight>. The voice recognition system as claimed in <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein said feature extraction module is disposed in said front end. </claim-text>
</claim>
<claim id="CLM-00018">
<claim-text><highlight><bold>18</bold></highlight>. The voice recognition system as claimed in <dependent-claim-reference depends_on="CLM-00011">claim 17</dependent-claim-reference>, wherein said front end is disposed in a subscriber terminal. </claim-text>
</claim>
<claim id="CLM-00019">
<claim-text><highlight><bold>19</bold></highlight>. In a voice recognition system comprising a front end and a back end a voice activity detection module, comprising: 
<claim-text>a processing sub-module; and </claim-text>
<claim-text>a voice activity detection sub-module communicatively coupled to said processing sub-module; </claim-text>
<claim-text>wherein a digital signal provided from said processing sub-module is downsampled in a downsampling module. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00020">
<claim-text><highlight><bold>20</bold></highlight>. The voice recognition system as claimed in <dependent-claim-reference depends_on="CLM-00011">claim 19</dependent-claim-reference>, wherein said downsampling module is disposed in said voice activity detection sub-module. </claim-text>
</claim>
<claim id="CLM-00021">
<claim-text><highlight><bold>21</bold></highlight>. The voice recognition system as claimed in <dependent-claim-reference depends_on="CLM-00022">claim 20</dependent-claim-reference>, further comprising: 
<claim-text>a first transformation module communicatively coupled to said downsampling module: </claim-text>
<claim-text>an estimation module communicatively coupled to said transformation module; </claim-text>
<claim-text>a threshold detector communicatively coupled to said estimation module; </claim-text>
<claim-text>a first filter module communicatively coupled to said threshold detector. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00022">
<claim-text><highlight><bold>22</bold></highlight>. The voice recognition system as claimed in <dependent-claim-reference depends_on="CLM-00022">claim 21</dependent-claim-reference> wherein said first transformation module is configured to perform discrete cosine transform. </claim-text>
</claim>
<claim id="CLM-00023">
<claim-text><highlight><bold>23</bold></highlight>. The voice recognition system as claimed in <dependent-claim-reference depends_on="CLM-00022">claim 21</dependent-claim-reference> wherein said estimation module comprises a neural network. </claim-text>
</claim>
<claim id="CLM-00024">
<claim-text><highlight><bold>24</bold></highlight>. The voice recognition system as claimed in <dependent-claim-reference depends_on="CLM-00022">claim 21</dependent-claim-reference> wherein said first filter module comprises a median filter module. </claim-text>
</claim>
<claim id="CLM-00025">
<claim-text><highlight><bold>25</bold></highlight>. The voice recognition system as claimed in <dependent-claim-reference depends_on="CLM-00011">claim 19</dependent-claim-reference>, wherein said processing sub-module comprises: 
<claim-text>a framing module; </claim-text>
<claim-text>a windowing module communicatively coupled to said framing module; </claim-text>
<claim-text>a second transformation module communicatively coupled to said windowing module; </claim-text>
<claim-text>a power spectrum module communicatively coupled to said transform module; </claim-text>
<claim-text>a second filter module communicatively coupled to said power spectrum module; and </claim-text>
<claim-text>a third transformation module communicatively coupled to said second filter module. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00026">
<claim-text><highlight><bold>26</bold></highlight>. The voice recognition system as claimed in <dependent-claim-reference depends_on="CLM-00022">claim 25</dependent-claim-reference>, wherein said framing module is configured to: 
<claim-text>accept speech signal; and </claim-text>
<claim-text>provide a frame of the speech signal. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00027">
<claim-text><highlight><bold>27</bold></highlight>. The voice recognition system as claimed in <dependent-claim-reference depends_on="CLM-00022">claim 25</dependent-claim-reference>, wherein said windowing module is configured to perform windowing by a Hamming function. </claim-text>
</claim>
<claim id="CLM-00028">
<claim-text><highlight><bold>28</bold></highlight>. The voice recognition system as claimed in <dependent-claim-reference depends_on="CLM-00022">claim 25</dependent-claim-reference>, wherein said second transformation module is configured to perform a fourier transform. </claim-text>
</claim>
<claim id="CLM-00029">
<claim-text><highlight><bold>29</bold></highlight>. The voice recognition system as claimed in <dependent-claim-reference depends_on="CLM-00022">claim 25</dependent-claim-reference>, wherein said power spectrum module is configured to perform a power spectrum determination. </claim-text>
</claim>
<claim id="CLM-00030">
<claim-text><highlight><bold>30</bold></highlight>. The voice recognition system as claimed in <dependent-claim-reference depends_on="CLM-00022">claim 25</dependent-claim-reference>, wherein said second filter module is configured to perform a MEL filtering. </claim-text>
</claim>
<claim id="CLM-00031">
<claim-text><highlight><bold>31</bold></highlight>. The voice recognition system as claimed in <dependent-claim-reference depends_on="CLM-00022">claim 25</dependent-claim-reference>, wherein said third transformation module is configured to perform a non-linear transformation. </claim-text>
</claim>
<claim id="CLM-00032">
<claim-text><highlight><bold>32</bold></highlight>. The voice recognition system as claimed in <dependent-claim-reference depends_on="CLM-00033">claim 31</dependent-claim-reference>, wherein said non-linear transformation is logarithmic transformation. </claim-text>
</claim>
<claim id="CLM-00033">
<claim-text><highlight><bold>33</bold></highlight>. The voice recognition system as claimed in <dependent-claim-reference depends_on="CLM-00011">claim 19</dependent-claim-reference>, wherein said voice activity detection module is disposed in said front end. </claim-text>
</claim>
<claim id="CLM-00034">
<claim-text><highlight><bold>34</bold></highlight>. The voice recognition system as claimed in <dependent-claim-reference depends_on="CLM-00033">claim 33</dependent-claim-reference>, wherein said front end is disposed in a subscriber terminal. </claim-text>
</claim>
<claim id="CLM-00035">
<claim-text><highlight><bold>35</bold></highlight>. A voice recognition system comprising a front end and a back end, comprising: 
<claim-text>a processing sub-module; </claim-text>
<claim-text>a feature extraction sub-module communicatively coupled to said processing sub-module, wherein a digital signal provided from said processing sub-module is downsampled in a first downsampling module; and </claim-text>
<claim-text>a voice activity detection sub-module communicatively coupled to said processing sub-module, wherein the digital signal provided from said processing sub-module is downsampled in a second downsampling module. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00036">
<claim-text><highlight><bold>36</bold></highlight>. The voice recognition system as claimed in <dependent-claim-reference depends_on="CLM-00033">claim 35</dependent-claim-reference>, wherein said first downsampling module is disposed in said feature extraction sub-module. </claim-text>
</claim>
<claim id="CLM-00037">
<claim-text><highlight><bold>37</bold></highlight>. The voice recognition system as claimed in <dependent-claim-reference depends_on="CLM-00033">claim 36</dependent-claim-reference> further comprising: 
<claim-text>a first filter module communicatively coupled to said processing sub-module and said first downsampling module. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00038">
<claim-text><highlight><bold>38</bold></highlight>. The voice recognition system as claimed in <dependent-claim-reference depends_on="CLM-00033">claim 37</dependent-claim-reference> wherein said first filter module is configured to perform filtering in accordance with linear discriminant analysis. </claim-text>
</claim>
<claim id="CLM-00039">
<claim-text><highlight><bold>39</bold></highlight>. The voice recognition system as claimed in <dependent-claim-reference depends_on="CLM-00033">claim 36</dependent-claim-reference> further comprising: 
<claim-text>a first transformation module communicatively coupled to said first downsampling module; and </claim-text>
<claim-text>a normalization module communicatively coupled to said first transformation module. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00040">
<claim-text><highlight><bold>40</bold></highlight>. The voice recognition system as claimed in <dependent-claim-reference depends_on="CLM-00033">claim 39</dependent-claim-reference> wherein said first transformation module is configured to perform discrete cosine transform. </claim-text>
</claim>
<claim id="CLM-00041">
<claim-text><highlight><bold>41</bold></highlight>. The voice recognition system as claimed in <dependent-claim-reference depends_on="CLM-00033">claim 39</dependent-claim-reference>, further comprising: 
<claim-text>a bitstream processor communicatively coupled to said normalization module. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00042">
<claim-text><highlight><bold>42</bold></highlight>. The voice recognition system as claimed in <dependent-claim-reference depends_on="CLM-00044">claim 41</dependent-claim-reference>, further comprising: 
<claim-text>a compressor communicatively coupled to said normalization module and said bitstream processor. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00043">
<claim-text><highlight><bold>43</bold></highlight>. The voice recognition system as claimed in <dependent-claim-reference depends_on="CLM-00033">claim 35</dependent-claim-reference>, wherein said second downsampling module is disposed in said voice activity detection sub-module. </claim-text>
</claim>
<claim id="CLM-00044">
<claim-text><highlight><bold>44</bold></highlight>. The voice recognition system as claimed in <dependent-claim-reference depends_on="CLM-00044">claim 43</dependent-claim-reference>, further comprising: 
<claim-text>a second transformation module communicatively coupled to said second downsampling module: </claim-text>
<claim-text>an estimation module communicatively coupled to said second transformation module; </claim-text>
<claim-text>a threshold detector communicatively coupled to said estimation module; </claim-text>
<claim-text>a second filter module communicatively coupled to said threshold detector. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00045">
<claim-text><highlight><bold>45</bold></highlight>. The voice recognition system as claimed in <dependent-claim-reference depends_on="CLM-00044">claim 44</dependent-claim-reference> wherein said second transformation module is configured to perform discrete cosine transform. </claim-text>
</claim>
<claim id="CLM-00046">
<claim-text><highlight><bold>46</bold></highlight>. The voice recognition system as claimed in <dependent-claim-reference depends_on="CLM-00044">claim 44</dependent-claim-reference> wherein said estimation module comprises a neural network. </claim-text>
</claim>
<claim id="CLM-00047">
<claim-text><highlight><bold>47</bold></highlight>. The voice recognition system as claimed in <dependent-claim-reference depends_on="CLM-00044">claim 44</dependent-claim-reference> wherein said second filter module comprises a median filter module. </claim-text>
</claim>
<claim id="CLM-00048">
<claim-text><highlight><bold>48</bold></highlight>. The voice recognition system as claimed in <dependent-claim-reference depends_on="CLM-00033">claim 35</dependent-claim-reference>, wherein said processing sub-module comprises: 
<claim-text>a framing module; </claim-text>
<claim-text>a windowing module communicatively coupled to said framing module; </claim-text>
<claim-text>a third transformation module communicatively coupled to said windowing module; </claim-text>
<claim-text>a power spectrum module communicatively coupled to said third transform module; </claim-text>
<claim-text>a third filter module communicatively coupled to said power spectrum module; and </claim-text>
<claim-text>a fourth transformation module communicatively coupled to said filtering module. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00049">
<claim-text><highlight><bold>49</bold></highlight>. The voice recognition system as claimed in <dependent-claim-reference depends_on="CLM-00044">claim 48</dependent-claim-reference>, wherein said framing module is configured to: 
<claim-text>accept speech signal; and </claim-text>
<claim-text>provide a frame of the speech signal. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00050">
<claim-text><highlight><bold>50</bold></highlight>. The voice recognition system as claimed in <dependent-claim-reference depends_on="CLM-00044">claim 48</dependent-claim-reference>, wherein said windowing module is configured to perform windowing by a Hamming function. </claim-text>
</claim>
<claim id="CLM-00051">
<claim-text><highlight><bold>51</bold></highlight>. The voice recognition system as claimed in <dependent-claim-reference depends_on="CLM-00044">claim 48</dependent-claim-reference>, wherein said third transformation module is configured to perform a fourier transform. </claim-text>
</claim>
<claim id="CLM-00052">
<claim-text><highlight><bold>52</bold></highlight>. The voice recognition system as claimed in <dependent-claim-reference depends_on="CLM-00044">claim 48</dependent-claim-reference>, wherein said power spectrum module is configured to perform a power spectrum determination. </claim-text>
</claim>
<claim id="CLM-00053">
<claim-text><highlight><bold>53</bold></highlight>. The voice recognition system as claimed in <dependent-claim-reference depends_on="CLM-00044">claim 48</dependent-claim-reference>, wherein said third filter module is configured to perform a MEL filtering. </claim-text>
</claim>
<claim id="CLM-00054">
<claim-text><highlight><bold>54</bold></highlight>. The voice recognition system as claimed in <dependent-claim-reference depends_on="CLM-00044">claim 48</dependent-claim-reference>, wherein said fourth transformation module is configured to perform a non-linear transformation. </claim-text>
</claim>
<claim id="CLM-00055">
<claim-text><highlight><bold>55</bold></highlight>. The voice recognition system as claimed in <dependent-claim-reference depends_on="CLM-00055">claim 54</dependent-claim-reference>, wherein said non-linear transformation is logarithmic transformation. </claim-text>
</claim>
<claim id="CLM-00056">
<claim-text><highlight><bold>56</bold></highlight>. The voice recognition system as claimed in <dependent-claim-reference depends_on="CLM-00033">claim 35</dependent-claim-reference>, further comprising a transmitter communicatively coupled to: 
<claim-text>said feature extraction module; and </claim-text>
<claim-text>said voice activity module. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00057">
<claim-text><highlight><bold>57</bold></highlight>. The voice recognition system as claimed in <dependent-claim-reference depends_on="CLM-00055">claim 56</dependent-claim-reference>, wherein said processing sub-module, said feature extraction module, said voice activity detection module and said transmitter are disposed in said front end. </claim-text>
</claim>
<claim id="CLM-00058">
<claim-text><highlight><bold>58</bold></highlight>. The voice recognition system as claimed in <dependent-claim-reference depends_on="CLM-00055">claim 57</dependent-claim-reference>, wherein said front end is disposed in a subscriber terminal. </claim-text>
</claim>
<claim id="CLM-00059">
<claim-text><highlight><bold>59</bold></highlight>. A voice recognition system comprising a front end and a back end, comprising: 
<claim-text>a framing module; </claim-text>
<claim-text>a windowing module communicatively coupled to said framing module; </claim-text>
<claim-text>a first transformation module communicatively coupled to said windowing module; </claim-text>
<claim-text>a power spectrum module communicatively coupled to said first transformation module; </claim-text>
<claim-text>a first filtering module communicatively coupled to said power spectrum module; </claim-text>
<claim-text>a second transformation module communicatively coupled to said first filtering module; </claim-text>
<claim-text>a second filter module communicatively coupled to said second transformation module; </claim-text>
<claim-text>a third filter module communicatively coupled to said second filter module; </claim-text>
<claim-text>a first downsampling module communicatively coupled to said second filter module; </claim-text>
<claim-text>a third transformation module communicatively coupled to said first downsampling module; </claim-text>
<claim-text>a normalization module communicatively coupled to said third transformation module. </claim-text>
<claim-text>a compressor module communicatively coupled to said normalization module; </claim-text>
<claim-text>a bitstream processor communicatively coupled to said compressor module; </claim-text>
<claim-text>a second downsampling module communicatively coupled to said second filter module; </claim-text>
<claim-text>a fourth transformation module communicatively coupled to said second downsampling module: </claim-text>
<claim-text>an estimation module communicatively coupled to said fourth transformation module; </claim-text>
<claim-text>a threshold detector communicatively coupled to said estimation module; </claim-text>
<claim-text>a fourth filter module communicatively coupled to said threshold detector. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00060">
<claim-text><highlight><bold>60</bold></highlight>. A method for extracting at least one feature from a speech signal, comprising: 
<claim-text>processing a speech signal; </claim-text>
<claim-text>downsampling said processed speech signal to provide a downsampled signal; and </claim-text>
<claim-text>extracting the at least one feature from said downsampled signal. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00061">
<claim-text><highlight><bold>61</bold></highlight>. The method as claimed in <dependent-claim-reference depends_on="CLM-00066">claim 60</dependent-claim-reference> further comprising: 
<claim-text>filtering said downsampled signal to provide a filtered signal; and </claim-text>
<claim-text>wherein said extracting the at least one feature comprises extracting the at least one feature from said filtered signal. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00062">
<claim-text><highlight><bold>62</bold></highlight>. The method as claimed in <dependent-claim-reference depends_on="CLM-00066">claim 61</dependent-claim-reference> wherein said filtering said downsampled signal to provide a filtered signal comprises: 
<claim-text>filtering in accordance with linear discriminant analysis. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00063">
<claim-text><highlight><bold>63</bold></highlight>. The method as claimed in <dependent-claim-reference depends_on="CLM-00066">claim 62</dependent-claim-reference>, further comprising: 
<claim-text>transforming said downsampled signal to provide transformed signal; </claim-text>
<claim-text>normalizing said transformed signal. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00064">
<claim-text><highlight><bold>64</bold></highlight>. The method as claimed in <dependent-claim-reference depends_on="CLM-00066">claim 63</dependent-claim-reference> wherein said transforming said downsampled signal to provide transformed signal comprises: 
<claim-text>transforming said downsampled signal by discrete cosine transform. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00065">
<claim-text><highlight><bold>65</bold></highlight>. The method as claimed in <dependent-claim-reference depends_on="CLM-00066">claim 63</dependent-claim-reference>, further comprising: 
<claim-text>processing said transformed signal to provide an output signal. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00066">
<claim-text><highlight><bold>66</bold></highlight>. The method as claimed in <dependent-claim-reference depends_on="CLM-00066">claim 65</dependent-claim-reference>, further comprising: 
<claim-text>compressing said transformed signal to provide a compressed signal; and </claim-text>
<claim-text>wherein said processing comprises processing said compressed signal to provide an output signal. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00067">
<claim-text><highlight><bold>67</bold></highlight>. The method as claimed in <dependent-claim-reference depends_on="CLM-00066">claim 60</dependent-claim-reference> wherein said processing a speech signal comprises: 
<claim-text>framing a speech signal to provide a frame of the speech signal; </claim-text>
<claim-text>windowing said framed signal to provide windowed signal; </claim-text>
<claim-text>transforming said windowed signal to provide transformed signal; </claim-text>
<claim-text>determinig a power spectrum of said transformed signal; </claim-text>
<claim-text>filtering said determined power spectrum; </claim-text>
<claim-text>transforming said filtered power spectrum. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00068">
<claim-text><highlight><bold>68</bold></highlight>. The method as claimed in <dependent-claim-reference depends_on="CLM-00066">claim 67</dependent-claim-reference>, wherein said transforming said windowed signal comprises: 
<claim-text>transforming said windowed signal by a fourier transform. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00069">
<claim-text><highlight><bold>69</bold></highlight>. The method as claimed in <dependent-claim-reference depends_on="CLM-00066">claim 67</dependent-claim-reference>, wherein said filtering said determined power spectrum comprises: 
<claim-text>filtering said determined power spectrum by a MEL filter. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00070">
<claim-text><highlight><bold>70</bold></highlight>. The method as claimed in <dependent-claim-reference depends_on="CLM-00066">claim 67</dependent-claim-reference>, wherein said transforming said filtered power spectrum comprises: 
<claim-text>transforming said filtered power spectrum by a non-linear transformation. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00071">
<claim-text><highlight><bold>71</bold></highlight>. The method as claimed in <dependent-claim-reference depends_on="CLM-00077">claim 70</dependent-claim-reference>, wherein said transforming said filtered power spectrum by a non-linear transformation comprises: 
<claim-text>transforming said filtered power spectrum by a logarithmic transformation. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00072">
<claim-text><highlight><bold>72</bold></highlight>. A method for voice activity detection, comprising: 
<claim-text>processing a speech signal; </claim-text>
<claim-text>downsampling said processed speech signal to provide a downsampled signal; and </claim-text>
<claim-text>detecting voice activity of said downsampled signal. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00073">
<claim-text><highlight><bold>73</bold></highlight>. The method as claimed in <dependent-claim-reference depends_on="CLM-00077">claim 72</dependent-claim-reference>, further comprising: 
<claim-text>transforming said downsampled signal to provide transformed signal; </claim-text>
<claim-text>estimating probability of said downsampled signal being speech; </claim-text>
<claim-text>applying a threshold to said estimation; </claim-text>
<claim-text>filtering said estimation after said applying the threshold. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00074">
<claim-text><highlight><bold>74</bold></highlight>. The method as claimed in <dependent-claim-reference depends_on="CLM-00077">claim 73</dependent-claim-reference> wherein said transforming said downsampled signal to provide transformed signal comprises: 
<claim-text>transforming said downsampled signal by discrete cosine transform. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00075">
<claim-text><highlight><bold>75</bold></highlight>. The method as claimed in <dependent-claim-reference depends_on="CLM-00077">claim 73</dependent-claim-reference> wherein said estimating probability of said downsampled signal being speech comprises: 
<claim-text>estimating probability by a neural network. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00076">
<claim-text><highlight><bold>76</bold></highlight>. The method as claimed in <dependent-claim-reference depends_on="CLM-00077">claim 73</dependent-claim-reference> wherein said filtering said estimation comprises: 
<claim-text>filtering said estimation by a median filter module. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00077">
<claim-text><highlight><bold>77</bold></highlight>. The method as claimed in <dependent-claim-reference depends_on="CLM-00077">claim 72</dependent-claim-reference> wherein said processing a speech signal comprises: 
<claim-text>framing a speech signal to provide a frame of the speech signal; </claim-text>
<claim-text>windowing said framed signal to provide windowed signal; </claim-text>
<claim-text>transforming said windowed signal to provide transformed signal; </claim-text>
<claim-text>determinig a power spectrum of said transformed signal; </claim-text>
<claim-text>filtering said determined power spectrum; </claim-text>
<claim-text>transforming said filtered power spectrum. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00078">
<claim-text><highlight><bold>78</bold></highlight>. The method as claimed in <dependent-claim-reference depends_on="CLM-00077">claim 77</dependent-claim-reference>, wherein said transforming said windowed signal comprises: 
<claim-text>transforming said windowed signal by a fourier transform. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00079">
<claim-text><highlight><bold>79</bold></highlight>. The method as claimed in <dependent-claim-reference depends_on="CLM-00077">claim 77</dependent-claim-reference>, wherein said filtering said determined power spectrum comprises: 
<claim-text>filtering said determined power spectrum by a MEL filter. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00080">
<claim-text><highlight><bold>80</bold></highlight>. The method as claimed in <dependent-claim-reference depends_on="CLM-00077">claim 77</dependent-claim-reference>, wherein said transforming said filtered power spectrum comprises: 
<claim-text>transforming said filtered power spectrum by a non-linear transformation. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00081">
<claim-text><highlight><bold>81</bold></highlight>. The method as claimed in <dependent-claim-reference depends_on="CLM-00088">claim 80</dependent-claim-reference>, wherein said transforming said filtered power spectrum by a non-linear transformation comprises: 
<claim-text>transforming said filtered power spectrum by a logarithmic transformation. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00082">
<claim-text><highlight><bold>82</bold></highlight>. A method for determining speech signal characteristics, comprising: 
<claim-text>processing a speech signal; </claim-text>
<claim-text>downsampling said processed speech signal by a first value to provide a first downsampled signal; </claim-text>
<claim-text>extracting the at least one feature from said first downsampled signal; </claim-text>
<claim-text>downsampling said processed speech signal by a second value to provide a second downsampled signal; and </claim-text>
<claim-text>detecting voice activity from said second downsampled signal. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00083">
<claim-text><highlight><bold>83</bold></highlight>. The method as claimed in <dependent-claim-reference depends_on="CLM-00088">claim 82</dependent-claim-reference>, wherein said downsampling said processed speech signal by a second value to provide a second downsampled signal comprises: 
<claim-text>downsampling said processed speech signal by the first value to provide the first downsampled signal. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00084">
<claim-text><highlight><bold>84</bold></highlight>. The method as claimed in <dependent-claim-reference depends_on="CLM-00088">claim 82</dependent-claim-reference> further comprising: 
<claim-text>filtering said first downsampled signal to provide a filtered signal; and </claim-text>
<claim-text>wherein said extracting the at least one feature comprises extracting the at least one feature from said filtered signal. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00085">
<claim-text><highlight><bold>85</bold></highlight>. The method as claimed in <dependent-claim-reference depends_on="CLM-00088">claim 84</dependent-claim-reference> wherein said filtering said first downsampled signal to provide a filtered signal comprises: 
<claim-text>filtering in accordance with linear discriminant analysis. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00086">
<claim-text><highlight><bold>86</bold></highlight>. The method as claimed in <dependent-claim-reference depends_on="CLM-00088">claim 84</dependent-claim-reference>, further comprising: 
<claim-text>transforming said first downsampled signal to provide transformed signal; </claim-text>
<claim-text>normalizing said transformed signal. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00087">
<claim-text><highlight><bold>87</bold></highlight>. The method as claimed in <dependent-claim-reference depends_on="CLM-00088">claim 86</dependent-claim-reference> wherein said transforming said downsampled signal to provide transformed signal comprises: 
<claim-text>transforming said first downsampled signal by discrete cosine transform. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00088">
<claim-text><highlight><bold>88</bold></highlight>. The method as claimed in <dependent-claim-reference depends_on="CLM-00088">claim 86</dependent-claim-reference>, further comprising: 
<claim-text>processing said transformed signal to provide an output signal. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00089">
<claim-text><highlight><bold>89</bold></highlight>. The method as claimed in <dependent-claim-reference depends_on="CLM-00088">claim 88</dependent-claim-reference>, further comprising: 
<claim-text>compressing said transformed signal to provide a compressed signal; and </claim-text>
<claim-text>wherein said processing comprises processing said compressed signal to provide an output signal. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00090">
<claim-text><highlight><bold>90</bold></highlight>. The method as claimed in <dependent-claim-reference depends_on="CLM-00088">claim 82</dependent-claim-reference>, further comprising: 
<claim-text>transforming said second downsampled signal to provide transformed signal; </claim-text>
<claim-text>estimating probability of said second downsampled signal being speech; </claim-text>
<claim-text>applying a threshold to said estimation; </claim-text>
<claim-text>filtering said estimation after applying the threshold. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00091">
<claim-text><highlight><bold>91</bold></highlight>. The method as claimed in <dependent-claim-reference depends_on="CLM-00099">claim 90</dependent-claim-reference> wherein said transforming said second downsampled signal to provide transformed signal comprises: 
<claim-text>transforming said second downsampled signal by discrete cosine transform. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00092">
<claim-text><highlight><bold>92</bold></highlight>. The method as claimed in <dependent-claim-reference depends_on="CLM-00099">claim 90</dependent-claim-reference> wherein said estimating probability of said second downsampled signal being speech comprises: 
<claim-text>estimating probability by a neural network. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00093">
<claim-text><highlight><bold>93</bold></highlight>. The method as claimed in <dependent-claim-reference depends_on="CLM-00099">claim 90</dependent-claim-reference> wherein said filtering said estimation after applying the threshold comprises: 
<claim-text>filtering said estimation by a median filter module. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00094">
<claim-text><highlight><bold>94</bold></highlight>. The method as claimed in <dependent-claim-reference depends_on="CLM-00088">claim 82</dependent-claim-reference> wherein said processing a speech signal comprises: 
<claim-text>framing a speech signal to provide a frame of the speech signal; </claim-text>
<claim-text>windowing said framed signal to provide windowed signal; </claim-text>
<claim-text>transforming said windowed signal to provide transformed signal; </claim-text>
<claim-text>determining a power spectrum of said transformed signal; </claim-text>
<claim-text>filtering said determined power spectrum; </claim-text>
<claim-text>transforming said filtered power spectrum. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00095">
<claim-text><highlight><bold>95</bold></highlight>. The method as claimed in <dependent-claim-reference depends_on="CLM-00099">claim 94</dependent-claim-reference>, wherein said transforming said windowed signal comprises: 
<claim-text>transforming said windowed signal by a fourier transform. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00096">
<claim-text><highlight><bold>96</bold></highlight>. The method as claimed in <dependent-claim-reference depends_on="CLM-00099">claim 94</dependent-claim-reference>, wherein said filtering said determined power spectrum comprises: 
<claim-text>filtering said determined power spectrum by a MEL filter. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00097">
<claim-text><highlight><bold>97</bold></highlight>. The method as claimed in <dependent-claim-reference depends_on="CLM-00099">claim 94</dependent-claim-reference>, wherein said transforming said filtered power spectrum comprises: 
<claim-text>transforming said filtered power spectrum by a non-linear transformation. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00098">
<claim-text><highlight><bold>98</bold></highlight>. The method as claimed in <dependent-claim-reference depends_on="CLM-00099">claim 97</dependent-claim-reference>, wherein said transforming said filtered power spectrum by a non-linear transformation comprises: 
<claim-text>transforming said filtered power spectrum by a logarithmic transformation. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00099">
<claim-text><highlight><bold>99</bold></highlight>. The method as claimed in <dependent-claim-reference depends_on="CLM-00099">claim 94</dependent-claim-reference>, further comprising; 
<claim-text>transmitting said extracted at least one feature and said detected voice activity. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00100">
<claim-text><highlight><bold>100</bold></highlight>. The method as claimed in <dependent-claim-reference depends_on="CLM-00099">claim 99</dependent-claim-reference>, wherein said detected voice activity is transmitted ahead of said extracted at least one feature. </claim-text>
</claim>
<claim id="CLM-00101">
<claim-text><highlight><bold>101</bold></highlight>. A system for processing speech, comprising: 
<claim-text>a terminal feature extraction submodule for extracting at least one feature from the speech; and </claim-text>
<claim-text>a terminal compression module for distinguishing the presence of voice activity from silence in the speech to determine voice activity data, compressing the at least one feature, and selectively combining and transmitting the at least one feature with selected voice activity data. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00102">
<claim-text><highlight><bold>102</bold></highlight>. The system of claim <highlight><bold>101</bold></highlight>, further comprising: 
<claim-text>a server decompression module for receiving and decompressing the selectively combined and transmitted at least one feature and selected voice activity data into decompression data; </claim-text>
<claim-text>a server feature vector generator for generating a feature vector from the decompression data; and </claim-text>
<claim-text>a speech recognition module for determining speech based on the feature vector. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00103">
<claim-text><highlight><bold>103</bold></highlight>. The system of claim <highlight><bold>101</bold></highlight>, wherein the terminal compression module comprises a voice activity detection module. </claim-text>
</claim>
<claim id="CLM-00104">
<claim-text><highlight><bold>104</bold></highlight>. The system of claim <highlight><bold>101</bold></highlight>, wherein the terminal feature extraction module and the terminal compression module reside on a subscriber unit. </claim-text>
</claim>
<claim id="CLM-00105">
<claim-text><highlight><bold>105</bold></highlight>. A distributed voice recognition system for transmitting speech activity, comprising: 
<claim-text>a subscriber unit, comprising: </claim-text>
<claim-text>a processing/feature extraction element receiving speech activity and converting the speech activity into features; </claim-text>
<claim-text>a voice activity detector for detecting voice activity within said speech and providing at least one voice activity indication; and </claim-text>
<claim-text>a processor for selectively combining the features with the at least one voice activity indication into advanced front end features; and </claim-text>
<claim-text>a transmitter for transmitting the advanced front end features to a remote device. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00106">
<claim-text><highlight><bold>106</bold></highlight>. The distributed voice recognition system of claim <highlight><bold>105</bold></highlight>, wherein said remote device comprises: 
<claim-text>a receiver for receiving the advanced front end features; </claim-text>
<claim-text>a word decoder for decoding the received information into words; and </claim-text>
<claim-text>a transmitter for transmitting the decoded words to an appropriate subscriber unit. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00107">
<claim-text><highlight><bold>107</bold></highlight>. A subscriber unit, comprising: 
<claim-text>means for extracting a plurality of features of a speech signal; </claim-text>
<claim-text>means for detecting voice activity with the speech signal and providing an indication of the detected voice activity; and </claim-text>
<claim-text>a transmitter coupled to the feature extraction means and the voice activity detection means and configured to selectively transmit indication of detected voice activity in selective combination with the plurality of features to a remote device. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00108">
<claim-text><highlight><bold>108</bold></highlight>. The subscriber unit of claim <highlight><bold>107</bold></highlight>, further comprising a means for combining the plurality of features with the indication of detected voice activity, wherein the indication of detected voice activity is ahead of the plurality of features. </claim-text>
</claim>
<claim id="CLM-00109">
<claim-text><highlight><bold>109</bold></highlight>. A system for generating feature vectors, comprising: 
<claim-text>a time derivative computation block for computing feature time derivatives; </claim-text>
<claim-text>a feature concatenation block for combining feature time derivatives with features; </claim-text>
<claim-text>a dual branch processor receiving data from said feature concatenation block, comprising: 
<claim-text>a first branch, comprising a multiple frame assembly module; and </claim-text>
<claim-text>a second branch comprising a nonlinear transformation module and a dimensionality reduction and decorrelation module; and </claim-text>
</claim-text>
<claim-text>a processing concatenation block for concatenating data computed by said first branch and said second branch.</claim-text>
</claim-text>
</claim>
</subdoc-claims>
<subdoc-drawings id="DRAWINGS">
<heading lvl="0" align="CENTER">Drawings</heading>
<representative-figure>2</representative-figure>
<figure id="figure-D00000">
<image id="EMI-D00000" file="US20030004720A1-20030102-D00000.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00001">
<image id="EMI-D00001" file="US20030004720A1-20030102-D00001.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00002">
<image id="EMI-D00002" file="US20030004720A1-20030102-D00002.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00003">
<image id="EMI-D00003" file="US20030004720A1-20030102-D00003.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00004">
<image id="EMI-D00004" file="US20030004720A1-20030102-D00004.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00005">
<image id="EMI-D00005" file="US20030004720A1-20030102-D00005.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00006">
<image id="EMI-D00006" file="US20030004720A1-20030102-D00006.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00007">
<image id="EMI-D00007" file="US20030004720A1-20030102-D00007.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00008">
<image id="EMI-D00008" file="US20030004720A1-20030102-D00008.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00009">
<image id="EMI-D00009" file="US20030004720A1-20030102-D00009.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00010">
<image id="EMI-D00010" file="US20030004720A1-20030102-D00010.TIF" imf="TIFF" ti="DR"/>
</figure>
</subdoc-drawings>
</patent-application-publication>
