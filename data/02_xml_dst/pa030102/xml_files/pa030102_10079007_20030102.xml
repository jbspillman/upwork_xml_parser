<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE patent-application-publication SYSTEM "pap-v16-2002-01-01.dtd" [
<!ENTITY US20030001835A1-20030102-M00001.NB SYSTEM "US20030001835A1-20030102-M00001.NB" NDATA NB>
<!ENTITY US20030001835A1-20030102-M00001.TIF SYSTEM "US20030001835A1-20030102-M00001.TIF" NDATA TIF>
<!ENTITY US20030001835A1-20030102-M00002.NB SYSTEM "US20030001835A1-20030102-M00002.NB" NDATA NB>
<!ENTITY US20030001835A1-20030102-M00002.TIF SYSTEM "US20030001835A1-20030102-M00002.TIF" NDATA TIF>
<!ENTITY US20030001835A1-20030102-M00003.NB SYSTEM "US20030001835A1-20030102-M00003.NB" NDATA NB>
<!ENTITY US20030001835A1-20030102-M00003.TIF SYSTEM "US20030001835A1-20030102-M00003.TIF" NDATA TIF>
<!ENTITY US20030001835A1-20030102-M00004.NB SYSTEM "US20030001835A1-20030102-M00004.NB" NDATA NB>
<!ENTITY US20030001835A1-20030102-M00004.TIF SYSTEM "US20030001835A1-20030102-M00004.TIF" NDATA TIF>
<!ENTITY US20030001835A1-20030102-M00005.NB SYSTEM "US20030001835A1-20030102-M00005.NB" NDATA NB>
<!ENTITY US20030001835A1-20030102-M00005.TIF SYSTEM "US20030001835A1-20030102-M00005.TIF" NDATA TIF>
<!ENTITY US20030001835A1-20030102-M00006.NB SYSTEM "US20030001835A1-20030102-M00006.NB" NDATA NB>
<!ENTITY US20030001835A1-20030102-M00006.TIF SYSTEM "US20030001835A1-20030102-M00006.TIF" NDATA TIF>
<!ENTITY US20030001835A1-20030102-D00000.TIF SYSTEM "US20030001835A1-20030102-D00000.TIF" NDATA TIF>
<!ENTITY US20030001835A1-20030102-D00001.TIF SYSTEM "US20030001835A1-20030102-D00001.TIF" NDATA TIF>
<!ENTITY US20030001835A1-20030102-D00002.TIF SYSTEM "US20030001835A1-20030102-D00002.TIF" NDATA TIF>
<!ENTITY US20030001835A1-20030102-D00003.TIF SYSTEM "US20030001835A1-20030102-D00003.TIF" NDATA TIF>
<!ENTITY US20030001835A1-20030102-D00004.TIF SYSTEM "US20030001835A1-20030102-D00004.TIF" NDATA TIF>
<!ENTITY US20030001835A1-20030102-D00005.TIF SYSTEM "US20030001835A1-20030102-D00005.TIF" NDATA TIF>
<!ENTITY US20030001835A1-20030102-D00006.TIF SYSTEM "US20030001835A1-20030102-D00006.TIF" NDATA TIF>
<!ENTITY US20030001835A1-20030102-D00007.TIF SYSTEM "US20030001835A1-20030102-D00007.TIF" NDATA TIF>
<!ENTITY US20030001835A1-20030102-D00008.TIF SYSTEM "US20030001835A1-20030102-D00008.TIF" NDATA TIF>
<!ENTITY US20030001835A1-20030102-D00009.TIF SYSTEM "US20030001835A1-20030102-D00009.TIF" NDATA TIF>
<!ENTITY US20030001835A1-20030102-D00010.TIF SYSTEM "US20030001835A1-20030102-D00010.TIF" NDATA TIF>
<!ENTITY US20030001835A1-20030102-D00011.TIF SYSTEM "US20030001835A1-20030102-D00011.TIF" NDATA TIF>
<!ENTITY US20030001835A1-20030102-D00012.TIF SYSTEM "US20030001835A1-20030102-D00012.TIF" NDATA TIF>
<!ENTITY US20030001835A1-20030102-D00013.TIF SYSTEM "US20030001835A1-20030102-D00013.TIF" NDATA TIF>
<!ENTITY US20030001835A1-20030102-D00014.TIF SYSTEM "US20030001835A1-20030102-D00014.TIF" NDATA TIF>
<!ENTITY US20030001835A1-20030102-D00015.TIF SYSTEM "US20030001835A1-20030102-D00015.TIF" NDATA TIF>
<!ENTITY US20030001835A1-20030102-D00016.TIF SYSTEM "US20030001835A1-20030102-D00016.TIF" NDATA TIF>
<!ENTITY US20030001835A1-20030102-D00017.TIF SYSTEM "US20030001835A1-20030102-D00017.TIF" NDATA TIF>
<!ENTITY US20030001835A1-20030102-D00018.TIF SYSTEM "US20030001835A1-20030102-D00018.TIF" NDATA TIF>
<!ENTITY US20030001835A1-20030102-D00019.TIF SYSTEM "US20030001835A1-20030102-D00019.TIF" NDATA TIF>
<!ENTITY US20030001835A1-20030102-D00020.TIF SYSTEM "US20030001835A1-20030102-D00020.TIF" NDATA TIF>
<!ENTITY US20030001835A1-20030102-D00021.TIF SYSTEM "US20030001835A1-20030102-D00021.TIF" NDATA TIF>
<!ENTITY US20030001835A1-20030102-D00022.TIF SYSTEM "US20030001835A1-20030102-D00022.TIF" NDATA TIF>
<!ENTITY US20030001835A1-20030102-D00023.TIF SYSTEM "US20030001835A1-20030102-D00023.TIF" NDATA TIF>
<!ENTITY US20030001835A1-20030102-D00024.TIF SYSTEM "US20030001835A1-20030102-D00024.TIF" NDATA TIF>
<!ENTITY US20030001835A1-20030102-D00025.TIF SYSTEM "US20030001835A1-20030102-D00025.TIF" NDATA TIF>
<!ENTITY US20030001835A1-20030102-D00026.TIF SYSTEM "US20030001835A1-20030102-D00026.TIF" NDATA TIF>
<!ENTITY US20030001835A1-20030102-D00027.TIF SYSTEM "US20030001835A1-20030102-D00027.TIF" NDATA TIF>
<!ENTITY US20030001835A1-20030102-D00028.TIF SYSTEM "US20030001835A1-20030102-D00028.TIF" NDATA TIF>
<!ENTITY US20030001835A1-20030102-D00029.TIF SYSTEM "US20030001835A1-20030102-D00029.TIF" NDATA TIF>
<!ENTITY US20030001835A1-20030102-D00030.TIF SYSTEM "US20030001835A1-20030102-D00030.TIF" NDATA TIF>
<!ENTITY US20030001835A1-20030102-D00031.TIF SYSTEM "US20030001835A1-20030102-D00031.TIF" NDATA TIF>
<!ENTITY US20030001835A1-20030102-D00032.TIF SYSTEM "US20030001835A1-20030102-D00032.TIF" NDATA TIF>
<!ENTITY US20030001835A1-20030102-D00033.TIF SYSTEM "US20030001835A1-20030102-D00033.TIF" NDATA TIF>
<!ENTITY US20030001835A1-20030102-D00034.TIF SYSTEM "US20030001835A1-20030102-D00034.TIF" NDATA TIF>
<!ENTITY US20030001835A1-20030102-D00035.TIF SYSTEM "US20030001835A1-20030102-D00035.TIF" NDATA TIF>
<!ENTITY US20030001835A1-20030102-D00036.TIF SYSTEM "US20030001835A1-20030102-D00036.TIF" NDATA TIF>
<!ENTITY US20030001835A1-20030102-D00037.TIF SYSTEM "US20030001835A1-20030102-D00037.TIF" NDATA TIF>
<!ENTITY US20030001835A1-20030102-D00038.TIF SYSTEM "US20030001835A1-20030102-D00038.TIF" NDATA TIF>
<!ENTITY US20030001835A1-20030102-D00039.TIF SYSTEM "US20030001835A1-20030102-D00039.TIF" NDATA TIF>
<!ENTITY US20030001835A1-20030102-D00040.TIF SYSTEM "US20030001835A1-20030102-D00040.TIF" NDATA TIF>
<!ENTITY US20030001835A1-20030102-D00041.TIF SYSTEM "US20030001835A1-20030102-D00041.TIF" NDATA TIF>
<!ENTITY US20030001835A1-20030102-D00042.TIF SYSTEM "US20030001835A1-20030102-D00042.TIF" NDATA TIF>
<!ENTITY US20030001835A1-20030102-D00043.TIF SYSTEM "US20030001835A1-20030102-D00043.TIF" NDATA TIF>
<!ENTITY US20030001835A1-20030102-D00044.TIF SYSTEM "US20030001835A1-20030102-D00044.TIF" NDATA TIF>
]>
<patent-application-publication>
<subdoc-bibliographic-information>
<document-id>
<doc-number>20030001835</doc-number>
<kind-code>A1</kind-code>
<document-date>20030102</document-date>
</document-id>
<publication-filing-type>new</publication-filing-type>
<domestic-filing-data>
<application-number>
<doc-number>10079007</doc-number>
</application-number>
<application-number-series-code>10</application-number-series-code>
<filing-date>20020220</filing-date>
</domestic-filing-data>
<technical-information>
<classification-ipc>
<classification-ipc-primary>
<ipc>G06T015/00</ipc>
</classification-ipc-primary>
<classification-ipc-edition>07</classification-ipc-edition>
</classification-ipc>
<classification-us>
<classification-us-primary>
<uspc>
<class>345</class>
<subclass>419000</subclass>
</uspc>
</classification-us-primary>
</classification-us>
<title-of-invention>Integrated system for quickly and accurately imaging and modeling three-dimensional objects</title-of-invention>
</technical-information>
<continuity-data>
<division-of>
<parent-child>
<child>
<document-id>
<doc-number>10079007</doc-number>
<kind-code>A1</kind-code>
<document-date>20020220</document-date>
</document-id>
</child>
<parent>
<document-id>
<doc-number>09177777</doc-number>
<document-date>19981023</document-date>
<country-code>US</country-code>
</document-id>
</parent>
<parent-status>PENDING</parent-status>
</parent-child>
</division-of>
<division-of>
<parent-child>
<child>
<document-id>
<doc-number>09177777</doc-number>
<document-date>19981023</document-date>
<country-code>US</country-code>
</document-id>
</child>
<parent>
<document-id>
<doc-number>PCT/US97/06793</doc-number>
<document-date>19970424</document-date>
<country-code>US</country-code>
</document-id>
</parent>
<parent-status>UNKNOWN</parent-status>
</parent-child>
</division-of>
<continuations>
<continuation-in-part-of>
<parent-child>
<child>
<document-id>
<doc-number>PCT/US97/06793</doc-number>
<document-date>19970424</document-date>
<country-code>US</country-code>
</document-id>
</child>
<parent>
<document-id>
<doc-number>08638961</doc-number>
<document-date>19960424</document-date>
<country-code>US</country-code>
</document-id>
</parent>
<parent-status>GRANTED</parent-status>
<parent-patent>
<document-id>
<doc-number>5988862</doc-number>
<country-code>US</country-code>
</document-id>
</parent-patent>
</parent-child>
</continuation-in-part-of>
</continuations>
</continuity-data>
<inventors>
<first-named-inventor>
<name>
<given-name>Jerry</given-name>
<family-name>Dimsdale</family-name>
</name>
<residence>
<residence-us>
<city>Berkeley</city>
<state>CA</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</first-named-inventor>
<inventor>
<name>
<given-name>Jonathan</given-name>
<middle-name>Apollo</middle-name>
<family-name>Kung</family-name>
</name>
<residence>
<residence-us>
<city>Berkeley</city>
<state>CA</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
</inventors>
<correspondence-address>
<name-1>STALLMAN &amp; POLLOCK LLP</name-1>
<name-2>Suite 290</name-2>
<address>
<address-1>121 Spear Street</address-1>
<city>San Francisco</city>
<state>CA</state>
<postalcode>94105</postalcode>
<country>
<country-code>US</country-code>
</country>
</address>
</correspondence-address>
</subdoc-bibliographic-information>
<subdoc-abstract>
<paragraph id="A-0001" lvl="0">An integrated system generates a model of a three-dimensional object. A scanning laser device scans the three-dimensional object and generates a point cloud. The points of the point cloud each indicate a location of a corresponding point on a surface of the object. A first model is generated, responsive to the point cloud, that generates a first model representing constituent geometric shapes of the object. A data file is generated, responsive to the first model, that can be inputted to a computer-aided design system. </paragraph>
</subdoc-abstract>
<subdoc-description>
<summary-of-invention>
<section>
<heading lvl="1">TECHNICAL FIELD </heading>
<paragraph id="P-0001" lvl="0"><number>&lsqb;0001&rsqb;</number> The present invention relates generally to systems that document the geometry and other attributes of objects in three dimensions and, specifically, to a system that employs a scanning lidar (range finding laser) to quickly and accurately sense the position in three-dimensional space of selected points on the surface of an object to generate a point cloud which represents the sensed positions of the selected points; that recognizes geometric shapes represented by groups of points in the point cloud, and that generates a model that represents these geometric shapes. The model may be transformed into a further model usable by computer-aided design (CAD) tools, including conventional CAD tools. </paragraph>
</section>
<section>
<heading lvl="1">BACKGROUND </heading>
<paragraph id="P-0002" lvl="0"><number>&lsqb;0002&rsqb;</number> Mapping the geometry (shape, dimensions and location) and other attributes (e.g., color, texture and reflectance intensity) of complex real objects (whether small components such as small mechanical parts or large objects such as buildings and sites) has conventionally been a tedious and time consuming process. That is, such measurement have traditionally been performed manually. In addition, transforming these measurements into drawings or computer models required manual drafting or input into a CAD system for the production of the drawing or computer models. </paragraph>
<paragraph id="P-0003" lvl="0"><number>&lsqb;0003&rsqb;</number> Recently innovations have endeavored to simplify this process, but all have fallen short of achieving full integration, automation, precision, speed and range. For example, in the building industry, mapping a structure conventionally requires three basic steps: </paragraph>
<paragraph id="P-0004" lvl="2"><number>&lsqb;0004&rsqb;</number> 1. Field data gathering </paragraph>
<paragraph id="P-0005" lvl="2"><number>&lsqb;0005&rsqb;</number> 2. Data reduction and preparation </paragraph>
<paragraph id="P-0006" lvl="2"><number>&lsqb;0006&rsqb;</number> 3. Drafting and CAD </paragraph>
<paragraph id="P-0007" lvl="7"><number>&lsqb;0007&rsqb;</number> The field data gathering step is performed by a team of surveyors who manually measure and record dimensions of pertinent components of the structure such as walls, ceilings, beams, columns, doors, windows, fixtures, pipes, conduits and equipment. The surveyors attempt to determine the geometry of the components as well as the relative location of the components in the structure. The surveyors recorded the data in a field notebook. The field-collected data is then organized and reduced to tables and organized sketches, and a CAD operator or drafter utilizes these tables to generate final drawings or models. </paragraph>
<paragraph id="P-0008" lvl="0"><number>&lsqb;0008&rsqb;</number> This process is labor intensive, time consuming, and error prone. In addition, using traditional surveying methods, the number of points which can actually be measured is very limited, due to the high cost of acquiring each point in terms of time and effort. Furthermore, if it is desired to acquire color, texture and other attribute information, additional field notes must be taken (e.g., still photographs and video). </paragraph>
<paragraph id="P-0009" lvl="0"><number>&lsqb;0009&rsqb;</number> Recently, the field step has been somewhat automated by using a laser ranging device built into or mounted on an electronic theodolite. Precision reflection targets (retro reflectors) are placed at the locations of the object for which measurements are desired. Then, the laser ranging device obtains a precise measurement of the distance between the instrument and the target, which the theodolite provides an accurate indication of the horizontal and vertical angle offsets to the point relative to a given coordinate system. The distance and angle data are either recorded automatically on a magnetic device connected to the instrument or are reduced within the instrument to Cartesian coordinates relative to the instrument axes. This procedure is then repeated as many times as necessary to map a desired number of points of the object. The collected coordinates data can then be plotted directly on a CAD system. </paragraph>
<paragraph id="P-0010" lvl="0"><number>&lsqb;0010&rsqb;</number> Unfortunately, the plot is of little practical use since it does not indicate the object geometry. Moreover, because of the requirement for retro reflectors which must be manually placed, and because of the relatively long time per reading required by the laser range finder, the gathering of sufficient points to describe most objects is very labor intensive, time consuming and error prone. </paragraph>
<paragraph id="P-0011" lvl="0"><number>&lsqb;0011&rsqb;</number> Another known field gathering data process employs stereo photography and aerial photogrammetry. That is, stereoscopic images are taken of the objects and the resulting stereo photographs are registered either manually or using computerized techniques to reproduce the relative location of the camera picture plane location at the time each photograph was taken. The data reduction and preparation step is performed manually by a specially trained operator. Specifically, with the aid of specially mounted stereoscopic viewing lenses, the operator digitizes the coordinates of a sufficient number of points to allow the definition of the objects using the stereo photographs. Again, the digitized data is input into a CAD system or is manually drawn on paper. </paragraph>
</section>
<section>
<heading lvl="1">SUMMARY </heading>
<paragraph id="P-0012" lvl="0"><number>&lsqb;0012&rsqb;</number> The present invention is an integrated system for generating a model of a three-dimensional object. A scanning laser device scans the three-dimensional object and generates a point cloud. The points of the point cloud each indicate a location of a corresponding point on a surface of the object. A first model is generated, responsive to the point cloud, representing constituent geometric shapes of the object. A data file is generated, responsive to the first model, that can be inputted to a computer-aided design system. </paragraph>
<paragraph id="P-0013" lvl="0"><number>&lsqb;0013&rsqb;</number> The subject invention further includes a method of controlling the timing of output pulse from a laser for use in a device which requires scanning of the laser output, wherein each output pulse is generated in response to a pump pulse comprising the steps of: monitoring the time delay between the initiation of the pump pulses and the subsequent generation of the associated output pulses; predicting the time delay between the initiation of next pump pulse and the associated output pulse based on the monitored time delays and; initiating the next pump pulse at a time selected to insure the output pulse is-generated at a time to permit proper positioning of the laser output during the scan of the beam. </paragraph>
<paragraph id="P-0014" lvl="0"><number>&lsqb;0014&rsqb;</number> The present invention further includes a method of manually separating from a plurality of clouds of points, representing three-dimensional features in a scene, a subset of the points that represents a desired feature in the scene, the method comprising: selecting all the point clouds that include at least some data points representing the desired feature; and changing a view of the clouds and drawing a polygonal lasso to refine a selected subset of points to be included in a point sub-cloud and repeating the refining as many times as required to obtain the desired sub-cloud. </paragraph>
<paragraph id="P-0015" lvl="0"><number>&lsqb;0015&rsqb;</number> The present invention further includes a method for automatically segmenting a scan field of a scene into subsets of points that represent different surfaces in the scene, comprising the steps of: separating the scan field into a depth grid that includes depth information for scanned points of surfaces in the scene and a normal grid that includes an estimate of a normal to scanned points of the surfaces; convolving the depth information of the depth grid to generate a depth rating image whose values represent a gradient of depth change from one scanned point to another scanned point in the scene; convolving the components of the normal grid to generate a scalar value for each component for each point of the normal grid; for each point of the normal grid, determining from the scalar values for the components of that particular point a gradient of the normal at that point, wherein the gradients determined for the points of the normal grid collectively constitute a normal rating image; converting the depth rating image to a binary depth image using a recursive thresholding technique; converting the normal rating image to a binary normal image using a recursive thresholding technique; combining the binary depth image and the binary normal image to determine a single edge image; and grouping subsets of non-edge points as belonging to corresponding surfaces of the scene. </paragraph>
<paragraph id="P-0016" lvl="0"><number>&lsqb;0016&rsqb;</number> The method can further include the steps of determining the type of geometric primitive that would best first each group of points; fitting the geometric primitive to the data points; and intersecting adjacent planar regions in the scene. </paragraph>
<paragraph id="P-0017" lvl="0"><number>&lsqb;0017&rsqb;</number> The subject matter further includes a method for fitting a point cloud representing a corner, comprising: determining a fit of three planes to the points of the point cloud and creating the planes for a model; determining the three lines at the intersection of pairs of planes and creating the lines for the model; and determining the vertex point at the intersection of the three planes and creating a vertex point for the model. </paragraph>
<paragraph id="P-0018" lvl="0"><number>&lsqb;0018&rsqb;</number> The subject invention further includes a method for modeling a three-dimensional scene, comprising: generating a plurality of points that each represent a point on a surface of the scene; determining a best fit of a cylinder for a group of the points using surface normal estimates and global error minimization. </paragraph>
<paragraph id="P-0019" lvl="0"><number>&lsqb;0019&rsqb;</number> The subject invention further includes a method for modeling a three-dimensional scene, comprising: generating a plurality of points that each represent a point on a surface of the scene; determining a best fit of a cylinder for a group of the points using a quadric surface fit and global error minimization. </paragraph>
<paragraph id="P-0020" lvl="0"><number>&lsqb;0020&rsqb;</number> The subject invention further includes a method for modeling a three-dimensional scene, comprising: generating a plurality of points that each represent a point on a surface of the scene; determining a best fit of a sphere for a group of the points using a quadric surface fit and global error minimization. </paragraph>
<paragraph id="P-0021" lvl="0"><number>&lsqb;0021&rsqb;</number> The subject invention further includes a method for modeling three-dimensional scene, comprising: generating a plurality of points that each represent a point on a surface of the scene; determining a best fit quadric surface for a group of points; and determining which geometric primitive of a plurality of the family described by the quadric surface best fits the group of points. </paragraph>
<paragraph id="P-0022" lvl="0"><number>&lsqb;0022&rsqb;</number> The subject invention further includes a method for merging two geometric primitives of the same type to form a single geometric primitive of the type, comprising: creating a new group of points by combining the points used to originally fit each of the two primitives; and fitting the new geometric primitive using any appropriate fitting technique and the newly generated point group with points from each of the original primitives. </paragraph>
<paragraph id="P-0023" lvl="0"><number>&lsqb;0023&rsqb;</number> The subject invention further includes a method of registering a first model, consisting of a plurality of points and geometric primitives and having a first coordinate system, with a second model, consisting of a plurality of points and geometric primitives and having a second coordinate system, comprising: identifying by a user common features of the first and second scenes; identifying a transformation between coordinate systems that is responsive to the identification; and transforming the objects of the second model so that they use the first coordinate system. </paragraph>
<paragraph id="P-0024" lvl="0"><number>&lsqb;0024&rsqb;</number> The subject invention further includes a method of warping, comprising: selecting one or more models represented by a plurality of point clouds and geometric primitives; specifying constraints on the locations of any number of points or geometric primitives; creating an artificial volume that surrounds the points and geometric primitives in each view and assigning mechanical material characteristics to the surrounding volume; computing a minimum energy configuration for the material in the surrounding volume in which the points or geometric primitives are embedded such that the configuration satisfies all applied constraints; and displacing the points and primitives in accordance with the computed minimum energy configuration of the surrounding volume of material. In the latter method, the constraints can be specified to eliminate closure errors. </paragraph>
<paragraph id="P-0025" lvl="0"><number>&lsqb;0025&rsqb;</number> The subject invention further includes an integrated system for generating a model of a three-dimensional scene, comprising: a scanning laser device that scans the three dimensional scene with pulsed laser beam wherein the pulses of light last less than 1 nanosecond with up to 0.2 &mgr;J in each pulse and measures the time delay, with a resolution of 30 psec or less, between each emitted pulse and a corresponding pulse that is returned from a surface of the scene and wherein said scanning laser device further tracks and measures the angular orientation of the beam during the scan; and means for generating a point cloud based upon the measured time delays and angle measurements, the point cloud comprising a plurality of data points that each represents a location of a corresponding point on the surface. </paragraph>
<paragraph id="P-0026" lvl="0"><number>&lsqb;0026&rsqb;</number> The subject invention further includes a system for calibrating the measuring electronics in a device which requires monitoring the time of flight of the output pulses from a laser comprising: a single mode optical fiber with one end thereof positioned to receive the output pulses of the laser, said single mode optical fiber having a known length; a detector positioned at one of the ends of the fiber for monitoring when the pulses exit the fiber and generating a signal in response thereto, said signal being passed through the measuring electronics; and a processor for calculating a theoretical length of the fiber based on the detection of the pulse exiting the fiber and comparing that -calculated length with known length of the fiber to calibrate the measuring electronics. </paragraph>
<paragraph id="P-0027" lvl="0"><number>&lsqb;0027&rsqb;</number> The optical fiber can include partial reflectors located at each end thereof so that for each laser pulse entering the fiber a train of pulses will exit the fiber and wherein said train of pulses are used to further calibrate the measuring electronics. </paragraph>
<paragraph id="P-0028" lvl="0"><number>&lsqb;0028&rsqb;</number> The system can further include delay measurement electronics and wherein the train of pulses have a fixed delay therebetween whereby the monitoring of the train of pulses can be used to calibrate the delay electronics. </paragraph>
<paragraph id="P-0029" lvl="0"><number>&lsqb;0029&rsqb;</number> The system can further include a means for varying the power of the pulses monitored by the detector and wherein said detector functions to generate a signal when the power of the detected light exceeds a predetermined threshold and wherein said processor functions to track the variation in the delay of the generation of the output signal by the detector as a function of the power of the output pulses, said processor further functioning to calibrate the measurement of the delay based on the measured power of successive pulses used for monitoring the time of flight. </paragraph>
<paragraph id="P-0030" lvl="0"><number>&lsqb;0030&rsqb;</number> The subject invention further includes an apparatus for obtaining position information about surface points of a three dimensional object comprising: a laser for generating an output beam; scanning system for moving the laser beam over the object; monitoring system for automatically measuring the range to the object based on the measurement of the reflection of the laser beam, said monitor system also tracking and measuring the angular position of the laser beam, said monitoring system having a positional accuracy for each point in three dimensional space equal to or better than six millimeters at one standard deviation over a range of up to 100 meters. </paragraph>
<paragraph id="P-0031" lvl="0"><number>&lsqb;0031&rsqb;</number> Each range measurement can be made in under 0.005 seconds. The laser can generate a pulsed output and the energy per pulse can be less than 0.2 micro joules and the average output power of the laser can be less than 1.0 milliwatts. </paragraph>
<paragraph id="P-0032" lvl="0"><number>&lsqb;0032&rsqb;</number> The subject invention further includes an apparatus for measuring the distance to an object comprising: a laser for generating a beam of output pulses; a monitoring system for measuring the distance to the object based on the reflection of the laser beam, said monitoring system having an accuracy equal to or better than 6 millimeters at one standard deviation over its entire range of up to 100 meters and wherein each measurement can be made in less than 0.005 seconds and wherein the laser has an energy per pulse of no more than 0.2 micro joules and an average power of no more than 1 milliwatt. If the object is provided with retro reflectors and where the range of operation is up to one mile. </paragraph>
<paragraph id="P-0033" lvl="0"><number>&lsqb;0033&rsqb;</number> The subject invention further includes an apparatus for acquiring three dimensional information from a remote object comprising: a scanning laser module for measuring position information of the object; a video module for capturing image information from the object; and a processor for rendering a model of the object which includes the position information and the image information. </paragraph>
<paragraph id="P-0034" lvl="0"><number>&lsqb;0034&rsqb;</number> The video image information can be collected in a spatially coincident manner with the measurement of position information. The video image information can be collected from points adjacent to the points where position information is obtained. </paragraph>
<paragraph id="P-0035" lvl="0"><number>&lsqb;0035&rsqb;</number> The subject invention further includes an apparatus for obtaining positional information about surface points of a three dimensional object comprising: a scanning module for measuring three dimensional position information about an object; a video module for capturing and displaying image information from the object; and a processor operating with the scanning and video modules and permitting the use of the image information captured by said video module to aid in targeting the scanning module. The processor can function to specify a portion of the object to be targeted by the scanning module by dragging the image of an outline over the video image of the area to be targeted. </paragraph>
<paragraph id="P-0036" lvl="0"><number>&lsqb;0036&rsqb;</number> The subject invention further includes an apparatus for obtaining positional information about surface points of a three dimensional object comprising: a scanning module for measuring three dimensional position information about an object; a video module for displaying image information obtained from the scanning module; a processor operating with the scanning and video modules and permitting the use of the image information displayed by said video module to further refine the targeting of the scanning module. </paragraph>
<paragraph id="P-0037" lvl="0"><number>&lsqb;0037&rsqb;</number> The subject invention further includes an apparatus for obtaining positional information about surface points of a three dimensional object comprising: a scanning module for measuring three dimensional position information about an object, said scanning module including a laser for emitting a beam of visible radiation; and a processor for controlling the scanning module and wherein said laser can be manually positioned so that the visible beam will target the portion of the object to be scanned in response to a control signal from the processor. </paragraph>
<paragraph id="P-0038" lvl="0"><number>&lsqb;0038&rsqb;</number> The subject invention further includes a system for calibrating the measuring electronics in a device which requires monitoring frequency changes in a light beam generated by a laser used to measure distance to an object, wherein said beam has frequency chirp imposed thereon comprising a single mode optical fiber with one end thereof positioned to receive light from the laser; a detector positioned to receive light which has traveled through and exited the fiber in combination which light from the laser which has not traveled through the fiber, said detector for monitoring the changes in the frequency of the combined beam; and processor for determining the linearity of the chirp on the beam based on uniformity of the frequency changes measured by the detector and using the result to calibrate the measuring electronics. </paragraph>
<paragraph id="P-0039" lvl="0"><number>&lsqb;0039&rsqb;</number> The fiber can have a known length and includes a partial reflector on said one end and at least a partial reflector on the other end, and wherein light reflected from said one end of the fiber which has not traveled in the fiber is measured by the detector and wherein the processor further functions to calculate a theoretical length of the fiber based on the frequency changes measured by the detector and compares that calculated length with the known length of the fiber to calibrate the measuring electronics.</paragraph>
</section>
</summary-of-invention>
<brief-description-of-drawings>
<section>
<heading lvl="1">BRIEF DESCRIPTION OF THE FIGURES </heading>
<paragraph id="P-0040" lvl="0"><number>&lsqb;0040&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 1</cross-reference> is a block diagram of a system in accordance with an embodiment of the invention. </paragraph>
<paragraph id="P-0041" lvl="0"><number>&lsqb;0041&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 1A</cross-reference> shows the overall flow of how one may use an embodiment of the invention to scan an object, organize acquired points, fit geometric shapes to the organized point, manipulate the fitted geometric shapes, and display the resulting manipulated geometric shapes. </paragraph>
<paragraph id="P-0042" lvl="0"><number>&lsqb;0042&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 2</cross-reference> is a more detailed block diagram of the system of <cross-reference target="DRAWINGS">FIG. 1</cross-reference>. </paragraph>
<paragraph id="P-0043" lvl="0"><number>&lsqb;0043&rsqb;</number> <cross-reference target="DRAWINGS">FIGS. 3A and 3B</cross-reference> show the physical arrangement of the FDV of the <cross-reference target="DRAWINGS">FIG. 1</cross-reference> system, and also shows how the FDV is coupled to the tripod by a fork mount. </paragraph>
<paragraph id="P-0044" lvl="0"><number>&lsqb;0044&rsqb;</number> <cross-reference target="DRAWINGS">FIGS. 4A and 4B</cross-reference> show an example coordinate system relative to the FDV of the <cross-reference target="DRAWINGS">FIG. 1</cross-reference> system. </paragraph>
<paragraph id="P-0045" lvl="0"><number>&lsqb;0045&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 5</cross-reference> is a block diagram of one embodiment of an FDV in accordance with the invention. </paragraph>
<paragraph id="P-0046" lvl="0"><number>&lsqb;0046&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 6</cross-reference> is a block diagram of the optical transceiver of the <cross-reference target="DRAWINGS">FIG. 5</cross-reference> FDV. </paragraph>
<paragraph id="P-0047" lvl="0"><number>&lsqb;0047&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 6A</cross-reference> shows a dual mirror arrangement of the scanner shown in <cross-reference target="DRAWINGS">FIG. 6</cross-reference>. </paragraph>
<paragraph id="P-0048" lvl="0"><number>&lsqb;0048&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 7</cross-reference> is a block diagram which shows an embodiment of the laser. </paragraph>
<paragraph id="P-0049" lvl="0"><number>&lsqb;0049&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 7A</cross-reference> is a block diagram of an embodiment of the beam expander shown in <cross-reference target="DRAWINGS">FIG. 6</cross-reference>. </paragraph>
<paragraph id="P-0050" lvl="0"><number>&lsqb;0050&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 8</cross-reference> shows an embodiment of the duplexer. </paragraph>
<paragraph id="P-0051" lvl="0"><number>&lsqb;0051&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 8A</cross-reference> shows a partially-reflecting duplexer. </paragraph>
<paragraph id="P-0052" lvl="0"><number>&lsqb;0052&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 9</cross-reference> shows an embodiment of the window of the <cross-reference target="DRAWINGS">FIG. 8</cross-reference> duplexer. </paragraph>
<paragraph id="P-0053" lvl="0"><number>&lsqb;0053&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 10</cross-reference> is a flowchart that shows calculations performed by the FDV DSP. </paragraph>
<paragraph id="P-0054" lvl="0"><number>&lsqb;0054&rsqb;</number> <cross-reference target="DRAWINGS">FIGS. 11A and 11B</cross-reference> show a unidirectional scan pattern and a bi-directional scan pattern, respectively. </paragraph>
<paragraph id="P-0055" lvl="0"><number>&lsqb;0055&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 12</cross-reference> is a block diagram of an embodiment of the FDV processor. </paragraph>
<paragraph id="P-0056" lvl="0"><number>&lsqb;0056&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 13</cross-reference> is a block diagram of example circuitry for determining a desired position of an FDV mirror. </paragraph>
<paragraph id="P-0057" lvl="0"><number>&lsqb;0057&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 14</cross-reference> is a block diagram of an example signal conditioning and energy integration circuit of the timing circuit shown in <cross-reference target="DRAWINGS">FIG. 12</cross-reference>. </paragraph>
<paragraph id="P-0058" lvl="0"><number>&lsqb;0058&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 15</cross-reference> is a detailed block diagram of the system of <cross-reference target="DRAWINGS">FIG. 1</cross-reference>. </paragraph>
<paragraph id="P-0059" lvl="0"><number>&lsqb;0059&rsqb;</number> <cross-reference target="DRAWINGS">FIGS. 16A and 16B</cross-reference> show two windows used to operate the CGP. </paragraph>
<paragraph id="P-0060" lvl="0"><number>&lsqb;0060&rsqb;</number> <cross-reference target="DRAWINGS">FIGS. 17A and 17B</cross-reference> show a targeting box and a point cloud. </paragraph>
<paragraph id="P-0061" lvl="0"><number>&lsqb;0061&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 18</cross-reference> shows a point cloud from the surface of a horse sculpture. </paragraph>
<paragraph id="P-0062" lvl="0"><number>&lsqb;0062&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 19</cross-reference> shows the point cloud of <cross-reference target="DRAWINGS">FIG. 18</cross-reference> color mapped with the laser return intensities. </paragraph>
<paragraph id="P-0063" lvl="0"><number>&lsqb;0063&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 20</cross-reference> shows a cloud of points from a corner feature. </paragraph>
<paragraph id="P-0064" lvl="0"><number>&lsqb;0064&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 21</cross-reference> shows the cloud of points of <cross-reference target="DRAWINGS">FIG. 20</cross-reference> and a polygonal lasso used for manual segmentation. </paragraph>
<paragraph id="P-0065" lvl="0"><number>&lsqb;0065&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 22</cross-reference> shows the cloud of points of <cross-reference target="DRAWINGS">FIG. 20</cross-reference> segmented into four subgroups, three subgroups on the surfaces of planes and a subgroup of edge points that are not part of the plane. </paragraph>
<paragraph id="P-0066" lvl="0"><number>&lsqb;0066&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 23</cross-reference> shows the cloud of points of <cross-reference target="DRAWINGS">FIG. 20</cross-reference> rendered as a triangulated mesh </paragraph>
<paragraph id="P-0067" lvl="0"><number>&lsqb;0067&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 24</cross-reference> shows the corner feature of <cross-reference target="DRAWINGS">FIG. 20</cross-reference> with planes fit to the groups of cloud points. </paragraph>
<paragraph id="P-0068" lvl="0"><number>&lsqb;0068&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 25</cross-reference> shows a point cloud from the surface of a cylinder. </paragraph>
<paragraph id="P-0069" lvl="0"><number>&lsqb;0069&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 26</cross-reference> shows a cylinder primitive that was fit to the points shown in <cross-reference target="DRAWINGS">FIG. 25</cross-reference>. </paragraph>
<paragraph id="P-0070" lvl="0"><number>&lsqb;0070&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 27</cross-reference> shows a cloud of points from the surfaces on a piping system. </paragraph>
<paragraph id="P-0071" lvl="0"><number>&lsqb;0071&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 28</cross-reference> shows cylinder primitives that were fit to the points shown in <cross-reference target="DRAWINGS">FIG. 27</cross-reference>. </paragraph>
<paragraph id="P-0072" lvl="0"><number>&lsqb;0072&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 29</cross-reference> shows the completed piping model, after extending pipes and adding elbows. </paragraph>
<paragraph id="P-0073" lvl="0"><number>&lsqb;0073&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 30</cross-reference> shows the result of a corner fit, giving three planes, three lines, and a vertex. </paragraph>
<paragraph id="P-0074" lvl="0"><number>&lsqb;0074&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 31</cross-reference> shows a cylinder primitive in a scene. </paragraph>
<paragraph id="P-0075" lvl="0"><number>&lsqb;0075&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 32</cross-reference> shows the cylinder form <cross-reference target="DRAWINGS">FIG. 31</cross-reference> extended to meet adjacent objects. </paragraph>
<paragraph id="P-0076" lvl="0"><number>&lsqb;0076&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 33</cross-reference> shows a cloud of points from the surface from a variety of objects. </paragraph>
<paragraph id="P-0077" lvl="0"><number>&lsqb;0077&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 34</cross-reference> shows a model containing primitives that were fit to the points shown in <cross-reference target="DRAWINGS">FIG. 33</cross-reference>. </paragraph>
<paragraph id="P-0078" lvl="0"><number>&lsqb;0078&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 35</cross-reference> shows configuration of a frequency adjustable laser. </paragraph>
<paragraph id="P-0079" lvl="0"><number>&lsqb;0079&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 36</cross-reference> shows block diagram of conventional FM chirp lidar. </paragraph>
<paragraph id="P-0080" lvl="0"><number>&lsqb;0080&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 37</cross-reference> shows block diagram of self-calibrating FM chirp lidar. </paragraph>
<paragraph id="P-0081" lvl="0"><number>&lsqb;0081&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 38</cross-reference> illustrates the relative timing at which a large and a small pulse cross a predetermined threshold. </paragraph>
<paragraph id="P-0082" lvl="0"><number>&lsqb;0082&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 39</cross-reference> illustrates one circuit for measuring pulse energy. </paragraph>
<paragraph id="P-0083" lvl="0"><number>&lsqb;0083&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 40</cross-reference> illustrates another circuit for measuring pulse energy. </paragraph>
</section>
</brief-description-of-drawings>
<detailed-description>
<section>
<heading lvl="1">DETAILED DESCRIPTION </heading>
<paragraph id="P-0084" lvl="0"><number>&lsqb;0084&rsqb;</number> A. Overview </paragraph>
<paragraph id="P-0085" lvl="0"><number>&lsqb;0085&rsqb;</number> 1. Overall System </paragraph>
<paragraph id="P-0086" lvl="0"><number>&lsqb;0086&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 1</cross-reference> is a block diagram that illustrates the invention in its broadest aspect. Referring to <cross-reference target="DRAWINGS">FIG. 1, a</cross-reference> Field Digital Vision (FDV) module <highlight><bold>10</bold></highlight> includes a scanning sensor for scanning an object <highlight><bold>20</bold></highlight> and for sensing the position in three-dimensional space of selected points on the surface of the object <highlight><bold>20</bold></highlight>. The FDV module <highlight><bold>10</bold></highlight> generates a point cloud <highlight><bold>30</bold></highlight> which represents the sensed positions of the selected points. The point cloud <highlight><bold>30</bold></highlight> also represents other attributes of the sensed positions, such as reflectivity, surface color and texture. </paragraph>
<paragraph id="P-0087" lvl="0"><number>&lsqb;0087&rsqb;</number> A Computer Graphics Perception (CGP) module <highlight><bold>40</bold></highlight> interacts with the FDV to provide control and targeting functions for the FDV module <highlight><bold>10</bold></highlight> sensor. In addition, using the point cloud, the CGP module <highlight><bold>40</bold></highlight> recognizes geometric shapes represented by groups of points in the point cloud <highlight><bold>30</bold></highlight>, and the CGP module generates a CGP model <highlight><bold>42</bold></highlight> that represents these geometric shapes. From the CGP model <highlight><bold>42</bold></highlight>, the CGP module <highlight><bold>40</bold></highlight> generates a further model usable by computer-aided design (CAD) tools <highlight><bold>50</bold></highlight>. The CAD tools may be conventional. </paragraph>
<paragraph id="P-0088" lvl="0"><number>&lsqb;0088&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 1A</cross-reference> shows the overall flow of how one may use an embodiment of the invention to scan an object, organize acquired points, fit geometric shapes to the organized point, manipulate the fitted geometric shapes, and display the resulting manipulated geometric shapes. </paragraph>
<paragraph id="P-0089" lvl="0"><number>&lsqb;0089&rsqb;</number> 2. FDVModule Overview </paragraph>
<paragraph id="P-0090" lvl="0"><number>&lsqb;0090&rsqb;</number> Referring to <cross-reference target="DRAWINGS">FIG. 2</cross-reference>, the FDV <highlight><bold>10</bold></highlight> includes a scanning laser system (lidar) <highlight><bold>210</bold></highlight> that scans points of the object <highlight><bold>20</bold></highlight> and that generates a lidar data signal that precisely represents the position in three-dimensional space of each scanned point. The lidar data signal for groups of scanned points collectively constitute the point cloud <highlight><bold>30</bold></highlight>. In addition, a video system <highlight><bold>220</bold></highlight>, preferably including both wide angle and narrow angle CCD cameras, is provided. The wide angle CCD camera of the video system <highlight><bold>220</bold></highlight> acquires a video image of the object <highlight><bold>20</bold></highlight> and provides, to the CGP <highlight><bold>40</bold></highlight> via a control/interface module <highlight><bold>230</bold></highlight> of the FDV <highlight><bold>10</bold></highlight>, a signal that represents the acquired video image. </paragraph>
<paragraph id="P-0091" lvl="0"><number>&lsqb;0091&rsqb;</number> In response to user input relative to the signal that represents the acquired video image, the CGP <highlight><bold>40</bold></highlight> provides a scanning control signal to the lidar <highlight><bold>210</bold></highlight>, via the control/interface module <highlight><bold>230</bold></highlight>, for controlling which points on the surface of the object <highlight><bold>20</bold></highlight> the lidar <highlight><bold>210</bold></highlight> scans. More particularly, the scanning control signal provided from the CGP <highlight><bold>40</bold></highlight> controls an accurate and repeatable beam steering mechanism to steer a laser beam of the lidar <highlight><bold>210</bold></highlight>. </paragraph>
<paragraph id="P-0092" lvl="0"><number>&lsqb;0092&rsqb;</number> In addition, the narrow angle CCD camera of the video system <highlight><bold>220</bold></highlight> captures the texture and color information, and provides this captured information to the CGP <highlight><bold>40</bold></highlight>. </paragraph>
<paragraph id="P-0093" lvl="0"><number>&lsqb;0093&rsqb;</number> 3. CGP Module Overview </paragraph>
<paragraph id="P-0094" lvl="0"><number>&lsqb;0094&rsqb;</number> Referring still to <cross-reference target="DRAWINGS">FIG. 2</cross-reference>, the CGP <highlight><bold>40</bold></highlight> is constituted of a data processing system (e.g., a notebook computer or a graphics workstation) and special purpose software that when executed conFig.s the CGP <highlight><bold>40</bold></highlight> data processing system to perform the FDV <highlight><bold>10</bold></highlight> control and targeting functions, and also to perform the CGP model <highlight><bold>42</bold></highlight> generation functions. </paragraph>
<paragraph id="P-0095" lvl="0"><number>&lsqb;0095&rsqb;</number> i. FDV Control </paragraph>
<paragraph id="P-0096" lvl="0"><number>&lsqb;0096&rsqb;</number> The CGP <highlight><bold>40</bold></highlight> controls the scanning lidar <highlight><bold>210</bold></highlight> of the FDV <highlight><bold>10</bold></highlight> by providing a lidar control signal to the FDV <highlight><bold>10</bold></highlight> that controls which points of the object <highlight><bold>20</bold></highlight> the FDV <highlight><bold>10</bold></highlight> scans. User input is provided to the CGP <highlight><bold>40</bold></highlight>, which defines what portions of the object <highlight><bold>20</bold></highlight> to scan, and at what resolution. </paragraph>
<paragraph id="P-0097" lvl="0"><number>&lsqb;0097&rsqb;</number> ii. Model Generation </paragraph>
<paragraph id="P-0098" lvl="0"><number>&lsqb;0098&rsqb;</number> Each data point in the point cloud <highlight><bold>30</bold></highlight> generated by the FDV <highlight><bold>10</bold></highlight> represents both distance to a corresponding laser impingement point from an FDV <highlight><bold>10</bold></highlight> &ldquo;origin point&rdquo; and the angle from the origin point to the laser impingement point. The CGP software conFig.s the CGP <highlight><bold>40</bold></highlight> computer to process the data points of the point cloud <highlight><bold>30</bold></highlight>, generated by the lidar <highlight><bold>210</bold></highlight> as a result of scanning the object <highlight><bold>20</bold></highlight>, to display and visualize the scanned portions of the object <highlight><bold>20</bold></highlight>. More specifically, the CGP software conFig.s the CGP <highlight><bold>40</bold></highlight> computer to recognize geometric shapes in the object <highlight><bold>20</bold></highlight> (&ldquo;graphic perception&rdquo;) and, using these recognized geometric shapes, to perform geometry construction, 3D model construction, 3D visualization, and database functions for automated acquisition or manual input of object attributes, generation of plans, sections, and dimensions, data query, and CAD interfaces, and networking options. </paragraph>
<paragraph id="P-0099" lvl="0"><number>&lsqb;0099&rsqb;</number> B. Details </paragraph>
<paragraph id="P-0100" lvl="0"><number>&lsqb;0100&rsqb;</number> 1. FDV Module Detail </paragraph>
<paragraph id="P-0101" lvl="0"><number>&lsqb;0101&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 5</cross-reference> is a block diagram of one embodiment of an FDV <highlight><bold>10</bold></highlight> in accordance with the invention. A lidar transceiver <highlight><bold>502</bold></highlight> includes a laser, transmit optics, receive optics and detector for generating ranging and intensity data. A scanning system <highlight><bold>504</bold></highlight> includes dual orthogonal scanning mirrors, galvo motors, and encoders for steering the laser beam and determining the azimuth and altitude angles of the laser beam from the positions of the mirrors. A wide angle video system <highlight><bold>506</bold></highlight> generates targeting video information and a narrow angle video system <highlight><bold>507</bold></highlight> generates color and texture information. Control/interface circuitry <highlight><bold>230</bold></highlight> handles the exchange of data between the FDV <highlight><bold>10</bold></highlight> and the CGP <highlight><bold>40</bold></highlight>. </paragraph>
<paragraph id="P-0102" lvl="0"><number>&lsqb;0102&rsqb;</number> If the laser beam is quasi-CW, always on with either intensity modulation (AM) or wavelength modulation (FM), the distance to the object <highlight><bold>20</bold></highlight> can be inferred by any of a number of techniques involving demodulation at the transceiver <highlight><bold>502</bold></highlight>. If the laser is pulsed, the distance to the object <highlight><bold>20</bold></highlight> is usually measured by the time of flight from the transceiver <highlight><bold>502</bold></highlight> to the object <highlight><bold>20</bold></highlight> and back. Other laser modulation schemes may be used. </paragraph>
<paragraph id="P-0103" lvl="0"><number>&lsqb;0103&rsqb;</number> In the time-of-flight embodiment, the laser is preferably of the type disclosed in U.S. Pat. Nos. 5,132,977; 5,386,427; and 5,381,431, assigned to Massachusetts Institute of Technology. In particular, the beam generated by such a laser has special properties such as being capable of producing pulse widths less than Insec. </paragraph>
<paragraph id="P-0104" lvl="0"><number>&lsqb;0104&rsqb;</number> A particular embodiment of the laser which has been used is particularly suitable for precision lidar since: </paragraph>
<paragraph id="P-0105" lvl="2"><number>&lsqb;0105&rsqb;</number> 1. The short pulsewidth provides high accuracy, since radar theory shows that the accuracy is proportional to the inverse of the pulse time. </paragraph>
<paragraph id="P-0106" lvl="2"><number>&lsqb;0106&rsqb;</number> 2. The laser itself is physically quite small, especially useful for portable applications. </paragraph>
<paragraph id="P-0107" lvl="2"><number>&lsqb;0107&rsqb;</number> 3. It has a diffraction limited beam, which implies that the spot size at a distance is not limited by the properties of the light, but only by the quality of the optics used to collimate or focus it </paragraph>
<paragraph id="P-0108" lvl="2"><number>&lsqb;0108&rsqb;</number> 4. Since the wavelength is quite short (532 nm), and Rayleigh range is inversely proportional to wavelength, the beam can be kept small over a large distance interval. In fact with a 1 cm exit aperture, the beam will remain less than 6 mm over 50 m. </paragraph>
<paragraph id="P-0109" lvl="0"><number>&lsqb;0109&rsqb;</number> In one preferred embodiment, the laser beam is directed by the orthogonal scanner mirrors to a laser impingement point on the surface of the object <highlight><bold>20</bold></highlight>. The range can be determined by any of a number of conventional lidar techniques. For example, the &ldquo;time of flight&rdquo; of a laser pulse to travel from the laser to the surface of the object <highlight><bold>20</bold></highlight>, and then back to the detector, is determined. The range is determined based on the constant speed of light, with appropriate adjustments being made for atmospheric factors. </paragraph>
<paragraph id="P-0110" lvl="0"><number>&lsqb;0110&rsqb;</number> A system in accordance with the present invention can provide high ranging accuracy at high acquisition rates. For example, at 100 m ranges, a 1 mm accuracy can be achieved on a single shot basis, with anywhere from 1000 to 5000 data points being acquired per second. </paragraph>
<paragraph id="P-0111" lvl="0"><number>&lsqb;0111&rsqb;</number> In other embodiments, a chirp lidar may be employed. The essential component of a chirp lidar can be modulated with a linear change of wavelength over time. Thus, the wavelength of the light emitting from the laser will be given by &lgr;(t)&equals;k(t&minus;t<highlight><subscript>0</subscript></highlight>)&plus;&lgr;<highlight><subscript>0</subscript></highlight>. In practice, such a laser is commonly manufactured by making a composite of two materials, a typical laser gain media such as NdYAG (<highlight><bold>3510</bold></highlight>), and substance which can have its index of refraction altered electrically such as Lithium Niobate (<highlight><bold>3520</bold></highlight>). (See <cross-reference target="DRAWINGS">FIG. 35</cross-reference>) This effectively changes the length of the laser cavity, and hence alters the emitted wavelength. Commercially available lasers can be modulated up to about 100 GHz with a voltage modulation of about 1 kV, so the frequency of the light will be approximately f(t)&equals;k (t&minus;t<highlight><subscript>0</subscript></highlight>)&plus;f<highlight><subscript>0</subscript></highlight>. </paragraph>
<paragraph id="P-0112" lvl="0"><number>&lsqb;0112&rsqb;</number> Referring to <cross-reference target="DRAWINGS">FIG. 36</cross-reference>, in a typical FM chirp system, a portion of the light emitted by the laser <highlight><bold>3610</bold></highlight> is sampled and recombined at the beam splitter <highlight><bold>3630</bold></highlight> with the light returning from the target <highlight><bold>3620</bold></highlight>. Since the light is delayed by the amount of time to contact the target and return, the light returning from the target will have a lower frequency than the light which is sampled from the laser. This difference will be apparent in the output of detector <highlight><bold>3610</bold></highlight> which measures the intensity of the combined beams. If the frequency ramp of light is exactly linear and the laser has a coherence length much greater than he distance to the target, combining the beams will produce a constant measured frequency from the detector which is proportional to the range: f&equals;k<highlight><subscript>1</subscript></highlight>*d&plus;d<highlight><subscript>0</subscript></highlight>. Chirped Yag lasers as described above have a coherence length of about 20 km, but the chirp is not linear, and this has severely limited the accuracy of existing FM chirp lidars. </paragraph>
<paragraph id="P-0113" lvl="0"><number>&lsqb;0113&rsqb;</number> Referring to <cross-reference target="DRAWINGS">FIG. 37, a</cross-reference> huge improvement in accuracy can be realized by adding a system to calibrate every range measurement. A fiber is prepared which has a partial reflector <highlight><bold>3771</bold></highlight> on one end, and a nearly total reflector <highlight><bold>3772</bold></highlight> on the other. Now a portion of the light emitted by the laser <highlight><bold>3710</bold></highlight> is sampled and recombined at beam splitter <highlight><bold>3740</bold></highlight> with the light returning from the target <highlight><bold>3720</bold></highlight>, and the intensity is measured by a detector <highlight><bold>3760</bold></highlight>. An additional sample of the beam emitted from the laser is sampled by beam splitter <highlight><bold>3730</bold></highlight> and introduced into the fiber at the partially reflected end <highlight><bold>3771</bold></highlight>. The beam propagates a fixed distance down the fiber and reflects off the end face and is recombined with the beam which is reflecting off the partially reflecting face <highlight><bold>3771</bold></highlight>, and is measured with a second detector <highlight><bold>3750</bold></highlight>. The linearity of the chirp is then measured by determining the deviation from a constant frequency of the output of detector <highlight><bold>3750</bold></highlight>, and this information is used to correct for the effects of the nonlinear chirp in the output of detector <highlight><bold>3760</bold></highlight>, which corresponds to the target range measurement. </paragraph>
<paragraph id="P-0114" lvl="0"><number>&lsqb;0114&rsqb;</number> Referring to <cross-reference target="DRAWINGS">FIGS. 3A and 3B</cross-reference>, in the embodiment, the FDV <highlight><bold>10</bold></highlight> is physically housed in a box <highlight><bold>330</bold></highlight> made of metal or other suitable housing material. The box <highlight><bold>330</bold></highlight> is suspended from its side panels by a fork mount mechanism <highlight><bold>310</bold></highlight>. The fork mount system is supported on a turntable <highlight><bold>340</bold></highlight>, and the turntable <highlight><bold>340</bold></highlight> is mountable on a tripod <highlight><bold>320</bold></highlight>. Using the fork mechanism <highlight><bold>310</bold></highlight>, the FDV <highlight><bold>10</bold></highlight> can be rotated horizontally (&ldquo;azimuth rotation&rdquo;) and vertically (&ldquo;elevation tilt&rdquo;, or &ldquo;altitude&rdquo;). Generally, a position of the tripod <highlight><bold>320</bold></highlight> is referred to as a &ldquo;setting&rdquo; or &ldquo;positioning&rdquo;; the rotation and tilt of the FDV <highlight><bold>10</bold></highlight> in the fork mount <highlight><bold>310</bold></highlight> is referred to as &ldquo;pointing&rdquo; or &ldquo;orientation&rdquo;. A &ldquo;view&rdquo; is generally associated with a given setting and orientation. </paragraph>
<paragraph id="P-0115" lvl="0"><number>&lsqb;0115&rsqb;</number> The fork mount system <highlight><bold>310</bold></highlight> preferably includes high accuracy azimuth and altitude rotation measuring devices (e.g., conventional theodolite-type optical or electronic encoders) to provide precise rotation and tilt data of the FDV <highlight><bold>10</bold></highlight>. This feature can allow the automatic integration of scans taken from the same tripod <highlight><bold>320</bold></highlight> setting, but with a different orientation of the FDV <highlight><bold>10</bold></highlight>. In the event these devices are not used, and for scans taken from different settings and orientations, these scans can be integrated using techniques described later in this disclosure. </paragraph>
<paragraph id="P-0116" lvl="0"><number>&lsqb;0116&rsqb;</number> It should be noted at this point that, while conventional surveying instruments should be leveled prior to operation in order to operate properly, this is not a requirement of the FDV <highlight><bold>10</bold></highlight>. This is due to the novel methods of this invention as embodied since its own internal coordinate system and the procedures utilized in its software that take advantage of its method of acquiring position data. The system, however, does have the ability to be leveled and to be used in a manner similar to a traditional theodolite. The Cartesian coordinates of the FDV <highlight><bold>10</bold></highlight> in the example embodiment are shown in <cross-reference target="DRAWINGS">FIGS. 4A and 4B</cross-reference>. </paragraph>
<paragraph id="P-0117" lvl="0"><number>&lsqb;0117&rsqb;</number> Referring still to <cross-reference target="DRAWINGS">FIGS. 3A and 3B</cross-reference>, in one embodiment, two orthogonal mirrors of the FDV <highlight><bold>10</bold></highlight> provide a field of view of approximately 40&deg; by 40&deg; (&ldquo;Field of View&rdquo;, or &ldquo;View&rdquo; is defined as the maximum size of the area projected by the laser maximum deflections of the beam in degrees). The field of view can be increased or decreased by resizing the mirrors and certain parts of the optical train The fork mount described above is utilized to allow pointing of the FDVs 40&deg;&times;40&deg; field of view anywhere over a projected sphere thus affording a wide range of flexibility in imaging large objects or groups of objects from the same setting. Other mounting methods may be used to accomplish the same purpose. </paragraph>
<paragraph id="P-0118" lvl="0"><number>&lsqb;0118&rsqb;</number> High accuracy and repeatability electronic encoders read the rotational angles of the orthogonal mirrors, and the readings of the mirror rotation angles are precisely timed to coincide with the range reading. Preferably, the system is Class II FDA eye safe. A first embodiment has 6 mm spatial accuracy over a range of &plusmn;50 m. In another embodiment, autofocus capability and 5-6 picosecond electronics are included, which extends the system&apos;s range accuracy to &plusmn;1 mm and &plusmn;1 mm spatial accuracy over &plusmn;50 m. The range (and accuracy) of the system can be significantly influenced by the choice of eye safety classification selected, but these limitations are not inherent limitations of the invention itself. </paragraph>
<paragraph id="P-0119" lvl="0"><number>&lsqb;0119&rsqb;</number> The following is a description of the key components of preferred embodiment of the FDV <highlight><bold>10</bold></highlight>. A block diagram of the optical transceiver <highlight><bold>502</bold></highlight> of the FDV <highlight><bold>10</bold></highlight> is shown in <cross-reference target="DRAWINGS">FIG. 6</cross-reference>. The optical transceiver <highlight><bold>502</bold></highlight> transmits an optical pulse to a spot on object <highlight><bold>20</bold></highlight>, and receives a reflected optical pulse from the object <highlight><bold>20</bold></highlight>. Given the constant speed of light, the optical transceiver calibrates the distance to the spot on the target. </paragraph>
<paragraph id="P-0120" lvl="0"><number>&lsqb;0120&rsqb;</number> Referring to <cross-reference target="DRAWINGS">FIG. 6</cross-reference>, the laser <highlight><bold>602</bold></highlight> fires an optical pulse which lasts less than 250 psec, in response to an external command provided from a laser controller <highlight><bold>604</bold></highlight>. The laser <highlight><bold>602</bold></highlight> produces a pulse, preferably at a wavelength of 532 nm, within 100-300 &mgr;sec after an external signal emanating from a digital signal processor which provides central control of real time events. The time delay is a complicated function of recent laser history and environmental conditions. This function is not completely known at present However, a software algorithm, which is described elsewhere, is employed to estimate the time delay with adequate accuracy for the required measurements. </paragraph>
<paragraph id="P-0121" lvl="0"><number>&lsqb;0121&rsqb;</number> The laser beam output of the laser <highlight><bold>602</bold></highlight> is transmitted through a beam expander <highlight><bold>606</bold></highlight> that is focused to adjust the size of a light spot that will eventually impinge upon a point on the object <highlight><bold>20</bold></highlight>. The focussed optical pulse is then transmitted through a duplexer <highlight><bold>608</bold></highlight>, which is an optical system for aligning the outgoing optical path with the incoming optical path. The duplexer <highlight><bold>608</bold></highlight> directs a significant first portion of the light energy of the outgoing optical pulse to a spot on the object <highlight><bold>20</bold></highlight> via a scanner <highlight><bold>614</bold></highlight>, but a second, much smaller portion, of the light energy of the outgoing optical pulse is directed to a receiver telescope <highlight><bold>610</bold></highlight>. The portion of the outgoing optical pulse that propagates to the object <highlight><bold>20</bold></highlight> impinges on the spot on the object <highlight><bold>20</bold></highlight>, and some of the energy of the optical pulse is reflected off the object <highlight><bold>20</bold></highlight> in a direction back to the duplexer <highlight><bold>608</bold></highlight>. The returning optical pulse is directed by the duplexer <highlight><bold>608</bold></highlight> to a receiver telescope <highlight><bold>610</bold></highlight>, which focuses the received energy onto a detector <highlight><bold>612</bold></highlight>. The detector <highlight><bold>612</bold></highlight> converts the received optical pulse energy into electrical energy, and the output of the detector <highlight><bold>612</bold></highlight> is a series of electrical pulses, the first (which is generated by the detector <highlight><bold>612</bold></highlight> in response to the second, small portion, of the transmitted pulse not directed toward the object <highlight><bold>20</bold></highlight>) occurs at a short fixed time (i.e., fixed by the length of the optical path through the beam expander <highlight><bold>606</bold></highlight>, the duplexer <highlight><bold>608</bold></highlight> and the receiver telescope <highlight><bold>610</bold></highlight>) and the second of which occurs as light energy returns from the object <highlight><bold>20</bold></highlight>. Both the second, small portion of the transmitted pulse not directed toward the object <highlight><bold>20</bold></highlight>, and the return optical pulse reflected from the spot on the object <highlight><bold>20</bold></highlight>, are provided to the timing circuit <highlight><bold>616</bold></highlight> which calculates the time of flight to the spot on the object <highlight><bold>20</bold></highlight>. The range to the spot on the object <highlight><bold>20</bold></highlight> can then be readily calculated from the calculated time of flight. </paragraph>
<paragraph id="P-0122" lvl="0"><number>&lsqb;0122&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 7</cross-reference> is a block diagram which shows an embodiment of the laser <highlight><bold>602</bold></highlight>. The heart of the laser system <highlight><bold>702</bold></highlight> is a conventional laser chip <highlight><bold>702</bold></highlight> that include two bonded crystals coated with antireflective dielectric coatings on their faces. The laser chip <highlight><bold>602</bold></highlight> is pumped with a solid state diode <highlight><bold>704</bold></highlight> operating at 808.5 nm&plusmn;3 nm. The output frequency of the diode pump <highlight><bold>704</bold></highlight> is adjusted by changing its temperature with a thermoelectric cooler <highlight><bold>706</bold></highlight>. The temperature of the diode pump <highlight><bold>704</bold></highlight> is measured with a thermistor <highlight><bold>708</bold></highlight>, and the measured temperature is fed back into the diode power supply <highlight><bold>710</bold></highlight>. The required temperature varies with each individual diode, but it typically ranges from 20&deg; to 30&deg; C. </paragraph>
<paragraph id="P-0123" lvl="0"><number>&lsqb;0123&rsqb;</number> The output power of the diode pump <highlight><bold>704</bold></highlight> is typically 1 watt, launched into a 100 &mgr;m core glass fiber. When continuously pumped, the output of the crystal laser <highlight><bold>602</bold></highlight> is approximately 35 mW average power at 1.064 &mgr;m, which corresponds to 2.4 &mgr;J pulses lasting about 280 psec at a repetition rate of 15 kHz. The multimode fiber is preferably terminated by an SMA<highlight><bold>905</bold></highlight> solid brass connector, with the crystal of the laser chip <highlight><bold>702</bold></highlight> glued to one end of the connector with an optical resin. Thus ensures adequate thermal dissipation from the crystal of the laser chip <highlight><bold>702</bold></highlight>, keeping the crystal <highlight><bold>702</bold></highlight> within the temperature range required for most efficient operation. </paragraph>
<paragraph id="P-0124" lvl="0"><number>&lsqb;0124&rsqb;</number> A piece of KTP frequency doubling crystal <highlight><bold>712</bold></highlight> is held within a few millimeters of the face of the laser chip crystal <highlight><bold>702</bold></highlight>. This provides an ultimate output from the laser <highlight><bold>602</bold></highlight> having a 12 mW average power at 532 nm, which corresponds to 0.8 &mgr;J pulses lasting approximately 218 psec. This ultimate output from the laser <highlight><bold>602</bold></highlight> is nearly diffraction limited (i.e., one which has theoretically minimum divergence, given a specific wavelength and waist diameter), with an apparent waist diameter of 561 &mgr;m. </paragraph>
<paragraph id="P-0125" lvl="0"><number>&lsqb;0125&rsqb;</number> Embodiments of the invention which meet FDA Class II eye safe system design specifications are potentially more commercially viable. In order to meet this specification, the maximum energy per pulse that can be transmitted at 532 nm is 0.2 &mgr;J. With this restriction, the average power transmitted is largely dependent on the pulse repetition rate, and is given by the following table  
<table-cwu id="TABLE-US-00001">
<number>1</number>
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="5">
<colspec colname="OFFSET" colwidth="14PT" align="left"/>
<colspec colname="1" colwidth="35PT" align="left"/>
<colspec colname="2" colwidth="49PT" align="center"/>
<colspec colname="3" colwidth="49PT" align="center"/>
<colspec colname="4" colwidth="70PT" align="center"/>
<thead>
<row>
<entry></entry>
<entry></entry>
</row>
<row>
<entry></entry>
<entry namest="OFFSET" nameend="4" align="center" rowsep="1"></entry>
</row>
<row>
<entry></entry>
<entry></entry>
<entry>ENERGY PER</entry>
<entry>AVERAGE</entry>
<entry>REPETITION</entry>
</row>
<row>
<entry></entry>
<entry>CLASS</entry>
<entry>PULSE</entry>
<entry>POWER</entry>
<entry>RATE</entry>
</row>
<row>
<entry></entry>
<entry namest="OFFSET" nameend="4" align="center" rowsep="1"></entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry></entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="7">
<colspec colname="OFFSET" colwidth="14PT" align="left"/>
<colspec colname="1" colwidth="35PT" align="left"/>
<colspec colname="2" colwidth="49PT" align="center"/>
<colspec colname="3" colwidth="28PT" align="right"/>
<colspec colname="4" colwidth="21PT" align="left"/>
<colspec colname="5" colwidth="35PT" align="right"/>
<colspec colname="6" colwidth="35PT" align="left"/>
<tbody valign="top">
<row>
<entry></entry>
<entry>I</entry>
<entry>.2 &mgr;J</entry>
<entry>.39</entry>
<entry>&mgr;W</entry>
<entry>1.95</entry>
<entry>Hz</entry>
</row>
<row>
<entry></entry>
<entry>IIA</entry>
<entry>.2 &mgr;J</entry>
<entry>3.9</entry>
<entry>&mgr;W</entry>
<entry>19.5</entry>
<entry>Hz</entry>
</row>
<row>
<entry></entry>
<entry>II</entry>
<entry>.2 &mgr;J</entry>
<entry>1.0</entry>
<entry>mW</entry>
<entry>5</entry>
<entry>kHz</entry>
</row>
<row>
<entry></entry>
<entry>IIIA</entry>
<entry>.2 &mgr;J</entry>
<entry>5.0</entry>
<entry>mW</entry>
<entry>25</entry>
<entry>kHz</entry>
</row>
<row>
<entry></entry>
<entry namest="OFFSET" nameend="6" align="center" rowsep="1"></entry>
</row>
</tbody>
</tgroup>
</table>
</table-cwu>
</paragraph>
<paragraph id="P-0126" lvl="0"><number>&lsqb;0126&rsqb;</number> In one embodiment of the invention, the beam expander <highlight><bold>606</bold></highlight> is entirely conventional (e.g., Melles Griot model number 09LBM013, 10&times; beam expander). The transceiver <highlight><bold>502</bold></highlight> has cross axis accuracy which is proportional to the size of the laser beam impinging on the intended target The base design of 6 mm accuracy has a simple beam expander. The laser <highlight><bold>602</bold></highlight> can be collimated with a fixed <highlight><bold>10</bold></highlight><highlight><italic>x </italic></highlight>beam expander <highlight><bold>606</bold></highlight> which has an aperture of &lt;1 cm to produce a beam whose 1/e<highlight><superscript>2</superscript></highlight>*<highlight><superscript>power </superscript></highlight>beam width is less than 6 mm over a range of 50 m. </paragraph>
<paragraph id="P-0127" lvl="0"><number>&lsqb;0127&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 7A</cross-reference> shows a further embodiment <highlight><bold>750</bold></highlight> of the beam expander <highlight><bold>606</bold></highlight> that includes features which allow the system of the invention to measure ranges at an accuracy of approximately 1 mm at 50 m. This is because the impingement spot on the object <highlight><bold>20</bold></highlight> of the laser beam expanded by a conventional beam expander is collimated, and produces a spot of no more than 6 mm over a 50 m range. However, a beam can be focused through a 50 mm aperture to a spot of size no more than 1 mm over a 50 m range&mdash;but the spot will be much larger at other ranges. Thus the beam expander <highlight><bold>750</bold></highlight> of a system having 1 mm accuracy at 50 m includes a movable optical element <highlight><bold>752</bold></highlight> which can change the size of the focused spot. Additionally, the beam expander <highlight><bold>750</bold></highlight> includes an adjustable aperture <highlight><bold>755</bold></highlight>, and means for controlling the adjustment, so that the distance, from the laser, over which the beam stays at 1 mm in diameter remains approximately constant The minimum diameter spot produced with a diffraction limited lens of focal length f and diameter D is d<highlight><subscript>0</subscript></highlight>&equals;2f&lgr;/D. The Rayleigh range of the focused spot, which is the depth of focus of the beam, is given by b&equals;2&pgr;<highlight><subscript>&ohgr;0</subscript></highlight><highlight><superscript>2</superscript></highlight>/&lgr;&equals;2&pgr;f<highlight><superscript>2</superscript></highlight>&lgr;/D<highlight><superscript>2</superscript></highlight>. Thus, if f/D is held constant, the depth of focus will not be a function of the range of the focused spot, f. </paragraph>
<paragraph id="P-0128" lvl="0"><number>&lsqb;0128&rsqb;</number> As the beam focus is changed, the elements should stay sufficiently aligned so as to prevent the beam from changing direction by more than a fraction of 1 mm at 50 m, or this will appear as an error in the placement of the point in space. In order to minimize this beam wander, a linear servo motor <highlight><bold>754</bold></highlight> (see <cross-reference target="DRAWINGS">FIG. 7A</cross-reference>) is employed for controlling the position of the focusing mechanism, and a transducer provides position feedback. The lens <highlight><bold>752</bold></highlight> is mounted in an annular ring <highlight><bold>753</bold></highlight>, which prevents it from rotating or misaligning while it is being translated. </paragraph>
<paragraph id="P-0129" lvl="7"><number>&lsqb;0129&rsqb;</number> Duplexer </paragraph>
<paragraph id="P-0130" lvl="0"><number>&lsqb;0130&rsqb;</number> An embodiment of the duplexer <highlight><bold>608</bold></highlight> is shown in <cross-reference target="DRAWINGS">FIG. 8</cross-reference>. The optical system of the duplexer <highlight><bold>608</bold></highlight> is conFig.d such that the outgoing beam from the beam expander <highlight><bold>606</bold></highlight> to the scanner <highlight><bold>504</bold></highlight> is coaxial with the return beam reflected from the object <highlight><bold>20</bold></highlight>. Thus, only one scanner <highlight><bold>504</bold></highlight> need be provided. In the embodiment of the duplexer shown in <cross-reference target="DRAWINGS">FIG. 8, a</cross-reference> window <highlight><bold>802</bold></highlight> is provided, with a 50% beamsplitter <highlight><bold>804</bold></highlight> attached over the window <highlight><bold>802</bold></highlight>. When an optical pulse is transmitted from the laser <highlight><bold>602</bold></highlight> and through the beam expander <highlight><bold>606</bold></highlight>, the pulse impinges upon the beam splitter <highlight><bold>804</bold></highlight>. Most of the light energy of the pulse is reflected off the beam splitter <highlight><bold>804</bold></highlight> and is passed to the scanner <highlight><bold>504</bold></highlight>, but some of the optical pulse proceeds through the beam splitter <highlight><bold>804</bold></highlight> and impinges upon a low reflectance beam block <highlight><bold>806</bold></highlight>. Due to the low (but non-zero) reflectance of the beam block <highlight><bold>806</bold></highlight>, a small fraction of the optical pulse hitting the beam block <highlight><bold>806</bold></highlight> returns to the beam splitter <highlight><bold>804</bold></highlight> and is reflected into the receiver <highlight><bold>610</bold></highlight>. </paragraph>
<paragraph id="P-0131" lvl="0"><number>&lsqb;0131&rsqb;</number> Moreover, as an optical pulse returns from the object <highlight><bold>20</bold></highlight>, since only the central portion of the return pulse is obscured by the prism <highlight><bold>804</bold></highlight>, most of the light impinging on the window <highlight><bold>802</bold></highlight> makes its way to the receiver <highlight><bold>610</bold></highlight>. </paragraph>
<paragraph id="P-0132" lvl="7"><number>&lsqb;0132&rsqb;</number> Partially Reflecting Duplexer </paragraph>
<paragraph id="P-0133" lvl="0"><number>&lsqb;0133&rsqb;</number> Referring now to <cross-reference target="DRAWINGS">FIG. 8</cross-reference>A, for the 1 mm accuracy embodiment, a partially-reflecting duplexer <highlight><bold>850</bold></highlight> is employed. With this duplexer, a fraction of the light pulse provided from the beam expander into a beam stop <highlight><bold>652</bold></highlight> and reflects off the duplexer window <highlight><bold>850</bold></highlight> to the receiver telescope <highlight><bold>610</bold></highlight>. The remainder of the light pulse proceeds to the object <highlight><bold>20</bold></highlight>. Most of the return light pulse from the object <highlight><bold>20</bold></highlight> continues on through the window <highlight><bold>850</bold></highlight> and is collected by the receiver telescope <highlight><bold>610</bold></highlight>. The window <highlight><bold>850</bold></highlight> is AR coated on the receiver side, and partially mirrored on the laser side. The entire window <highlight><bold>850</bold></highlight> is used to steer the outgoing beam, since a 50 mm aperture is required to focus the spot to 1 mm at 50 m. The partial reflectance is chosen in view of the laser transmission power and the applicable eye-safe classification level. For example, if the laser transmission power is four times the allowable level of the applicable eye-safe level, then the partial mirroring is chosen to reflect 25% and absorb 75%. </paragraph>
<paragraph id="P-0134" lvl="0"><number>&lsqb;0134&rsqb;</number> Referring now to <cross-reference target="DRAWINGS">FIG. 9</cross-reference>, in the 6 mm embodiment, improved efficiency can be achieved in collecting the return optical pulse if only the center of the window <highlight><bold>802</bold></highlight> is coated <highlight><bold>904</bold></highlight> to reflect the outgoing pulse, and the remainder of the window <highlight><bold>802</bold></highlight> is anti-reflective coated <highlight><bold>906</bold></highlight>. In this way, the return optical pulse is not reflected out of the receiver by the part of the window <highlight><bold>802</bold></highlight> that is antireflection coated <highlight><bold>906</bold></highlight>. </paragraph>
<paragraph id="P-0135" lvl="0"><number>&lsqb;0135&rsqb;</number> Preferably, the laser <highlight><bold>602</bold></highlight> emits a strongly polarized beam so that the reflective coating <highlight><bold>904</bold></highlight> can be optimized to have slightly different reflection coefficients for the two planar polarizations (20%-S and 30%-P). In such an embodiment, the power of the beam impinged onto the object <highlight><bold>20</bold></highlight> can be fine tuned merely by physically rotating the laser body. </paragraph>
<paragraph id="P-0136" lvl="7"><number>&lsqb;0136&rsqb;</number> Receiver Telescope </paragraph>
<paragraph id="P-0137" lvl="0"><number>&lsqb;0137&rsqb;</number> Referring again to <cross-reference target="DRAWINGS">FIG. 6</cross-reference>, after the returning pulse has passed through the duplexer <highlight><bold>608</bold></highlight>, it is collected by the receiver telescope <highlight><bold>610</bold></highlight>, which optimizes the amount of signal provided to the detector <highlight><bold>612</bold></highlight>. The receiver telescope <highlight><bold>610</bold></highlight> may be a simple 50 mm aperture lens. The lens is preferably selected so that the variation in pulse energy entering the detector <highlight><bold>612</bold></highlight> does not change as a function of the distance to the object <highlight><bold>20</bold></highlight> over the range of distances for which the instrument is designed. A multiple element lens can be designed to minimize the variation in received pulse energy as a function of range somewhat more effectively than a single element lens. That is, at the greatest expected distance, the focal length of the lens is such that all the incoming light, which is effectively collimated since it is generated by a point source in the far field, is focused to completely fill the detector <highlight><bold>612</bold></highlight>. As the object <highlight><bold>20</bold></highlight> becomes closer to the telescope <highlight><bold>610</bold></highlight>, the spot of return light becomes larger than the detector <highlight><bold>612</bold></highlight>. The power incident on the detector <highlight><bold>612</bold></highlight> increases as the square of the distance from the telescope <highlight><bold>610</bold></highlight> to the object <highlight><bold>20</bold></highlight>, up to the maximum expected distance. Moreover, the power returning from the object <highlight><bold>20</bold></highlight> decreases as the square of the distance from the telescope <highlight><bold>610</bold></highlight> to the object <highlight><bold>20</bold></highlight>. Thus, in practice, these two effects approximately cancel each other. This minimizes the variation in optical power incident on the detector <highlight><bold>612</bold></highlight> over the range of anticipated use. In the 1 mm option, the receiver optics can be improved in some cases by using a two element, adjustable focus, Newtonian telescope (e.g., similar to the 1 mm beam expander). </paragraph>
<paragraph id="P-0138" lvl="7"><number>&lsqb;0138&rsqb;</number> Detector </paragraph>
<paragraph id="P-0139" lvl="0"><number>&lsqb;0139&rsqb;</number> The detector <highlight><bold>612</bold></highlight> converts optical pulses to electrical pulses which can be processed by the elapsed time measurement electronics (timing circuit <highlight><bold>616</bold></highlight>). In one embodiment, the detector <highlight><bold>612</bold></highlight> is an avalanche photodiode (APD) with greater than 1 GHz electrical bandwidth. In addition to the time between the start and any stop pulses, the intensities of all the pulses are recorded. The intensity information is used to make a correction to the range measurement derived from the timing information. </paragraph>
<paragraph id="P-0140" lvl="7"><number>&lsqb;0140&rsqb;</number> Scanner </paragraph>
<paragraph id="P-0141" lvl="0"><number>&lsqb;0141&rsqb;</number> The scanner <highlight><bold>504</bold></highlight> may be conventional. The scanner <highlight><bold>504</bold></highlight> directs the outgoing pulses from the duplexer <highlight><bold>608</bold></highlight> to a desired position on the object <highlight><bold>20</bold></highlight> and directs the incoming return pulse into the receiver telescope <highlight><bold>610</bold></highlight>. The scanner <highlight><bold>504</bold></highlight> directs light to the narrow field ccd camera <highlight><bold>507</bold></highlight> to collect color and texture in the immediate vicinity of the scanned laser points, which provides for precise registration of color and texture obtained with the lidar acquired point geometry. In one embodiment, the scanner <highlight><bold>504</bold></highlight> includes a dual mirror arrangement (see <cross-reference target="DRAWINGS">FIG. 6A</cross-reference>) for beam steering, although any conventional high accuracy and repeatability beam steering mechanism may be employed. The dual mirror arrangement includes two mirrors which are rotated on orthogonal axes by moving coil motors. These motors have an integral position decoder which has angular repeatability of less than 1 microradian. The mount for the scanners is integrally formed with the supports for the laser and other optics. This system provides 40 degrees of optical motion in both altitude (elevation) and azimuth at several Hertz. </paragraph>
<paragraph id="P-0142" lvl="7"><number>&lsqb;0142&rsqb;</number> Electronics </paragraph>
<paragraph id="P-0143" lvl="0"><number>&lsqb;0143&rsqb;</number> A. Timing Circuit </paragraph>
<paragraph id="P-0144" lvl="0"><number>&lsqb;0144&rsqb;</number> Another embodiment of the scanner <highlight><bold>504</bold></highlight> mechanism consists of a single mirror rotating about a central axis, mounted on a rotating turret In this configuration, the physical coordinate system would be spherical, with the faster (due to the less inertia) mirror providing the elevational angle and the more slowly rotating turret providing azimuthal motion. A system such as this could provide a field of view of more than 90 degrees in a vertical plane and a full 360 degrees in a horizontal plane (both planes being relative to some chosen scanner coordinate system. </paragraph>
<paragraph id="P-0145" lvl="7"><number>&lsqb;0145&rsqb;</number> Electronics </paragraph>
<paragraph id="P-0146" lvl="0"><number>&lsqb;0146&rsqb;</number> Ranging Electronics </paragraph>
<paragraph id="P-0147" lvl="0"><number>&lsqb;0147&rsqb;</number> The function of the ranging electronics is to compute the range from the FDV <highlight><bold>10</bold></highlight> to the object <highlight><bold>20</bold></highlight> based upon the electrical output of the detector <highlight><bold>612</bold></highlight>. Several possible methods may be used, including a demodulator in the case of a quasi-CW modulated laser system. For the preferred time of flight embodiment, an interval timer (timing circuit) measures the relative time interval between an initial (start) pulse reflected directly into the receiver <highlight><bold>610</bold></highlight> by the duplexor <highlight><bold>608</bold></highlight>, and the pulse reflected off of the object <highlight><bold>20</bold></highlight> back into the receiver <highlight><bold>610</bold></highlight>. </paragraph>
<paragraph id="P-0148" lvl="0"><number>&lsqb;0148&rsqb;</number> Reflectivity Electronics </paragraph>
<paragraph id="P-0149" lvl="0"><number>&lsqb;0149&rsqb;</number> In many cases, it is useful to know not only the position in space of a point on the object <highlight><bold>20</bold></highlight>, but also know the reflectivity (at some particular wavelength) of that point The reflectivity electronics measure the amount of light reflected from the object <highlight><bold>20</bold></highlight> into the receiver <highlight><bold>610</bold></highlight> and detector <highlight><bold>612</bold></highlight>. This data can be used to provide corrections to the range information as well as the information on the material and/or finish of the surface of the object <highlight><bold>20</bold></highlight>. </paragraph>
<paragraph id="P-0150" lvl="0"><number>&lsqb;0150&rsqb;</number> Digital Signal Processor </paragraph>
<paragraph id="P-0151" lvl="0"><number>&lsqb;0151&rsqb;</number> A digital signal processor integrated circuit controls all the time critical is functions of the FDV&mdash;scanner control, laser firing. It also provides fast floating point computation capability for making geometry corrections, calibration corrections, and video lens corrections, and video compression. The digital signal processor is interrupted at regular time intervals, typically about 10 usec. At each of these time intervals, a check is made to see what real time calculations are outstanding. </paragraph>
<paragraph id="P-0152" lvl="0"><number>&lsqb;0152&rsqb;</number> Scanner Control </paragraph>
<paragraph id="P-0153" lvl="0"><number>&lsqb;0153&rsqb;</number> The electronics for the scanner are a simple precision PD controller which are driven by a digital signal from the DSP. When driving this system quickly, there is noticeable lag in the ability of the scanner to follow the driving signal. However, the controller circuit does not have an error signal output An external precision analog differential amplifier provides an error signal (the difference between the command signal and the actual displacement), which is sampled by the DSP at low resolution. The DSP then computes the exact scan position by computing the sum of the command signal and the error signal. The advantage of this method is that it requires only a low resolution A/D converter and a precision D/A converter, rather than a far more expensive precision A/D. </paragraph>
<paragraph id="P-0154" lvl="0"><number>&lsqb;0154&rsqb;</number> The digital signal processor generates the trajectories for the analog scanner controller, and makes measurements of the difference between the desired trajectory and the actual position. It predicts the time at which the laser pump is turned on so that the laser will fire at the desired angle. These predictions are made at regular time intervals. <cross-reference target="DRAWINGS">FIG. 10</cross-reference> is a flow chart that shows the calculations performed at each time interval. </paragraph>
<paragraph id="P-0155" lvl="0"><number>&lsqb;0155&rsqb;</number> Trajectory Computation </paragraph>
<paragraph id="P-0156" lvl="0"><number>&lsqb;0156&rsqb;</number> The user defines areas within the view of the scanner that are to be scanned, and indicates the density of points to sample within the scanned region. There are several scan patterns which can be used, and these require a specific pattern of mirror movement, known as the trajectory. The objective of picking a good trajectory are the conflicting needs of doing the move quickly and accurately. Accurate movement requires minimum torque, which would otherwise deform the apparatus. This limits the speed with which motions can be made. At equal time increments, a calculation is performed to determine the current position of each mirror. The particular calculation used depends upon the type of scanning employed. </paragraph>
<paragraph id="P-0157" lvl="0"><number>&lsqb;0157&rsqb;</number> Raster Scanning </paragraph>
<paragraph id="P-0158" lvl="0"><number>&lsqb;0158&rsqb;</number> When the desired scan field is a polygon, one of two raster scanning patterns is used. In the first, scanning is uni-directional (i.e., always proceeds from left to right, or right to left, on parallel lines). <cross-reference target="DRAWINGS">FIG. 11A</cross-reference> shows such a unidirectional scan pattern. In between scan lines, the scan mirror retraces to the beginning of the next line without making any range measurements. The retrace can proceed quite quickly since no measurements are being made during the retrace. </paragraph>
<paragraph id="P-0159" lvl="0"><number>&lsqb;0159&rsqb;</number> A slightly more efficient means of raster scanning is bi-directional, in which scanning is also performed on the retrace. <cross-reference target="DRAWINGS">FIG. 11B</cross-reference> shows such a bi-directional scan pattern This is not as efficient as it might seem because the retrace time is used for other calculations, and because the resulting scan pattern is not as regular. </paragraph>
<paragraph id="P-0160" lvl="0"><number>&lsqb;0160&rsqb;</number> Both raster scanning methods require traversing a straight line in the minimum time, starting at zero velocity and ending at zero velocity. The torque applied to the mirror is proportional to the angular acceleration, which must zero at the beginning and end of the scan since the mirror is at result. It can be shown that the trajectory that makes such a minimum energy move between two points is given by the sum of a straight line and a full cycle of a sin. However, this is closely approximated with much less computation by the minimum degree polynomial, with boundary conditions p(t0)&equals;p0, p&prime;(t0)&equals;0, p&Prime;(t0)&equals;0, p(t1)&equals;p1, p&prime;(t1)&equals;0, and p&prime;(t1)&equals;0 which is the fifth order polynomial: p(t)&equals;(p<highlight><subscript>1</subscript></highlight>&minus;p<highlight><subscript>0</subscript></highlight>)t&prime;<highlight><superscript>3</superscript></highlight>(6t&prime;<highlight><superscript>2</superscript></highlight>&minus;15t&prime;&plus;10)&plus;p<highlight><subscript>0</subscript></highlight>, where t&prime;&equals;(t&minus;t<highlight><subscript>0</subscript></highlight>)/(t<highlight><subscript>1</subscript></highlight>&minus;t<highlight><subscript>0</subscript></highlight>). </paragraph>
<paragraph id="P-0161" lvl="0"><number>&lsqb;0161&rsqb;</number> Spiral Scanning </paragraph>
<paragraph id="P-0162" lvl="0"><number>&lsqb;0162&rsqb;</number> A disadvantage of raster scanning is that since the speed of the trajectory is varying, the scanning efficiency is not optimal. A spiral pattern can achieve a constant speed trajectory which permits a uniform point distribution. </paragraph>
<paragraph id="P-0163" lvl="0"><number>&lsqb;0163&rsqb;</number> Seeking </paragraph>
<paragraph id="P-0164" lvl="0"><number>&lsqb;0164&rsqb;</number> In addition to scanning a range image, the system is capable of performing a number of functions which are common in surveying. The scanner can be made to search for important features, or locations of high reflectivities. This allows the system to perform normal surveying functions by finding a target whose location is approximated identified, and reporting its exact angles and position. </paragraph>
<paragraph id="P-0165" lvl="0"><number>&lsqb;0165&rsqb;</number> Angle Calibration </paragraph>
<paragraph id="P-0166" lvl="0"><number>&lsqb;0166&rsqb;</number> The capacitive encoders in the moving coil motors have tremendous repeatability, but relatively poor accuracy. A number of calibration activities need to be continuously performed to ensure system accuracy. </paragraph>
<paragraph id="P-0167" lvl="0"><number>&lsqb;0167&rsqb;</number> Before use, each scanner is calibrated over its complete range of angles. At a number of discrete temperatures, a map is created and stored of the measurements of apparent angles for thousands of accurately measured points using an external resolver that is traceable to NBS standards. The DSP linearly interpolates between these measured points on every angle measurement. Preferably, the accuracy of angle measurement is improved by determining scale or offset errors in the encoder during operation. Commercially available scanners can drift significantly with environment changes. This results in a shift in the effective zero and full scale range of the angle measurement, while maintaining the overall shape of the calibration curve obtained by making careful laboratory measurements before the scanner is installed in the system. The environmental effect is reduced by providing a means for determining when the scanner is at a known and repeatable angle. In one preferred embodiment of such a system, two optical references which are fixed with regard to the case of the instrument are aimed at the back of each scanning mirror. There are a variety of mechanisms for providing the optical reference, but in one preferred embodiment, a pair of autocollimators are aimed at a reflective surface on the back of the scanning mirrors and will provide a highly repeatable measurement of when the mirror is normal to the axis of each autocollimator. Each autocollimator gives a-reference angle to within approximately 10 &mgr;rad. Periodically, the scanner is moved under computer control to the position at which the mirror is closes to being normal to the autocollimator axis, and the apparent angle is measured. The measurements are compared with the measurements taken when the scanners were calibrated, and a linear correction is calculated and applied to every subsequent measurement. </paragraph>
<paragraph id="P-0168" lvl="0"><number>&lsqb;0168&rsqb;</number> In an alternative embodiment, a pair of mechanical stops is provided just past the normal range of motion of the scanning mirror. Periodically, the mirror is driven until it touches a mechanical stop. Then, the scanning mirror is driven with a known current, which corresponds to a known force. The mirror arrives at equilibrium at a very repeatable position, and this is used to calculate a linear correction to the mirror calibration curves. </paragraph>
<paragraph id="P-0169" lvl="0"><number>&lsqb;0169&rsqb;</number> Range Calibration Fibers </paragraph>
<paragraph id="P-0170" lvl="0"><number>&lsqb;0170&rsqb;</number> The timing circuits have a certain amount of offset and scale drift with time and temperature, and a provision has been included to compensate for these variations. When an optical pulse is emitted from the laser <highlight><bold>602</bold></highlight> a small amount of the energy is sampled by a beam splitter <highlight><bold>810</bold></highlight> and introduced into a single mode optical fiber <highlight><bold>830</bold></highlight> by focusing the beam using a lens <highlight><bold>833</bold></highlight> on the fiber face <highlight><bold>831</bold></highlight>. The other face of the fiber <highlight><bold>832</bold></highlight> is arranged so that the beam which comes out of it is collimated into a beam which enters the lidar receiver <highlight><bold>610</bold></highlight>. The fiber can either be produced so that its length does not vary with temperature, or its variation in length with temperature can be accurately characterized. When a single mode fiber is used, the variation in propagation delay will be less than a few picoseconds and the pulse shape emitted by the fiber will be nearly identical to the pulse shape going into the fiber. Periodically, the timing circuits are used to measure the propagation delay through the fiber, and corresponding adjustments are made to the range measurements taken from external surfaces. </paragraph>
<paragraph id="P-0171" lvl="0"><number>&lsqb;0171&rsqb;</number> The fibers can be manufactured so that the end at which the pulse is launched <highlight><bold>833</bold></highlight> and from which the pulse is emitted <highlight><bold>834</bold></highlight> are partially reflecting. When this is done, the pulse enters <highlight><bold>833</bold></highlight> and is propagated to the opposite end <highlight><bold>834</bold></highlight>, at which point only some of the energy is released and the rest returns to the first end <highlight><bold>833</bold></highlight>. Again, a fraction of the light is emitted, and the rest reflected, which eventually is emitted into the receiver. This process repeats until the remaining energy in the fiber falls to negligible levels. The result is a sequence of pulses, commonly 3-10, being applied to the receiver, which have delays all repeatable to within a few picoseconds. Periodically, the timing circuits are used to measure these pulse trains from the fiber, and corresponding adjustments are made to the range measurements taken from external surfaces. </paragraph>
<paragraph id="P-0172" lvl="0"><number>&lsqb;0172&rsqb;</number> Range Walk Calibration </paragraph>
<paragraph id="P-0173" lvl="0"><number>&lsqb;0173&rsqb;</number> The lidar system measures the range of surfaces by timing the delay between the laser pulse being emitted and returning from the surface. This delay is measured electronically by imposing a sample of the outgoing pulse, and the return pulse on an optically sensitive electronic detector <highlight><bold>612</bold></highlight> embedded in the receiver <highlight><bold>610</bold></highlight>. In one, embodiment, the electronic timing circuit measures the time between when the outgoing pulse exceeds a set threshold voltage, and when the return pulse exceeds the same voltage. The outgoing pulse will be the same intensity within a few percent. However, many surfaces vary greatly in the amount of light that will be reflected. The result is that the apparent relative time for two pulses which occur at the same range but have different intensities may appear to be at different ranges. The measured time for a small pulse <highlight><bold>3810</bold></highlight> to first exceed the threshold level will be later than the measured time for a large pulse <highlight><bold>3830</bold></highlight> to exceed the same threshold, even when the pulses return from objects at the same range. Thus highly reflective objects or objects at distances of maximum transceiver sensitivity will appear slightly closer. This creates an apparent &ldquo;range walk&rdquo; as a function of intensity. The range walk can be corrected if the shape of the optical return is always the same and the energy of the return is known. The extremely repeatable shape of the pulses generated by the passively Q-switched microchip laser makes this possible. </paragraph>
<paragraph id="P-0174" lvl="0"><number>&lsqb;0174&rsqb;</number> Part of the timing circuit estimates the energy in each detected pulse. A table of corrections is maintained to improve the range estimates. Two different circuits have been employed to make a measurement of the pulse energy for this purpose. The first is a gated integrator, the gate being open at the beginning of the pulse, and closed at the end. The signal is applied to a comparator <highlight><bold>3920</bold></highlight> which closes the switch <highlight><bold>3930</bold></highlight> when the signal exceeds a selected level, and closes it when the signal falls below the same level. The signal is also applied to a delay <highlight><bold>3910</bold></highlight>, and the output of the delay goes through the switch <highlight><bold>3930</bold></highlight> when it is closed, and is applied to the integrator <highlight><bold>3940</bold></highlight> over the period of time the switch is closed. The delay is chosen to compensate for the time lag in the comparator and switch. When the pulse is complete, the value of the integrator is sampled by an analog to digital converter <highlight><bold>3950</bold></highlight>. The second consists of a integrator with a time constant scaled to the width of the pulse <highlight><bold>4010</bold></highlight>, followed by a peak detector <highlight><bold>4020</bold></highlight> which has a time constant much longer than the pulse width. The output of the peak detector is sampled shortly after the pulse is detected. </paragraph>
<paragraph id="P-0175" lvl="0"><number>&lsqb;0175&rsqb;</number> Periodically, the timing circuit is used to measure a sequence of pulses which have been delayed by the single mode fibers <highlight><bold>830</bold></highlight> used to calibrate the offset and scale factors associated with the time circuits. Additionally, the intensity of these pulses are varied over a broad range by a variable attenuator <highlight><bold>820</bold></highlight>. By altering the amount of light coupled into the fiber, the energy of the detected pulses can be varied over the dynamic range of the receiver, at one particular time delay. The intensity and the measured time delay values produce a map of the range walk correction required for each intensity, and this correction is applied to subsequent measurements. This correction can provide accuracy of 1 mm over the dynamic range of the instrument, particularly as a result of the great repeatability of the laser pulse waveform. This function is then used to correct the measured range of external surfaces as a function of light intensity returned from those surfaces. </paragraph>
<paragraph id="P-0176" lvl="0"><number>&lsqb;0176&rsqb;</number> Geometry Calculation </paragraph>
<paragraph id="P-0177" lvl="0"><number>&lsqb;0177&rsqb;</number> The output of the FDV after a range scan consists of points in spherical coordinates with respect to a coordinate system in the scanner. However, the raw data consists of mirror angles and time intervals. The DSP computes the spherical coordinates of the scanned points by talking into account scanner geometry (mirror thickness, optic axes, mirror offsets, etc.) and all the appropriate calibration adjustments. </paragraph>
<paragraph id="P-0178" lvl="0"><number>&lsqb;0178&rsqb;</number> Laser Control </paragraph>
<paragraph id="P-0179" lvl="0"><number>&lsqb;0179&rsqb;</number> Delay Prediction </paragraph>
<paragraph id="P-0180" lvl="0"><number>&lsqb;0180&rsqb;</number> The digital signal processor is responsible for controlling the firing of the pulsed laser, but it can only do so indirectly. The processor has control of the timing for starting the pump diode, which causes the passive q-switch to fire after saturation has occurred. However there is a variable delay between turning on the pump and having the laser fire. The delay is a function of junction temperature, which in turn is a complex function of ambient temperature and recent history of laser firing. The delay generally ranges between 100-300 usecs. </paragraph>
<paragraph id="P-0181" lvl="0"><number>&lsqb;0181&rsqb;</number> Fortunately, it is primarily necessary to know the scanning mirror angle at the precise moment the laser fires. After the laser has been fired just a few times, the pump delay does not change quickly if the firing rate does not change quickly. As a result, accuracy of a few microseconds can be achieved by estimating the next pump delay to be the same as that in the previous firing cycle. The digital signal processor measures the pump delay by reading an internal counter when the pump is started and when the laser actually fires, causing an interrupt. Since the interrupt latency is less than a microsecond, this becomes the timing accuracy to which the pump delay can be measured. </paragraph>
<paragraph id="P-0182" lvl="0"><number>&lsqb;0182&rsqb;</number> A more sophisticated dynamic model of the thermal properties of the laser could lead to slightly enhanced scanning pattern regularity, but is probably equally limited by the time resolution of the processor interrupts. </paragraph>
<paragraph id="P-0183" lvl="0"><number>&lsqb;0183&rsqb;</number> Firing Control </paragraph>
<paragraph id="P-0184" lvl="0"><number>&lsqb;0184&rsqb;</number> Given a time vs. angle trajectory for a scanning axis, w(t), a desired angle to fire the laser, and an interrupt interval Dt, the decision to fire the laser amounts to computing the time at which point the pump diode is started. </paragraph>
<paragraph id="P-0185" lvl="0"><number>&lsqb;0185&rsqb;</number> Computer Control </paragraph>
<paragraph id="P-0186" lvl="0"><number>&lsqb;0186&rsqb;</number> The FDV is designed to perform under the control of a remote host computer which contains graphical controls for a user to specify areas to be scanned. The remote machine controls the FDV through a bi directional serial byte stream, which is effected in any of a number of media: Ethernet, EPP parallel port serial port A processor in the FDV is assigned the task of decoding messages, and scheduling the required activity. <cross-reference target="DRAWINGS">FIG. 12</cross-reference> is a block diagram of the FDV processor. </paragraph>
<paragraph id="P-0187" lvl="0"><number>&lsqb;0187&rsqb;</number> Host Communications Interface </paragraph>
<paragraph id="P-0188" lvl="0"><number>&lsqb;0188&rsqb;</number> The host machine acts as a master, sending a well defined message protocol to command the FDV. When actions are completed, the FDV responds with data and status information. Among the actions which can be requested are: </paragraph>
<paragraph id="P-0189" lvl="2"><number>&lsqb;0189&rsqb;</number> Point the scanner </paragraph>
<paragraph id="P-0190" lvl="2"><number>&lsqb;0190&rsqb;</number> measure a distance </paragraph>
<paragraph id="P-0191" lvl="2"><number>&lsqb;0191&rsqb;</number> range scan a box </paragraph>
<paragraph id="P-0192" lvl="2"><number>&lsqb;0192&rsqb;</number> fire the laser n times </paragraph>
<paragraph id="P-0193" lvl="2"><number>&lsqb;0193&rsqb;</number> take a video image </paragraph>
<paragraph id="P-0194" lvl="0"><number>&lsqb;0194&rsqb;</number> Scanner Control </paragraph>
<paragraph id="P-0195" lvl="0"><number>&lsqb;0195&rsqb;</number> Referring to <cross-reference target="DRAWINGS">FIG. 13</cross-reference>, in normal operation, each scanner in the dual mirror system requires a 16 to 18 bit digital word to set the desired position, which is applied to a precision digital to analog converter to create a voltage proportional to the desired position. However, there will be some error between the position commanded by the output of this converter and the actual position of the scanner, which is reflected by the output of the position encoder. A precision difference signal is generated, and the difference is measured to 12 bit accuracy. This provides an economic method of making 18 bit position measurements while only using an inexpensive 12 bit converter. </paragraph>
<paragraph id="P-0196" lvl="0"><number>&lsqb;0196&rsqb;</number> Commercially available galvo scanners have microradian repeatability, but have relatively poor scale and offset performance, particularly over temperature. A calibration mode has been incorporated into the system to permit making measurements at two precise angles, and using the two measured data points the offset and scale drift of the scanner can be calculated. </paragraph>
<paragraph id="P-0197" lvl="0"><number>&lsqb;0197&rsqb;</number> Two methods have been developed for this purpose: an optical and a mechanical means. In the mechanical method, the scanner shaft is gently placed against one of two mechanical stops, and the current in the scanner controller is adjusted to a specific value, which provides a known force. The position signal is adjusted until there is no position error, and this gives the calibrated position measurement. In the optical method, two autocollimators are aimed at the back of the scanner mirrors, which have also been polished and mirror coated. When the scanner mirrors are exactly aligned with one of the collimators, the output from the split photodetector in the autocollimator is balanced. By placing the scanner in each of these precise angles in turn, an offset and scale correction for the scanner encoder can be calculated. </paragraph>
<paragraph id="P-0198" lvl="7"><number>&lsqb;0198&rsqb;</number> Timing Circuit </paragraph>
<paragraph id="P-0199" lvl="0"><number>&lsqb;0199&rsqb;</number> The purpose of the timing circuit is to provide the relative time between the start pulse and the stop pulse, in picoseconds. There are two subsystems in the timing circuit: a signal conditioning and energy integration circuit (an embodiment of which is shown in <cross-reference target="DRAWINGS">FIG. 14</cross-reference>), and a time interval analyzer. Both communicate directly with the DSP. Initially, systems have been produced with a commercial timing instrument, the Stanford Research Systems SR620 time interval analyzer. The interface to this instrument is through an IEEE488 interface. In a preferred embodiment, the communications interface to the Stanford Research Systems SR620 time interval analyzer is IEEE488. </paragraph>
<paragraph id="P-0200" lvl="0"><number>&lsqb;0200&rsqb;</number> A custom time interval measurement circuit has been developed which utilizes a separately patented interpolation technology. The circuit employs a clock, typically operating at &gt;100 mhz, which is used to make a coarse count of 10 nsec intervals between stop and start pulses. Additionally, there is an interpolator which divides each 10 nsec coarse count into 1000 smaller increments, providing 10 psec resolution. This system has approximately 5 psec jitter. Differential time measurements can be made with less than 20 psec RMS error, which corresponds to about 3 mm. This circuit communicates with the DSP using a dedicated serial bus, and employs a packet protocol: the DSP arms the circuit by sending a single byte. When the timing circuit completes its task, it sends a sequence of bytes which represent both the time delay between start and stop pulses, and the intensity of each pulse. </paragraph>
<paragraph id="P-0201" lvl="0"><number>&lsqb;0201&rsqb;</number> Laser Firing </paragraph>
<paragraph id="P-0202" lvl="0"><number>&lsqb;0202&rsqb;</number> The DSP has three lines for laser control: one starts the laser pump, the second indicates that the laser has fired, and the third indicates that the return pulse from a target has been detected. When the laser fires, the DSP samples the analog pulse amplitude signal. This happens typically within 1 &mgr;sec. </paragraph>
<paragraph id="P-0203" lvl="0"><number>&lsqb;0203&rsqb;</number> Video </paragraph>
<paragraph id="P-0204" lvl="0"><number>&lsqb;0204&rsqb;</number> For targeting, the user is provided on the host a video representation of the scene from which he can choose a portion to be range scanned. In most cases this will correspond to the scene rendered in ambient illumination. </paragraph>
<paragraph id="P-0205" lvl="0"><number>&lsqb;0205&rsqb;</number> Capture </paragraph>
<paragraph id="P-0206" lvl="0"><number>&lsqb;0206&rsqb;</number> One way the video is captured is by using the scanner to aim single sensitive detector across the scene with the laser turned off. This permits acquiring an image which has very accurate spatial alignment with subsequent range scans. However, image capture can be quite slow in comparison to commercially available cameras. </paragraph>
<paragraph id="P-0207" lvl="0"><number>&lsqb;0207&rsqb;</number> A second approach is to utilize standard commercial CCD video cameras to acquire an image. One CCD camera with a wide angle lens is aligned with the range scanner with as small an offset as possible. A second camera with a 5 degree field of view is placed so that its optic axis is coaxial with the transceiver. Thus, a much smaller field of view is accessible through the scanner, and can be scanned with the same resolution as the transceiver. This allows targeting small or distant objects. </paragraph>
<paragraph id="P-0208" lvl="0"><number>&lsqb;0208&rsqb;</number> Alignment </paragraph>
<paragraph id="P-0209" lvl="0"><number>&lsqb;0209&rsqb;</number> The wide angle lens introduces a fish-bowl effect in the image captured by the CCD sitting behind the lens. Straight lines in the world are not straight in the image. This distortion increases with the distance from the center of the lens. This distortion is removed by comparing the image the camera produces when aimed at a carefully designed and printed calibration target image. The difference in the anticipated image and the recorded image provides the information needed to warp subsequently acquired images to eliminate the distortion. </paragraph>
<paragraph id="P-0210" lvl="0"><number>&lsqb;0210&rsqb;</number> Compression </paragraph>
<paragraph id="P-0211" lvl="0"><number>&lsqb;0211&rsqb;</number> Each video image is compressed prior to transfer. Currently we are using JPEG standard image compression. It is relatively fast, and creates reasonably small compressed images for communication. Another desirable feature is that the algorithm operates on blocks, which permits us to do interleave image capture, alignment, compression, and transmission in parallel&mdash;significantly enhancing throughput. </paragraph>
<paragraph id="P-0212" lvl="0"><number>&lsqb;0212&rsqb;</number> Point Video </paragraph>
<paragraph id="P-0213" lvl="0"><number>&lsqb;0213&rsqb;</number> A second camera, with a narrow field of view (e.g., approximately 5&deg;) is placed such that it is coaxial with the scanning laser beam. The field of view is adjusted so that the pixel resolution is approximately the same as the voxel resolution of the lidar system. The camera can be operated while the laser is activated. When this is done, a small group of pixels will be illuminated by the laser, and the centroid of these pixels will correspond to the point which would be measured by the lidar. When a video image is capture it can be mapped onto a surface which is estimated by a lidar scan. </paragraph>
<paragraph id="P-0214" lvl="0"><number>&lsqb;0214&rsqb;</number> Computer Graphics Perception (CGP) Software </paragraph>
<paragraph id="P-0215" lvl="0"><number>&lsqb;0215&rsqb;</number> Referring to <cross-reference target="DRAWINGS">FIG. 15</cross-reference>, the CGP <highlight><bold>40</bold></highlight> is a software system that runs on a CGP Computer <highlight><bold>1500</bold></highlight> and communicates with the FDV <highlight><bold>10</bold></highlight>. The CGP <highlight><bold>40</bold></highlight> runs on many different types of computers, including laptops and workstations. The CGP <highlight><bold>40</bold></highlight> operates on a computer <highlight><bold>1500</bold></highlight> with a suitable display device <highlight><bold>1510</bold></highlight>, such as a color graphic display terminal, a suitable character input device <highlight><bold>1520</bold></highlight>, such as a keyboard, and a suitable pointing device <highlight><bold>1530</bold></highlight>, such as a mouse. The software can use any number of standard 3-D graphics rendering libraries to interactively present the acquired 3-D data in a window on the display device. The portion of the CGP <highlight><bold>40</bold></highlight> user interface that involves 3-D view manipulation and data projection into a window is handled by the 3-D library. </paragraph>
<paragraph id="P-0216" lvl="0"><number>&lsqb;0216&rsqb;</number> The CGP <highlight><bold>40</bold></highlight> performs real time 3-D data acquisition and modeling in the field. The CGP&apos;s <highlight><bold>40</bold></highlight> functionality includes high level FDV <highlight><bold>10</bold></highlight> control, targeting and data acquisition; display and visualization of scanned points; surface segmentation and fitting; manual 3-D model construction; 3-D visualization; interaction with part and model databases; and the ability to export data in standard data exchange formats to other CAD systems for further processing. The integration of hardware and software, as described here, enables major improvements in productivity and quality in the overall process of three dimensional modeling of reality. </paragraph>
<paragraph id="P-0217" lvl="0"><number>&lsqb;0217&rsqb;</number> With reference to <cross-reference target="DRAWINGS">FIG. 1</cross-reference>A, the data acquisition and modeling process divides into the following steps: FDV <highlight><bold>10</bold></highlight> control, point acquisition, segmentation, geometry fitting, modeling by manipulating the geometry, scene registration with or without warping, model annotation, and geometry display and query. </paragraph>
<paragraph id="P-0218" lvl="0"><number>&lsqb;0218&rsqb;</number> With reference to <cross-reference target="DRAWINGS">FIGS. 16A and 16B</cross-reference>, the foregoing operations may be performed in at least two graphic display windows. One window <highlight><bold>1610</bold></highlight> (<cross-reference target="DRAWINGS">FIG. 16A</cross-reference>) displays a video image of the target scene used to define regions to be scanned by the FDV <highlight><bold>10</bold></highlight> while the other window <highlight><bold>1620</bold></highlight> (<cross-reference target="DRAWINGS">FIG. 16B</cross-reference>) displays an interactive 2D projection of the 3-D model consisting of the scanned points and constructed surface geometry as well as other information about the scene. Additional windows may be used to provide multiple views of the data. In addition, the CGP <highlight><bold>40</bold></highlight> provides additional windows for controlling the FDV <highlight><bold>10</bold></highlight> hardware and for setting and displaying the status parameters of the system. </paragraph>
<paragraph id="P-0219" lvl="0"><number>&lsqb;0219&rsqb;</number> Scan Control </paragraph>
<paragraph id="P-0220" lvl="0"><number>&lsqb;0220&rsqb;</number> Referring to <cross-reference target="DRAWINGS">FIG. 15</cross-reference>, prior to using the integrated hardware/software system the FDV <highlight><bold>10</bold></highlight> is positioned to point in the direction of the objects <highlight><bold>20</bold></highlight> of interest. </paragraph>
<paragraph id="P-0221" lvl="0"><number>&lsqb;0221&rsqb;</number> Scan control is the process of indicating which portions of the scene that are visible to the scanner are to be scanned. Different parts of the visible scene can be scanned at different densities, since simple geometric objects, such as planes, cylinders and spheres can be accurately modeled with a fairly low number of scan points. Therefore, the region in front of the scanner is often captured in multiple scans, rather than in one high resolution scan. Only regions with high levels of detail need high resolution scans. </paragraph>
<paragraph id="P-0222" lvl="0"><number>&lsqb;0222&rsqb;</number> Referring to <cross-reference target="DRAWINGS">FIG. 17</cross-reference>A, one of the means of scan control is the use of a video image <highlight><bold>1710</bold></highlight> of the scene acquired from the FDV <highlight><bold>10</bold></highlight>. Using a pointing device, such as a mouse, one can indicate the region to be scanned by any number of methods, such as dragging out a rectangle <highlight><bold>1720</bold></highlight> on the video image. The CGP <highlight><bold>40</bold></highlight> instructs the FDV <highlight><bold>10</bold></highlight> to measure the range of whatever object exists at the center of the user specified targeting region to assist in specifying the scan density, since the angle between points is determined by both the desired density on the surface and the distance from the scanner. A means for specifying the desired scan parameters, such as a dialog box, is provided and allows the user to specify the scan parameters in a variety of ways, including point density, point spacing, or total number of points in each of the vertical and horizontal directions. </paragraph>
<paragraph id="P-0223" lvl="0"><number>&lsqb;0223&rsqb;</number> The CGP <highlight><bold>40</bold></highlight> then translates the region and scan resolution information into a set of commands for the FDV <highlight><bold>10</bold></highlight>. These commands are communicated to the FDV <highlight><bold>10</bold></highlight> using a means of communications, such as a TCP/IP network connection, and the acquired data is also returned to the CGP Computer <highlight><bold>1500</bold></highlight> using the same means. </paragraph>
<paragraph id="P-0224" lvl="0"><number>&lsqb;0224&rsqb;</number> Additional scans at different densities can be initiated in the same way, or one can use previously scanned data points rather than the video image to specify new scan regions. If the view of the scanned data is oriented so that it is exactly aligned with the scanner direction, then a scan region can be indicated by methods such as dragging out a rectangular box. When the data is aligned to the scanner in this way most of the 3-D information is difficult to see, therefore, the software can display the points with the intensity of the returned laser light at each point color mapped as described in the next section. The intensity information is often sufficient to identify objects in the data window, so that new scan regions can be defined. Alternatively, the user can model and/or color some of the objects in the scene to help locate regions of interest in the window. Using the data window to define new scan regions avoids any parallax errors, since the view is aligned with the scanner. </paragraph>
<paragraph id="P-0225" lvl="0"><number>&lsqb;0225&rsqb;</number> Scan control can also be achieved by using the pointing device to move the laser beam and highlight points in the actual scene. Any number of methods could be used to describe the desired scan region by moving the laser beam and identifying points of interest by a user action, such as clicking a mouse button. Methods could include operations such as: indicating a bounding box by moving the laser to diagonally opposite corners of the desired scan regions; indicating the top, bottom, left and right bounds of the scene; indicating a sequence of points that represent the bounding polygon of the scan region; indicating the center of the scan region and using other means, such as dialog boxes to describe the extent of the desired scan region. </paragraph>
<paragraph id="P-0226" lvl="0"><number>&lsqb;0226&rsqb;</number> Point Acquisition </paragraph>
<paragraph id="P-0227" lvl="0"><number>&lsqb;0227&rsqb;</number> With reference to <cross-reference target="DRAWINGS">FIG. 17</cross-reference>B, the data returned by the FDV <highlight><bold>10</bold></highlight> consist of the coordinates of the points and their intensity values. In one preferred embodiment, the scanning is performed in such a way that the data returned lies in an ordered grid of three dimensional points <highlight><bold>1730</bold></highlight>. Viewed from the scanner, these points appear as a regular rectangular grid, much like a bitmap. However, each point consists of its coordinates in three-space as well as the intensity of the reflected laser pulse at that location. </paragraph>
<paragraph id="P-0228" lvl="0"><number>&lsqb;0228&rsqb;</number> Each point returned is displayed in the data window <highlight><bold>1620</bold></highlight> as it is transmitted by the FDV <highlight><bold>10</bold></highlight>. The CGP <highlight><bold>40</bold></highlight> lets the user interactively change the 3-D view of the data while the data is arriving to get a better idea of the spatial layout of the data. Also, to help visualize different features in the data, of the CGP <highlight><bold>40</bold></highlight> can allow each point to be color mapped from the intensity of the reflected laser pulse at that location. A scan cloud <highlight><bold>1810</bold></highlight> is shown in <cross-reference target="DRAWINGS">FIG. 18</cross-reference> representing the surface of a horse sculpture. Instead of having all the points in a single color, as shown in <cross-reference target="DRAWINGS">FIG. 18</cross-reference>, one can map different laser return intensity values to different colors, and produce a multicolored scan field <highlight><bold>1910</bold></highlight> as shown in <cross-reference target="DRAWINGS">FIG. 19</cross-reference>. The intensity color mapping provides considerable extra surface feedback to the user and this is useful in both targeting and modeling, as described later. </paragraph>
<paragraph id="P-0229" lvl="0"><number>&lsqb;0229&rsqb;</number> The ordered grid of points generated is referred to as a scan field. Multiple, possibly overlapping scan fields may be gathered and simultaneously displayed in the manner described above. The data structures within the CGP <highlight><bold>40</bold></highlight> maintain the list of scan fields, so each data point is always associated with a scan field. The scan fields usually contain data points from the surfaces of many different objects, so they need to be partitioned into smaller groups of points, as described in the next section. </paragraph>
<paragraph id="P-0230" lvl="0"><number>&lsqb;0230&rsqb;</number> Segmentation </paragraph>
<paragraph id="P-0231" lvl="0"><number>&lsqb;0231&rsqb;</number> Segmentation is the process of grouping together points that were scanned from the surface of the same object The points from a single object may be a small portion of a scan field, or may occur across multiple scan fields. The segmentation process may be manual, as described below, or autornated, as described later in the auto-segmentation section. </paragraph>
<paragraph id="P-0232" lvl="0"><number>&lsqb;0232&rsqb;</number> Referring to <cross-reference target="DRAWINGS">FIG. 20</cross-reference>, the first step of the manual segmentation process is to select one or more scan fields <highlight><bold>2010</bold></highlight> that contain scan points on the object of interest. Selecting one or more scan fields can be performed by any of the conventional means, such as using a pointing device, possibly together with keyboard keys. Selecting a scan field selects all of the points in the scan field. The group of points resulting from this step form a pool of candidate points that can now be trimmed to remove points on other objects. Each point in the pool is initially marked as selected, and the operations described below can be used to toggle the point states between selected and deselected. </paragraph>
<paragraph id="P-0233" lvl="0"><number>&lsqb;0233&rsqb;</number> Referring to <cross-reference target="DRAWINGS">FIG. 21</cross-reference>, scan points <highlight><bold>2010</bold></highlight> from a desired object surface can be cut out using one or more lasso operations from possibly different views. The user can manipulate the view direction as needed to give a clear view of the desired subset of the point pool. The user then uses the pointing device to draw a polygonal lasso region <highlight><bold>2110</bold></highlight>, which divides the screen into two regions: the interior and exterior of the lasso polygon. The following operations are supported: mark all points in a region as selected and all of the other points as deselected, mark all of the points in a region as selection without affecting the other points, and mark all of the points in a region as deselected without affecting the other points. The lasso operation may be repeated as many times as necessary to refine the selection, possibly changing the view of the scene between lasso operations. The user can the cut the currently selected set of points out to form a new point set The new point set acts like a scan field in that it is, and can take part in the fitting operations described in the next section. In <cross-reference target="DRAWINGS">FIG. 22</cross-reference>, three new groups of points <highlight><bold>2210</bold></highlight>, <highlight><bold>2220</bold></highlight> and <highlight><bold>2230</bold></highlight> have been created using the manual segmentation method described here, and some points near the intersection of the planes are left from the original cloud of points. </paragraph>
<paragraph id="P-0234" lvl="0"><number>&lsqb;0234&rsqb;</number> Geometry Fitting </paragraph>
<paragraph id="P-0235" lvl="0"><number>&lsqb;0235&rsqb;</number> In one preferred embodiment, the CGP <highlight><bold>40</bold></highlight> can contain many geometric primitives that can be used to simulate the actual surfaces of the objects scanned. The geometric primitives include any number of standard graphics primitives, such as triangulated meshes, planes, cylinders, spheres, torii, lines, and points. The simplest form of geometry fitting involves using a triangulated mesh to connect the scan points to show the surface features of the objects scanned. The scan cloud <highlight><bold>1810</bold></highlight> in <cross-reference target="DRAWINGS">FIG. 18</cross-reference> can be meshed <highlight><bold>2310</bold></highlight> and rendered as shown in <cross-reference target="DRAWINGS">FIG. 23</cross-reference>. Since the scan data are acquired in a regular grid, it is simple to create a triangular mesh by connecting neighboring points. The user can also set discontinuity tolerances in depth and angle to avoid meshing adjacent points separated by more than the specified threshold. Breaking the mesh in this way provides a more realistic looking surface, referred to as a shrinkwrap surface, because artificial mesh surfaces at occlusion edges do not occur. A wide variety of known mesh operations can be applied to the resulting mesh, such as smoothing (noise reduction) and mesh simplification (to reduce the mesh density in smooth areas that do not require a fine mesh grid). Mesh vertices may also be colored with information such as intensity. </paragraph>
<paragraph id="P-0236" lvl="0"><number>&lsqb;0236&rsqb;</number> As stated above, the CGP <highlight><bold>40</bold></highlight> includes many standard geometric primitives. Before fitting the points to such objects, the point clouds must be segmented as described above. Once segmented, each group of points represents a single surface that can be fit by a geometric object. The fitting can be guided by the user, who may know the type of shape to be fit. For instance, after scanning the corner of a room it is clear to the user that the points on a wall can be fit by a plane and the points on a pipe can be fit by a cylinder, so the user can request the fit of a specific object It is also possible to semi-automate this process to identify which shape best fits a particular point group. </paragraph>
<paragraph id="P-0237" lvl="0"><number>&lsqb;0237&rsqb;</number> Fitting a plane to a set of points is a simple problem that has many well-known solutions. The extent of the patch used in the CGP <highlight><bold>40</bold></highlight> to represent the plane can be determined by the convex hull of the points in the plane. For instance, the three point groups <highlight><bold>2210</bold></highlight>,<highlight><bold>2220</bold></highlight> and <highlight><bold>2230</bold></highlight> shown in <cross-reference target="DRAWINGS">FIG. 22</cross-reference> can each be separately fit to planes <highlight><bold>2410</bold></highlight>, <highlight><bold>2420</bold></highlight> and <highlight><bold>2430</bold></highlight> as shown in <cross-reference target="DRAWINGS">FIG. 24</cross-reference> using any available fitting algorithm. </paragraph>
<paragraph id="P-0238" lvl="0"><number>&lsqb;0238&rsqb;</number> Many standard approaches are available for fitting more complex shapes. In one preferred embodiment, two phases are involved: a parameter estimation phase to get a starting point, and an optimization phase, where the parameters are varied to minimize the overall error. The total error is the sum of the squares of the distance between each scan point and the nearest point on the surface of the object being fit The optimization phase uses conventional optimization methods to reduce the error between the object, as defined by its parameters, and the data given by the scan points. </paragraph>
<paragraph id="P-0239" lvl="0"><number>&lsqb;0239&rsqb;</number> A cylinder fitter can convert a cloud of points <highlight><bold>2510</bold></highlight> as shown in <cross-reference target="DRAWINGS">FIG. 25</cross-reference> into a cylinder object <highlight><bold>2610</bold></highlight> as shown in <cross-reference target="DRAWINGS">FIG. 26</cross-reference>. All fitted objects, including the cylinder, reference the original points that were used to fit the object The user may choose to view the resulting cylinder <highlight><bold>2610</bold></highlight> or the original points <highlight><bold>2510</bold></highlight>, or both, at any time. Using manual or automatic segmentation methods, it is possible to convert the scan clouds, <highlight><bold>2710</bold></highlight> in <cross-reference target="DRAWINGS">FIG. 27</cross-reference>, representing many cylinders into the best fit cylinders <highlight><bold>2810</bold></highlight> shown in <cross-reference target="DRAWINGS">FIG. 28</cross-reference>. Once each cylinder&apos;s diameter and axis are established, it is possible to manually or automatically add elbows <highlight><bold>2910</bold></highlight> in <cross-reference target="DRAWINGS">FIG. 29</cross-reference> to complete the modeling process. </paragraph>
<paragraph id="P-0240" lvl="0"><number>&lsqb;0240&rsqb;</number> A cylinder is described by five parameters: a normalized vector describing the cylinder axis (two independent parameters), the radius, and two additional parameters that are used to locate the line of action of the cylinder axis in space. The length of the resulting cylinder can be determined by projecting the scan points onto the cylinder axis and noting the extreme values of this projection. </paragraph>
<paragraph id="P-0241" lvl="0"><number>&lsqb;0241&rsqb;</number> Two novel methods for estimating cylinder parameters are implemented in one preferred embodiment. The first way to find initial parameter estimates for a cylinder is to find approximate surface normals, as described in the auto-segmentation section. If all of the normals are set to unit length, then they can all be consider to be vectors from the origin to a point on the surface of the unit sphere. If one uses each normal vector and its to accumulate a group of points on the unit sphere, then one can fit a plane through the resulting group of points. The resulting plane normal is roughly parallel to the cylinder axis. Given the cylinder axis and the plane from the previous step, one can project the scan points onto the plane. The projected points will be well described by a circle in this plane, since the plane is normal to the cylinder axis. A best fit circle can be calculated using the projected points on the plane to give an estimate of the cylinder radius. The center of the circle on the plane can be converted to a 3-D point to give a point on the cylinder axis. </paragraph>
<paragraph id="P-0242" lvl="0"><number>&lsqb;0242&rsqb;</number> The second way to estimate the cylinder parameters is to fit the set of point to a quadric surface, which is described by the implicit equation: </paragraph>
<paragraph lvl="0"><in-line-formula><highlight><italic>F</italic></highlight>(<highlight><italic>p</italic></highlight>)&equals;0<highlight><italic>&equals;c</italic></highlight><highlight><subscript>1</subscript></highlight><highlight><italic>p</italic></highlight><highlight><subscript>1</subscript></highlight><highlight><superscript>2</superscript></highlight><highlight><italic>&plus;c</italic></highlight><highlight><subscript>2</subscript></highlight><highlight><italic>p</italic></highlight><highlight><subscript>2</subscript></highlight><highlight><superscript>2</superscript></highlight><highlight><italic>&plus;c</italic></highlight><highlight><subscript>3</subscript></highlight><highlight><italic>p</italic></highlight><highlight><subscript>3</subscript></highlight><highlight><superscript>2</superscript></highlight><highlight><italic>&plus;c</italic></highlight><highlight><subscript>4</subscript></highlight><highlight><italic>p</italic></highlight><highlight><subscript>1</subscript></highlight><highlight><italic>p</italic></highlight><highlight><subscript>2</subscript></highlight><highlight><italic>&plus;c</italic></highlight><highlight><subscript>5</subscript></highlight><highlight><italic>p</italic></highlight><highlight><subscript>1</subscript></highlight><highlight><italic>p</italic></highlight><highlight><subscript>3</subscript></highlight><highlight><italic>&plus;c</italic></highlight><highlight><subscript>6</subscript></highlight><highlight><italic>p</italic></highlight><highlight><subscript>2</subscript></highlight><highlight><italic>p</italic></highlight><highlight><subscript>3</subscript></highlight><highlight><italic>&plus;c</italic></highlight><highlight><subscript>7</subscript></highlight><highlight><italic>p</italic></highlight><highlight><subscript>1</subscript></highlight><highlight><italic>&plus;c</italic></highlight><highlight><subscript>8</subscript></highlight><highlight><italic>p</italic></highlight><highlight><subscript>2</subscript></highlight><highlight><italic>&plus;c</italic></highlight><highlight><subscript>9</subscript></highlight><highlight><italic>p</italic></highlight><highlight><subscript>3</subscript></highlight><highlight><italic>&plus;c</italic></highlight><highlight><subscript>10</subscript></highlight>&emsp;&emsp;(1) </in-line-formula></paragraph>
<paragraph id="P-0243" lvl="0"><number>&lsqb;0243&rsqb;</number> where p&equals;&lcub;p<highlight><subscript>1</subscript></highlight>, p<highlight><subscript>2</subscript></highlight>, p<highlight><subscript>3</subscript></highlight>&rcub; is a point on the quadric surface. </paragraph>
<paragraph id="P-0244" lvl="0"><number>&lsqb;0244&rsqb;</number> One can then take c<highlight><subscript>10</subscript></highlight>&equals;&minus;1 since the equation is implicit and perform a least squares fit with all of the data points to determine the other nine parameters. After determining the best fit quadric surface for a given set of points the next step is to find a point actually on the new surface (p<highlight><subscript>s</subscript></highlight>) that is in the vicinity of the other points. This is achieved by finding the centroid of the scan points (p<highlight><subscript>c</subscript></highlight>) and then finding the closest point on the surface of the quadric to give p<highlight><subscript>s</subscript></highlight>. The normal to the surface at point pecan be determined using: </paragraph>
<paragraph lvl="0"><in-line-formula>N<highlight><subscript>p</subscript></highlight>&equals;D<highlight><subscript>1</subscript></highlight>p<highlight><subscript>s</subscript></highlight>&plus;D<highlight><subscript>2</subscript></highlight>&emsp;&emsp;(2) </in-line-formula></paragraph>
<paragraph id="P-0245" lvl="0"><number>&lsqb;0245&rsqb;</number> where  
<math-cwu id="MATH-US-00001">
<number>1</number>
<math>
<mrow>
  <mrow>
    <msub>
      <mi>D</mi>
      <mn>1</mn>
    </msub>
    <mo>=</mo>
    <mrow>
      <mo>[</mo>
      <mtable>
        <mtr>
          <mtd>
            <mrow>
              <mn>2</mn>
              <mo>&it;</mo>
              <msub>
                <mi>c</mi>
                <mn>1</mn>
              </msub>
            </mrow>
          </mtd>
          <mtd>
            <msub>
              <mi>c</mi>
              <mn>4</mn>
            </msub>
          </mtd>
          <mtd>
            <msub>
              <mi>c</mi>
              <mn>5</mn>
            </msub>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <msub>
              <mi>c</mi>
              <mn>4</mn>
            </msub>
          </mtd>
          <mtd>
            <mrow>
              <mn>2</mn>
              <mo>&it;</mo>
              <msub>
                <mi>c</mi>
                <mn>2</mn>
              </msub>
            </mrow>
          </mtd>
          <mtd>
            <msub>
              <mi>c</mi>
              <mn>6</mn>
            </msub>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <msub>
              <mi>c</mi>
              <mn>5</mn>
            </msub>
          </mtd>
          <mtd>
            <msub>
              <mi>c</mi>
              <mn>6</mn>
            </msub>
          </mtd>
          <mtd>
            <mrow>
              <mn>2</mn>
              <mo>&it;</mo>
              <msub>
                <mi>c</mi>
                <mn>3</mn>
              </msub>
            </mrow>
          </mtd>
        </mtr>
      </mtable>
      <mo>]</mo>
    </mrow>
  </mrow>
  <mo>,</mo>
  <mrow>
    <msub>
      <mi>D</mi>
      <mn>2</mn>
    </msub>
    <mo>=</mo>
    <mrow>
      <mo>[</mo>
      <mtable>
        <mtr>
          <mtd>
            <msub>
              <mi>c</mi>
              <mn>7</mn>
            </msub>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <msub>
              <mi>c</mi>
              <mn>8</mn>
            </msub>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <msub>
              <mi>c</mi>
              <mn>9</mn>
            </msub>
          </mtd>
        </mtr>
      </mtable>
      <mo>]</mo>
    </mrow>
  </mrow>
</mrow>
</math>
<mathematica-file id="MATHEMATICA-00001" file="US20030001835A1-20030102-M00001.NB"/>
<image id="EMI-M00001" wi="216.027" he="31.9221" file="US20030001835A1-20030102-M00001.TIF" imf="TIFF" ti="MF"/>
</math-cwu>
</paragraph>
<paragraph id="P-0246" lvl="0"><number>&lsqb;0246&rsqb;</number> Two unit vectors, u<highlight><subscript>1 </subscript></highlight>and u<highlight><subscript>2</subscript></highlight>, are then found such that they are normal to both each other and N<highlight><subscript>p</subscript></highlight>. These vectors form a basis for the surface at the point under consideration, and additional vectors on the surface can be found as: </paragraph>
<paragraph lvl="0"><in-line-formula><highlight><italic>v</italic></highlight><highlight><subscript>&agr;</subscript></highlight><highlight><italic>&equals;u</italic></highlight><highlight><subscript>1 </subscript></highlight>cos&agr;&plus;<highlight><italic>u</italic></highlight><highlight><subscript>2 </subscript></highlight>sin&agr;, 0&lE;&agr;&lE;2&pgr;&emsp;&emsp;(3) </in-line-formula></paragraph>
<paragraph id="P-0247" lvl="0"><number>&lsqb;0247&rsqb;</number> The unit principle vectors v<highlight><subscript>&agr;</subscript></highlight> are then found by determining the rotation &agr; that satisfies: </paragraph>
<paragraph lvl="0"><in-line-formula><highlight><italic>v</italic></highlight><highlight><subscript>&agr;&middot;(</subscript></highlight><highlight><italic>N</italic></highlight><highlight><subscript>p</subscript></highlight><highlight><italic>&times;D</italic></highlight><highlight><subscript>1</subscript></highlight><highlight><italic>v</italic></highlight><highlight><subscript>&agr;</subscript></highlight>)&equals;0&emsp;&emsp;(4) </in-line-formula></paragraph>
<paragraph id="P-0248" lvl="0"><number>&lsqb;0248&rsqb;</number> There are two solutions to this equation that yield the orthogonal unit principal vectors v<highlight><subscript>1 </subscript></highlight>and v<highlight><subscript>2</subscript></highlight>. The surface curvatures &kgr; in these two principle directions are then given by:  
<math-cwu id="MATH-US-00002">
<number>2</number>
<math>
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mi>&kappa;</mi>
          <mo>&af;</mo>
          <mrow>
            <mo>(</mo>
            <msub>
              <mi>v</mi>
              <mi>i</mi>
            </msub>
            <mo>)</mo>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mrow>
          <mrow>
            <mo>-</mo>
            <msub>
              <mi>v</mi>
              <mi>i</mi>
            </msub>
          </mrow>
          <mo>&CenterDot;</mo>
          <mrow>
            <mo>(</mo>
            <mfrac>
              <mrow>
                <msub>
                  <mi>D</mi>
                  <mn>1</mn>
                </msub>
                <mo>&it;</mo>
                <msub>
                  <mi>v</mi>
                  <mi>i</mi>
                </msub>
              </mrow>
              <mrow>
                <mo>&LeftDoubleBracketingBar;</mo>
                <msub>
                  <mi>N</mi>
                  <mi>p</mi>
                </msub>
                <mo>&RightDoubleBracketingBar;</mo>
              </mrow>
            </mfrac>
            <mo>)</mo>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>5</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
<mathematica-file id="MATHEMATICA-00002" file="US20030001835A1-20030102-M00002.NB"/>
<image id="EMI-M00002" wi="216.027" he="19.93005" file="US20030001835A1-20030102-M00002.TIF" imf="TIFF" ti="MF"/>
</math-cwu>
</paragraph>
<paragraph id="P-0249" lvl="0"><number>&lsqb;0249&rsqb;</number> For cylindrical surfaces one of the principal curvatures will be near zero, and the radius of the cylinder is the reciprocal of the absolute value of the nonzero curvature. A method to determine the radius (r) and axis of the cylinder has been described, and only the location of the axis needs to be determined. A unit surface normal can be calculated as {circumflex over (n)}&equals;N<highlight><subscript>p</subscript></highlight>/&par;N<highlight><subscript>p</subscript></highlight>&par;. The sense of the normal can be adjusted so that it points towards the interior of the cylinder by ensuring that {circumflex over (n)}&middot;(p<highlight><subscript>c</subscript></highlight>&minus;p<highlight><subscript>s</subscript></highlight>)&gt;0 since the centroid of the points lies on the interior of the cylinder. A point on the axis is then given by p<highlight><subscript>s</subscript></highlight>&plus;r {circumflex over (n)}. These starting parameters can then be used in a minimization process to find the best fit parameters for the cylinder. </paragraph>
<paragraph id="P-0250" lvl="0"><number>&lsqb;0250&rsqb;</number> The novel method described above for curvature estimation using the quadric surface formulation is further used in a novel way for automated object type determination. If the points being fit are well represented by a plane then both principal curvatures will be near zero. If the points being fit are from a cylindrical surface then one curvature will be near zero and the other will be nonzero. If the points are from a sphere then both curvatures will be nonzero and their magnitudes will be approximately equal. Combining the automatic detection of object type and the auto-segmentation algorithm, described later, allows the CGP <highlight><bold>40</bold></highlight> to have a novel method for automatic fitting of many objects that occur in typical scanned scenes. </paragraph>
<paragraph id="P-0251" lvl="0"><number>&lsqb;0251&rsqb;</number> A further use of the curvature estimation is sphere fitting, which is achieved by using the quadric surface approach to approximate the radius and location of the center point, and then using a four parameter (center point and radius) minimization to reduce the error between the sphere model and the measured points. The novel method described above for finding a point on the axis of a cylinder is also used in the preferred embodiment to find the center of a sphere. </paragraph>
<paragraph id="P-0252" lvl="0"><number>&lsqb;0252&rsqb;</number> The segmentation techniques disclosed above can be used to create a variety of useful fitting tool based on combinations of the previously described shapes. For instance, a corner, consisting of an intersection of three planes which may or may not be orthogonal, is a very common feature to scan. Knowing that the specified point group contains three intersecting planes, such as the points <highlight><bold>2010</bold></highlight> in <cross-reference target="DRAWINGS">FIG. 20</cross-reference>, the points are automatically segmented, using the technique described later, into three subgroups of points that each lie on separate planes. Then, any available plane-fitting algorithm can be used to fit the planes to the scan points in each group. Unlike the more general auto-segmentation algorithm described later, if one knows that the corner object consists of three planes, the fitting algorithm need not try to fit cylinders, spheres or other objects and check which produces the best fit, only planar fits are required. Referring to <cross-reference target="DRAWINGS">FIG. 30</cross-reference>, the corner fitting tool not only fits the planes <highlight><bold>3010</bold></highlight>, <highlight><bold>3020</bold></highlight>, <highlight><bold>3030</bold></highlight> but can also intersect them to complete the corner. As shown in <cross-reference target="DRAWINGS">FIG. 30</cross-reference> additional useful information can be given to the user by introducing lines <highlight><bold>3040</bold></highlight>, <highlight><bold>3050</bold></highlight>, <highlight><bold>3060</bold></highlight> at the intersection of pairs of planes, and a vertex point <highlight><bold>3070</bold></highlight> that represents the location of the corner. This vertex point is much more accurate than a single scan point, because each plane is fit using many data points, and the vertex is generated by intersecting the planes. The above novel method for automatically creating a corner with its intersection lines and vertex can be used as a tool of the CGP <highlight><bold>40</bold></highlight>. </paragraph>
<paragraph id="P-0253" lvl="0"><number>&lsqb;0253&rsqb;</number> Each object stores information about the quality of the fit, so that the user can query the object and examine the mean, standard deviation, and worst 10 errors. Knowing the accuracy of the FDV <highlight><bold>10</bold></highlight>, the CGP or the user can then decide if an error has been made during the fit Errors can arise when the wrong type of primitive is fit, or when extraneous points that were not actually scanned from the desired surface remain in the data set. In addition, the objects store their geometric parameters so the users can query for radius, length or other values of interest. </paragraph>
<paragraph id="P-0254" lvl="0"><number>&lsqb;0254&rsqb;</number> In addition to the class of general object fitters, which are given close to no initial information other than the points to fit, there is a class of fitters that can take advantage of foreknowledge about objects in the scene. An area in which such foreknowledge exists is that of the construction industry, where parts used follow standards of dimensions and design. For example, the external diameter of pipes from a particular manufacturer may come in five different sizes: 4&Prime;, 5&Prime;, 6.5&Prime;, 8&Prime;, and 10&Prime;. This information typically resides in tables that describe relevant attributes of these parts. The cylinder fitter can take advantage of the information in these tables to significantly reduce the solution space to be searched: the fitter need only search for solutions involving cylinders of one of those diameters. Another way to use such table lookups is to have the fitter come up with a general solution, then match against entries in the object tables to find the entry with the closest parameters. For example, a pipe fit by a cylinder with 7.8&Prime; diameter would be matched against the 8&Prime; entry in the table from the example above; the user (or fitter) then has the option of refitting an 8&Prime; cylinder to the pipe, or accepting the 7.8&Prime; cylinder. Yet another use is for the user to manually select a specific entry (or collection of entries) from the table and tell the fitter to use its parameters in the fit, which also reduces the fitter&apos;s solution space (which can decrease the time taken). </paragraph>
<paragraph id="P-0255" lvl="0"><number>&lsqb;0255&rsqb;</number> Modeling </paragraph>
<paragraph id="P-0256" lvl="0"><number>&lsqb;0256&rsqb;</number> The fitting of geometric primitives, as described in the previous section does not usually complete the modeling process. It is often the case that only a portion of the objects surface is scanned, such as one end of a cylinder or a portion of a wall, and further operations are required to complete the 3-D model. Modeling is the process of completing the construction of the 3-D model given some fitted geometric primitives. </paragraph>
<paragraph id="P-0257" lvl="0"><number>&lsqb;0257&rsqb;</number> Many common CAD operations such as extension, intersection (mutual extension) and trimming are available in the CGP <highlight><bold>40</bold></highlight>. For example, the cylinder <highlight><bold>3110</bold></highlight> in <cross-reference target="DRAWINGS">FIG. 31</cross-reference> does not initially extend to the plane of the floor <highlight><bold>3120</bold></highlight>. In <cross-reference target="DRAWINGS">FIG. 32</cross-reference> the cylinder <highlight><bold>3220</bold></highlight> has been extended to the floor plane <highlight><bold>3120</bold></highlight>. These simple operations enable rapid completion of portions of the model from the objects created by geometry fitting. For instance, given three planar patches that were fit from scan data near a corner, it is easy to mutually extend the three planes to complete the corner feature. </paragraph>
<paragraph id="P-0258" lvl="0"><number>&lsqb;0258&rsqb;</number> Object extensions can be accomplished in several ways. One way is to select the geometric object to be extended and tell it to extend to a subsequently selected object. The nature of the extension is determined by both the type of object to be extended and the second object selected. For example, a cylinder extends the end closer to the second object along its centerline until its end intersects with the infinite plane defined by the second object&apos;s geometry (in the case of a planar patch, the infinite plane is that of the patch&apos;s plane, and for a cylinder, the infinite plane is that containing the centerline and as orthogonal to the extending cylinder as possible). </paragraph>
<paragraph id="P-0259" lvl="0"><number>&lsqb;0259&rsqb;</number> Another way is to make use of object handles, which are nodes that the user can grab. These handles are tied to an object&apos;s definition (position, orientation, and size) where appropriate, and by moving a handle, the object&apos;s definition changes accordingly. Again, taking the cylinder as an example, the same extension described above can be accomplished by grabbing the handle on the end to be extended, and then moving the handle (and extending the cylinder) to the desired position. A handle&apos;s motion depends on the part of the object with which it is tied; a handle on the centerline of the cylinder is constrained to move only along that centerline, while a handle on the boundary of a planar patch is constrained to move only inside the plane of the patch For some objects, handles may be inserted and removed, changing the definition of the shape of the object (for example, handles on a planar patch have a one-to-one correspondence to vertices on the planar patch&apos;s boundary). Other handles can provide rotational control over an object The control of handles is interactive and dynamically updates, so the user can see the intermediate results of the redefinition. </paragraph>
<paragraph id="P-0260" lvl="0"><number>&lsqb;0260&rsqb;</number> A new operation, called merging, has been developed to allow different portions of a single object surface to be joined to form a single object in the CGP <highlight><bold>40</bold></highlight>. It is often the case that one&apos;s view of an object is obscured by other objects in front of it. For instance, the view of a back wall in a room may be divided into two pieces because of a column in the foreground. A scan of the region will result in different groups of points on the particular object. If auto-segmentation is used, as described later, rather than manual methods where a user knows that the points belong to the same object, then separate point groups would be formed. Each point group would then be fit to a separate object, resulting in multiple pieces of the same surface. The two objects in the CGP <highlight><bold>40</bold></highlight> that are known to be on the surface of the same feature, such as the two planar patches of wall obscured by a column, can be merged to form a single object. Each object stores a reference to the data points that define it, so when a merge request is received a new geometric fit is performed on all the underlying data points that were part of the constituent geometries to achieve the best overall fit This novel method of increasing the accuracy of object fitting is used in the merging operations of one preferred embodiment of the invention. The characteristics of the two primitive objects that were merged do not affect the outcome of the merge; only the underlying point positions are considered. </paragraph>
<paragraph id="P-0261" lvl="0"><number>&lsqb;0261&rsqb;</number> Using the manual or automatic methods, the user can take a cloud of points <highlight><bold>3320</bold></highlight> from the surfaces of many objects, such as the points on the pyramid <highlight><bold>3310</bold></highlight> shown in <cross-reference target="DRAWINGS">FIG. 33</cross-reference> and convert them to a set of geometric objects <highlight><bold>3420</bold></highlight>, such as the planar face of the pyramid <highlight><bold>3410</bold></highlight> in <cross-reference target="DRAWINGS">FIG. 34</cross-reference>. The modeled scene shown in <cross-reference target="DRAWINGS">FIG. 34</cross-reference> accurately represents the features of the original objects that were scanned, and allows measurements to be made between any particular locations in the modeled scene. </paragraph>
<paragraph id="P-0262" lvl="0"><number>&lsqb;0262&rsqb;</number> Scene Registration </paragraph>
<paragraph id="P-0263" lvl="0"><number>&lsqb;0263&rsqb;</number> The initial position of each scan point is described in a local coordinate system whose origin is that of the FDV <highlight><bold>10</bold></highlight>, and whose axes are fixed relative to the FDV <highlight><bold>10</bold></highlight>. Therefore, scan fields taken without moving the FDV <highlight><bold>10</bold></highlight> are inherently registered in that all the scan points use the same coordinate system However, if the FDV <highlight><bold>10</bold></highlight> is moved between scanning operations, additional effort is required to transform the data to use a shared coordinate system. This is a six parameter rigid body transformation of a data set, involving three translation and three rotation parameters, and is easy to apply to the data once the transformation is known. </paragraph>
<paragraph id="P-0264" lvl="0"><number>&lsqb;0264&rsqb;</number> A novel process is used to register the scan fields from different FDV <highlight><bold>10</bold></highlight> positions. The novel registration process requires the user to identify pairs of points, lines or planes in two different scenes that represent the same feature. It is also possible to use different features, such as the back and front face of a wall, if the user knows the offset between them. The planes and lines are converted to point pairs, as described below, and the process operates entirely on point pairs. The points used for registration may be actual scan points or may be constructed points, such as a corner at the intersection of three planes. </paragraph>
<paragraph id="P-0265" lvl="0"><number>&lsqb;0265&rsqb;</number> Given a set of point pairs, the registration process searches the set of candidate points for three pairs that are not colinear. Using the three point pairs, one can construct the transformation required to convert the coordinate system in one view to that used in the other view, which in turn can be used to transform the scan points to all share a single coordinate system. For convenience, the process will be described in terms of the first data set remaining fixed while the second data set is transformed to use the coordinate system of the first. The process works equally well if the user fixes the second data set and transforms the first. </paragraph>
<paragraph id="P-0266" lvl="0"><number>&lsqb;0266&rsqb;</number> The first step is to apply a rigid body translation to the second data set to make the first pair of points coincide in terms of their x, y and z components. The second step is to rotate the second data set about its first point until the lines formed by points one and two in both data sets are colinear. The third step is to rotate the second data set about the line established in the previous step until the planes defined by points one, two and three in both data sets are coplanar. </paragraph>
<paragraph id="P-0267" lvl="0"><number>&lsqb;0267&rsqb;</number> Once an initial estimate is made one can use all the point pairs and an error minimization method to reduce the sum of the squares of the distances between each point pair. </paragraph>
<paragraph id="P-0268" lvl="0"><number>&lsqb;0268&rsqb;</number> In order to use the point registration method described above, the CGP uses a novel method to convert lines, planes, and planes with offsets to sets of point pairs. Whenever a nonzero plane offset is present the new points introduced are shifted in the second scene to a position where they will match exactly with the corresponding points in the first scene. The replacement of planes and lines with points makes it simple to write the error function for minimization, since only point errors are involved, rather than angular and distance errors simultaneously. </paragraph>
<paragraph id="P-0269" lvl="0"><number>&lsqb;0269&rsqb;</number> In replacing planes and lines, one can only introduce points that are at location relative to the user specified objects, since the origins of the two data sets are different. For instance, introducing a new point pair in a plane at the location closest to the origin would not result in points that actually match in space, since the origin is arbitrary. However, introducing a point pair at a plane-line intersection will give matching points in the two data sets. Some pairs of objects, like parallel lines, should not be used to introduce new points so an angular tolerance, called ATOL below, is used to ignore poor object pairs. ATOL is initially set to ten degrees but other values can be used to generate fewer or more artificial point pairs as needed. The point pairs are introduced in the following order: </paragraph>
<paragraph id="P-0270" lvl="2"><number>&lsqb;0270&rsqb;</number> For all plane-line pairs where the angle between the line and plane is greater than ATOL, introduce two new point pairs. The first new point is inserted at the intersection point of the line and plane, and the second point pair is inserted along the line of action at a fixed distance away from the first point, here taken to be the minimum of the line lengths in the two views. </paragraph>
<paragraph id="P-0271" lvl="2"><number>&lsqb;0271&rsqb;</number> For all pairs of planes and points, introduce a new point pair on the plane such that the plane normal passes through the new point and specified point. </paragraph>
<paragraph id="P-0272" lvl="2"><number>&lsqb;0272&rsqb;</number> For all plane pairs whose normals are at least ATOL apart, generate a new line segment along the intersection of the planes and make the line segments length equal to the minimum extent that any plane has along the line. The new line segment has no direction, but has both length and position information. After this step the planes are no longer needed. </paragraph>
<paragraph id="P-0273" lvl="2"><number>&lsqb;0273&rsqb;</number> For all pairs of lines and points, introduce a new point on the line at the location where it is closest to the specified point. </paragraph>
<paragraph id="P-0274" lvl="2"><number>&lsqb;0274&rsqb;</number> For all pairs of lines separated by an angle greater than ATOL, introduce four new pairs of points. The new points are the ends of line segments along the original line of action, but centered on the location of closest approach of the two lines. The distance between the new line points is equal to the minimum length of the line segment lengths along that line of action from the two data sets. After this step the lines are no longer needed. </paragraph>
<paragraph id="P-0275" lvl="0"><number>&lsqb;0275&rsqb;</number> The result of the plane and line replacements as described above is a set of point pairs that retains the direction information associated with the original planes and lines. The augmented set of point pairs can then be used for the registration process that is described above. </paragraph>
<paragraph id="P-0276" lvl="0"><number>&lsqb;0276&rsqb;</number> After registration of the two scenes, primitives from the two individual views which represent the same physical object can be combined using the merging technique described previously. In particular, matching planar patches representing the same surface can be combined into one extended planar patch. Similarly, pieces of matching cylindrical surfaces can be merged to form a single cylinder. </paragraph>
<paragraph id="P-0277" lvl="0"><number>&lsqb;0277&rsqb;</number> Warping Data Sets </paragraph>
<paragraph id="P-0278" lvl="0"><number>&lsqb;0278&rsqb;</number> The registration process described above is a rigid body transformation that does not modify the relative locations of objects within either data set. After registration, most of the point, line or plane pairs that were identified will still have small errors, since the minimization process reduces the total mean square error. A novel method is presented that allows the user to force the identified pairs to exactly match by deforming the scene volumes. </paragraph>
<paragraph id="P-0279" lvl="0"><number>&lsqb;0279&rsqb;</number> As with any measured data, there is some level of error associated with each scan point location. The magnitude of the error associated with a point location will vary with the measuring technology used, but some error will always be present. Since the data under consideration here describes surface features of objects, the data errors will manifest themselves as surface irregularities. For example, a set of points acquired from an actual planar surface may not all lie on a plane, but will have some small scatter away from the real plane location. Calculating a best fit plane through the set of measured points may not give the real plane location or orientation due to the errors in the point data set. </paragraph>
<paragraph id="P-0280" lvl="0"><number>&lsqb;0280&rsqb;</number> The errors in recovered features, such as planes, cause errors in the relationships between the recovered objects as well. For instance, if data is collected from two planes that have an exactly ninety degree angle between them, the best-fit planes generated from the data points may not be exactly ninety degrees apart. Similarly, cylinders that were parallel in the real scene may result in best-fit cylinders that are not parallel after fitting from the scanned points. These inconsistencies in the recovered features, that occur due to measurement errors, will appear whether the data points are collected from a single scan position or are a union of scans from a variety of different positions. </paragraph>
<paragraph id="P-0281" lvl="0"><number>&lsqb;0281&rsqb;</number> The lack of fit problem may actually grow if several different sets of scan data are registered using a relative system. If a series of sequential scans are collected, and each scan is registered with respect to some recognizable sequence of data points in a previous scan, then the absolute errors in each scan may grow. If at the end of the sequence of scans the locations of features are exactly known, then one must adjust the scanned data points so that they fit the known locations. In surveying both the 2-D closure problem and the 3-D benchmark matching problems are similar in nature to the problems described above. In the surveying closure application, when one surveys a sequence of locations and arrives back at the starting location one typically finds that through cumulative measurement errors the starting and finishing locations are not at exactly the same location. The closure error, which is the distance between the starting in finishing locations, is distributed using well known surveying techniques throughout the other data points collected such that the first and last end points meet after the correction is made. Similarly, when surveying benchmarks of known location arc introduced into a surveying data set the data set must be adjusted to accommodate the known benchmark locations. Both the closure problem and the benchmark matching problem can be solved by the method described here since they can be described in terms of displacement constraints. </paragraph>
<paragraph id="P-0282" lvl="0"><number>&lsqb;0282&rsqb;</number> The novel method described here to correct location errors in measured 3-D data sets and distributes the errors throughout the point sets by applying solid mechanics principles to a volume surrounding the data points. The method provides a technique for satisfying a wide variety of displacement constraints on 3-D data sets and also distributes the measurement errors throughout the data sets. The process of deforming the data sets to achieve these goals is called warping. The displacement constraints can be specified in terms of both control points, whose absolute coordinates are known in space and do not move, and tie points, which represent the same location in two or more data sets, but whose absolute location is unknown. One can describe constraints involving more complex objects, for instance, line segments by specifying two points, and reality, the constraints themselves can be used to anchor multiple volumes together. Mechanics principles allow one to determine the minimum energy deformation of the volume that satisfies the stated constraints, which mimics what would actually happen to a real deformable object subjected to the same constraints. </paragraph>
<paragraph id="P-0283" lvl="0"><number>&lsqb;0283&rsqb;</number> In a particular embodiment, the warping method uses principles of solid mechanics to deform a volume containing points of interest in order to satisfy a set of constraints applied to the data points. Not only are the constraints satisfied, but the effects of the initial location errors are spread throughout the volumes operated on. </paragraph>
<paragraph id="P-0284" lvl="0"><number>&lsqb;0284&rsqb;</number> The finite element method is used to apply the principles of solid mechanics to the volumes enclosing the points. The volume is discretized into a set of points or vertices and a set of elements that connect to the vertices. Four node tetrahedral elements are used to discretize the volume. </paragraph>
<paragraph id="P-0285" lvl="0"><number>&lsqb;0285&rsqb;</number> The first step of the process is to collect the set of constraints that apply to one or more data sets. During this phase, one must identify constraints that are to be satisfied by the warping process. These constraints include the identification of points that represent the same physical location in different data sets (tie points), like the corner of a cube, which are to appear at the same location when the warping process is completed. Some of the tie points may not be points that were scanned in the original data set, but may be constructed from groups of other points. For instance, if one had a series of points that represented three planes intersecting at a corner, then one could fit three planes to the points, and use the resulting corner point as a tie point. The constraints are specified in terms of pairs of objects, such as points, lines and planes, as well as the desired offset and angle between them. The two objects involved in the constraint can be contained in a single data set or can occur in different data sets. Within a single data set, one could specify that lines or planes remain parallel, or that the distance between two points be a specified amount Between multiple data sets, one could write similar constraints, or indicate that the features seen in two data sets represent the same object. One might also know the actual location of some points very accurately (benchmarks) and constrain points in the data set to lie at the known locations. Using these benchmark points to anchor different points in the data sets enables the closure problem to be solved, since the data sets will be warped so that the measured data points move exactly to the desired control point locations and the errors in the data set will be smoothed over all volumes. </paragraph>
<paragraph id="P-0286" lvl="0"><number>&lsqb;0286&rsqb;</number> The second step in the warping process is to register all the data sets involved, as described in the previous section. </paragraph>
<paragraph id="P-0287" lvl="0"><number>&lsqb;0287&rsqb;</number> The third step in the warping process is to select a volume that surrounds the region of interest, and describe the volume in terms of a set of new points. The region that can be scanned by the FDV <highlight><bold>10</bold></highlight> is called the view volume and is shaped like a pyramid, with the tip of the pyramid located at the origin of the scanning device. A pyramidal shape can be used to bound the view region for the purpose of warping, and the pyramid is easily described by five points, using the same coordinate system as the data points. These new points do not become part of the data set, but are used in the warping process. The convex hull of these points represents the new volume surface, and should enclose all the data points on the interior. This operation is performed separately for each data set. </paragraph>
<paragraph id="P-0288" lvl="0"><number>&lsqb;0288&rsqb;</number> The fourth step is to mesh each of the data volumes. Meshing involves filling the volume with finite elements that leave no gaps and that do not overlap. The finite elements span between the points or vertices that have been defined on the volume boundary and those that are involved in constraints on the interior. The points in the data set do not all need to be included in the warping process, only those that are used in constraint specifications and those that define the volume boundary need to be used. The elements in the initial mesh may be of poor quality due to their shape. Long sliver elements, for instance, are known to give poor results in finite element analysis. Therefore, the meshing process is actually iterative. New points are inserted into the mesh, and then old elements are removed and new elements are introduced so that the mesh quality improves. This iterative process continues until one is satisfied with the overall quality of the mesh. In one preferred embodiment, four node tetrahedral elements are used. The initial mesh is constructed by applying a 3-D Delaunay triangulation on the starting set of points. The iterative process identifies poorly shaped elements using an element quality measure, and introduces new points and remeshes the region. The process terminates when all elements meet a minimum quality criteria. The preferred implementation uses longest edge bisection to introduce new points that improve the mesh, but other methods can be used. </paragraph>
<paragraph id="P-0289" lvl="0"><number>&lsqb;0289&rsqb;</number> The fifth step processes the constraints described in step one into a system of linear constraints. In the preferred embodiment, the final system of constraints is linear in terms of the nodal displacements at the vertices of the tetrahedral elements. The desired form of the constraints is: </paragraph>
<paragraph lvl="0"><in-line-formula><highlight><italic>Cu&equals;q</italic></highlight>&emsp;&emsp;(6) </in-line-formula></paragraph>
<paragraph id="P-0290" lvl="0"><number>&lsqb;0290&rsqb;</number> The matrix C contains constant coefficients. The number of rows of C is equal to the number of constraints in the system. The vector u represents the 3-D displacements of the vertices of the tetrahedral elements. The vector q contains constant coefficients. If the constraints are homogenous then each element of q will be 0. The form of constraint specification given in Equation (6) allows arbitrary linear multipoint (involving more than one vertex) constraints. </paragraph>
<paragraph id="P-0291" lvl="0"><number>&lsqb;0291&rsqb;</number> The conversion of the constraints specified in step one into the form shown above depends on the type of constraints involved. For two points to be tied together the constraint would be: </paragraph>
<paragraph lvl="0"><in-line-formula><highlight><italic>p</italic></highlight><highlight><subscript>1</subscript></highlight><highlight><italic>&plus;u</italic></highlight><highlight><subscript>1</subscript></highlight><highlight><italic>&equals;P</italic></highlight><highlight><subscript>2</subscript></highlight><highlight><italic>&plus;u</italic></highlight><highlight><subscript>2</subscript></highlight>&emsp;&emsp;(7) </in-line-formula></paragraph>
<paragraph lvl="0"><in-line-formula>or <highlight><italic>u</italic></highlight><highlight><subscript>1</subscript></highlight><highlight><italic>&minus;u</italic></highlight><highlight><subscript>2</subscript></highlight><highlight><italic>&equals;p</italic></highlight><highlight><subscript>2</subscript></highlight><highlight><italic>&minus;p</italic></highlight><highlight><subscript>1</subscript></highlight>&emsp;&emsp;(8) </in-line-formula></paragraph>
<paragraph id="P-0292" lvl="0"><number>&lsqb;0292&rsqb;</number> In these equations, p<highlight><subscript>1 </subscript></highlight>and P<highlight><subscript>2 </subscript></highlight>are vectors from the origin to the vertices of interest, while u<highlight><subscript>1 </subscript></highlight>and u<highlight><subscript>2 </subscript></highlight>are the displacements of the same vertices during warping. Equation (7) demands that the final location of each vertex, equal to the starting point plus the displacement during warping, be equal to the final location of the other point Equation (8) is in the form of Equation (6), with q&equals;p<highlight><subscript>2</subscript></highlight>&minus;p<highlight><subscript>1 </subscript></highlight>and results in three linear constraints, in terms of the x, y, and z components of the nodal displacements. Expanding Equation (8) into three equations in the form of Equation (6) then gives:  
<math-cwu id="MATH-US-00003">
<number>3</number>
<math>
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mrow>
            <mo>[</mo>
            <mtable>
              <mtr>
                <mtd>
                  <mn>1</mn>
                </mtd>
                <mtd>
                  <mn>0</mn>
                </mtd>
                <mtd>
                  <mn>0</mn>
                </mtd>
                <mtd>
                  <mrow>
                    <mo>-</mo>
                    <mn>1</mn>
                  </mrow>
                </mtd>
                <mtd>
                  <mn>0</mn>
                </mtd>
                <mtd>
                  <mn>0</mn>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <mn>0</mn>
                </mtd>
                <mtd>
                  <mn>1</mn>
                </mtd>
                <mtd>
                  <mn>0</mn>
                </mtd>
                <mtd>
                  <mn>0</mn>
                </mtd>
                <mtd>
                  <mrow>
                    <mo>-</mo>
                    <mn>1</mn>
                  </mrow>
                </mtd>
                <mtd>
                  <mn>0</mn>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <mn>0</mn>
                </mtd>
                <mtd>
                  <mn>0</mn>
                </mtd>
                <mtd>
                  <mn>1</mn>
                </mtd>
                <mtd>
                  <mn>0</mn>
                </mtd>
                <mtd>
                  <mn>0</mn>
                </mtd>
                <mtd>
                  <mrow>
                    <mo>-</mo>
                    <mn>1</mn>
                  </mrow>
                </mtd>
              </mtr>
            </mtable>
            <mo>]</mo>
          </mrow>
          <mo>&it;</mo>
          <mrow>
            <mo>{</mo>
            <mtable>
              <mtr>
                <mtd>
                  <msub>
                    <mi>u</mi>
                    <mrow>
                      <mn>1</mn>
                      <mo>&it;</mo>
                      <mi>x</mi>
                    </mrow>
                  </msub>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <msub>
                    <mi>u</mi>
                    <mrow>
                      <mn>1</mn>
                      <mo>&it;</mo>
                      <mi>y</mi>
                    </mrow>
                  </msub>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <msub>
                    <mi>u</mi>
                    <mrow>
                      <mn>1</mn>
                      <mo>&it;</mo>
                      <mi>z</mi>
                    </mrow>
                  </msub>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <msub>
                    <mi>u</mi>
                    <mrow>
                      <mn>2</mn>
                      <mo>&it;</mo>
                      <mi>x</mi>
                    </mrow>
                  </msub>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <msub>
                    <mi>u</mi>
                    <mrow>
                      <mn>2</mn>
                      <mo>&it;</mo>
                      <mi>y</mi>
                    </mrow>
                  </msub>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <msub>
                    <mi>u</mi>
                    <mrow>
                      <mn>2</mn>
                      <mo>&it;</mo>
                      <mi>z</mi>
                    </mrow>
                  </msub>
                </mtd>
              </mtr>
            </mtable>
            <mo>}</mo>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mrow>
          <mo>{</mo>
          <mtable>
            <mtr>
              <mtd>
                <mn>0</mn>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <mn>0</mn>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <mn>0</mn>
              </mtd>
            </mtr>
          </mtable>
          <mo>}</mo>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>9</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
<mathematica-file id="MATHEMATICA-00003" file="US20030001835A1-20030102-M00003.NB"/>
<image id="EMI-M00003" wi="216.027" he="66.9627" file="US20030001835A1-20030102-M00003.TIF" imf="TIFF" ti="MF"/>
</math-cwu>
</paragraph>
<paragraph id="P-0293" lvl="0"><number>&lsqb;0293&rsqb;</number> Other constraints, like the distance between two points, are non-linear in nature. The nonlinear constraints can use the existing geometry of the system as well as small deformation assumptions to produce linear multipoint constraints. For example, to specify the desired distance between two points to be some specified value x, one could determine the vector v<highlight><subscript>21 </subscript></highlight>between the final points locations: </paragraph>
<paragraph lvl="0"><in-line-formula><highlight><italic>v</italic></highlight><highlight><subscript>21</subscript></highlight>&equals;(<highlight><italic>p</italic></highlight><highlight><subscript>2</subscript></highlight><highlight><italic>&plus;u</italic></highlight><highlight><subscript>2</subscript></highlight>)&minus;(<highlight><italic>p</italic></highlight><highlight><subscript>1</subscript></highlight><highlight><italic>&plus;u</italic></highlight><highlight><subscript>1</subscript></highlight>)&emsp;&emsp;(10) </in-line-formula></paragraph>
<paragraph id="P-0294" lvl="0"><number>&lsqb;0294&rsqb;</number> and then specify the desired length of the vector: </paragraph>
<paragraph lvl="0"><in-line-formula>&par;<highlight><italic>v</italic></highlight><highlight><subscript>21</subscript></highlight><highlight><italic>&par;&equals;x</italic></highlight>&emsp;&emsp;(11) </in-line-formula></paragraph>
<paragraph id="P-0295" lvl="0"><number>&lsqb;0295&rsqb;</number> or, using the vector dot product: </paragraph>
<paragraph lvl="0"><in-line-formula><highlight><italic>v</italic></highlight><highlight><subscript>21</subscript></highlight><highlight><italic>&middot;v</italic></highlight><highlight><subscript>21</subscript></highlight><highlight><italic>&equals;x</italic></highlight><highlight><superscript>2</superscript></highlight>&emsp;&emsp;(12) </in-line-formula></paragraph>
<paragraph id="P-0296" lvl="0"><number>&lsqb;0296&rsqb;</number> Both Equations (11) and (12) are nonlinear in terms of the displacements of the nodes, u<highlight><subscript>1 </subscript></highlight>and u<highlight><subscript>2</subscript></highlight>. To linearize the constraint we can specify the desired length along the original line of action to be equal to the desired offset:  
<math-cwu id="MATH-US-00004">
<number>4</number>
<math>
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mi>let</mi>
          <mo>&it;</mo>
          <mstyle>
            <mtext>&emsp;</mtext>
          </mstyle>
          <mo>&it;</mo>
          <msub>
            <mi>n</mi>
            <mn>21</mn>
          </msub>
        </mrow>
        <mo>=</mo>
        <mfrac>
          <mrow>
            <mo>(</mo>
            <mrow>
              <msub>
                <mi>p</mi>
                <mn>2</mn>
              </msub>
              <mo>-</mo>
              <msub>
                <mi>p</mi>
                <mn>1</mn>
              </msub>
            </mrow>
            <mo>)</mo>
          </mrow>
          <mrow>
            <mo>&LeftDoubleBracketingBar;</mo>
            <mrow>
              <msub>
                <mi>p</mi>
                <mn>2</mn>
              </msub>
              <mo>-</mo>
              <msub>
                <mi>p</mi>
                <mn>1</mn>
              </msub>
            </mrow>
            <mo>&RightDoubleBracketingBar;</mo>
          </mrow>
        </mfrac>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>13</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
<mathematica-file id="MATHEMATICA-00004" file="US20030001835A1-20030102-M00004.NB"/>
<image id="EMI-M00004" wi="216.027" he="18.96615" file="US20030001835A1-20030102-M00004.TIF" imf="TIFF" ti="MF"/>
</math-cwu>
</paragraph>
<paragraph lvl="0"><in-line-formula><highlight><italic>v</italic></highlight><highlight><subscript>21</subscript></highlight><highlight><italic>&middot;n</italic></highlight><highlight><subscript>21</subscript></highlight><highlight><italic>&equals;x</italic></highlight>&emsp;&emsp;(14) </in-line-formula></paragraph>
<paragraph lvl="0"><in-line-formula>or &lsqb;(<highlight><italic>p</italic></highlight><highlight><subscript>2</subscript></highlight><highlight><italic>&plus;u</italic></highlight><highlight><subscript>2</subscript></highlight>)&minus;(<highlight><italic>p</italic></highlight><highlight><subscript>1</subscript></highlight><highlight><italic>&plus;u</italic></highlight><highlight><subscript>1</subscript></highlight>)&rsqb;&middot;<highlight><italic>n</italic></highlight><highlight><subscript>21</subscript></highlight><highlight><italic>&equals;x</italic></highlight>&emsp;&emsp;(15) </in-line-formula></paragraph>
<paragraph lvl="0"><in-line-formula><highlight><italic>u</italic></highlight><highlight><subscript>2</subscript></highlight><highlight><italic>&middot;n</italic></highlight><highlight><subscript>21</subscript></highlight><highlight><italic>&minus;u</italic></highlight><highlight><subscript>1</subscript></highlight><highlight><italic>&middot;n</italic></highlight><highlight><subscript>21</subscript></highlight><highlight><italic>&equals;x&minus;&par;p</italic></highlight><highlight><subscript>2</subscript></highlight><highlight><italic>&minus;p</italic></highlight><highlight><subscript>1</subscript></highlight>&par;&emsp;&emsp;(16) </in-line-formula></paragraph>
<paragraph id="P-0297" lvl="0"><number>&lsqb;0297&rsqb;</number> The term on the right hand side of equation (16) is the desired distance between the points minus the current distance between the points. The x, y and z components of n<highlight><subscript>21 </subscript></highlight>are constraints. Equation (16) can be written as a single constraint in the proper form as:  
<math-cwu id="MATH-US-00005">
<number>5</number>
<math>
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mrow>
            <mo>&LeftAngleBracket;</mo>
            <mtable>
              <mtr>
                <mtd>
                  <mrow>
                    <mo>-</mo>
                    <msub>
                      <mi>n</mi>
                      <mrow>
                        <mn>21</mn>
                        <mo>&it;</mo>
                        <mi>x</mi>
                      </mrow>
                    </msub>
                  </mrow>
                </mtd>
                <mtd>
                  <mrow>
                    <mo>-</mo>
                    <msub>
                      <mi>n</mi>
                      <mrow>
                        <mn>21</mn>
                        <mo>&it;</mo>
                        <mi>y</mi>
                      </mrow>
                    </msub>
                  </mrow>
                </mtd>
                <mtd>
                  <mrow>
                    <mo>-</mo>
                    <msub>
                      <mi>n</mi>
                      <mrow>
                        <mn>21</mn>
                        <mo>&it;</mo>
                        <mi>z</mi>
                      </mrow>
                    </msub>
                  </mrow>
                </mtd>
                <mtd>
                  <msub>
                    <mi>n</mi>
                    <mrow>
                      <mn>21</mn>
                      <mo>&it;</mo>
                      <mi>x</mi>
                    </mrow>
                  </msub>
                </mtd>
                <mtd>
                  <msub>
                    <mi>n</mi>
                    <mrow>
                      <mn>21</mn>
                      <mo>&it;</mo>
                      <mi>y</mi>
                    </mrow>
                  </msub>
                </mtd>
                <mtd>
                  <msub>
                    <mi>n</mi>
                    <mrow>
                      <mn>21</mn>
                      <mo>&it;</mo>
                      <mi>z</mi>
                    </mrow>
                  </msub>
                </mtd>
              </mtr>
            </mtable>
            <mo>&RightAngleBracket;</mo>
          </mrow>
          <mo>&it;</mo>
          <mrow>
            <mo>{</mo>
            <mtable>
              <mtr>
                <mtd>
                  <msub>
                    <mi>u</mi>
                    <mrow>
                      <mn>1</mn>
                      <mo>&it;</mo>
                      <mi>x</mi>
                    </mrow>
                  </msub>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <msub>
                    <mi>u</mi>
                    <mrow>
                      <mn>1</mn>
                      <mo>&it;</mo>
                      <mi>y</mi>
                    </mrow>
                  </msub>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <msub>
                    <mi>u</mi>
                    <mrow>
                      <mn>1</mn>
                      <mo>&it;</mo>
                      <mi>z</mi>
                    </mrow>
                  </msub>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <msub>
                    <mi>u</mi>
                    <mrow>
                      <mn>2</mn>
                      <mo>&it;</mo>
                      <mi>x</mi>
                    </mrow>
                  </msub>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <msub>
                    <mi>u</mi>
                    <mrow>
                      <mn>2</mn>
                      <mo>&it;</mo>
                      <mi>y</mi>
                    </mrow>
                  </msub>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <msub>
                    <mi>u</mi>
                    <mrow>
                      <mn>2</mn>
                      <mo>&it;</mo>
                      <mi>z</mi>
                    </mrow>
                  </msub>
                </mtd>
              </mtr>
            </mtable>
            <mo>}</mo>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mrow>
          <mi>x</mi>
          <mo>-</mo>
          <mrow>
            <mo>&LeftDoubleBracketingBar;</mo>
            <mrow>
              <msub>
                <mi>p</mi>
                <mn>2</mn>
              </msub>
              <mo>-</mo>
              <msub>
                <mi>p</mi>
                <mn>1</mn>
              </msub>
            </mrow>
            <mo>&RightDoubleBracketingBar;</mo>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>17</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
<mathematica-file id="MATHEMATICA-00005" file="US20030001835A1-20030102-M00005.NB"/>
<image id="EMI-M00005" wi="216.027" he="66.9627" file="US20030001835A1-20030102-M00005.TIF" imf="TIFF" ti="MF"/>
</math-cwu>
</paragraph>
<paragraph id="P-0298" lvl="0"><number>&lsqb;0298&rsqb;</number> In Step <highlight><bold>6</bold></highlight> the final system of linear equations is assembled. There are two parts to this step: first, assembling the element stiffnesses for each of the tetrahedral elements, and second, selecting and applying a constraint handling technique. The calculation and assembly of element stiffnesses follow standard finite element procedures. Using constraints in the form of Equation (6) involves a constraint processing method. The Lagrange Multipliers technique can be used to introduce the effect of the linear constraints, but any other method, such as penalty or transformation techniques, could be used equally effectively. </paragraph>
<paragraph id="P-0299" lvl="0"><number>&lsqb;0299&rsqb;</number> Using Lagrange Multipliers, one introduces a new variable into the final system of equations for each constraint in the system. One then modifies the static equilibrium equations for the unconstrained system, which are given by: </paragraph>
<paragraph lvl="0"><in-line-formula><highlight><italic>Ku&equals;r</italic></highlight>&emsp;&emsp;(18) </in-line-formula></paragraph>
<paragraph id="P-0300" lvl="0"><number>&lsqb;0300&rsqb;</number> In Equation (18), K is the system stiffness matrix, assembled from the individual element stiffness contributions, u is the displacement vector that is the solution of the problem, and r is a vector of externally applied loads. In this embodiment of the invention, there are no externally applied loads, so the r vector contains only zeroes. Equation (18) does not include the effect of any constraints, but these can be included using the Lagrange Multipliers technique to give the system of equations:  
<math-cwu id="MATH-US-00006">
<number>6</number>
<math>
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mrow>
            <mo>[</mo>
            <mtable>
              <mtr>
                <mtd>
                  <mi>K</mi>
                </mtd>
                <mtd>
                  <msup>
                    <mi>C</mi>
                    <mi>T</mi>
                  </msup>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <mi>C</mi>
                </mtd>
                <mtd>
                  <mn>0</mn>
                </mtd>
              </mtr>
            </mtable>
            <mo>]</mo>
          </mrow>
          <mo>&it;</mo>
          <mrow>
            <mo>{</mo>
            <mtable>
              <mtr>
                <mtd>
                  <mi>u</mi>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <msub>
                    <mi>u</mi>
                    <mi>L</mi>
                  </msub>
                </mtd>
              </mtr>
            </mtable>
            <mo>}</mo>
          </mrow>
        </mrow>
        <mo>=</mo>
        <mrow>
          <mo>{</mo>
          <mtable>
            <mtr>
              <mtd>
                <mi>r</mi>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <mi>q</mi>
              </mtd>
            </mtr>
          </mtable>
          <mo>}</mo>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>19</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
<mathematica-file id="MATHEMATICA-00006" file="US20030001835A1-20030102-M00006.NB"/>
<image id="EMI-M00006" wi="216.027" he="23.04855" file="US20030001835A1-20030102-M00006.TIF" imf="TIFF" ti="MF"/>
</math-cwu>
</paragraph>
<paragraph id="P-0301" lvl="0"><number>&lsqb;0301&rsqb;</number> In Equation (19) K, C, u, r, and q are as previously defined, and UL is a vector containing the additional Lagrange Multiplier variables that are introduced using this method. The matrix C<highlight><superscript>T </superscript></highlight>is the transpose of C, and 0 is a matrix of zeroes. The solution of Equation (19) gives the displacements u that satisfy the linear constraints described by C and q. Note that these constraints may be linearizations of nonlinear constraints, and the nonlinear constraints might not be satisfied by the solution at this point. </paragraph>
<paragraph id="P-0302" lvl="0"><number>&lsqb;0302&rsqb;</number> If penalty or transformation methods were used instead of Lagrange Multipliers, a system of linear equations different from those shown in Equation (19) would be produced, but the solution of the linear system of equations will give similar values for the displacement vector u. </paragraph>
<paragraph id="P-0303" lvl="0"><number>&lsqb;0303&rsqb;</number> In Step <highlight><bold>7</bold></highlight> Equation (19) is solved to give u and UL. There are many methods available to solve large systems of linear equations, and the preferred embodiment uses a symmetric solver with a profile storage scheme. The different types of solvers that could be used give essentially the same results, but optimize speed and memory usage differently. </paragraph>
<paragraph id="P-0304" lvl="0"><number>&lsqb;0304&rsqb;</number> The preferred embodiment uses a direct solver, but iterative sparse solvers could be used as well. The system of equations shown in Equation (19) is sparse, so significant speed enhancements can be achieved by selecting the proper solver. However, the results of the warping process overall are unaffected by this choice. </paragraph>
<paragraph id="P-0305" lvl="0"><number>&lsqb;0305&rsqb;</number> In Step <highlight><bold>8</bold></highlight>, one must check if the current displacement satisfies the constraints to a desired level of accuracy. If the current deformed shape violates the offset or angle in any of the constraints collected in Step <highlight><bold>1</bold></highlight> by more than a user specified tolerance, then steps <highlight><bold>5</bold></highlight> through <highlight><bold>7</bold></highlight> must be repeated, starting with the new deformed shape. The linearizations of the shape may change on each iteration since the geometry of the volume changes with the cumulative deformations. When all constraints are satisfied within the given tolerance, then one can proceed to step <highlight><bold>9</bold></highlight>. </paragraph>
<paragraph id="P-0306" lvl="0"><number>&lsqb;0306&rsqb;</number> Step <highlight><bold>9</bold></highlight> uses the nodal deformations u calculated in Step <highlight><bold>7</bold></highlight> to determine the deformation of any point of interest within the volumes. For each point of interest, one must find an finite element that includes the point on its surface or interior. If the point is internal to an element then only one such element exists. If the point is on the surface of an element or along the edge of an element, then several elements could be considered to contain the point. Any of these elements can be selected to determine where the point of interest moves. If the point is shared between elements, then the use of any of the elements to find the point displacement will give the same results. Once an element is identified, the vertex displacements of that element are extracted from u and are used to determine the displacement of any point on the interior using an interpolation process. This procedure uses the finite element shape functions which are linear in the preferred embodiment, and is a common operation in finite element analysis. </paragraph>
<paragraph id="P-0307" lvl="0"><number>&lsqb;0307&rsqb;</number> Auto-Segmentation </paragraph>
<paragraph id="P-0308" lvl="0"><number>&lsqb;0308&rsqb;</number> The novel auto-segmentation process, as presented below, involves a similar sequence of operations to the manual modeling process described previously. A point cloud is segmented, geometric primitive objects are fit to the point groups, and then modeling operations, such as extension and intersection are used to complete the model. In this novel process, automation is applied to each of these steps, as well as the entire process, as described below. </paragraph>
<paragraph id="P-0309" lvl="0"><number>&lsqb;0309&rsqb;</number> It is possible to automatically partition the scan points into groups representing primitive geometrical shapes by using variations of common machine vision techniques. A gridded scan field is stored in a two dimensional array of points, much like a regular bitmap. The scan field differs from a bitmap in that more information is stored at each location than just a color. Each point stores its location in space, from which the distance to the scanner can be calculated, as well as the intensity of the return laser pulse. The depth information calculated from the three dimensional position stored at the points is crucial to the automated segmentation algorithm described here, even though many operations, such as filtering, rating, thresholding and thinning are commonly used image manipulation operations. </paragraph>
<paragraph id="P-0310" lvl="0"><number>&lsqb;0310&rsqb;</number> The first stage of the auto-segmentation process is to estimate the surface normal at each point in the grid. This can be achieved using many different techniques, the current embodiment of the software fits a plane to the nearest neighbors of the point in the 3 by 3 grid surrounding it The normal of the resulting plane is taken as the normal at the center point. Each point in the grid has a normal calculated in the same way, except that edge and corner points ignore the missing neighbors in the normal calculation. The normal stored at each point is a three dimensional vector and is normalized to have unit length. </paragraph>
<paragraph id="P-0311" lvl="0"><number>&lsqb;0311&rsqb;</number> In the second phase two rating images are created by convolving standard edge detection filters over the grid. The first rating image is created by convolving the depth of the grid point with an edge detection filter to identify depth discontinuities, such as those that would occur at an occluded edge. A variety of edge detection filters can be used, but rather than operate on color or intensity the filter operates on the depth information stored at each grid point. </paragraph>
<paragraph id="P-0312" lvl="0"><number>&lsqb;0312&rsqb;</number> The second rating image is created by convolving the normal with an edge detection filter. The normal rating image is actually composed of 3 subimages created from a convolution with the normal&apos;s x, y, and z components. The resulting three values are combined by taking the square root of the sum of the squares to give a per-point scalar value. The second rating image is used to identify normal discontinuities, such as those that would occur at the edge between a wall and a floor. Again, a wide variety of edge detection filters can be used, but the values used are normal coefficients rather than color or intensity. </paragraph>
<paragraph id="P-0313" lvl="0"><number>&lsqb;0313&rsqb;</number> Once the two rating images have been created they must separately be converted to binary images. Conventional machine vision algorithms, such as recursive thresholding can be used to achieve this task. Each point in the depth and normal rating images contains an estimate of the gradient of the depth and normal respectively. Recursive thresholding can be used to isolate the regions of highest gradient. In the resulting binary images the points in the regions of highest gradient are marked as edge points while the rest of the points are marked as non-edge. </paragraph>
<paragraph id="P-0314" lvl="0"><number>&lsqb;0314&rsqb;</number> A final binary image is created by marking a point as an edge point if it is marked as an edge point in either or both of the two binary images created by recursive thresholding above. All other points are marked as non-edge. This image contains all edge points that delineate the boundaries between groups of points on different surfaces. </paragraph>
<paragraph id="P-0315" lvl="0"><number>&lsqb;0315&rsqb;</number> The final step of the point partitioning process is to use a connected components algorithm to collect the points into groups separated by edges. Points are considered to be connected only if they are vertically or horizontally adjacent in the grid, diagonal adjacency is not used. Very simple algorithms can be used to identify the unique groups of non-edge points in the image. Each group of connected points is then cut from the initial point set to form a new group of points. The result of this algorithm is the partitioning of the point set into multiple point groups that each represents a single surface. Each of the new point groups can be fit by a geometric primitive as described in the next section. </paragraph>
<paragraph id="P-0316" lvl="0"><number>&lsqb;0316&rsqb;</number> Once the scan cloud has been partitioned into groups of scan points that lie on different surfaces, the next step is to fit objects to the desired surfaces. A variety of methods can be used to achieve this task. The current embodiment of the software can perform the object fitting process in two different ways. The first method fits a series of objects to each group of points, and selects the objects that produces the smallest distance errors between the measured points and the fitted object surfaces. The second method uses the quadric surface fit described previously, and resulting principle curvatures, to determine if a plane, cylinder or sphere should be fit to a particular point group. Other variations of these approaches could also be used, such as progressive commitment, where objects are fitted in order from simplest to most complicated, and the process stops whenever the errors associated with the particular fit drop to acceptable levels. </paragraph>
<paragraph id="P-0317" lvl="0"><number>&lsqb;0317&rsqb;</number> The last stage of auto-segmentation process extends primitive objects, where possible, to create complete object intersections, rather than stopping at scan point boundaries. Using the gridded nature of the original data and the edge information from the point partitioning algorithm described above, it is possible to extend and intersect objects. For all edges that result from surface intersections, which are the surface normal discontinuity edges described above, one can extend the objects on either side of the edge to form an intersection. </paragraph>
<paragraph id="P-0318" lvl="0"><number>&lsqb;0318&rsqb;</number> Model Annotation </paragraph>
<paragraph id="P-0319" lvl="0"><number>&lsqb;0319&rsqb;</number> In order to compose a semantically rich 3-D model, individual parts in the above geometrical model can be annotated with additional, possibly non-geometric, information, such as material references or part numbers. Tthis information can be entered manually through a special window for displaying object attributes. </paragraph>
<paragraph id="P-0320" lvl="0"><number>&lsqb;0320&rsqb;</number> The user may click on an individual part in the geometrical model and recover such additional information through other windows. Similarly, the user may request that all parts which meet some selection criteria are to be highlighted. </paragraph>
<paragraph id="P-0321" lvl="0"><number>&lsqb;0321&rsqb;</number> A novel method is also used for automatic model annotation. This method uses the FDV <highlight><bold>10</bold></highlight> to scan bar codes containing pertinent information relating to any given object. Standard bar code reading and decoding techniques are used to convert optical information to useful digital information that is associated with a given object scanned at the same time as the bar code. The captured information can be displayed as described above for the manual method. </paragraph>
<paragraph id="P-0322" lvl="0"><number>&lsqb;0322&rsqb;</number> Geometry Display and Query </paragraph>
<paragraph id="P-0323" lvl="0"><number>&lsqb;0323&rsqb;</number> The model is accessible in a variety of ways, including access through the data window <highlight><bold>1610</bold></highlight> where the model is rendered. Many standard graphic interface techniques can be used to manipulate the view of the model; in one preferred embodiment, a crystal ball interface is used. Any object in view can be selected using the pointing device; its geometrical attributes can then be displayed, in addition to other annotations added to the object. Traversing the data set can be simplified by placing objects in different layers, and then displaying only the layers of interest. Reducing the number of rendered-objects in this way increases the interactive performance of the program. In addition to querying the geometric properties of individual objects several standard tools for measuring distances and angles between objects can be employed. Additional standard techniques can be used for operations such as contour generation, 2-D section cutting, and automated dimensioning. </paragraph>
<paragraph id="P-0324" lvl="0"><number>&lsqb;0324&rsqb;</number> The resulting model can be exported to any of a number of CAD programs for further editing or designing. In the preferred embodiment, the CGP <highlight><bold>40</bold></highlight> can create a CAD file in a format compatible with several commercial CAD programs, and can then start the CAD program, having it load the generated data file. With both the CGP and the CAD program running, the user can then view and work with the model using either program. </paragraph>
<paragraph id="P-0325" lvl="0"><number>&lsqb;0325&rsqb;</number> The following documents form an integral part of this specification: </paragraph>
<paragraph id="P-0326" lvl="2"><number>&lsqb;0326&rsqb;</number> Modular Decomposition </paragraph>
<paragraph id="P-0327" lvl="2"><number>&lsqb;0327&rsqb;</number> Summary of Proposed CGP Specification </paragraph>
<paragraph id="P-0328" lvl="2"><number>&lsqb;0328&rsqb;</number> Cyrax Software Specification </paragraph>
<paragraph id="P-0329" lvl="2"><number>&lsqb;0329&rsqb;</number> Product Overview </paragraph>
<paragraph id="P-0330" lvl="2"><number>&lsqb;0330&rsqb;</number> Collection of View Slides </paragraph>
<paragraph id="P-0331" lvl="2"><number>&lsqb;0331&rsqb;</number> Introduction and Overview </paragraph>
</section>
</detailed-description>
</subdoc-description>
<subdoc-claims>
<claim id="CLM-00001">
<claim-text><highlight><bold>1</bold></highlight>. A method of manually separating from a plurality of clouds of points, representing three-dimensional features in a scene, a subset of the points that represents a desired feature in the scene, the method comprising: 
<claim-text>selecting all the point clouds that include at least some data points representing the desired feature; and </claim-text>
<claim-text>changing a view of the clouds and drawing a polygonal lasso to refine a selected subset of points to be included in a point sub-cloud and repeating the refining as many times as required to obtain the desired sub-cloud. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00002">
<claim-text><highlight><bold>2</bold></highlight>. A method for automatically segmenting a scan field of a scene into subsets of points that represent different surfaces in the scene, comprising the steps of: 
<claim-text>separating the scan field into a depth grid that includes depth information for scanned points of surfaces in the scene and a normal grid that includes an estimate of a normal to scanned points of the surfaces; </claim-text>
<claim-text>convolving the depth information of the depth grid to generate a depth rating image whose values represent a gradient of depth change from one scanned point to another scanned point in the scene; </claim-text>
<claim-text>convolving the components of the normal grid to generate a scalar value for each component for each point of the normal grid, </claim-text>
<claim-text>for each point of the normal grid, determining from the scalar values for the components of that particular point a gradient of the normal at that point, wherein the gradients determined for the points of the normal grid collectively constitute a normal rating image; </claim-text>
<claim-text>converting the depth rating image to a binary depth image using a recursive thresholding technique; </claim-text>
<claim-text>converting the normal rating image to a binary normal image using a recursive thresholding technique; </claim-text>
<claim-text>combining the binary depth image and the binary normal image to determine a single edge image; and </claim-text>
<claim-text>grouping subsets of non-edge points as belonging to corresponding surfaces of the scene. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00003">
<claim-text><highlight><bold>3</bold></highlight>. A method as recited in <dependent-claim-reference depends_on="CLM-00002">claim 2</dependent-claim-reference> further including the steps of: 
<claim-text>determining the type of geometric primitive that would best first each group of points; and </claim-text>
<claim-text>fitting the geometric primitive to the data points. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00004">
<claim-text><highlight><bold>4</bold></highlight>. A method as recited in <dependent-claim-reference depends_on="CLM-00003">claim 3</dependent-claim-reference> further including the step of intersecting adjacent planar regions in the scene. </claim-text>
</claim>
<claim id="CLM-00005">
<claim-text><highlight><bold>5</bold></highlight>. A method for fitting a point cloud representing a corner, comprising: 
<claim-text>determining a fit of three planes to the points of the point cloud and creating the planes for a model; </claim-text>
<claim-text>determining the three lines at the intersection of pairs of planes and creating the lines for the model; and </claim-text>
<claim-text>determining the vertex point at the intersection of the three planes and creating a vertex point for the model. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00006">
<claim-text><highlight><bold>6</bold></highlight>. A method for modeling a three-dimensional scene, comprising: 
<claim-text>generating a plurality of points that each represent a point on a surface of the scene; </claim-text>
<claim-text>determining a best fit of a cylinder for a group of the points using surface normal estimates and global error minimization. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00007">
<claim-text><highlight><bold>7</bold></highlight>. A method for modeling a three-dimensional scene, comprising: 
<claim-text>generating a plurality of points that each represent a point on a surface of the scene; </claim-text>
<claim-text>determining a best fit of a cylinder for a group of the points using a quadric surface fit and global error minimization. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00008">
<claim-text><highlight><bold>8</bold></highlight>. A method for modeling a three-dimensional scene, comprising: 
<claim-text>generating a plurality of points that each represent a point on a surface of the scene; </claim-text>
<claim-text>determining a best fit of a sphere for a group of the points using a quadric surface fit and global error minimization. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00009">
<claim-text><highlight><bold>9</bold></highlight>. A method for modeling a three-dimensional scene, comprising: 
<claim-text>generating a plurality of points that each represent a point on a surface of the scene; </claim-text>
<claim-text>determining a best fit quadric surface for a group of points; and </claim-text>
<claim-text>determining which geometric primitive of a plurality of the family described by the quadric surface best fits the group of points. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00010">
<claim-text><highlight><bold>10</bold></highlight>. A method for merging two geometric primitives of the same type to form a single geometric primitive of the type, comprising: 
<claim-text>creating a new group of points by combining the points used to originally fit each of the two primitives; and </claim-text>
<claim-text>fitting the new geometric primitive using any appropriate fitting technique and the newly generated point group with points from each of the original primitives. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00011">
<claim-text><highlight><bold>11</bold></highlight>. A method of registering a first model, consisting of a plurality of points and geometric primitives and having a first coordinate system, with a second model, consisting of a plurality of points and geometric primitives and having a second coordinate system, comprising: 
<claim-text>identifying by a user common features of the first and second scenes; </claim-text>
<claim-text>identifying a transformation between coordinate systems that is responsive to the identification; and </claim-text>
<claim-text>transforming the objects of the second model so that they use the first coordinate system. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00012">
<claim-text><highlight><bold>12</bold></highlight>. A method of warping, comprising: 
<claim-text>selecting one or more models represented by a plurality of point clouds and geometric primitives; </claim-text>
<claim-text>specifying constraints on the locations of any number of points or geometric primitives; </claim-text>
<claim-text>creating an artificial volume that surrounds the points and geometric primitives in each view and assigning mechanical material characteristics to the surrounding volume; </claim-text>
<claim-text>computing a minimum energy configuration for the material in the surrounding volume in which the points or geometric primitives are embedded such that the configuration satisfies all applied constraints; and </claim-text>
<claim-text>displacing the points and primitives in accordance with the computed minimum energy configuration of the surrounding volume of material. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00013">
<claim-text><highlight><bold>13</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00011">claim 12</dependent-claim-reference>, wherein the constraints are specified to eliminate closure errors. </claim-text>
</claim>
<claim id="CLM-00014">
<claim-text><highlight><bold>14</bold></highlight>. An apparatus for acquiring three dimensional information from a remote object comprising: 
<claim-text>a scanning laser module for measuring position information of the object; </claim-text>
<claim-text>a video module for capturing image information from the object; and </claim-text>
<claim-text>a processor for rendering a model of the object which includes the position information and the image information. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00015">
<claim-text><highlight><bold>15</bold></highlight>. An apparatus as recited in <dependent-claim-reference depends_on="CLM-00011">claim 14</dependent-claim-reference> wherein the video image information is collected in a spatially coincident manner with the measurement of position information. </claim-text>
</claim>
<claim id="CLM-00016">
<claim-text><highlight><bold>16</bold></highlight>. An approach as recited in <dependent-claim-reference depends_on="CLM-00011">claim 15</dependent-claim-reference> wherein the video image information is collected from points adjacent to the points where position information is obtained.</claim-text>
</claim>
</subdoc-claims>
<subdoc-drawings id="DRAWINGS">
<heading lvl="0" align="CENTER">Drawings</heading>
<representative-figure>1A</representative-figure>
<figure id="figure-D00000">
<image id="EMI-D00000" file="US20030001835A1-20030102-D00000.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00001">
<image id="EMI-D00001" file="US20030001835A1-20030102-D00001.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00002">
<image id="EMI-D00002" file="US20030001835A1-20030102-D00002.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00003">
<image id="EMI-D00003" file="US20030001835A1-20030102-D00003.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00004">
<image id="EMI-D00004" file="US20030001835A1-20030102-D00004.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00005">
<image id="EMI-D00005" file="US20030001835A1-20030102-D00005.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00006">
<image id="EMI-D00006" file="US20030001835A1-20030102-D00006.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00007">
<image id="EMI-D00007" file="US20030001835A1-20030102-D00007.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00008">
<image id="EMI-D00008" file="US20030001835A1-20030102-D00008.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00009">
<image id="EMI-D00009" file="US20030001835A1-20030102-D00009.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00010">
<image id="EMI-D00010" file="US20030001835A1-20030102-D00010.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00011">
<image id="EMI-D00011" file="US20030001835A1-20030102-D00011.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00012">
<image id="EMI-D00012" file="US20030001835A1-20030102-D00012.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00013">
<image id="EMI-D00013" file="US20030001835A1-20030102-D00013.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00014">
<image id="EMI-D00014" file="US20030001835A1-20030102-D00014.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00015">
<image id="EMI-D00015" file="US20030001835A1-20030102-D00015.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00016">
<image id="EMI-D00016" file="US20030001835A1-20030102-D00016.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00017">
<image id="EMI-D00017" file="US20030001835A1-20030102-D00017.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00018">
<image id="EMI-D00018" file="US20030001835A1-20030102-D00018.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00019">
<image id="EMI-D00019" file="US20030001835A1-20030102-D00019.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00020">
<image id="EMI-D00020" file="US20030001835A1-20030102-D00020.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00021">
<image id="EMI-D00021" file="US20030001835A1-20030102-D00021.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00022">
<image id="EMI-D00022" file="US20030001835A1-20030102-D00022.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00023">
<image id="EMI-D00023" file="US20030001835A1-20030102-D00023.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00024">
<image id="EMI-D00024" file="US20030001835A1-20030102-D00024.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00025">
<image id="EMI-D00025" file="US20030001835A1-20030102-D00025.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00026">
<image id="EMI-D00026" file="US20030001835A1-20030102-D00026.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00027">
<image id="EMI-D00027" file="US20030001835A1-20030102-D00027.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00028">
<image id="EMI-D00028" file="US20030001835A1-20030102-D00028.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00029">
<image id="EMI-D00029" file="US20030001835A1-20030102-D00029.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00030">
<image id="EMI-D00030" file="US20030001835A1-20030102-D00030.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00031">
<image id="EMI-D00031" file="US20030001835A1-20030102-D00031.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00032">
<image id="EMI-D00032" file="US20030001835A1-20030102-D00032.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00033">
<image id="EMI-D00033" file="US20030001835A1-20030102-D00033.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00034">
<image id="EMI-D00034" file="US20030001835A1-20030102-D00034.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00035">
<image id="EMI-D00035" file="US20030001835A1-20030102-D00035.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00036">
<image id="EMI-D00036" file="US20030001835A1-20030102-D00036.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00037">
<image id="EMI-D00037" file="US20030001835A1-20030102-D00037.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00038">
<image id="EMI-D00038" file="US20030001835A1-20030102-D00038.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00039">
<image id="EMI-D00039" file="US20030001835A1-20030102-D00039.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00040">
<image id="EMI-D00040" file="US20030001835A1-20030102-D00040.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00041">
<image id="EMI-D00041" file="US20030001835A1-20030102-D00041.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00042">
<image id="EMI-D00042" file="US20030001835A1-20030102-D00042.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00043">
<image id="EMI-D00043" file="US20030001835A1-20030102-D00043.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00044">
<image id="EMI-D00044" file="US20030001835A1-20030102-D00044.TIF" imf="TIFF" ti="DR"/>
</figure>
</subdoc-drawings>
</patent-application-publication>
