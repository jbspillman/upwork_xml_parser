<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE patent-application-publication SYSTEM "pap-v16-2002-01-01.dtd" [
<!ENTITY US20030002051A1-20030102-D00000.TIF SYSTEM "US20030002051A1-20030102-D00000.TIF" NDATA TIF>
<!ENTITY US20030002051A1-20030102-D00001.TIF SYSTEM "US20030002051A1-20030102-D00001.TIF" NDATA TIF>
<!ENTITY US20030002051A1-20030102-D00002.TIF SYSTEM "US20030002051A1-20030102-D00002.TIF" NDATA TIF>
<!ENTITY US20030002051A1-20030102-D00003.TIF SYSTEM "US20030002051A1-20030102-D00003.TIF" NDATA TIF>
<!ENTITY US20030002051A1-20030102-D00004.TIF SYSTEM "US20030002051A1-20030102-D00004.TIF" NDATA TIF>
<!ENTITY US20030002051A1-20030102-D00005.TIF SYSTEM "US20030002051A1-20030102-D00005.TIF" NDATA TIF>
<!ENTITY US20030002051A1-20030102-D00006.TIF SYSTEM "US20030002051A1-20030102-D00006.TIF" NDATA TIF>
<!ENTITY US20030002051A1-20030102-D00007.TIF SYSTEM "US20030002051A1-20030102-D00007.TIF" NDATA TIF>
<!ENTITY US20030002051A1-20030102-D00008.TIF SYSTEM "US20030002051A1-20030102-D00008.TIF" NDATA TIF>
<!ENTITY US20030002051A1-20030102-D00009.TIF SYSTEM "US20030002051A1-20030102-D00009.TIF" NDATA TIF>
<!ENTITY US20030002051A1-20030102-D00010.TIF SYSTEM "US20030002051A1-20030102-D00010.TIF" NDATA TIF>
<!ENTITY US20030002051A1-20030102-D00011.TIF SYSTEM "US20030002051A1-20030102-D00011.TIF" NDATA TIF>
<!ENTITY US20030002051A1-20030102-D00012.TIF SYSTEM "US20030002051A1-20030102-D00012.TIF" NDATA TIF>
<!ENTITY US20030002051A1-20030102-D00013.TIF SYSTEM "US20030002051A1-20030102-D00013.TIF" NDATA TIF>
<!ENTITY US20030002051A1-20030102-D00014.TIF SYSTEM "US20030002051A1-20030102-D00014.TIF" NDATA TIF>
<!ENTITY US20030002051A1-20030102-D00015.TIF SYSTEM "US20030002051A1-20030102-D00015.TIF" NDATA TIF>
<!ENTITY US20030002051A1-20030102-D00016.TIF SYSTEM "US20030002051A1-20030102-D00016.TIF" NDATA TIF>
<!ENTITY US20030002051A1-20030102-D00017.TIF SYSTEM "US20030002051A1-20030102-D00017.TIF" NDATA TIF>
<!ENTITY US20030002051A1-20030102-D00018.TIF SYSTEM "US20030002051A1-20030102-D00018.TIF" NDATA TIF>
<!ENTITY US20030002051A1-20030102-D00019.TIF SYSTEM "US20030002051A1-20030102-D00019.TIF" NDATA TIF>
<!ENTITY US20030002051A1-20030102-D00020.TIF SYSTEM "US20030002051A1-20030102-D00020.TIF" NDATA TIF>
]>
<patent-application-publication>
<subdoc-bibliographic-information>
<document-id>
<doc-number>20030002051</doc-number>
<kind-code>A1</kind-code>
<document-date>20030102</document-date>
</document-id>
<publication-filing-type>new</publication-filing-type>
<domestic-filing-data>
<application-number>
<doc-number>10116994</doc-number>
</application-number>
<application-number-series-code>10</application-number-series-code>
<filing-date>20020405</filing-date>
</domestic-filing-data>
<technical-information>
<classification-ipc>
<classification-ipc-primary>
<ipc>G01B011/24</ipc>
</classification-ipc-primary>
<classification-ipc-edition>07</classification-ipc-edition>
</classification-ipc>
<classification-us>
<classification-us-primary>
<uspc>
<class>356</class>
<subclass>601000</subclass>
</uspc>
</classification-us-primary>
</classification-us>
<title-of-invention>Non-contacting mensuration system</title-of-invention>
</technical-information>
<continuity-data>
<non-provisional-of-provisional>
<document-id>
<doc-number>60293676</doc-number>
<document-date>20010525</document-date>
<country-code>US</country-code>
</document-id>
</non-provisional-of-provisional>
</continuity-data>
<inventors>
<first-named-inventor>
<name>
<given-name>Ralph</given-name>
<middle-name>F.</middle-name>
<family-name>Cohn</family-name>
</name>
<residence>
<residence-us>
<city>Brookline</city>
<state>MA</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</first-named-inventor>
</inventors>
<correspondence-address>
<name-1>Iandiorio &amp; Teska</name-1>
<name-2></name-2>
<address>
<address-1>260 Bear Hill Road</address-1>
<city>Waltham</city>
<state>MA</state>
<postalcode>02451-1018</postalcode>
<country>
<country-code>US</country-code>
</country>
</address>
</correspondence-address>
</subdoc-bibliographic-information>
<subdoc-abstract>
<paragraph id="A-0001" lvl="0">A non-contacting mensuration system including a light source for projecting light on a target; an imager having an image plane; a first optical subsystem configured to project a first image of the light on the target onto a first portion of the image plane of the imager; and a second optical subsystem configured to project a second image of the light on the target onto a second portion of the image plane of the imager so that two different images are obtained from different aspects simultaneously. </paragraph>
</subdoc-abstract>
<subdoc-description>
<cross-reference-to-related-applications>
<heading lvl="1">PRIORITY CLAIM </heading>
<paragraph id="P-0001" lvl="0"><number>&lsqb;0001&rsqb;</number> This invention claims priority of Provisional Patent Application Serial No. 60/293,676 filed May 25, 2001.</paragraph>
</cross-reference-to-related-applications>
<summary-of-invention>
<section>
<heading lvl="1">FIELD OF THE INVENTION </heading>
<paragraph id="P-0002" lvl="0"><number>&lsqb;0002&rsqb;</number> This invention relates to a non-contacting measurement system useful for imaging targets of various types and for measuring the contours and features thereof. In one example, the contours and features of a plaster model of a human face is scanned and the resulting data used to fabricate a custom-made oxygen mask. </paragraph>
</section>
<section>
<heading lvl="1">BACKGROUND OF THE INVENTION </heading>
<paragraph id="P-0003" lvl="0"><number>&lsqb;0003&rsqb;</number> Imaging and measuring the contours and features of various targets without touching them, with a probe for example, is known in the art in a variety of fields. </paragraph>
<paragraph id="P-0004" lvl="0"><number>&lsqb;0004&rsqb;</number> One basic means by which a non-contact optical system can be used to acquire three-dimensional surface information is triangulation. In plane geometry, triangulation determines the distance by observing a point on an object from two different positions separated by a known base line distance. By determining the angles from the two observations sites to a single point on the surface, the length of one side of the triangle and all of the angles are known. This information is sufficient to determine the remaining length of the sides of the triangles and the heights above the base line using trigonometry. A laser can be used to project a beam at a target from a known position and angle. The location of the projected spot is imaged using a CCD camera also fixed at a known location. From the location of the spot on the image captured by the camera, the required azimuth and elevation angles can be determined providing the necessary information relative to the location in three dimensions of that point on the target surface. Since only a single data point is collected for each CCD image processed, however, the surface must be scanned in two dimensions which results in a lengthy data collection/processing duration. </paragraph>
<paragraph id="P-0005" lvl="0"><number>&lsqb;0005&rsqb;</number> Structured light systems provide a more efficient means of collecting scanned data of the surface of an object. In such a system, a line of light or &ldquo;light stripe&rdquo; is projected on the surface. The line is oriented so that stripe is at a constant known azimuth angle, the critical angle that must be known for triangulation. The light fans out over a wide image of elevation angles making this unknown. The light stripe is then imaged by the CCD camera, which captures a two-dimensional image of the projected stripe on the surface. The deviation of the image stripe from a known base line provides a measurement of the azimuth angle at the camera&apos;s observation point. The elevation angle may be determined from the distance in the orthogonal direction. Because there is only one elevation angle in this geometry, the system is completely determined and the structured light provides a complete scan along one direction. The mechanical surface scanning process is thus decreased from two dimensions to one dimension and the number of CCD images which must be processed is reduced from n<highlight><superscript>2 </superscript></highlight>to n for a square scan. Scanning is typically accomplished by rotating the specimen while holding the optical system in a fixed position. </paragraph>
<paragraph id="P-0006" lvl="0"><number>&lsqb;0006&rsqb;</number> One problem frequently encountered by this approach is shadowing. Shadowing is the obscuring of the image of the projected light by three-dimensional features on the surface of the target object such as peaks and holes. </paragraph>
<paragraph id="P-0007" lvl="0"><number>&lsqb;0007&rsqb;</number> One solution to the shadowing problem is to view the object from more than one position selecting the viewing angles so that at least one view will be unobscured by any three dimensional features on the surface of the object. This approach works particularly well when the object is roughly symmetrical about the vertical axis and is rotated to achieve the desired surface scan. While such systems work fairly well, there are some drawbacks. The use of two cameras significantly increases the system cost, including additional processing electronics and data handling/storage capacity. The use of two cameras also doubles the volume of the digitized image of the object and doubles the processing requirements. </paragraph>
<paragraph id="P-0008" lvl="0"><number>&lsqb;0008&rsqb;</number> In U.S. Pat. No. 4,737,023, incorporated herein by this reference, the contour line formed when the light stripe intersects the target is viewed from two sides but then both of these images are combined by a beam splitter so that the camera only sees one image. Optical alignment of the various optical components, however, is critical in this design and thus the resulting system is not very robust and is difficult to use in factory settings and/or by non-skilled technicians and operators. Any physical disturbance, such as shock, vibration, or thermal cycling, can cause one or more optical components to shift misaligning the system. This misalignment will produce erroneous reading by the system and which can only be corrected by a trained technician with specific skills with optics and optical systems. The lack of robustness in the design of the &apos;023 patent limits its applicability. </paragraph>
</section>
<section>
<heading lvl="1">SUMMARY OF THE INVENTION </heading>
<paragraph id="P-0009" lvl="0"><number>&lsqb;0009&rsqb;</number> It is therefore an object of this invention to provide a more robust non-contacting mensuration system. </paragraph>
<paragraph id="P-0010" lvl="0"><number>&lsqb;0010&rsqb;</number> It is a further object of this invention to provide such a non-contacting mensuration system which is easier to use and can be operated by non-skilled technicians and operators. </paragraph>
<paragraph id="P-0011" lvl="0"><number>&lsqb;0011&rsqb;</number> It is a further object of this invention to provide such a non-contacting mensuration system in which optical alignment of the optical components need not be as precise as the requirements associated with the prior art. </paragraph>
<paragraph id="P-0012" lvl="0"><number>&lsqb;0012&rsqb;</number> It is a further object of this invention to provide such a non-contacting mensuration system which requires only one imaging camera. </paragraph>
<paragraph id="P-0013" lvl="0"><number>&lsqb;0013&rsqb;</number> It is a further object of this invention to provide such a system which can be calibrated by non-skilled technicians and operators. </paragraph>
<paragraph id="P-0014" lvl="0"><number>&lsqb;0014&rsqb;</number> This invention results from the realization that instead of viewing the contour line from two sides and then combining both images into a single image which necessarily results in precise optical alignment requirements, a more robust system can be devised if two different images of the contour line are separately directed onto different portions of the image plane of a single imager. </paragraph>
<paragraph id="P-0015" lvl="0"><number>&lsqb;0015&rsqb;</number> This invention features a non-contacting mensuration system. A light source projects light on a target and there is an imager having an image plane. A first optical subsystem is configured to project a first image of the light on the target onto a first portion of the image plane of the imager and a second optical subsystem is configured to project a second image of the light on the target onto a second portion of the image plane of the imager so that two different images are obtained from different aspects simultaneously. </paragraph>
<paragraph id="P-0016" lvl="0"><number>&lsqb;0016&rsqb;</number> Typically, the light source is a laser configured to project a two-dimensional light pattern such as a stripe on the target. In one example, the imager is a single CCD camera. In the preferred embodiment, the first optical subsystem includes a first mirror on one side of the light source oriented to view the first image from a first aspect and a second mirror behind the light source oriented to direct the first image onto the first portion of the image plane. The second optical subsystem then includes a third mirror on another side of the light source oriented to view the second image from a second aspect and a fourth mirror behind the light source oriented to direct the second image onto the second portion of the image plane. In one example, the second mirror and the fourth mirror are opposing faces of a prism. </paragraph>
<paragraph id="P-0017" lvl="0"><number>&lsqb;0017&rsqb;</number> In the prototype, the first and second mirrors were oriented to provide a 30&deg; observation angle, the light source was positioned directly in front of the image plane of the imager, and the prism was positioned between the light source and the image plane of the imager. Further included may be means for moving the target relative the light source such as a turntable upon which the target is placed. In one embodiment, the non-contacting mensuration system comprises a light source for projecting light onto a target; an imager having an image plane; one mirror located on one side of the light source and oriented to view the target from a first aspect; another mirror located on another side of the light source and oriented to view the target from a second aspect; and a prism positioned between the light source the image plane of the imager having opposing mirror faces oriented to direct images from the two mirrors onto first and second portions on the image plane of the imager simultaneously. </paragraph>
</section>
</summary-of-invention>
<brief-description-of-drawings>
<section>
<heading lvl="1">BRIEF DESCRIPTION OF THE DRAWINGS </heading>
<paragraph id="P-0018" lvl="0"><number>&lsqb;0018&rsqb;</number> Other objects, features and advantages will occur to those skilled in the art from the following description of a preferred embodiment and the accompanying drawings, in which: </paragraph>
<paragraph id="P-0019" lvl="0"><number>&lsqb;0019&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 1</cross-reference> is a schematic view showing the primary components associated with a prior art imaging system wherein a spot projected by a laser is imaged using a CCD camera; </paragraph>
<paragraph id="P-0020" lvl="0"><number>&lsqb;0020&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 2</cross-reference> is a schematic view showing the primary components associated with a prior art imaging system wherein a light stripe projected by a laser is imaged by a CCD camera; </paragraph>
<paragraph id="P-0021" lvl="0"><number>&lsqb;0021&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 3</cross-reference> is a schematic view showing the primary components associated with another prior art imaging system wherein the system of <cross-reference target="DRAWINGS">FIG. 2</cross-reference> is modified to include a second CCD camera to overcome the shadowing problem associated with the system shown in <cross-reference target="DRAWINGS">FIG. 2</cross-reference>; </paragraph>
<paragraph id="P-0022" lvl="0"><number>&lsqb;0022&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 4</cross-reference> is a schematic view showing the primary components associated with the imaging system disclosed in U.S. Pat. No. 4,737,032; </paragraph>
<paragraph id="P-0023" lvl="0"><number>&lsqb;0023&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 5</cross-reference> is a schematic view showing the primary optical components associated with the non-contacting mensuration system of the subject invention; </paragraph>
<paragraph id="P-0024" lvl="0"><number>&lsqb;0024&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 6</cross-reference> is a schematic view showing the primary optical components associated with the working embodiment of the mensuration system shown in <cross-reference target="DRAWINGS">FIG. 5</cross-reference>; </paragraph>
<paragraph id="P-0025" lvl="0"><number>&lsqb;0025&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 7</cross-reference> is a view showing the image seen by the camera of the mensuration system of the subject invention with the laser line centered in the middle of the target; </paragraph>
<paragraph id="P-0026" lvl="0"><number>&lsqb;0026&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 8</cross-reference> is a view showing the image seen by the camera of the mensuration system of this invention with the target rotated and one view of the laser line shaded by a surface feature on the target; </paragraph>
<paragraph id="P-0027" lvl="0"><number>&lsqb;0027&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 9</cross-reference> is a graph showing the contours of the target depicted in <cross-reference target="DRAWINGS">FIGS. 7 and 8</cross-reference> measured by the non-contacting mensuration system of the subject invention; </paragraph>
<paragraph id="P-0028" lvl="0"><number>&lsqb;0028&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 10</cross-reference> is a graph showing in more detail one particular target surface feature measured by the non-contacting mensuration system of the subject invention; </paragraph>
<paragraph id="P-0029" lvl="0"><number>&lsqb;0029&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 11</cross-reference> is a graph showing in more detail another surface feature of the target shown in <cross-reference target="DRAWINGS">FIGS. 7 and 8</cross-reference> measured by the non-contacting mensuration system of the subject invention; </paragraph>
<paragraph id="P-0030" lvl="0"><number>&lsqb;0030&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 12</cross-reference> is a graph showing the slight differences between the two profiles in accordance with the non-contacting mensuration system of the subject invention; </paragraph>
<paragraph id="P-0031" lvl="0"><number>&lsqb;0031&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 13</cross-reference> is a depiction of a complete surface scan of the target shown in <cross-reference target="DRAWINGS">FIGS. 7 and 8</cross-reference> output by the non-contacting mensuration system of the subject invention; and </paragraph>
<paragraph id="P-0032" lvl="0"><number>&lsqb;0032&rsqb;</number> FIGS. <highlight><bold>14</bold></highlight>-<highlight><bold>24</bold></highlight> are flow charts depicting the primary steps associated with the non-contacting mensuration software in accordance with the subject invention.</paragraph>
</section>
</brief-description-of-drawings>
<detailed-description>
<section>
<heading lvl="1">DISCLOSURE OF THE PREFERRED EMBODIMENT </heading>
<paragraph id="P-0033" lvl="0"><number>&lsqb;0033&rsqb;</number> As discussed in the background section above, prior art mensuration system <highlight><bold>5</bold></highlight> includes laser <highlight><bold>14</bold></highlight> which projects a vertical light stripe on target <highlight><bold>10</bold></highlight> imaged by camera <highlight><bold>12</bold></highlight>. In this example, target <highlight><bold>10</bold></highlight> is a plaster cast of a human face. As shown in <cross-reference target="DRAWINGS">FIG. 2</cross-reference>, however, when surface feature <highlight><bold>13</bold></highlight> (the nose of the face) obscures the vertical stripe projected by laser <highlight><bold>14</bold></highlight> on target <highlight><bold>10</bold></highlight>, however, another camera such as camera <highlight><bold>16</bold></highlight>, <cross-reference target="DRAWINGS">FIG. 3</cross-reference> must be used so that at least one view will be unobscured. </paragraph>
<paragraph id="P-0034" lvl="0"><number>&lsqb;0034&rsqb;</number> The problem with the two camera system shown in <cross-reference target="DRAWINGS">FIG. 3</cross-reference>, however, is that the system cost is higher, and additional processing electronics and data handling/storage capacity must be provided. The system shown in <cross-reference target="DRAWINGS">FIG. 3</cross-reference> also doubles the volume of digitized images of the objects and doubles the processing requirements. </paragraph>
<paragraph id="P-0035" lvl="0"><number>&lsqb;0035&rsqb;</number> In <cross-reference target="DRAWINGS">FIG. 4</cross-reference>, target <highlight><bold>10</bold></highlight> rests on turntable <highlight><bold>40</bold></highlight>. Light source <highlight><bold>42</bold></highlight> provides a plane of light intersecting target <highlight><bold>10</bold></highlight> providing a contour line. Mirrors <highlight><bold>44</bold></highlight> and <highlight><bold>46</bold></highlight> view the contour line from different aspect angles and beamsplitter <highlight><bold>48</bold></highlight> combines both images so that only one resultant image is directed to sensor (camera) <highlight><bold>49</bold></highlight>. See U.S. Pat. No. 4,737,032. The shortcomings associated with this system are adequately explained in the background section above. </paragraph>
<paragraph id="P-0036" lvl="0"><number>&lsqb;0036&rsqb;</number> In the subject invention, in contrast, non-contacting mensuration system <highlight><bold>50</bold></highlight>, <cross-reference target="DRAWINGS">FIG. 5</cross-reference> includes source <highlight><bold>52</bold></highlight> for projecting light on target <highlight><bold>10</bold></highlight>. Typically, light source <highlight><bold>52</bold></highlight> is a laser such a class II diode laser 1.6 mW, 670 n configured to project a line beam on target <highlight><bold>10</bold></highlight> with a 30&deg; fan angle. System <highlight><bold>50</bold></highlight> also includes imager <highlight><bold>55</bold></highlight> having an image plane such as a monochrome CCD camera <highlight><bold>54</bold></highlight> (available from Cohu, Inc. as part no. 212-1000/0000) with lens <highlight><bold>56</bold></highlight>. In accordance with this invention, first optical subsystem <highlight><bold>60</bold></highlight> is configured to project a first image of the light on target <highlight><bold>10</bold></highlight> onto a first portion of the image plane of camera <highlight><bold>54</bold></highlight>. And, second optical subsystem <highlight><bold>62</bold></highlight> is configured to project a second image of the light on target <highlight><bold>10</bold></highlight> onto a second portion of the image plane of camera <highlight><bold>54</bold></highlight>. In this way, two different images are obtained from different aspects but only one camera is typically used and yet, at the same time, system <highlight><bold>50</bold></highlight> is more robust and less sensitive to optical misalignments. In the preferred embodiment, first optical subsystem <highlight><bold>60</bold></highlight> includes first mirror <highlight><bold>64</bold></highlight> on one side of light source <highlight><bold>52</bold></highlight> oriented to view the first image from a first aspect and second mirror <highlight><bold>66</bold></highlight> behind the light source <highlight><bold>52</bold></highlight> oriented to direct the first image onto the first portion of the image plane of camera <highlight><bold>54</bold></highlight>. The second optical subsystem <highlight><bold>62</bold></highlight> then includes third mirror <highlight><bold>68</bold></highlight> on the other side of light source <highlight><bold>52</bold></highlight> oriented to view the second image from a second aspect and fourth mirror <highlight><bold>70</bold></highlight> behind light source <highlight><bold>52</bold></highlight> oriented to direct the second image onto the second portion of the image plane of CCD camera <highlight><bold>54</bold></highlight>. </paragraph>
<paragraph id="P-0037" lvl="0"><number>&lsqb;0037&rsqb;</number> In the preferred embodiment, mirror <highlight><bold>66</bold></highlight> and mirror <highlight><bold>70</bold></highlight> are opposing faces of 90&deg; angle prism <highlight><bold>72</bold></highlight>. Typically, mirror <highlight><bold>64</bold></highlight> is oriented to provide a 30&deg; observation angle and mirror <highlight><bold>68</bold></highlight> is also oriented to provide a 30&deg; observation angle. As shown, it is preferred that light source <highlight><bold>52</bold></highlight> is positioned directly in front of the image plane of camera <highlight><bold>54</bold></highlight> and prism <highlight><bold>72</bold></highlight> is positioned between the light source and the image plane of the imager. Further included may be means for moving the target relative to the light source such as turntable <highlight><bold>74</bold></highlight> upon which the target is placed. </paragraph>
<paragraph id="P-0038" lvl="0"><number>&lsqb;0038&rsqb;</number> A complete system is shown in <cross-reference target="DRAWINGS">FIG. 6</cross-reference>. Target <highlight><bold>10</bold></highlight> in this example is again a cast of a human face mounted on turntable <highlight><bold>10</bold></highlight> powered by a stepper motor with an angle of 1.8&deg; per step and half step resolution through 100:1 ratio worm gear/worm wheel. In other embodiments, a translation stage may be used or the whole system can be moved with respect to a stationary object. Laser <highlight><bold>52</bold></highlight> is mounted on an aluminum laser holder <highlight><bold>80</bold></highlight> which also acts as a heat sink. The laser holder is fixed on a post that is held by a post holder mounted on base <highlight><bold>90</bold></highlight>. Camera <highlight><bold>54</bold></highlight> is fixed to support <highlight><bold>86</bold></highlight> mounted on base <highlight><bold>90</bold></highlight>. Lens <highlight><bold>56</bold></highlight> is a 25 mm, F1.4 lense with manual focus and iris. Mirrors <highlight><bold>64</bold></highlight> and <highlight><bold>68</bold></highlight> are held on mirror holders by nylon bushings and screws. The mirror holders are attached to vertical supports with a spring-loaded shoulder bolt which allows precise adjustment of the angular position of the mirrors by turning three adjusting screws. The vertical support is mounted on base <highlight><bold>90</bold></highlight> via a support bracket. 90&deg; angle prism <highlight><bold>72</bold></highlight> is externally mirrored and has one inch square faces <highlight><bold>66</bold></highlight> and <highlight><bold>70</bold></highlight>, <cross-reference target="DRAWINGS">FIG. 5</cross-reference> to function as a precise 90&deg; mirror. Prism <highlight><bold>72</bold></highlight> is mounted on a kinematic mount and held in place by both an adhesive and a clamp provided with a nylon tip clamping screw. The kinematic mount provides tilt control in two axes enabling precise alignment of the leveling of the base of the prism so that the edge that faces the CCD camera is perfectly aligned. The kinematic mount is assembled through a post and post holder system to an x-y fine adjustment translation table that provides precise positioning of the prism in relation to the lens of the camera. </paragraph>
<paragraph id="P-0039" lvl="0"><number>&lsqb;0039&rsqb;</number> Also associated with the system shown in <cross-reference target="DRAWINGS">FIG. 6</cross-reference> are a controller/driver for the stepper motor turntable <highlight><bold>74</bold></highlight>, a universal power supply with an input of 90 to 260 VAC at a frequency of 47 to 440 Hz to supply 5 VDC for the laser, 12 VDC for the camera, and 24 VDC for the controller and turntable. A reference position for turntable <highlight><bold>74</bold></highlight> is determined using a photosensor which detects a white line painted on a predetermined radius of the underside of the turntable tray. The photosensor is mounted on a small PC board where there is associated circuitry and this board is connected through a cable to the motor controller. </paragraph>
<paragraph id="P-0040" lvl="0"><number>&lsqb;0040&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 7</cross-reference> shows the dual images <highlight><bold>90</bold></highlight> and <highlight><bold>92</bold></highlight> viewed by camera <highlight><bold>54</bold></highlight>, <cross-reference target="DRAWINGS">FIGS. 5 and 6</cross-reference> with the laser lines centered in the middle of target <highlight><bold>10</bold></highlight>. Camera <highlight><bold>54</bold></highlight>, <cross-reference target="DRAWINGS">FIGS. 5 and 6</cross-reference> is positioned on its side to make the best use of the available 640 horizontal and 480 vertical pixels. Note, however, that the system of this invention does not precisely spilt the camera&apos;s field of view into two separately defined images, but rather overlaps and fades the two images together. Typically, a darkened enclosure is included to eliminate any interference from other illumination sources. Therefore, the images of the two views may freely overlap because the object being measured is dark and no interference results. The images of the lines shown in <cross-reference target="DRAWINGS">FIG. 7</cross-reference>, however, remain separable. This is carried out by a careful geometric design ensuring that over the full range of surface heights to be measured there is no overlap. When the target is rotated via turntable <highlight><bold>74</bold></highlight>, <cross-reference target="DRAWINGS">FIGS. 5 and 6</cross-reference>, image <highlight><bold>92</bold></highlight>&prime; is incomplete as shown at <highlight><bold>94</bold></highlight> due to feature <highlight><bold>96</bold></highlight> present on target <highlight><bold>10</bold></highlight>. By using the two 90&deg; faces of right angle prism <highlight><bold>72</bold></highlight>, <cross-reference target="DRAWINGS">FIGS. 5 and 6</cross-reference> coated with highly reflective aluminum as mirrors to provide split frame imaging, the target is observed from both sides simultaneously. A 30&deg; observation angle is used which provides both excellent precision and adequate measuring range. </paragraph>
<paragraph id="P-0041" lvl="0"><number>&lsqb;0041&rsqb;</number> The software associated with the system, discussed in reference to FIGS. <highlight><bold>14</bold></highlight>-<highlight><bold>24</bold></highlight>, processes the split display, yielding two relatively independent measurements of the profile whenever shadowing does not occur and a single result when shadowing does occur as shown in <cross-reference target="DRAWINGS">FIG. 8</cross-reference>. The system averages the results to improve accuracy whenever redundant measurements are available and uses a single result whenever shadowing occurs. </paragraph>
<paragraph id="P-0042" lvl="0"><number>&lsqb;0042&rsqb;</number> FIGS. <highlight><bold>9</bold></highlight>-<highlight><bold>11</bold></highlight> shows the results obtained when processing a single trace of the profile of target <highlight><bold>10</bold></highlight> shown in FIGS. <highlight><bold>5</bold></highlight>-<highlight><bold>8</bold></highlight>. Shadow causing feature <highlight><bold>96</bold></highlight> and feature <highlight><bold>97</bold></highlight> are shown in more detail in <cross-reference target="DRAWINGS">FIGS. 11 and 10</cross-reference>, respectively. In all three figures, the horizontal axis is the pixel location and the vertical axis is the feature height in inches. Everywhere else, the system averages the two results to improve accuracy whenever redundant results are available. <cross-reference target="DRAWINGS">FIG. 9</cross-reference> shows the results obtained when processing a single trace of the profile of the lower half of the target demonstrating the excellent correspondence between the two simultaneous measurements. <cross-reference target="DRAWINGS">FIG. 13</cross-reference> shows a complete surface scan of the target, in this example, the casting of the lower half of the face shown in the previous figures. The resulting data can be used in one example, to manufacture an oxygen mask for a soldier or pilot. </paragraph>
<paragraph id="P-0043" lvl="0"><number>&lsqb;0043&rsqb;</number> FIGS. <highlight><bold>14</bold></highlight>-<highlight><bold>24</bold></highlight> depict the primary steps associated with the operation and programming of the non-contacting mensuration system of the subject invention. The system is initialized and turntable <highlight><bold>74</bold></highlight> is rotated to the loading position. Once a zero angle rotation degree is obtained, the image is captured and then the centroid is determined to find the height for each column. By subtracting the base line from the result, the average deviation from the base line is obtained, step <highlight><bold>110</bold></highlight>. If the average deviation is below a predefined limit, step <highlight><bold>112</bold></highlight>, the previous calibration settings may be retained, step <highlight><bold>114</bold></highlight> (or calibration can be accomplished again as shown at step <highlight><bold>116</bold></highlight>) and the system displays on the output terminal the system status whereupon the user selects the appropriate task including calibration, scanning, loading, Go to load position, or exit, step <highlight><bold>118</bold></highlight>. If calibration is not required, step <highlight><bold>120</bold></highlight>, scanning is initiated by rotating the turntable platform to the loading position, updating the screen to alert the user to the motion, and providing a signal when stopped with a message step <highlight><bold>122</bold></highlight> (&ldquo;Insert cast and close door and press start button&rdquo;). </paragraph>
<paragraph id="P-0044" lvl="0"><number>&lsqb;0044&rsqb;</number> In step <highlight><bold>124</bold></highlight>, the system prompts the user to insert the cast. The user can set an option to save scan images by checking the box provided. When the option is selected, the user sets the directory and base file name. The program reads the number of scan lines, and the start and end angles from the computer disk file named &ldquo;settings.txt&rdquo;. After inserting the cast and closing the door, the user starts the processing with a key press. The program then tests to confirm the laser is on and to confirm the enclosure door is closed. </paragraph>
<paragraph id="P-0045" lvl="0"><number>&lsqb;0045&rsqb;</number> In step <highlight><bold>126</bold></highlight>, the platform holding the cast is next rotated to the starting position, the counter is initialized, and the program reads the calibration data from a disk file. Step <highlight><bold>128</bold></highlight> represents a decision as to whether all the scans have been completed for the current run, the counter is checked to see if all the scan images have been collected, if more images must be recorded the program proceeds to step <highlight><bold>130</bold></highlight>, <cross-reference target="DRAWINGS">FIG. 15</cross-reference> in the flowchart. If all the scans have been completed the program proceeds to step <highlight><bold>190</bold></highlight>, <cross-reference target="DRAWINGS">FIG. 17</cross-reference> in the flowchart. </paragraph>
<paragraph id="P-0046" lvl="0"><number>&lsqb;0046&rsqb;</number> In step <highlight><bold>130</bold></highlight>, <cross-reference target="DRAWINGS">FIG. 15</cross-reference>, information is displayed on the screen so that the user can keep track of the current status of the scan. The displayed information consists of the angle of the stage, image number, and time. Next, in step <highlight><bold>132</bold></highlight>, a single monochrome image with 8-bit intensity resolution is acquired by the frame grabber, this image is transferred to an array in the computer memory. If the option was previously selected by the user, the image is stored in file with the name determined by the current stage angle (e.g. data-70<highlight><subscript>&mdash;</subscript></highlight>5.tif for &minus;70.5 degrees). Once this is complete, the stage is commanded to move to the next scan angle. This motion will continue while the acquired structured light data is processed. </paragraph>
<paragraph id="P-0047" lvl="0"><number>&lsqb;0047&rsqb;</number> The processing of the image data to determine the height of the surface of the cast proceeds in the following fashion. In step <highlight><bold>134</bold></highlight> processing begins on the region of the image array in the computer memory containing the upper light stripe on the screen. During this step, the image is searched and the brightest pixel in each column is identified. Next, in step <highlight><bold>136</bold></highlight>, is a decision: is the pixel in each column greater than a preset threshold indicating it represents valid data and is the pixel in a valid position within the image&quest; If the data is valid, the program proceeds to step <highlight><bold>140</bold></highlight>. If the data is below threshold or outside the valid position, the program continues at step <highlight><bold>138</bold></highlight>. The program loops through each column in the image to complete this process. In step <highlight><bold>138</bold></highlight>, the data in the column is invalid and therefore the location of the centroid of the light stripe in the image and the computed height of the cast are both set to zero. The program then proceeds to step <highlight><bold>142</bold></highlight>. In step <highlight><bold>140</bold></highlight>, the data is valid and the following operations take place. First, the centroid of the light stripe in the column is computed using the procedure detailed in <cross-reference target="DRAWINGS">FIG. 23</cross-reference>. The centroid computation uses multiple pixels of variable brightness across the light stripe to estimate the center of the stripe with sub pixel accuracy. Next, the baseline light stripe position from a calibration measurement of the back plate alone is subtracted to determine the position of the stripe relative to the reference height. Finally, the height calibration detailed in <cross-reference target="DRAWINGS">FIG. 24</cross-reference> is applied to determine the height of the surface in that column of the upper image (H<highlight><subscript>U</subscript></highlight>). The baseline and calibration data come from the stored calibration file. The program then proceeds to step <highlight><bold>142</bold></highlight>, <cross-reference target="DRAWINGS">FIG. 15</cross-reference>. </paragraph>
<paragraph id="P-0048" lvl="0"><number>&lsqb;0048&rsqb;</number> In step <highlight><bold>142</bold></highlight>, the program checks the centroid values to verify that they fall within preset maximum and minimum values. Any centroid values that fall outside of range to the limits will be set to the corresponding maximum or minimum value. The centroid values are used together with calibration data to compute the x-axis position (X<highlight><subscript>U</subscript></highlight>) of each column in the light stripe. In step <highlight><bold>144</bold></highlight>, the program proceeds to process the region of the image array in the computer memory containing the Upper light stripe on the screen. During step <highlight><bold>144</bold></highlight>, the image is searched and the brightest pixel in each column is identified. Next in step <highlight><bold>148</bold></highlight> is a decision: is the pixel in each column greater than a preset threshold indicating it represents valid data and is the pixel in a valid position within the image&quest; If the data is valid, the program proceeds to step <highlight><bold>146</bold></highlight>. If the data is below threshold or outside the valid position, the program continues at step <highlight><bold>150</bold></highlight>. The program loops through each column in the image to complete this process. In step <highlight><bold>146</bold></highlight>, the data in the column is invalid therefore the location of the centroid of the light stripe in the image and the computed height of the cast are both set to zero. The program proceeds to step <highlight><bold>152</bold></highlight>. In step <highlight><bold>150</bold></highlight>, the data is valid and the following operations take place. First, the centroid of the light stripe in the column is computed using the procedure detailed in <cross-reference target="DRAWINGS">FIG. 23</cross-reference>. The centroid computation uses multiple pixels of variable brightness across the light stripe to estimate the center of the stripe with sub pixel accuracy. Next, the baseline light stripe position, from a calibration measurement of the back plate alone, is subtracted to determine the position of the stripe relative to the reference height. Finally, the height calibration detailed in <cross-reference target="DRAWINGS">FIG. 24</cross-reference> is applied to determine the height of the surface in that column of the lower image (H<highlight><subscript>L</subscript></highlight>). The baseline and calibration data come from the stored calibration file. The program then proceeds to step <highlight><bold>152</bold></highlight>, <cross-reference target="DRAWINGS">FIG. 15</cross-reference>. </paragraph>
<paragraph id="P-0049" lvl="0"><number>&lsqb;0049&rsqb;</number> The centroid values are checked, step <highlight><bold>152</bold></highlight>, to verify that they fall within preset maximum and minimum values. Any centroid values that fall outside of range to the limits will be set to the corresponding maximum or minimum value. The centroid values are used together with calibration data to compute the x-axis position (X<highlight><subscript>L</subscript></highlight>) of each column in the light stripe. </paragraph>
<paragraph id="P-0050" lvl="0"><number>&lsqb;0050&rsqb;</number> In step <highlight><bold>160</bold></highlight>, <cross-reference target="DRAWINGS">FIG. 16</cross-reference>, the program verifies that the scan motion initiated in step <highlight><bold>132</bold></highlight> is complete. If the stage is still in motion, the program waits at this step until it is completed. Step <highlight><bold>162</bold></highlight> is a decision&mdash;if more columns remain to be processed, the program proceeds to step <highlight><bold>164</bold></highlight>, however if no additional columns remain, the flow is directed to step <highlight><bold>168</bold></highlight>. Steps <highlight><bold>164</bold></highlight>, <highlight><bold>166</bold></highlight>, <highlight><bold>167</bold></highlight>, <highlight><bold>170</bold></highlight>, <highlight><bold>172</bold></highlight>, and <highlight><bold>174</bold></highlight> define the operations that are used to combine the data from the upper and lower images to produce the height (H) and longitudinal (X) positions. These operation average the values together if two legitimate values exist or take the available valid data point when only one legitimate value exists. In step <highlight><bold>164</bold></highlight> a decision is made&mdash;if both the H<highlight><subscript>U </subscript></highlight>and H<highlight><subscript>L </subscript></highlight>values are greater than zero&mdash;proceed at step <highlight><bold>170</bold></highlight>, otherwise, if one of these values is zero, proceed with step <highlight><bold>166</bold></highlight>. In step <highlight><bold>166</bold></highlight> another decision is made: if the H<highlight><subscript>U </subscript></highlight>value is greater than zero, proceed to step <highlight><bold>172</bold></highlight>, otherwise, proceed with step <highlight><bold>167</bold></highlight>. In step <highlight><bold>167</bold></highlight>, set variable H&equals;H<highlight><subscript>L </subscript></highlight>and variable X&equals;X<highlight><subscript>L</subscript></highlight>; then return to step <highlight><bold>162</bold></highlight>. In step <highlight><bold>172</bold></highlight>, set variable H&equals;H<highlight><subscript>U </subscript></highlight>and variable X&equals;X<highlight><subscript>U</subscript></highlight>; then return to step <highlight><bold>162</bold></highlight>. In step <highlight><bold>172</bold></highlight> both values are valid so take the average as follows, set variable H&equals;(H<highlight><subscript>U</subscript></highlight>&plus;H<highlight><subscript>L</subscript></highlight>)/2 and variable X&equals;(X<highlight><subscript>U</subscript></highlight>&plus;X<highlight><subscript>L</subscript></highlight>)/2. In step <highlight><bold>174</bold></highlight>, the difference between the heights from the upper and lower images are computed and then summed with previous values from the current angle as follows, variable Test&equals;Test&plus;(H<highlight><subscript>U</subscript></highlight>&minus;H<highlight><subscript>L</subscript></highlight>). A count of the number of points included in the sum is then incremented (count&plus;&plus;). When this is complete processing returns to step <highlight><bold>162</bold></highlight>. In step <highlight><bold>168</bold></highlight> the average value of the difference in height between the upper and lower estimates of the surface height is computed, Test&equals;Test/count. </paragraph>
<paragraph id="P-0051" lvl="0"><number>&lsqb;0051&rsqb;</number> Step <highlight><bold>176</bold></highlight> is a decision wherein the magnitude of the surface height difference computed in step <highlight><bold>168</bold></highlight> is compared to a limit defined in the Settings.txt file, &verbar;Test&verbar;&lt;Limit. If the magnitude is less than the limit, proceed to step <highlight><bold>180</bold></highlight>, if not go to step <highlight><bold>178</bold></highlight>. In step <highlight><bold>178</bold></highlight>, the count of the number of lines that failed the test in <highlight><bold>176</bold></highlight> is incremented. In step <highlight><bold>180</bold></highlight>, the scan angle and height value (H) are used to compute the Y-axis and Z-axis values for each pixel as follows. Y&equals;H*sin(angle)*(H &excl;&equals;&minus;center_offset) and Z&equals;H*cos(angle)&plus;center_offset. Then the X, Y, &amp; Z values, which completely define each point on the surface in Cartesian coordinates, are stored in an array in the computer&apos;s memory. Control now returns to step <highlight><bold>128</bold></highlight>, <cross-reference target="DRAWINGS">FIG. 4</cross-reference> where the decision will be made to either process the next scan angle or be ultimately directed to step <highlight><bold>190</bold></highlight>, <cross-reference target="DRAWINGS">FIG. 17</cross-reference> if the scan is complete. </paragraph>
<paragraph id="P-0052" lvl="0"><number>&lsqb;0052&rsqb;</number> In step <highlight><bold>190</bold></highlight>, first display on the computer screen, &ldquo;Scan Complete, Processing Data&rdquo;. Then clean up locations where Z&equals;0 and Y&equals;0 by estimating the correct X and Y coordinates so that the Z&equals;0 value will fall in the correct location on the surface plot. In step <highlight><bold>192</bold></highlight>, the program rotates the stage to bring the cast to the door of the enclosure. The user is prompted to either remove the cast or repeat the scan. </paragraph>
<paragraph id="P-0053" lvl="0"><number>&lsqb;0053&rsqb;</number> The number of lines that failed the upper-lower measurement difference test and the total number of scan lines are displayed on the computer screen. The program recommends that the user consider recalibration if a specified number of allowed scan lines, specified in the Settings.txt file is exceeded. </paragraph>
<paragraph id="P-0054" lvl="0"><number>&lsqb;0054&rsqb;</number> Step <highlight><bold>194</bold></highlight> prompts the user for a file directory and name for the newly acquired data set and accepts the response from the User. In step <highlight><bold>196</bold></highlight>, the X, Y, and Z arrays are saved in the disk file in a format that can be read by the CAD (computer aided design) system. Finally, in step <highlight><bold>198</bold></highlight>, the scan data is displayed on the screen in a surface or mesh plot so that the user can evaluate the data quality. Three viewpoint options are provided which the user can select. The scan is now complete and program control returns to Step <highlight><bold>118</bold></highlight>, <cross-reference target="DRAWINGS">FIG. 14</cross-reference>. </paragraph>
<paragraph id="P-0055" lvl="0"><number>&lsqb;0055&rsqb;</number> The calibration routine, <cross-reference target="DRAWINGS">FIG. 18</cross-reference>, ensures the system is calibrated after it is first set up and the calibration data is stored in a disk file. The calibration only needs to be updated when the measurement accuracy decreases, which will be detect by the measurement of the difference in the two scan lines computed in steps <highlight><bold>168</bold></highlight> and <highlight><bold>176</bold></highlight>, <cross-reference target="DRAWINGS">FIG. 16</cross-reference>. </paragraph>
<paragraph id="P-0056" lvl="0"><number>&lsqb;0056&rsqb;</number> Step <highlight><bold>200</bold></highlight>, <cross-reference target="DRAWINGS">FIG. 18</cross-reference> begins the calibration process by collecting data from the supplied calibration targets and saving the data to disk. First, the stage is rotated to the loading position at the enclosure doorway. Then a message is displayed on the computer screen to instruct the user to confirm stage is empty and clean. The bare backplane provides a zero height reference level. The program wait for a key entry from the user then it tests to confirm that the laser is turned on and that the door is closed and latched. Once this is confirmed, the program rotates the stage to the zero degree angle. When motion is stopped an image of the light stripe on the zero height baseline is captured and saved to disk. Finally, the camera image is display to allow the User to verify that it is satisfactory. </paragraph>
<paragraph id="P-0057" lvl="0"><number>&lsqb;0057&rsqb;</number> Step <highlight><bold>210</bold></highlight> is a decision point, the user is asked to accept the zero height calibration image. If the image is unacceptable (typically because the stage wasn&apos;t cleared) the program returns to step <highlight><bold>200</bold></highlight> and repeats the process. If the image is acceptable, the program continues on to step <highlight><bold>212</bold></highlight>. Step <highlight><bold>212</bold></highlight> continues the calibration process by collecting data from the next calibration target and saving the data to disk. First, the stage is rotated to the loading position at the enclosure doorway. Then a message is displayed on the computer screen to instruct the user to insert the one-inch high calibration target. The program wait for a key entry from the user then it tests to confirm that the laser is turned on and that the door is closed and latched. Once this is confirmed, the program rotates the stage to the zero degree angle. When motion is stopped an image of the light stripe on the one-inch high calibration target is captured and saved to disk. Finally, the camera image is display to allow the user to verify that it is satisfactory. Step <highlight><bold>214</bold></highlight> is a decision point wherein the user is asked to accept the one-inch high calibration image. If the image is unacceptable (typically because the wrong target was in place) the program returns to step <highlight><bold>212</bold></highlight> and repeats the process. If the image is acceptable, the program continues on to step <highlight><bold>216</bold></highlight>. Step <highlight><bold>216</bold></highlight> continues the calibration process by collecting data from the next calibration target and saving the data to disk. First, the stage is rotated to the loading position at the enclosure doorway. Then a message is displayed on the computer screen to instruct the user to insert the three-inch high calibration target. The program wait for a key entry from the user then it tests to confirm that the laser is turned on and that the door is closed and latched. Once this is confirmed, the program rotates the stage to the zero degree angle. When motion is stopped an image of the light stripe on the three-inch high calibration target is captured and saved to disk. Finally, the camera image is display to allow the user to verify that it is satisfactory. Step <highlight><bold>218</bold></highlight> is a decision point wherein the user is asked to accept the three-inch high calibration image. If the image is unacceptable (typically because the wrong target was in place) the program returns to step <highlight><bold>216</bold></highlight> and repeats the process. If the image is acceptable, the program continues on to step <highlight><bold>220</bold></highlight>. </paragraph>
<paragraph id="P-0058" lvl="0"><number>&lsqb;0058&rsqb;</number> In Step <highlight><bold>220</bold></highlight>, <cross-reference target="DRAWINGS">FIG. 19</cross-reference>, the processing of the stored calibration images begins. The zero height calibration image is read into memory from the file in which it was previously stored. Then the valid column specifications for the calibration files are read from the &ldquo;Settings.txt&rdquo; file. In step <highlight><bold>222</bold></highlight>, the upper light stripe in the zero height calibration image is processed. The brightest pixel in each valid column of the upper image is identified. Then the centroid routine detailed in <cross-reference target="DRAWINGS">FIG. 23</cross-reference> is applied to compute the centroid for each column. Using only valid data columns, the program computes a third-order least squares fit to the baseline data. Lastly, the program uses the least squares fit to compute a noise free zero-height baseline value for all columns. In step <highlight><bold>224</bold></highlight>, the lower light stripe in the zero height calibration image is processed. The brightest pixel in each valid column of the lower image is identified. Then the centroid routine detailed in <cross-reference target="DRAWINGS">FIG. 23</cross-reference> is applied to compute the centroid for each column. Using only valid data columns, the program computes a third-order least squares fit to the baseline data. Lastly, the program uses the least squares fit to compute a noise free zero-height baseline value for all columns. In step <highlight><bold>226</bold></highlight>, the one-inch high calibration image is read into memory from the file in which it was previously stored. In step <highlight><bold>230</bold></highlight>, the upper light stripe in the one-inch high calibration image is processed. The brightest pixel in each valid column of the upper image is identified. Then the centroid routine detailed in <cross-reference target="DRAWINGS">FIG. 23</cross-reference> is applied to compute the centroid for each column. Using only valid data columns, the program computes a third-order least squares fit to the one-inch high data. Lastly, the program uses the least squares fit to compute a noise free one-inch high distance value for all columns. In step <highlight><bold>232</bold></highlight>, the lower light stripe in the one-inch high calibration image is processed. The brightest pixel in each valid column of the lower image is identified. Then the centroid routine detailed in <cross-reference target="DRAWINGS">FIG. 23</cross-reference> is applied to compute the centroid for each column. Using only valid data columns, the program computes a third-order least squares fit to the one-inch high data. Lastly, the program uses the least squares fit to compute a noise free one-inch high distance value for all columns. </paragraph>
<paragraph id="P-0059" lvl="0"><number>&lsqb;0059&rsqb;</number> In step <highlight><bold>234</bold></highlight>, <cross-reference target="DRAWINGS">FIG. 20</cross-reference>, the three-inch high calibration image is read into memory from the file in which it was previously stored. In step <highlight><bold>236</bold></highlight>, the upper light stripe in the three-inch high calibration image is processed. The brightest pixel in each valid column of the upper image is identified. Then the centroid routine detailed in <cross-reference target="DRAWINGS">FIG. 23</cross-reference> is applied to compute the centroid for each column. Using only valid data columns, the program computes a third-order least squares fit to the three-inch high data. Lastly, the program uses the least squares fit to compute a noise free three-inch high distance value for all columns. In step <highlight><bold>238</bold></highlight>, the lower light stripe in the three-inch high calibration image is processed. The brightest pixel in each valid column of the lower image is identified. Then the centroid routine detailed in <cross-reference target="DRAWINGS">FIG. 23</cross-reference> is applied to compute the centroid for each column. Using only valid data columns, the program computes a third-order least squares fit to the three-inch high data. Lastly, the program uses the least squares fit to compute a noise free three-inch high distance value for all columns. In step <highlight><bold>240</bold></highlight> the calibration values for the upper screen image are calculated by performing the following calculation on each column (n) in the image. </paragraph>
<paragraph lvl="0"><in-line-formula>Form the matrix <highlight><italic>M</italic></highlight><highlight><subscript>U</subscript></highlight><highlight><italic>&equals;&lsqb;H</italic></highlight>3true <highlight><italic>H</italic></highlight>3true*<highlight><italic>P</italic></highlight>3dist<highlight><subscript>U</subscript></highlight>(<highlight><italic>n</italic></highlight>) <highlight><italic>H</italic></highlight>1true <highlight><italic>H</italic></highlight>1true*<highlight><italic>P</italic></highlight>1dist<highlight><subscript>U</subscript></highlight>(<highlight><italic>n</italic></highlight>)&rsqb;; form the vector A<highlight><subscript>U</subscript></highlight><highlight><italic>&equals;&lsqb;P</italic></highlight>3dist<highlight><subscript>U</subscript></highlight>(<highlight><italic>n</italic></highlight>) <highlight><italic>P</italic></highlight>1dist<highlight><subscript>U</subscript></highlight>(<highlight><italic>n</italic></highlight>)&rsqb;<highlight><superscript>T</superscript></highlight>; and then compute &lsqb;<highlight><italic>u</italic></highlight><highlight><subscript>U</subscript></highlight><highlight><italic>v</italic></highlight><highlight><subscript>U</subscript></highlight>&rsqb;<highlight><superscript>T</superscript></highlight><highlight><italic>&equals;M</italic></highlight><highlight><subscript>U</subscript></highlight><highlight><superscript>&minus;1</superscript></highlight><highlight><italic>A</italic></highlight><highlight><subscript>U </subscript></highlight></in-line-formula></paragraph>
<paragraph id="P-0060" lvl="7"><number>&lsqb;0060&rsqb;</number> where H3true is the exact measured height of the three-inch high calibration target, H1true is the exact measured height of the one-inch high calibration target, P3dist<highlight><subscript>U</subscript></highlight>(n) is the difference in measured three-inch pixel position processed in step <highlight><bold>236</bold></highlight> by taking the centroid of the calibration light stripe subtracting the baseline level then removing the noise using a third-order least squares fit, and P1 dist<highlight><subscript>U</subscript></highlight>(n) is the difference in measured one-inch pixel position processed in step <highlight><bold>230</bold></highlight> by taking the centroid of the calibration light stripe subtracting the baseline level then removing the noise using a third-order least squares fit. </paragraph>
<paragraph id="P-0061" lvl="0"><number>&lsqb;0061&rsqb;</number> There will be a pair of u v calibration values for each column. These u v values are saved to use in calibrating the object height in inches from the centroid data of the light stripes. In step <highlight><bold>242</bold></highlight>, the calibration values for the upper screen image are calculated by performing the following calculation on each column (n) in the image. </paragraph>
<paragraph lvl="0"><in-line-formula>Form the matrix <highlight><italic>M</italic></highlight><highlight><subscript>L</subscript></highlight><highlight><italic>&equals;&lsqb;H</italic></highlight>3true <highlight><italic>H</italic></highlight>3true*<highlight><italic>P</italic></highlight>3dist<highlight><subscript>L</subscript></highlight>(<highlight><italic>n</italic></highlight>) <highlight><italic>H</italic></highlight>1true <highlight><italic>H</italic></highlight>1true*<highlight><italic>P</italic></highlight>1dist<highlight><subscript>L</subscript></highlight>(<highlight><italic>n</italic></highlight>)&rsqb;; form the vector A<highlight><subscript>L</subscript></highlight><highlight><italic>&equals;&lsqb;P</italic></highlight>3dist<highlight><subscript>L</subscript></highlight>(<highlight><italic>n</italic></highlight>) <highlight><italic>P</italic></highlight>1dist<highlight><subscript>L</subscript></highlight>(<highlight><italic>n</italic></highlight>)&rsqb;<highlight><superscript>T</superscript></highlight>; and then compute &lsqb;<highlight><italic>u</italic></highlight><highlight><subscript>L</subscript></highlight><highlight><italic>v</italic></highlight><highlight><subscript>L</subscript></highlight>&rsqb;<highlight><superscript>T</superscript></highlight><highlight><italic>&equals;M</italic></highlight><highlight><subscript>L</subscript></highlight><highlight><superscript>&minus;1</superscript></highlight><highlight><italic>A</italic></highlight><highlight><subscript>L </subscript></highlight></in-line-formula></paragraph>
<paragraph id="P-0062" lvl="7"><number>&lsqb;0062&rsqb;</number> where H3true is the exact measured height of the three-inch high calibration target, H1true is the exact measured height of the one-inch high calibration target, P3dist<highlight><subscript>L</subscript></highlight>(n) is the difference in measured three-inch pixel position processed in Step <highlight><bold>238</bold></highlight> by taking the centroid of the calibration light stripe subtracting the baseline level then removing the noise using a third-order least squares fit, and PI dist<highlight><subscript>L</subscript></highlight>(n) is the difference in measured one-inch pixel position processed in Step <highlight><bold>232</bold></highlight> by taking the centroid of the calibration light stripe subtracting the baseline level then removing the noise using a third-order least squares fit. There will be a pair of u v calibration values for each column. These u v values are saved to use in calibrating the object height in inches from the centroid data of the light stripes. </paragraph>
<paragraph id="P-0063" lvl="0"><number>&lsqb;0063&rsqb;</number> Step <highlight><bold>246</bold></highlight>, <cross-reference target="DRAWINGS">FIG. 21</cross-reference>, begins the process of computing the X axis calibration. This is done to compensate for the distortion in apparent X axis position caused by the height of the surface of the cast. An image of a grid pattern with <highlight><bold>5</bold></highlight> lines per inch is stored on the computer&apos;s disk. This image only needs to be updated when a full system realignment is performed, therefore there is no automatic provision in the software for capturing new images of the grid. In step <highlight><bold>246</bold></highlight> the computer first reads the upper grid image from disk for X axis calibration. Then the valid range of pixels, in both X and Y coordinates, is read form the Settings.txt file. The data will be processed one row at a time, &lcub;i&rcub; specifies the row index and &lcub;j&rcub; specifies the column index. Step <highlight><bold>248</bold></highlight> is a decision: are there more rows to process&quest; If there are more rows, proceed to step <highlight><bold>262</bold></highlight>, if no additional rows remain, the program moves to step <highlight><bold>250</bold></highlight>. In step <highlight><bold>262</bold></highlight>, any lighting nonuniformity in the grid image is removed. First a third-order least squares fit is calculated for the data in row R(i). The least squares fit coefficients are next used to calculate the pixel values for an average noise free row MR(i), where i is the row index variable. The data is then corrected for lighting nonuniformities by calculating a corrected row CR(i)&equals;MR(i) R(i). In step <highlight><bold>266</bold></highlight>, a threshold level is determined for the CR(i) data that will be used to detect the black threshold lines against the white background. First find the 6 highest and lowest pixel values in the row and drop them. Then find the 10 next highest and lowest pixel values and average them independently to determine the mean high and low levels. The threshold is set at 50% of the range between the mean high and mean low pixel values. In step <highlight><bold>268</bold></highlight>, the gridlines are located in the image. First all values in CR(i) less than the threshold level are set equal to zero. Then a peak detection algorithm is applied because the grid line images are wider than a single pixel. Calculate the following quantity (P) that will be equal to 2 for pixels that are greater than their neighbors to the left and right P&equals;CR&gt;CR(shifted left 1 pixel)&plus;CR&gt;CR(shifted right 1 pixel). Step <highlight><bold>270</bold></highlight> is a decision on each threshold computation. If P(j) equals 2 and the pixel values exceeds the threshold by at least 10 units (CR(i,j)&gt;thresh&plus;10) then the data is a grid line center and the program proceeds to step <highlight><bold>276</bold></highlight>. If both of these conditions are not true it is not a grid line center and the program proceeds to step <highlight><bold>272</bold></highlight>. If reaching step <highlight><bold>273</bold></highlight>, the pixel is not a grid center and is therefore set to zero (PP(j))&equals;0). The program now moves to step <highlight><bold>280</bold></highlight>. Step <highlight><bold>276</bold></highlight> computes a Peak Quality Factor (1-3) which is no longer used in subsequent processing (PP(j))&equals;1&plus;P(j&minus;1)&plus;P(j&plus;1)). In step <highlight><bold>278</bold></highlight> a list of peak locations is built by adding the new location to an array (Loc) in the computer&apos;s memory. The program now moves to step <highlight><bold>280</bold></highlight>. </paragraph>
<paragraph id="P-0064" lvl="0"><number>&lsqb;0064&rsqb;</number> In step <highlight><bold>280</bold></highlight>, <cross-reference target="DRAWINGS">FIG. 22</cross-reference>, the distance between peaks of the grid lines is calculated. Using vector calculation notation, the values of the location vector (Loc) are shifted and then subtracted forming a vector of the distances between peaks (Dist) as follows: Dist&equals;Loc(2:end)&minus;Loc(1:end&minus;1). Step <highlight><bold>282</bold></highlight> is a decision used to detect rows containing bad data. First, all values in the vector (Dist) are averaged to find the mean distance between grid lines. Then the mean is subtracted from each element in the vector (Dist) the absolute value is taken and each of these magnitudes are then tested to see if they are greater than 5, abs(Dist&minus;mean(Dist))&gt;5. If any value is greater than 5 proceed to step <highlight><bold>286</bold></highlight>, while if all the values are less than or equal to 5 go to step <highlight><bold>284</bold></highlight>. If the program moves to step <highlight><bold>284</bold></highlight>, the data in this row of pixels from the grid is good, now compute the number of inches per pixel, using the known constant of 0.2 inches per gridline, &lcub;0.2 inches per gridline&rcub;/mean(Dist). Store this value in an array in the computer&apos;s memory paired with the row number. The program now loops back to step <highlight><bold>248</bold></highlight> to process the next row. If the program moves to step <highlight><bold>286</bold></highlight>, the test has failed and this row contains bad data and is discarded. The program now loops back to step <highlight><bold>248</bold></highlight> to process the next row. </paragraph>
<paragraph id="P-0065" lvl="0"><number>&lsqb;0065&rsqb;</number> Step <highlight><bold>250</bold></highlight> follows from step <highlight><bold>248</bold></highlight> when the processing of all rows of the grid image is complete. Now the longitudinal distance calibration must be completed. The software calculates the second-order least squares fit to the data array containing the pixel row number and the inches per pixel data. The three polynomial coefficients from the second-order least squares fit are saved in a disk file for use in calibrating future scan data. Step <highlight><bold>252</bold></highlight> is not really a simple step but a placeholder for the grid calibration process on the lower grid Image. The calibration process previously described, which ran from steps <highlight><bold>246</bold></highlight> through <highlight><bold>286</bold></highlight>, is repeated on the lower grid image. This provides three calibration coefficients for the data displayed in the lower half of the computer&apos;s screen. Step <highlight><bold>254</bold></highlight> is a test on the new calibration coefficients. The new height calibrations are applied to the data from the 0, 1, and 3 inch calibration targets, then statistics are calculated relative to the known heights. Heights are calculated at each pixel along the light stripe and the mean value and standard deviations are calculated for each calibration target. The mean values represent accuracy and should fall within a very small margin of the known true height. The standard deviations present the random error in the reading and must be a small value when the system is working properly. In step <highlight><bold>256</bold></highlight>, the user has the opportunity choose to accept or reject the new calibration values based on the statistics presented in step <highlight><bold>254</bold></highlight>. Pressing on the accept or reject buttons on the computer screen makes the selection. If the calibration is accepted proceed to step <highlight><bold>258</bold></highlight>. If it is rejected the data will not be saved to disk, the prior calibration remains, and control proceeds to step <highlight><bold>260</bold></highlight>. Step <highlight><bold>258</bold></highlight> the user has elected to use the new calibration. The new calibration data is now saved to the disk in place of the previous data. If the new calibration is not accepted the old calibration is retained and the program proceed to step <highlight><bold>260</bold></highlight>. The calibration process is now complete, return to step <highlight><bold>118</bold></highlight>, <cross-reference target="DRAWINGS">FIG. 14</cross-reference>. </paragraph>
<paragraph id="P-0066" lvl="0"><number>&lsqb;0066&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 23</cross-reference> provides detail on the operation called out in step <highlight><bold>140</bold></highlight>, <cross-reference target="DRAWINGS">FIG. 15</cross-reference> for computing the centroid on the light stripe image. The light stripe is several pixels wide with the brightest pixels in the center and the irradiance level fades toward either edge of the stripe. The actual light distribution is continuous, but when imaged with the camera and digitized this is broken into a number of discrete levels. The centroid algorithm uses the information in this digitized distribution to estimate the true center of the distribution, which is also the center of the light stripe. This centroid estimation has sub-pixel accuracy, which significantly enhances the accuracy of the surface height measurement. </paragraph>
<paragraph id="P-0067" lvl="0"><number>&lsqb;0067&rsqb;</number> Step <highlight><bold>300</bold></highlight>, <cross-reference target="DRAWINGS">FIG. 23</cross-reference> represents the entry point to the start of the centroid function. Note that the following steps apply to a single light stripe and this process must be repeated to process all the images in the scan data set. Step <highlight><bold>302</bold></highlight> defines the counter variables used in this process, i represents the row count and j represents the column count. The light stripe image will be processed column by column. The light stripe is horizontal across the image so each column represents a transverse cut through the light stripe. Step <highlight><bold>302</bold></highlight> represents a decision, the column counter j is evaluated to see if more columns remain to be processed. If no more columns remain, proceed to step <highlight><bold>306</bold></highlight>, otherwise proceed to step <highlight><bold>308</bold></highlight> to process more columns. The centroid calculation is complete, return to the calling routine in the program. Step <highlight><bold>308</bold></highlight> there are more columns to process, clear the following variables i&equals;0, Pmax&equals;0, Cmax(j)&equals;0. Step <highlight><bold>310</bold></highlight> represents a decision wherein the row counter (i) is evaluated to see if more rows remain to be processed. If no more rows remain, proceed to step <highlight><bold>316</bold></highlight>, otherwise proceed to step <highlight><bold>312</bold></highlight> to process the next row. Step <highlight><bold>312</bold></highlight> represents a decision. Is the current pixel (row i, column j) greater than a threshold value set in the file Settings.txt, (P(i,j)&gt;Threshold) and is the current pixel greater than the maximum amplitude pixel (P(i,j)&gt;Pmax) for the current row being processed&quest; If both conditions are true, we have a new maximum pixel, proceed to step <highlight><bold>314</bold></highlight>. If either of the conditions are not true, proceed to step <highlight><bold>310</bold></highlight> to process the next row. Step <highlight><bold>314</bold></highlight> updates the maximum pixel by saving its location in Cmax, (Cmax(j)&equals;i) and replacing the value in Pmax with the current pixel amplitude. (Pmax&equals;P(i,j)). Then proceed to step <highlight><bold>310</bold></highlight> to process the next row. Step <highlight><bold>316</bold></highlight> represents a decision made after all the rows in the current column have been processed, does the location of the maximum pixel in the current column fall within the allowed limits&quest;</paragraph>
<paragraph lvl="0"><in-line-formula>Cmax&gt;&equals;Row min &amp;&amp; Cmax&lt;&equals;Row max </in-line-formula></paragraph>
<paragraph id="P-0068" lvl="7"><number>&lsqb;0068&rsqb;</number> If both conditions are true, we have a valid maximum light stripe pixel, proceed to step <highlight><bold>318</bold></highlight> to compute the centroid. If either of the conditions is not true, proceed to step <highlight><bold>322</bold></highlight>. In Step <highlight><bold>322</bold></highlight>, the conditions in step <highlight><bold>316</bold></highlight> have not been met and there is no centroid value for this column. Therefore, the element in the centroid vector for this the jth column is set to zero (Cent(j)&equals;0) and control moves to step <highlight><bold>304</bold></highlight> to process the next column. In step <highlight><bold>318</bold></highlight> a loop counter n is set up and two summation variables are cleared as follows, </paragraph>
<paragraph lvl="0"><in-line-formula><highlight><italic>n&equals;</italic></highlight>Cmax(<highlight><italic>j</italic></highlight>)&minus;10, Sum1&equals;0, Sum2&equals;0. </in-line-formula></paragraph>
<paragraph id="P-0069" lvl="0"><number>&lsqb;0069&rsqb;</number> The centroid calculation will be performed on a span of 20 pixels centered around the brightest pixel in the light stripe. This &plus;/&minus; 10 pixel band will completely cover the entire range of the light stripe with a margin of dark pixels on each side. Step <highlight><bold>320</bold></highlight> contains a decision that forms a loop counter for the &plus;/&minus; 10 pixel band around the light stripe. The loop is terminated when counter n (which began at center value minus ten) exceeds the center pixel value plus ten (n&gt;Cmax(j))&plus;10). If n is still inside the &plus;/&minus;10 pixel band control moves to step <highlight><bold>324</bold></highlight> to process the next pixel, otherwise control moves to step <highlight><bold>326</bold></highlight> to complete the centroid calculation for that column. In step <highlight><bold>324</bold></highlight> two sums are computed. Sum1 is the sum of the pixel value multiplied by the row number, while Sum2 is the direct sum of the pixel values. The row counter n is also incremented as follows: Sum1&equals;Sum1&plus;P(n,j)*n, Sum2&equals;Sum2 &plus;P(n,j), n&equals;n&plus;1. </paragraph>
<paragraph id="P-0070" lvl="0"><number>&lsqb;0070&rsqb;</number> Control now returns to Step <highlight><bold>320</bold></highlight> to process the next pixel in the centroid calculation. Reaching step <highlight><bold>326</bold></highlight>, the centroid calculation for this column is complete and the centroid value is found by dividing Sum1 by Sum2 as follows Cent(j))&equals;Sum1/Sum2. Control then moves to Step <highlight><bold>304</bold></highlight> to process the next column. </paragraph>
<paragraph id="P-0071" lvl="0"><number>&lsqb;0071&rsqb;</number> The flowchart of <cross-reference target="DRAWINGS">FIG. 24</cross-reference> provides detail on the height calibration routine, called out in step <highlight><bold>140</bold></highlight>, <cross-reference target="DRAWINGS">FIG. 15</cross-reference> that takes the centroid data from one light strip image and computes an accurate estimate of the object&apos;s height at each centroid value. This calibration is called twice for each image in the scan, once for the upper image and the second time for the lower image. This routine applies the arrays of u and v calibration values computed in the calibration routine to process the centroid data. </paragraph>
<paragraph id="P-0072" lvl="0"><number>&lsqb;0072&rsqb;</number> Step <highlight><bold>330</bold></highlight>, <cross-reference target="DRAWINGS">FIG. 24</cross-reference> is the start of the height calibration routine. Processing is done element by element on the vector of results from the centroid operation. Each element represents an image column and the upper and lower light stripes are processed separately with different baseline and calibration values. When the 640&times;480 image is split at line 240, pixels 1:240 are processed for the top stripe and pixels 240:480 for bottom stripe, then the value BToffset&equals;0 is used when processing the top portion and BToffset&equals;239 when processing the bottom portion of the image. Step <highlight><bold>332</bold></highlight> is a decision, do more elements (i) remain in the vector of centroid values to be processed&quest; If no more elements remain, proceed to step <highlight><bold>334</bold></highlight>, otherwise continue on step <highlight><bold>336</bold></highlight>. Step <highlight><bold>334</bold></highlight> processing of the set of centroid values is complete, return to the Step <highlight><bold>140</bold></highlight> with the vector (Z) of the calibrated heights. In step <highlight><bold>336</bold></highlight> the difference pixel position (dP) is calculated by subtracting the baseline value, determined in the calibration routine, and a bottom offset (BToffset) that compensates for bottom or top image position, in the following calculation dP(i)&equals;Cent(i)&minus;Baseline(i)&plus;BToffset. In step <highlight><bold>338</bold></highlight> a decision is made to detect centroid values that were set to zero. If dP(i) equals (BToffset&minus;Baseline(i)) then there is no data in this element, proceed to step <highlight><bold>340</bold></highlight>, otherwise proceed to step <highlight><bold>342</bold></highlight>. Step <highlight><bold>340</bold></highlight>, because there is no data at this index (i) location in the centroid vector, set the height values equal to zero (Z(i)&equals;0) and proceed to step <highlight><bold>332</bold></highlight> for the next element (i) in the centroid vector (Cent(i)). Step <highlight><bold>342</bold></highlight> the data is valid and the height calibration will be performed. Calibration is performed using the following equation, &lcub;ul indicates upper or lower index&rcub;</paragraph>
<paragraph lvl="0"><in-line-formula><highlight><italic>Z</italic></highlight>(<highlight><italic>i</italic></highlight>)&equals;<highlight><italic>dP</italic></highlight>(<highlight><italic>i</italic></highlight>)/&lsqb;<highlight><italic>U</italic></highlight><highlight><subscript>ul</subscript></highlight>(<highlight><italic>i</italic></highlight>)&plus;<highlight><italic>V</italic></highlight><highlight><subscript>ul</subscript></highlight>(<highlight><italic>i</italic></highlight>)*<highlight><italic>dP</italic></highlight>(<highlight><italic>i</italic></highlight>)&rsqb;. </in-line-formula></paragraph>
<paragraph id="P-0073" lvl="7"><number>&lsqb;0073&rsqb;</number> Now proceed to step <highlight><bold>332</bold></highlight> for the next element (i) in the centroid vector (Cent(i)). </paragraph>
<paragraph id="P-0074" lvl="0"><number>&lsqb;0074&rsqb;</number> Although specific features of the invention are shown in some drawings and not in others, this is for convenience only as each feature may be combined with any or all of the other features in accordance with the invention. The words &ldquo;including&rdquo;, &ldquo;comprising&rdquo;, &ldquo;having&rdquo;, and &ldquo;with&rdquo; as used herein are to be interpreted broadly and comprehensively and are not limited to any physical interconnection. Moreover, any embodiments disclosed in the subject application are not to be taken as the only possible embodiments. </paragraph>
<paragraph id="P-0075" lvl="0"><number>&lsqb;0075&rsqb;</number> Other embodiments will occur to those skilled in the art and are within the following claims:</paragraph>
</section>
</detailed-description>
</subdoc-description>
<subdoc-claims>
<heading lvl="1">What is claimed is: </heading>
<claim id="CLM-00001">
<claim-text><highlight><bold>1</bold></highlight>. A non-contacting mensuration system comprising: 
<claim-text>a light source for projecting light on a target; </claim-text>
<claim-text>an imager having an image plane; </claim-text>
<claim-text>a first optical subsystem configured to project a first image of the light on the target onto a first portion of the image plane of the imager; and </claim-text>
<claim-text>a second optical subsystem configured to project a second image of the light on the target onto a second portion of the image plane of the imager so that two different images are obtained from different aspects simultaneously. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00002">
<claim-text><highlight><bold>2</bold></highlight>. The non-contacting mensuration system of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference> in which the light source is a laser. </claim-text>
</claim>
<claim id="CLM-00003">
<claim-text><highlight><bold>3</bold></highlight>. The non-contacting mensuration system of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference> in which the light source is configured to project a two-dimensional light pattern on the target. </claim-text>
</claim>
<claim id="CLM-00004">
<claim-text><highlight><bold>4</bold></highlight>. The non-contacting mensuration system of <dependent-claim-reference depends_on="CLM-00003">claim 3</dependent-claim-reference> in which the two dimensional light pattern is a stripe. </claim-text>
</claim>
<claim id="CLM-00005">
<claim-text><highlight><bold>5</bold></highlight>. The non-contacting mensuration system of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference> in which the imager is a single CCD camera. </claim-text>
</claim>
<claim id="CLM-00006">
<claim-text><highlight><bold>6</bold></highlight>. The non-contacting mensuration system of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference> in which the first optical subsystem includes a first mirror on one side of the light source oriented to view the first image from a first aspect and a second mirror behind the light source oriented to direct the first image onto the first portion of the image plane. </claim-text>
</claim>
<claim id="CLM-00007">
<claim-text><highlight><bold>7</bold></highlight>. The non-contacting mensuration system of <dependent-claim-reference depends_on="CLM-00006">claim 6</dependent-claim-reference> in which the second optical subsystem includes a third mirror on another side of the light source oriented to view the second image from a second aspect and a fourth mirror behind the light source oriented to direct the second image onto the second portion of the image plane. </claim-text>
</claim>
<claim id="CLM-00008">
<claim-text><highlight><bold>8</bold></highlight>. The non-contacting mensuration system of <dependent-claim-reference depends_on="CLM-00007">claim 7</dependent-claim-reference> in which the second mirror and the fourth mirror are opposing faces of a prism. </claim-text>
</claim>
<claim id="CLM-00009">
<claim-text><highlight><bold>9</bold></highlight>. The non-contacting mensuration system of <dependent-claim-reference depends_on="CLM-00006">claim 6</dependent-claim-reference> in which the first mirror is oriented to provide a 30&deg; observation angle. </claim-text>
</claim>
<claim id="CLM-00010">
<claim-text><highlight><bold>10</bold></highlight>. The non-contacting mensuration system of <dependent-claim-reference depends_on="CLM-00007">claim 7</dependent-claim-reference> in which the third mirror is oriented to provide a 30&deg; observation angle. </claim-text>
</claim>
<claim id="CLM-00011">
<claim-text><highlight><bold>11</bold></highlight>. The non-contacting mensuration system of <dependent-claim-reference depends_on="CLM-00008">claim 8</dependent-claim-reference> in which the light source is positioned directly in front of the image plane of the imager. </claim-text>
</claim>
<claim id="CLM-00012">
<claim-text><highlight><bold>12</bold></highlight>. The non-contacting mensuration system of <dependent-claim-reference depends_on="CLM-00011">claim 11</dependent-claim-reference> in which the prism is positioned between the light source and the image plane of the imager. </claim-text>
</claim>
<claim id="CLM-00013">
<claim-text><highlight><bold>13</bold></highlight>. The non-contacting mensuration system of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference> further including means for moving the target relative the light source. </claim-text>
</claim>
<claim id="CLM-00014">
<claim-text><highlight><bold>14</bold></highlight>. The non-contacting mensuration system of <dependent-claim-reference depends_on="CLM-00011">claim 13</dependent-claim-reference> in which the means for moving the target includes a turntable upon which the target is placed. </claim-text>
</claim>
<claim id="CLM-00015">
<claim-text><highlight><bold>15</bold></highlight>. A non-contacting mensuration system comprising: 
<claim-text>a light source for projecting light onto a target; </claim-text>
<claim-text>an imager having an image plane; </claim-text>
<claim-text>one mirror located on one side of the light source and oriented to view the target from a first aspect; </claim-text>
<claim-text>another mirror located on another side of the light source and oriented to view the target from a second aspect; and </claim-text>
<claim-text>a prism positioned between the light source and the image plane of the imager having opposing mirror faces oriented to direct images from the two mirrors onto first and second portions on the image plane of the imager simultaneously.</claim-text>
</claim-text>
</claim>
</subdoc-claims>
<subdoc-drawings id="DRAWINGS">
<heading lvl="0" align="CENTER">Drawings</heading>
<representative-figure>5</representative-figure>
<figure id="figure-D00000">
<image id="EMI-D00000" file="US20030002051A1-20030102-D00000.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00001">
<image id="EMI-D00001" file="US20030002051A1-20030102-D00001.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00002">
<image id="EMI-D00002" file="US20030002051A1-20030102-D00002.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00003">
<image id="EMI-D00003" file="US20030002051A1-20030102-D00003.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00004">
<image id="EMI-D00004" file="US20030002051A1-20030102-D00004.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00005">
<image id="EMI-D00005" file="US20030002051A1-20030102-D00005.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00006">
<image id="EMI-D00006" file="US20030002051A1-20030102-D00006.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00007">
<image id="EMI-D00007" file="US20030002051A1-20030102-D00007.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00008">
<image id="EMI-D00008" file="US20030002051A1-20030102-D00008.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00009">
<image id="EMI-D00009" file="US20030002051A1-20030102-D00009.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00010">
<image id="EMI-D00010" file="US20030002051A1-20030102-D00010.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00011">
<image id="EMI-D00011" file="US20030002051A1-20030102-D00011.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00012">
<image id="EMI-D00012" file="US20030002051A1-20030102-D00012.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00013">
<image id="EMI-D00013" file="US20030002051A1-20030102-D00013.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00014">
<image id="EMI-D00014" file="US20030002051A1-20030102-D00014.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00015">
<image id="EMI-D00015" file="US20030002051A1-20030102-D00015.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00016">
<image id="EMI-D00016" file="US20030002051A1-20030102-D00016.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00017">
<image id="EMI-D00017" file="US20030002051A1-20030102-D00017.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00018">
<image id="EMI-D00018" file="US20030002051A1-20030102-D00018.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00019">
<image id="EMI-D00019" file="US20030002051A1-20030102-D00019.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00020">
<image id="EMI-D00020" file="US20030002051A1-20030102-D00020.TIF" imf="TIFF" ti="DR"/>
</figure>
</subdoc-drawings>
</patent-application-publication>
