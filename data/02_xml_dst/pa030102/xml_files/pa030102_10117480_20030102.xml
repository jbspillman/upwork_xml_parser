<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE patent-application-publication SYSTEM "pap-v16-2002-01-01.dtd" [
<!ENTITY US20030004724A1-20030102-M00001.NB SYSTEM "US20030004724A1-20030102-M00001.NB" NDATA NB>
<!ENTITY US20030004724A1-20030102-M00001.TIF SYSTEM "US20030004724A1-20030102-M00001.TIF" NDATA TIF>
<!ENTITY US20030004724A1-20030102-D00000.TIF SYSTEM "US20030004724A1-20030102-D00000.TIF" NDATA TIF>
<!ENTITY US20030004724A1-20030102-D00001.TIF SYSTEM "US20030004724A1-20030102-D00001.TIF" NDATA TIF>
<!ENTITY US20030004724A1-20030102-D00002.TIF SYSTEM "US20030004724A1-20030102-D00002.TIF" NDATA TIF>
<!ENTITY US20030004724A1-20030102-D00003.TIF SYSTEM "US20030004724A1-20030102-D00003.TIF" NDATA TIF>
<!ENTITY US20030004724A1-20030102-D00004.TIF SYSTEM "US20030004724A1-20030102-D00004.TIF" NDATA TIF>
<!ENTITY US20030004724A1-20030102-D00005.TIF SYSTEM "US20030004724A1-20030102-D00005.TIF" NDATA TIF>
<!ENTITY US20030004724A1-20030102-D00006.TIF SYSTEM "US20030004724A1-20030102-D00006.TIF" NDATA TIF>
<!ENTITY US20030004724A1-20030102-D00007.TIF SYSTEM "US20030004724A1-20030102-D00007.TIF" NDATA TIF>
<!ENTITY US20030004724A1-20030102-D00008.TIF SYSTEM "US20030004724A1-20030102-D00008.TIF" NDATA TIF>
<!ENTITY US20030004724A1-20030102-D00009.TIF SYSTEM "US20030004724A1-20030102-D00009.TIF" NDATA TIF>
<!ENTITY US20030004724A1-20030102-D00010.TIF SYSTEM "US20030004724A1-20030102-D00010.TIF" NDATA TIF>
<!ENTITY US20030004724A1-20030102-D00011.TIF SYSTEM "US20030004724A1-20030102-D00011.TIF" NDATA TIF>
<!ENTITY US20030004724A1-20030102-D00012.TIF SYSTEM "US20030004724A1-20030102-D00012.TIF" NDATA TIF>
<!ENTITY US20030004724A1-20030102-D00013.TIF SYSTEM "US20030004724A1-20030102-D00013.TIF" NDATA TIF>
]>
<patent-application-publication>
<subdoc-bibliographic-information>
<document-id>
<doc-number>20030004724</doc-number>
<kind-code>A1</kind-code>
<document-date>20030102</document-date>
</document-id>
<publication-filing-type>new</publication-filing-type>
<domestic-filing-data>
<application-number>
<doc-number>10117480</doc-number>
</application-number>
<application-number-series-code>10</application-number-series-code>
<filing-date>20020405</filing-date>
</domestic-filing-data>
<foreign-priority-data>
<priority-application-number>
<doc-number>PCT/US01/1760</doc-number>
</priority-application-number>
<filing-date>20010531</filing-date>
<country-code></country-code>
</foreign-priority-data>
<technical-information>
<classification-ipc>
<classification-ipc-primary>
<ipc>G10L013/08</ipc>
</classification-ipc-primary>
<classification-ipc-secondary>
<ipc>G10L011/00</ipc>
</classification-ipc-secondary>
<classification-ipc-edition>07</classification-ipc-edition>
</classification-ipc>
<classification-us>
<classification-us-primary>
<uspc>
<class>704</class>
<subclass>260000</subclass>
</uspc>
</classification-us-primary>
<classification-us-secondary>
<uspc>
<class>704</class>
<subclass>278000</subclass>
</uspc>
</classification-us-secondary>
</classification-us>
<title-of-invention>Speech recognition program mapping tool to align an audio file to verbatim text</title-of-invention>
</technical-information>
<continuity-data>
<continuations>
<continuation-in-part-of>
<parent-child>
<child>
<document-id>
<doc-number>10117480</doc-number>
<kind-code>A1</kind-code>
<document-date>20020405</document-date>
</document-id>
</child>
<parent>
<document-id>
<doc-number>10014677</doc-number>
<document-date>20011211</document-date>
<country-code>US</country-code>
</document-id>
</parent>
<parent-status>PENDING</parent-status>
</parent-child>
</continuation-in-part-of>
</continuations>
<non-provisional-of-provisional>
<document-id>
<doc-number>60118949</doc-number>
<document-date>19990205</document-date>
<country-code>US</country-code>
</document-id>
</non-provisional-of-provisional>
<non-provisional-of-provisional>
<document-id>
<doc-number>60120997</doc-number>
<document-date>19990219</document-date>
<country-code>US</country-code>
</document-id>
</non-provisional-of-provisional>
<non-provisional-of-provisional>
<document-id>
<doc-number>60208878</doc-number>
<document-date>20000601</document-date>
<country-code>US</country-code>
</document-id>
</non-provisional-of-provisional>
<non-provisional-of-provisional>
<document-id>
<doc-number>60208994</doc-number>
<document-date>20000601</document-date>
<country-code>US</country-code>
</document-id>
</non-provisional-of-provisional>
<non-provisional-of-provisional>
<document-id>
<doc-number>60253632</doc-number>
<document-date>20001128</document-date>
<country-code>US</country-code>
</document-id>
</non-provisional-of-provisional>
</continuity-data>
<inventors>
<first-named-inventor>
<name>
<given-name>Jonathan</given-name>
<family-name>Kahn</family-name>
</name>
<residence>
<residence-us>
<city>Crown Point</city>
<state>IN</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</first-named-inventor>
<inventor>
<name>
<given-name>Michael</given-name>
<middle-name>C.</middle-name>
<family-name>Huttinger</family-name>
</name>
<residence>
<residence-us>
<city>Valparaiso</city>
<state>IN</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
<inventor>
<name>
<given-name>Stephen</given-name>
<middle-name>J.</middle-name>
<family-name>Scalpone</family-name>
</name>
<residence>
<residence-us>
<city>Portland</city>
<state>OR</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
</inventors>
<correspondence-address>
<name-1>SONNENSCHEIN NATH &amp; ROSENTHAL</name-1>
<name-2>Wacker Drive Station, Sears Tower</name-2>
<address>
<address-1>P.O. Box 061080</address-1>
<city>Chicago</city>
<state>IL</state>
<postalcode>60606-1080</postalcode>
<country>
<country-code>US</country-code>
</country>
</address>
</correspondence-address>
</subdoc-bibliographic-information>
<subdoc-abstract>
<paragraph id="A-0001" lvl="0">The invention includes a method to determine time location of at least one audio segment in an original audio file comprising: (a) receiving the original audio file; (b) transcribing a current audio segment from the original audio file using speech recognition software; (c) extracting a transcribed element and a binary audio stream corresponding to the transcribed element from the speech recognition software; (d) saving an association between the transcribed element and the corresponding binary audio stream; (e) repeating (b) through (d) for each audio segment in the original audio file; (f) for each transcribed element, searching for the associated binary audio stream in the original audio file, while tracking an end time location of that search within the original audio file; and (g) inserting the end time location for each binary audio stream into the transcribed element-corresponding binary audio stream association. </paragraph>
</subdoc-abstract>
<subdoc-description>
<summary-of-invention>
<section>
<heading lvl="1">BACKGROUND OF THE INVENTION </heading>
<paragraph id="P-0001" lvl="0"><number>&lsqb;0001&rsqb;</number> 1. Field of the Invention </paragraph>
<paragraph id="P-0002" lvl="0"><number>&lsqb;0002&rsqb;</number> The present invention relates to speech recognition and to a system to use word mapping between verbatim text and computer transcribed text to increase speech engine accuracy. </paragraph>
<paragraph id="P-0003" lvl="0"><number>&lsqb;0003&rsqb;</number> 2. Background Information </paragraph>
<paragraph id="P-0004" lvl="0"><number>&lsqb;0004&rsqb;</number> Speech recognition programs that automatically convert speech into text have been under continuous development since the 1980s. The first programs required the speaker to speak with clear pauses between each word to help the program separate one word from the next. One example of such a program was DragonDictate, a discrete speech recognition program originally produced by Dragon Systems, Inc. (Newton, Mass.). </paragraph>
<paragraph id="P-0005" lvl="0"><number>&lsqb;0005&rsqb;</number> In 1994, Philips Dictation Systems of Vienna, Austria introduced the first commercial, continuous speech recognition system. See, Judith A. Markowitz, Using Speech Recognition (1996), pp. 200-06. Currently, the two most widely used off-the-shelf continuous speech recognition programs are Dragon NaturallySpeaking&trade; (now produced by ScanSoft, Inc., Peabody, Mass.) and IBM Viavoice&trade; (manufactured by IBM, Armonk, N.Y.). The focus of the off-the-shelf Dragon NaturallySpeaking&trade; and IBM Viavoice&trade; products has been direct dictation into the computer and correction by the user of misrecognized text. Both the Dragon NaturallySpeaking&trade; and IBM Viavoice&trade; programs are available in a variety of languages and versions and have a software development kit (&ldquo;SDK&rdquo;) available for independent speech vendors. </paragraph>
<paragraph id="P-0006" lvl="0"><number>&lsqb;0006&rsqb;</number> Conventional continuous speech recognition programs are speaker dependent and require creation of an initial speech user profile by each speaker. This &ldquo;enrollment&rdquo; generally takes about a half-hour for each user. It usually includes calibration, text reading (dictation), and vocabulary selection. With calibration, the speaker adjusts the microphone output to insure adequate audio signal and minimal background noise. Then the speaker dictates a standard text provided by the program into a microphone connected to a handheld recorder or computer. The speech recognition program correlates the spoken word with the pre-selected text excerpt. It uses the correlation to establish an initial speech user profile based on that user&apos;s speech characteristics. </paragraph>
<paragraph id="P-0007" lvl="0"><number>&lsqb;0007&rsqb;</number> If the speaker uses different types of microphones or handheld recorders, an enrollment must be completed for each since the acoustic characteristics of each input device differ substantially. In fact, it is recommended a separate enrollment be performed on each computer having a different manufacturer&apos;s or type of sound card because the different characteristics of the analog to digital conversion may substantially affect recognition accuracy. For this reason, many speech recognition manufacturers advocate a speaker&apos;s use of a single microphone that can digitize the analog signal external to the sound card, thereby obviating the problem of dictating at different computers with different sound cards. </paragraph>
<paragraph id="P-0008" lvl="0"><number>&lsqb;0008&rsqb;</number> Finally, the speaker must specify the reference vocabulary that will be used by the program in selecting the words to be transcribed. Various vocabularies like &ldquo;General English,&rdquo; &ldquo;Medical,&rdquo; &ldquo;Legal,&rdquo; and &ldquo;Business&rdquo; are usually available. Sometimes the program can add additional words from the user&apos;s documents or analyze these documents for word use frequency. Adding the user&apos;s words and analyzing the word use pattern can help the program better understand what words the speaker is most likely to use. </paragraph>
<paragraph id="P-0009" lvl="0"><number>&lsqb;0009&rsqb;</number> Once enrollment is completed, the user may begin dictating into the speech recognition program or applications such as conventional word processors like MS Word&trade; (Microsoft Corporation, Redmond, Wash.) or Wordperfect&trade; (Corel Corporation, Ottawa, Ontario, Canada). Recognition accuracy is often low, for example, 60-70%. To improve accuracy, the user may repeat the process of reading a standard text provided by the speech recognition program. The speaker may also select a word and record the audio for that word into the speech recognition program. In addition, written-spokens may be created. The speaker selects a word that is often incorrectly transcribed and types in the word&apos;s phonetic pronunciation in a special speech recognition window. </paragraph>
<paragraph id="P-0010" lvl="0"><number>&lsqb;0010&rsqb;</number> Most commonly, &ldquo;corrective adaptation&rdquo; is used whereby the system learns from its mistakes. The user dictates into the system. It transcribes the text. The user corrects the misrecognized text in a special correction window. In addition to seeing the transcribed text, the speaker may listen to the aligned audio by selecting the desired text and depressing a play button provided by the speech recognition program. Listening to the audio, the speaker can make a determination as to whether the transcribed text matches the audio or whether the text has been misrecognized. With repeated correction, system accuracy often gradually improves, sometimes up to as high as 95-98%. Even with 90% accuracy, the user must correct about one word a sentence, a process that slows down a busy dictating lawyer, physician, or business user. Due to the long training time and limited accuracy, many users have given up using speech recognition in frustration. Many current users are those who have no other choice, for example, persons who are unable to type, such as paraplegics or patients with severe repetitive stress disorder. </paragraph>
<paragraph id="P-0011" lvl="0"><number>&lsqb;0011&rsqb;</number> In the correction process, whether performed by the speaker or editor, it is important that verbatim text is used to correct the misrecognized text. Correction using the wrong word will incorrectly &ldquo;teach&rdquo; the system and result in decreased accuracy. Very often the verbatim text is substantially different from the final text for a printed report or document. Any experienced transcriptionist will testify as to the frequent required editing of text to correct errors that the speaker made or other changes necessary to improve grammar or content. For example, the speaker may say &ldquo;left&rdquo; when he or she meant &ldquo;right,&rdquo; or add extraneous instructions to the dictation that must be edited out, such as, &ldquo;Please send a copy of this report to Mr. Smith.&rdquo; Consequently, the final text can often not be used as verbatim text to train the system. </paragraph>
<paragraph id="P-0012" lvl="0"><number>&lsqb;0012&rsqb;</number> With conventional speech recognition products, generation of verbatim text by an editor during &ldquo;delegated correction&rdquo; is often not easy or convenient. First, after a change is made in the speech recognition text processor, the audio-text alignment in the text may be lost. If a change was made to generate a final report or document, the editor does not have an easy way to play back the audio and hear what was said. Once the selected text in the speech recognition text window is changed, the audio text alignment may not be maintained. For this reason, the editor often cannot select the corrected text and listen to the audio to generate the verbatim text necessary for training. Second, current and previous versions of off-the-shelf Dragon NaturallySpeaking&trade; and IBM Viavoice&trade; SDK programs, for example, do not provide separate windows to prepare and separately save verbatim text and final text. If the verbatim text is entered into the text processor correction window, this is the text that appears in the application window for the final document or report, regardless of how different it is from the verbatim text. Similar problems may be found with products developed by independent speech vendors using, for example, the IBM Viavoice&trade; speech recognition engine and providing for editing in commercially available word processors such as Word or WordPerfect. </paragraph>
<paragraph id="P-0013" lvl="0"><number>&lsqb;0013&rsqb;</number> Another problem with conventional speech recognition programs is the large size of the session files. As noted above, session files include text and aligned audio. By opening a session file, the text appears in the application text processor window. If the speaker selects a word or phrase to play the associated audio, the audio can be played back using a hot key or button. For Dragon NaturallySpeaking&trade; and IBM Viavoice&trade; SDK session files, the session files reach about a megabyte for every minute of dictation. For example, if the dictation is 30 minutes long, the resulting session file will be approximately 30 megabytes. These files cannot be substantially compressed using standard software techniques. Even if the task of correcting a session file could be delegated to an editor in another city, state, or country, there would be substantial bandwidth problems in transmitting the session file for correction by that editor. The problem is obviously compounded if there are multiple, long dictations to be sent. Until sufficient high-speed Internet connection or other transfer protocol come into existence, it may be difficult to transfer even a single dictation session file to a remote editor. A similar problem would be encountered in attempting to implement the remote editing features using the standard session files available in the Dragon NaturallySpeaking&trade; and IBM Viavoice&trade; SDK. </paragraph>
<paragraph id="P-0014" lvl="0"><number>&lsqb;0014&rsqb;</number> Accordingly, it is an object of the present invention to provide a system that offers training of the speech recognition program transparent to the end-users by performing an enrollment for them. It is an associated object to develop condensed session files for rapid transmission to remote editors. An additional associated object is to develop a convenient system for generation of verbatim text for speech recognition training through use of multiple linked windows in a text processor. It is another associated object to facilitate speech recognition training by use of a word mapping system for transcribed and verbatim text that has the effect of permanently aligning the audio with the verbatim text. </paragraph>
<paragraph id="P-0015" lvl="0"><number>&lsqb;0015&rsqb;</number> These and other objects will be apparent to those of ordinary skill in the art having the present drawings, specifications, and claims before them. </paragraph>
</section>
<section>
<heading lvl="1">SUMMARY OF THE INVENTION </heading>
<paragraph id="P-0016" lvl="0"><number>&lsqb;0016&rsqb;</number> The present invention relates to a method to determine time location of at least one audio segment in an original audio file. The method includes (a) receiving the original audio file; (b) transcribing a current audio segment from the original audio file using speech recognition software; (c) extracting a transcribed element and a binary audio stream corresponding to the transcribed element from the speech recognition software; (d) saving an association between the transcribed element and the corresponding binary audio stream; (e) repeating (b) through (d) for each audio segment in the original audio file; (f) for each transcribed element, searching for the associated binary audio stream in the original audio file, while tracking an end time location of that search within the original audio file; and (g) inserting the end time location for each binary audio stream into the transcribed element-corresponding binary audio stream association. </paragraph>
<paragraph id="P-0017" lvl="0"><number>&lsqb;0017&rsqb;</number> In a preferred embodiment of the invention, searching includes removing any DC offset from the corresponding binary audio stream. Removing the DC offset may include taking a derivative of the corresponding binary audio stream to produce a derivative binary audio stream. The method may further include taking a derivative of a segment of the original audio file to produce a derivative audio segment; and searching for the derivative binary audio stream in the derivative audio segment. </paragraph>
<paragraph id="P-0018" lvl="0"><number>&lsqb;0018&rsqb;</number> In another preferred embodiment, the method may include saving each transcribed element-corresponding binary audio stream association in a single file. The single file may include, for each word saved, a text for the transcribed element and a pointer to the binary audio stream. </paragraph>
<paragraph id="P-0019" lvl="0"><number>&lsqb;0019&rsqb;</number> In yet another embodiment, extracting may be performed by using the Microsoft Speech API as an interface to the speech recognition software, wherein the speech recognition software does not return a word with a corresponding audio stream. </paragraph>
<paragraph id="P-0020" lvl="0"><number>&lsqb;0020&rsqb;</number> The invention also includes 15 a system for determining a time location of at least one audio segment in an original audio file. The system may include a storage device for storing the original audio file and a speech recognition engine to transcribe a current audio segment from the original audio file. The system also includes a program that extracts a transcribed element and a binary audio stream file corresponding to the transcribed element from the speech recognition software; saves an association between the transcribed element and the corresponding binary audio stream into a session file; searches for the binary audio stream audio stream in the original audio file; and inserts the end time location for each binary audio stream into the transcribed element-corresponding binary audio stream association. </paragraph>
<paragraph id="P-0021" lvl="0"><number>&lsqb;0021&rsqb;</number> The invention further includes a system for determining a time location of at least one audio segment in an original audio file comprising means for receiving the original audio file; means for transcribing a current audio segment from the original audio file using speech recognition software; means for extracting a transcribed element and a binary audio stream corresponding to the transcribed element from the speech recognition program; means for saving an association between the transcribed element and the corresponding binary audio stream; means for searching for the associated binary audio stream in the original audio file, while tracking an end time location of that search within the original audio file; and means for inserting the end time location for the binary audio stream into the transcribed element-corresponding binary audio stream association. </paragraph>
</section>
</summary-of-invention>
<brief-description-of-drawings>
<section>
<heading lvl="1">BRIEF DESCRIPTION OF THE DRAWINGS </heading>
<paragraph id="P-0022" lvl="0"><number>&lsqb;0022&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 1</cross-reference> is a block diagram of one potential embodiment of a computer within a system <highlight><bold>100</bold></highlight>; </paragraph>
<paragraph id="P-0023" lvl="0"><number>&lsqb;0023&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 2</cross-reference> includes a flow diagram that illustrates a process <highlight><bold>200</bold></highlight> of the invention; </paragraph>
<paragraph id="P-0024" lvl="0"><number>&lsqb;0024&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 3</cross-reference> of the drawings is a view of an exemplary graphical user interface <highlight><bold>300</bold></highlight> to support the present invention; </paragraph>
<paragraph id="P-0025" lvl="0"><number>&lsqb;0025&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 4</cross-reference> illustrates a text A <highlight><bold>400</bold></highlight>; </paragraph>
<paragraph id="P-0026" lvl="0"><number>&lsqb;0026&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 5</cross-reference> illustrates a text B <highlight><bold>500</bold></highlight>; </paragraph>
<paragraph id="P-0027" lvl="0"><number>&lsqb;0027&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 6</cross-reference> of the drawings is a view of an exemplary graphical user interface <highlight><bold>600</bold></highlight> to support the present invention; </paragraph>
<paragraph id="P-0028" lvl="0"><number>&lsqb;0028&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 7</cross-reference> illustrates an example of a mapping window <highlight><bold>700</bold></highlight>; </paragraph>
<paragraph id="P-0029" lvl="0"><number>&lsqb;0029&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 8</cross-reference> illustrates options <highlight><bold>800</bold></highlight> having automatic mapping options for the word mapping tool <highlight><bold>235</bold></highlight> of the invention; </paragraph>
<paragraph id="P-0030" lvl="0"><number>&lsqb;0030&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 9</cross-reference> of the drawings is a view of an exemplary graphical user interface <highlight><bold>900</bold></highlight> to support the present invention; </paragraph>
<paragraph id="P-0031" lvl="0"><number>&lsqb;0031&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 10</cross-reference> is a flow diagram that illustrates a process <highlight><bold>1000</bold></highlight> of the invention; </paragraph>
<paragraph id="P-0032" lvl="0"><number>&lsqb;0032&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 11</cross-reference> is a flow diagram illustrating step <highlight><bold>1060</bold></highlight> of process <highlight><bold>1000</bold></highlight>; and </paragraph>
<paragraph id="P-0033" lvl="0"><number>&lsqb;0033&rsqb;</number> <cross-reference target="DRAWINGS">FIGS. 12</cross-reference><highlight><italic>a</italic></highlight>-<highlight><bold>12</bold></highlight><highlight><italic>c </italic></highlight>illustrate one example of the process <highlight><bold>1000</bold></highlight>. </paragraph>
</section>
</brief-description-of-drawings>
<detailed-description>
<section>
<heading lvl="1">DETAILED DESCRIPTION OF THE INVENTION </heading>
<paragraph id="P-0034" lvl="0"><number>&lsqb;0034&rsqb;</number> While the present invention may be embodied in many different forms, the drawings and discussion are presented with the understanding that the present disclosure is an exemplification of the principles of the invention and is not intended to limit the invention to the embodiments illustrated. </paragraph>
<paragraph id="P-0035" lvl="7"><number>&lsqb;0035&rsqb;</number> I. System <highlight><bold>100</bold></highlight> </paragraph>
<paragraph id="P-0036" lvl="0"><number>&lsqb;0036&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 1</cross-reference> is a block diagram of one potential embodiment of a computer within a system <highlight><bold>100</bold></highlight>. The system <highlight><bold>100</bold></highlight> may be part of a speech recognition system of the invention. Alternatively, the speech recognition system of the invention may be employed as part of the system <highlight><bold>100</bold></highlight>. </paragraph>
<paragraph id="P-0037" lvl="0"><number>&lsqb;0037&rsqb;</number> The system <highlight><bold>100</bold></highlight> may include input/output devices, such as a digital recorder <highlight><bold>102</bold></highlight>, a microphone <highlight><bold>104</bold></highlight>, a mouse <highlight><bold>106</bold></highlight>, a keyboard <highlight><bold>108</bold></highlight>, and a video monitor <highlight><bold>110</bold></highlight>. The microphone <highlight><bold>104</bold></highlight> may include, but not be limited to, microphone on telephone. Moreover, the system <highlight><bold>100</bold></highlight> may include a computer <highlight><bold>120</bold></highlight>. As a machine that performs calculations automatically, the computer <highlight><bold>120</bold></highlight> may include input and output (I/O) devices, memory, and a central processing unit (CPU). </paragraph>
<paragraph id="P-0038" lvl="0"><number>&lsqb;0038&rsqb;</number> Preferably the computer <highlight><bold>120</bold></highlight> is a general-purpose computer, although the computer <highlight><bold>120</bold></highlight> may be a specialized computer dedicated to a speech recognition program (sometimes &ldquo;speech engine&rdquo;). In one embodiment, the computer <highlight><bold>120</bold></highlight> may be controlled by the WINDOWS 9.x operating system. It is contemplated, however, that the system <highlight><bold>100</bold></highlight> would work equally well using a MACINTOSH operating system or even another operating system such as a WINDOWS CE, UNIX or a JAVA based operating system, to name a few. </paragraph>
<paragraph id="P-0039" lvl="0"><number>&lsqb;0039&rsqb;</number> In one arrangement, the computer <highlight><bold>120</bold></highlight> includes a memory <highlight><bold>122</bold></highlight>, a mass storage <highlight><bold>124</bold></highlight>, a speaker input interface <highlight><bold>126</bold></highlight>, a video processor <highlight><bold>128</bold></highlight>, and a microprocessor <highlight><bold>130</bold></highlight>. The memory <highlight><bold>122</bold></highlight> may be any device that can hold data in machine-readable format or hold programs and data between processing jobs in memory segments <highlight><bold>129</bold></highlight> such as for a short duration (volatile) or a long duration (non-volatile). Here, the memory <highlight><bold>122</bold></highlight> may include or be part of a storage device whose contents are preserved when its power is off. </paragraph>
<paragraph id="P-0040" lvl="0"><number>&lsqb;0040&rsqb;</number> The mass storage <highlight><bold>124</bold></highlight> may hold large quantities of data through one or more devices, including a hard disc drive (HDD), a floppy drive, and other removable media devices such as a CD-ROM drive, DITTO, ZIP or JAZ drive (from Iomega Corporation of Roy, Utah). </paragraph>
<paragraph id="P-0041" lvl="0"><number>&lsqb;0041&rsqb;</number> The microprocessor <highlight><bold>130</bold></highlight> of the computer <highlight><bold>120</bold></highlight> may be an integrated circuit that contains part, if not all, of a central processing unit of a computer on one or more chips. Examples of single chip microprocessors include the Intel Corporation PENTIUM, AMD K6, Compaq Digital Alpha, or Motorola 68000 and Power PC series. In one embodiment, the microprocessor <highlight><bold>130</bold></highlight> includes an audio file receiver <highlight><bold>132</bold></highlight>, a sound card <highlight><bold>134</bold></highlight>, and an audio preprocessor <highlight><bold>136</bold></highlight>. </paragraph>
<paragraph id="P-0042" lvl="0"><number>&lsqb;0042&rsqb;</number> In general, the audio file receiver <highlight><bold>132</bold></highlight> may function to receive a pre-recorded audio file, such as from the digital recorder <highlight><bold>102</bold></highlight> or an audio file in the form of live, stream speech from the microphone <highlight><bold>104</bold></highlight>. Examples of the audio file receiver <highlight><bold>132</bold></highlight> include a digital audio recorder, an analog audio recorder, or a device to receive computer files through a data connection, such as those that are on magnetic media. The sound card <highlight><bold>134</bold></highlight> may include the functions of one or more sound cards produced by, for example, Creative Labs, Trident, Diamond, Yamaha, Guillemot, NewCom, Inc., Digital Audio Labs, and Voyetra Turtle Beach, Inc. </paragraph>
<paragraph id="P-0043" lvl="0"><number>&lsqb;0043&rsqb;</number> Generally, an audio file can be thought of as a &ldquo;.WAV&rdquo; file. Waveform (wav) is a sound format developed by Microsoft and used extensively in Microsoft Windows. Conversion tools are available to allow most other operating systems to play .wav files. .wav files are also used as the sound source in wavetable synthesis, e.g. in E-mu&apos;s SoundFont. In addition, some Musical Instrument Digital Interface (MIDI) sequencers as add-on audio also support .wav files. That is, pre-recorded .wav files may be played back by control commands written in the sequence script. </paragraph>
<paragraph id="P-0044" lvl="0"><number>&lsqb;0044&rsqb;</number> A &ldquo;.WAV&rdquo; file may be originally created by any number of sources, including digital audio recording software; as a byproduct of a speech recognition program; or from a digital audio recorder. Other audio file formats, such as MP2, MP3, RAW, CD, MOD, MIDI, AIFF, mu-law, WMA, or DSS, may be used to format the audio file, without departing from the spirit of the present invention. </paragraph>
<paragraph id="P-0045" lvl="0"><number>&lsqb;0045&rsqb;</number> The microprocessor <highlight><bold>130</bold></highlight> may also include at least one speech recognition program, such as a first speech recognition program <highlight><bold>138</bold></highlight> and a second speech recognition program <highlight><bold>140</bold></highlight>. Preferably, the first speech recognition program <highlight><bold>138</bold></highlight> and the second speech recognition program <highlight><bold>140</bold></highlight> would transcribe the same audio file to produce two transcription files that are more likely to have differences from one another. The invention may exploit these differences to develop corrected text. In one embodiment, the first speech recognition program <highlight><bold>138</bold></highlight> may be Dragon NaturallySpeaking&trade; and the second speech recognition program <highlight><bold>140</bold></highlight> may be IBM Viavoice&trade;. </paragraph>
<paragraph id="P-0046" lvl="0"><number>&lsqb;0046&rsqb;</number> In some cases, it may be necessary to pre-process the audio files to make them acceptable for processing by speech recognition software. The audio preprocessor <highlight><bold>136</bold></highlight> may serve to present an audio file from the audio file receiver <highlight><bold>132</bold></highlight> to each program <highlight><bold>138</bold></highlight>, <highlight><bold>140</bold></highlight> in a form that is compatible with each program <highlight><bold>138</bold></highlight>, <highlight><bold>140</bold></highlight>. For instance, the audio preprocessor <highlight><bold>136</bold></highlight> may selectively change an audio file from a DSS or RAW file format into a WAV file format. Also, the audio preprocessor <highlight><bold>136</bold></highlight> may upsample or downsample the sampling rate of a digital audio file. Software to accomplish such preprocessing is available from a variety of sources including Syntrillium Corporation, Olympus Corporation, or Custom Speech USA, Inc. </paragraph>
<paragraph id="P-0047" lvl="0"><number>&lsqb;0047&rsqb;</number> The microprocessor <highlight><bold>130</bold></highlight> may also include a pre-correction program <highlight><bold>142</bold></highlight>, a segmentation correction program <highlight><bold>144</bold></highlight>, a word processing program <highlight><bold>146</bold></highlight>, and assorted automation programs <highlight><bold>148</bold></highlight>. </paragraph>
<paragraph id="P-0048" lvl="0"><number>&lsqb;0048&rsqb;</number> A machine-readable medium includes any mechanism for storing or transmitting information in a form readable by a machine (e.g., a computer). For example, a machine-readable medium includes read only memory (ROM); random access memory (RAM); magnetic disk storage media; optical storage media; flash memory devices; electrical, optical, acoustical or other form of propagated signals (e.g., carrier waves, infrared signals, digital signals, etc.). Methods or processes in accordance with the various embodiments of the invention may be implemented by computer readable instructions stored in any media that is readable and executable by a computer system. For example, a machine-readable medium having stored thereon instructions, which when executed by a set of processors, may cause the set of processors to perform the methods of the invention. </paragraph>
<paragraph id="P-0049" lvl="7"><number>&lsqb;0049&rsqb;</number> II. Process <highlight><bold>200</bold></highlight> </paragraph>
<paragraph id="P-0050" lvl="0"><number>&lsqb;0050&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 2</cross-reference> includes a flow diagram that illustrates a process <highlight><bold>200</bold></highlight> of the invention. The process <highlight><bold>200</bold></highlight> includes simultaneous use of graphical user interface (GUI) windows to create both a verbatim text for speech engine training and a final text to be distributed as a document or report. The process <highlight><bold>200</bold></highlight> also includes steps to create a file that maps transcribed text to verbatim text. In turn, this mapping file may be used to facilitate a training event for a speech engine, where this training event permits a subsequent iterative correction process to reach a higher accuracy that would be possible were this training event never to occur. Importantly, the mapping file, the verbatim text, and the final text may be created simultaneously through the use of arranged GUI windows. </paragraph>
<paragraph id="P-0051" lvl="7"><number>&lsqb;0051&rsqb;</number> A. Non-Enrolled User Profile </paragraph>
<paragraph id="P-0052" lvl="0"><number>&lsqb;0052&rsqb;</number> The process <highlight><bold>200</bold></highlight> begins at step <highlight><bold>202</bold></highlight>. At step <highlight><bold>204</bold></highlight>, a speaker may create an audio file <highlight><bold>205</bold></highlight>, such as by using the microphone <highlight><bold>104</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 1</cross-reference>. The process <highlight><bold>200</bold></highlight> then may determine whether a user profile exists for this particular speaker at step <highlight><bold>206</bold></highlight>. A user profile may include basic identification information about the speaker, such as a name, preferred reference vocabulary, information on the way in which a speaker pronounces particular words (acoustic information), and information on the way in which a speaker tends to use words (language model). </paragraph>
<paragraph id="P-0053" lvl="0"><number>&lsqb;0053&rsqb;</number> Most conventional speech engines for continuous dictation are manufactured with a generic user profile file comprising a generic name (e.g. &ldquo;name&rdquo;), generic acoustic information, and a generic language model. The generic acoustic information and the generic language model may be thought of as a generic speech model that is applicable to the entire class of speakers who use a particular speech engine. </paragraph>
<paragraph id="P-0054" lvl="0"><number>&lsqb;0054&rsqb;</number> Conventional speech engines for continuous dictation have been understood in the art to be speaker dependent so as to require manual creation of an initial speech user profile by each speaker. That is to say, in addition to the generic speech model that is generic to all users, conventional speech engines have been viewed as requiring the speaker to create speaker acoustic information and a speaker language model. The initial manual creation of speaker acoustic information and a speaker language model by the speaker may be referred to as enrollment. This process generally takes about a half-hour for each speaker. </paragraph>
<paragraph id="P-0055" lvl="0"><number>&lsqb;0055&rsqb;</number> The collective of the generic speech model, as modified by user profile information, may be copied into a set of user speech files. By supplying these speech files with acoustic and language information, for example, the accuracy of a speech engine may be increased. </paragraph>
<paragraph id="P-0056" lvl="0"><number>&lsqb;0056&rsqb;</number> In one experiment to better understand the roll enrollment plays in the accuracy growth of a speech engine, the inventors of the invention twice processed an audio file through a speech engine and measured the accuracy. In the first run, the speech engine had a user profile that consisted of (i) the user&apos;s name, (ii) generic acoustic information, and (iii) a generic language model. Here, the enrollment process was skipped and the speech engine was forced to process the audio file without the benefit of the enrollment process. In this run, the accuracy was low, often as low or lower than 30%. </paragraph>
<paragraph id="P-0057" lvl="0"><number>&lsqb;0057&rsqb;</number> In the second run, enrollment was performed and the speech engine had a user profile within which went (i) the user&apos;s name, (ii) generic acoustic information, (iii) a generic language model, (iv) speaker acoustic information, and (v) a speaker language model. The accuracy was generally higher and might measure approximately 60%, about twice as great from the run where the enrollment process was skipped. </paragraph>
<paragraph id="P-0058" lvl="0"><number>&lsqb;0058&rsqb;</number> Based on the above results, a skilled person would conclude that enrollment is necessary to present the speaker with a speech engine product from which the accuracy reasonably may be grown. In fact, conventional speech engine programs require enrollment. However, as discussed in more detail below, the inventors have discovered that iteratively processing an audio file with a non-enrolled user profile through the correction session of the invention surprisingly increased the accuracy of the speech engine to a point at which the speaker may be presented with a speech product from which the accuracy reasonably may be improved. </paragraph>
<paragraph id="P-0059" lvl="0"><number>&lsqb;0059&rsqb;</number> This process has been designed to make speech recognition more user friendly by reducing the time required for enrollment essentially to zero and to facilitate the off-site transcription of audio by speech recognition systems. The off-site facility can begin transcription virtually immediately after presentation of an audio file by creating a user. A user does not have to &ldquo;enroll&rdquo; before the benefits of speech recognition can be obtained. User accuracy can subsequently be improved through off-site corrective adaptation and other techniques. Characteristics of the input (e.g., telephone, type of microphone or handheld recorder) can be recorded and input specific speech files developed and trained for later use by the remote transcription facility. In addition, once trained to a sufficient accuracy level, these speech files can be transferred back to the speaker for on-site use using standard export or import controls. These are available in off-the-shelf speech recognition software or applications produced by a, for example, Dragon NaturallySpeaking&trade; or IBM Viavoice&trade; software development kit. The user can import the speech files and then calibrate his or her local system using the microphone and background noise &ldquo;wizards&rdquo; provided, for example, by standard, off-the-shelf Dragon NaturallySpeaking&trade; and IBM Viavoice&trade; speech recognition products. </paragraph>
<paragraph id="P-0060" lvl="0"><number>&lsqb;0060&rsqb;</number> In the co-pending application U.S. Non-Provisional application Ser. No. 09/889,870, the assignee of the present invention developed a technique to make the enrollment process transparent to the speaker. U.S. Non-Provisional application Ser. No. 09/889,870 discloses a system for substantially automating transcription services for one or more voice users is disclosed. This system receives a voice dictation file from a current user, which is automatically converted into a first written text based on a first set of conversion variables. The same voice dictation is automatically converted into a second written text based on a second set of conversion variables. The first and second sets of conversion variables have at least one difference, such as different speech recognition programs, different vocabularies, and the like. The system further includes a program for manually editing a copy of the first and second written texts to create a verbatim text of the voice dictation file. This verbatim text can then be delivered to the current user as transcribed text. A method for this approach is also disclosed. </paragraph>
<paragraph id="P-0061" lvl="0"><number>&lsqb;0061&rsqb;</number> What the above U.S. Non-Provisional application Ser. No. 09/889,870 demonstrates is that at the time U.S. Non-Provisional application Ser. No. 09/889,870 was filed, the assignee of the invention believed that the enrollment process was necessary to begin using a speech engine. In the present patent, the assignee of the invention has demonstrated the surprising conclusion that the enrollment process is not necessary. </paragraph>
<paragraph id="P-0062" lvl="0"><number>&lsqb;0062&rsqb;</number> Returning to step <highlight><bold>206</bold></highlight>, if no user profile is created, then the process <highlight><bold>200</bold></highlight> may create a user profile at step <highlight><bold>208</bold></highlight>. In creating the user profile at step <highlight><bold>208</bold></highlight>, the process <highlight><bold>200</bold></highlight> may employ the preexisting enrollment process of a speech engine and create an enrolled user profile. For example, a user profile previously created by the speaker at a local site, or speech files subsequently trained by the speaker with standard corrective adaptation and other techniques, can be transferred on a local area or wide area network to the transcription site for use by the speech recognition engine. This, again, can be accomplished using standard export and import controls available with off-the-shelf products or a software development kit. In a preferred embodiment, the process <highlight><bold>200</bold></highlight> may create a non-enrolled user profile and process this non-enrolled user profile through the correction session of the invention. </paragraph>
<paragraph id="P-0063" lvl="0"><number>&lsqb;0063&rsqb;</number> If a user profile has already been created, then the process <highlight><bold>200</bold></highlight> proceeds from step <highlight><bold>206</bold></highlight> to the transcribe audio file step <highlight><bold>210</bold></highlight>. </paragraph>
<paragraph id="P-0064" lvl="7"><number>&lsqb;0064&rsqb;</number> B. Compressed Session File </paragraph>
<paragraph id="P-0065" lvl="0"><number>&lsqb;0065&rsqb;</number> From step <highlight><bold>210</bold></highlight>, recorded audio file <highlight><bold>205</bold></highlight> may be converted into written, transcribed text by a speech engine, such a Dragon NaturallySpeaking&trade; or IBM Viavoice&trade;. The information then may be saved. Due to the time involved in correcting text and training the system, some manufacturers, e.g., Dragon NaturallySpeaking&trade; and IBM Viavoice&trade;, have now made &ldquo;delegated correction&rdquo; available. The speaker dictates into the speech recognition program. Text is transcribed. The program creates a &ldquo;session file&rdquo; that includes the text and audio that goes with it. The user saves the session file. This file may be opened later by another operator in the speech recognition text processor or in a commercially available word processor such as Word or WORDPERFECT. The secondary operator can select text, play back the audio associated with it, and make any required changes in the text. If the correction window is opened, the operator can correct the misrecognized words and train the system for the initial user. Unless the editor is very familiar with the speaker&apos;s dictation style and content (such as the dictating speaker&apos;s secretary), the editor usually does not know exactly what was dictated and must listen to the entire audio to find and correct the inevitable mistakes. Especially if the accuracy is low, the gains from automated transcription by the computer are partially, if not completely, offset by the time required to edit and correct. </paragraph>
<paragraph id="P-0066" lvl="0"><number>&lsqb;0066&rsqb;</number> The invention may employ one, two, three, or more speech engines, each transcribing the same audio file. Because of variations in programming or other factors, each speech engine may create a different transcribed text from the same audio file <highlight><bold>205</bold></highlight>. Moreover, with different configurations and parameters, the same speech engine used as both a first speech engine <highlight><bold>211</bold></highlight> and a second speech engine <highlight><bold>213</bold></highlight> may create a different transcribed text for the same audio. Accordingly, the invention may permit each speech engine to create its own transcribed text for a given audio file <highlight><bold>205</bold></highlight>. </paragraph>
<paragraph id="P-0067" lvl="0"><number>&lsqb;0067&rsqb;</number> From step <highlight><bold>210</bold></highlight>, the audio file <highlight><bold>205</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 2</cross-reference> may be received into a speech engine. In this example, the audio file <highlight><bold>205</bold></highlight> may be received into the first speech engine <highlight><bold>211</bold></highlight> at step <highlight><bold>212</bold></highlight>, although the audio file <highlight><bold>205</bold></highlight> alternatively (or simultaneously) may be received into the second speech engine <highlight><bold>213</bold></highlight>. At step <highlight><bold>214</bold></highlight>, the first speech engine <highlight><bold>211</bold></highlight> may output a transcribed text &ldquo;A&rdquo;. The transcribed text &ldquo;A&rdquo; may represent the best efforts of the first speech engine <highlight><bold>211</bold></highlight> at this stage in the process <highlight><bold>200</bold></highlight> to create a written text that may result from the words spoken by the speaker and recorded in the audio file <highlight><bold>205</bold></highlight> based on the language model presently used by the first speech engine <highlight><bold>211</bold></highlight> for that speaker. Each speech engine produces its own transcribed text &ldquo;A,&rdquo; the content of which usually differs by engine. </paragraph>
<paragraph id="P-0068" lvl="0"><number>&lsqb;0068&rsqb;</number> In addition to the transcribed text &ldquo;A&rdquo;, the first speech engine <highlight><bold>211</bold></highlight> may also create an audio tag. The audio tag may include information that maps or aligns the audio file <highlight><bold>205</bold></highlight> to the transcribed text &ldquo;A&rdquo;. Thus, for a given transcribed text segment, the associated audio segment may be played by employing the audio tag information. </paragraph>
<paragraph id="P-0069" lvl="0"><number>&lsqb;0069&rsqb;</number> Preferably, the audio tag information for each transcribed element (i.e. words, symbols, punctuation, formatting instructions etc.) contains information regarding a start time location and a stop time location of the associated audio segment in the original audio file. In order to determine the start time location and stop time location of each associated audio segment, the invention may employ Microsoft&apos;s Speech API (&ldquo;SAPI&rdquo;). As an exemplary embodiment, the following is described with respect to the Dragon NaturallySpeaking&trade; speech recognition program, version 5.0 and Microsoft SAPI SDK version 4.0a. As would be understood by those of ordinary skill in the art, other speech recognition engines will interface with this and other version of the Microsoft SAPI. For instance, Dragon NaturallySpeaking&trade; version 6 will interface with SAPI version 4.0a, IBM Viavoice&trade; version 8 will also interface with SAPI version 4.0a, and IBM Viavoice&trade; version 9 will interface with SAPI version 5. </paragraph>
<paragraph id="P-0070" lvl="0"><number>&lsqb;0070&rsqb;</number> With reference to <cross-reference target="DRAWINGS">FIG. 10</cross-reference>, Process <highlight><bold>1000</bold></highlight> uses the SAPI engine as a front end to interface with the Dragon NaturallySpeaking&trade; SDK modules in order to obtain information that is not readily provided by Dragon NaturallySpeaking&trade;. In step <highlight><bold>1010</bold></highlight>, an audio file is received by the speech recognition software. For instance, the speaker may dictate into the speech recognition program, using any input device such as a microphone, handheld recorder, or telephone, to produce an original audio file as previously described. The dictated audio is then transcribed using the first and/or second speech recognition program in conjunction with SAPI to produce a transcribed text. In step <highlight><bold>1020</bold></highlight>, a transcribed element (word, symbol, punctuation, or formatting instruction) is transcribed from a current audio segment in the original audio file. The SAPI then returns the text of the transcribed element and a binary audio stream, preferably in WAV PCM format, that the speech recognition software corresponds to the transcribed word.(step <highlight><bold>1030</bold></highlight>). The transcribed element text and a link to the associated binary audio stream are saved.(Step <highlight><bold>1040</bold></highlight>). In step <highlight><bold>1050</bold></highlight>, if there are more audio segments in the original audio file, the process returns to step <highlight><bold>1020</bold></highlight>. In a preferred approach, the transcribed text may be saved in a single session file, with each other transcribed word and points to each associated separate binary audio stream file. </paragraph>
<paragraph id="P-0071" lvl="0"><number>&lsqb;0071&rsqb;</number> Step <highlight><bold>1060</bold></highlight> then searches the original audio file for each separate binary audio stream to determine the stop time location and the start time location for that separate audio stream and end with its associated transcribed element. The stop time location for each transcribed element is then inserted into the single session file. Since the binary audio stream produced by the SAPI engine has a DC offset when compared to the original audio file, it is not possible to directly search the original audio file for each binary audio segment. As such, in a preferred approach the step <highlight><bold>1060</bold></highlight> searches for matches between the mathematical derivatives of each portion of audio, as described in further detail in <cross-reference target="DRAWINGS">FIG. 11</cross-reference>. </paragraph>
<paragraph id="P-0072" lvl="0"><number>&lsqb;0072&rsqb;</number> Referring to <cross-reference target="DRAWINGS">FIG. 11</cross-reference>, step <highlight><bold>1110</bold></highlight> sets a start position S to S&equals;0, and an end position E to E&equals;0. At step <highlight><bold>1112</bold></highlight>, a binary audio stream corresponding to the first association in the single session file is read into an array X, which is comprised of a series of sample points from time location <highlight><bold>0</bold></highlight> to time location N. In one approach, the number of sample points in the binary audio stream is determined in relation to the sampling rate and the duration of the binary audio stream. For example, if the binary audio stream is 1 second long and has a sampling rate of 11 samples/sec, the number of sample points in array X is 11. </paragraph>
<paragraph id="P-0073" lvl="0"><number>&lsqb;0073&rsqb;</number> At Step <highlight><bold>1114</bold></highlight> the mathematical derivative of the array X is computed in order to produce a derivative audio stream Dx(0 to N&minus;1). In one approach the mathematical derivative may be a discrete derivative, which is determined by taking the difference between a number of discrete points in the array X. In this approach, the discrete derivative may be defined as follows:  
<math-cwu id="MATH-US-00001">
<number>1</number>
<math>
<mrow>
  <mrow>
    <mi>Dx</mi>
    <mo>&af;</mo>
    <mrow>
      <mo>(</mo>
      <mrow>
        <mrow>
          <mn>0</mn>
          <mo>&it;</mo>
          <mstyle>
            <mtext>&emsp;</mtext>
          </mstyle>
          <mo>&it;</mo>
          <mi>to</mi>
          <mo>&it;</mo>
          <mstyle>
            <mtext>&emsp;</mtext>
          </mstyle>
          <mo>&it;</mo>
          <mi>N</mi>
        </mrow>
        <mo>-</mo>
        <mn>1</mn>
      </mrow>
      <mo>)</mo>
    </mrow>
  </mrow>
  <mo>=</mo>
  <mfrac>
    <mrow>
      <mrow>
        <mi>K</mi>
        <mo>&af;</mo>
        <mrow>
          <mo>(</mo>
          <mrow>
            <mi>n</mi>
            <mo>+</mo>
            <mn>1</mn>
          </mrow>
          <mo>)</mo>
        </mrow>
      </mrow>
      <mo>-</mo>
      <mrow>
        <mi>K</mi>
        <mo>&af;</mo>
        <mrow>
          <mo>(</mo>
          <mi>n</mi>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mrow>
    <mi>Tn</mi>
  </mfrac>
</mrow>
</math>
<mathematica-file id="MATHEMATICA-00001" file="US20030004724A1-20030102-M00001.NB"/>
<image id="EMI-M00001" wi="216.027" he="17.03835" file="US20030004724A1-20030102-M00001.TIF" imf="TIFF" ti="MF"/>
</math-cwu>
</paragraph>
<paragraph id="P-0074" lvl="7"><number>&lsqb;0074&rsqb;</number> where n is an integer from 1 to N, K(n&plus;1) is a sample point taken at time location n&plus;1, K(n) is a previous sample point take at time location n, and Tn is the time base between K(n) and K(n&minus;1). In a preferred approach, the time base Tn between two consecutive sample points is always equal to 1. Thus, simplifying the calculation of the discrete derivative to Dx(0 to N&minus;1)&equals;K(n&plus;1)&minus;K(n). </paragraph>
<paragraph id="P-0075" lvl="0"><number>&lsqb;0075&rsqb;</number> In step <highlight><bold>1116</bold></highlight>, a segment of the original audio file is read into an array Y starting at position S, which was previously set to 0. In a preferred approach, array Y is twice as wide as array X such that the audio segment read into the array Y extends from time position S to time position S&plus;2N. At Step <highlight><bold>1118</bold></highlight> the discrete derivative of array Y is computed to produce a derivative audio segment array Dy(S to S&plus;2N&minus;1) by employing the same method as described above for array X. </paragraph>
<paragraph id="P-0076" lvl="0"><number>&lsqb;0076&rsqb;</number> In step <highlight><bold>1120</bold></highlight>, a counter P is set to P&equals;0. Step <highlight><bold>1122</bold></highlight> then begins to search for the derivative audio stream array Dx(0 to N&minus;1) within the derivative audio segment array Dy(S to S&plus;2N&minus;1). The derivative audio stream array Dx(0 to N&minus;1) is compared sample by sample to a portion of the derivative audio segment array defined by Dy(S&plus;P to S&plus;P&plus;N&minus;1). If every sample point in the derivative audio stream is not an exact match with this portion of the derivative audio segment, the process proceeds to step <highlight><bold>1124</bold></highlight>. At Step <highlight><bold>1124</bold></highlight>, if P is less than N, P is incremented by 1, and the process returns to step <highlight><bold>1122</bold></highlight> to compare the derivative audio stream array with the next portion of the derivative audio segment array. If P is equal to N in Step <highlight><bold>1124</bold></highlight>, the start position S is incremented by N such that S&equals;S&plus;N, and the process returns to step <highlight><bold>1116</bold></highlight> where a new segment from the original audio file is read into array Y. </paragraph>
<paragraph id="P-0077" lvl="0"><number>&lsqb;0077&rsqb;</number> When the derivative audio stream Dx(0 to N&minus;1) matches the portion of the derivative audio segment Dy(S&plus;P to S&plus;P&plus;N&minus;1) at step <highlight><bold>1122</bold></highlight> sample point for sample point, the start time location of the audio tag for the transcribed word associated with the current binary audio stream is set as the previous end position E, and the stop time location end<highlight><subscript>z </subscript></highlight>of the audio tag is set to S&plus;P&plus;N&minus;1 (step <highlight><bold>1130</bold></highlight>). These values are saved as the audio tag information for the associated transcribed element in the session file. Using these values and the original audio file, an audio segment from that original audio file can be played back. In a preferred approach, only the end time location for each transcribed element is saved in the session file. In this approach, the start time location of each associated audio segment is simply determined by the end time location of the previous audio segment. However, in an alternative approach, the start time location and the end time location may be saved for each transcribed element in the session file. </paragraph>
<paragraph id="P-0078" lvl="0"><number>&lsqb;0078&rsqb;</number> In step <highlight><bold>1132</bold></highlight>, if there are more word tags in the session file, the process proceeds to step <highlight><bold>1134</bold></highlight>. In step <highlight><bold>1134</bold></highlight>, S is set to E&equals;S&plus;P&plus;N&minus;1 and in step <highlight><bold>1136</bold></highlight>, S is set to S&equals;E. The process then returns to step <highlight><bold>1112</bold></highlight> where a binary audio stream associated with the next word tag is read into array X from the appropriate file, and the next segment from the original audio file is read into array Y beginning at a time location corresponding to the new value of S. Once there are no more word tags in the session file, the process may proceed to step <highlight><bold>218</bold></highlight> in <cross-reference target="DRAWINGS">FIG. 2</cross-reference>. </paragraph>
<paragraph id="P-0079" lvl="0"><number>&lsqb;0079&rsqb;</number> When the process shown in <cross-reference target="DRAWINGS">FIG. 11</cross-reference> is completed, each transcribed element in the transcribed text will be associated with an audio tag that has at least the stop time location end<highlight><subscript>z </subscript></highlight>of each associated audio segment in the original audio file. Since the start position of each audio tag corresponds to the end position of the audio tag for the previous word, the above described process ensures that the audio tags associated with the transcribed words include each portion of the original audio file even if the speech engine failed to transcribe some audio portion thereof. As such, by using the audio tags created by the playback of the associated audio segments will also play back any portion of the original audio file that was not originally transcribed by the speech recognition software. </paragraph>
<paragraph id="P-0080" lvl="0"><number>&lsqb;0080&rsqb;</number> Although the above described process utilizes the derivative of the binary audio stream and original audio file to compensate for offsets, the above process may alternatively be practiced by determining that relative DC offset between the binary audio stream and the original audio file. This relative DC offset would then be removed from the binary audio stream and the compensated binary audio stream would be compared directly to the original audio file. </paragraph>
<paragraph id="P-0081" lvl="0"><number>&lsqb;0081&rsqb;</number> It is also contemplated that the size of array Y can be varied with the understanding that making the size of this array too small may require additional complexity the matching of audio that spans across a nominal array boundary. </paragraph>
<paragraph id="P-0082" lvl="0"><number>&lsqb;0082&rsqb;</number> <cross-reference target="DRAWINGS">FIGS. 12</cross-reference><highlight><italic>a</italic></highlight>-<highlight><bold>12</bold></highlight><highlight><italic>c </italic></highlight>show one exemplary embodiment of the above described process. <cross-reference target="DRAWINGS">FIG. 12</cross-reference><highlight><italic>a </italic></highlight>shows one example of a session file <highlight><bold>1210</bold></highlight> and a series of binary audio streams <highlight><bold>1220</bold></highlight> corresponding to each transcribed element saved in the session file. In this example, the process has already determined the end time locations for each of the files <highlight><bold>0000</bold></highlight>.wav, <highlight><bold>0001</bold></highlight>.wav, and <highlight><bold>0002</bold></highlight>.wav and the process is now reading file <highlight><bold>0003</bold></highlight>.wave into Array X. As shown in <cross-reference target="DRAWINGS">FIG. 12</cross-reference><highlight><italic>b</italic></highlight>, array X has 11 sample points ranging from time location 0 to time location N. The discrete derivative of Array X(<highlight><bold>0</bold></highlight> to <highlight><bold>10</bold></highlight>) is then taken to produce a derivative audio stream array Dx(<highlight><bold>0</bold></highlight> to <highlight><bold>9</bold></highlight>) as described in step <highlight><bold>1114</bold></highlight> above. </paragraph>
<paragraph id="P-0083" lvl="0"><number>&lsqb;0083&rsqb;</number> The values in the arrays X, Y, Dx, and Dy, shown in <cross-reference target="DRAWINGS">FIGS. 12</cross-reference><highlight><italic>a</italic></highlight>-<highlight><bold>12</bold></highlight><highlight><italic>c</italic></highlight>, are represented as integers to clearly present the invention. However, in practice, the values may be represented in binary, ones complement, twos complement, sign-magnitude or any other method for representing values. </paragraph>
<paragraph id="P-0084" lvl="0"><number>&lsqb;0084&rsqb;</number> With further reference to <cross-reference target="DRAWINGS">FIGS. 12</cross-reference><highlight><italic>a </italic></highlight>and <highlight><bold>12</bold></highlight><highlight><italic>b</italic></highlight>, as the end time location for the previous binary audio stream <highlight><bold>0002</bold></highlight>.wav was determined to be time location <highlight><bold>40</bold></highlight>, end position E is set to E&equals;40(step <highlight><bold>1134</bold></highlight>) and start position S is also set to S&equals;40(step <highlight><bold>1136</bold></highlight>). Therefore, an audio segment ranging from S to S&plus;2N, or time location <highlight><bold>40</bold></highlight> to time location <highlight><bold>60</bold></highlight> in the original audio file, is read into array Y (step <highlight><bold>1116</bold></highlight>). The discrete derivative of array Y is then taken, resulting in Dy(<highlight><bold>40</bold></highlight> to <highlight><bold>59</bold></highlight>). </paragraph>
<paragraph id="P-0085" lvl="0"><number>&lsqb;0085&rsqb;</number> The derivative audio stream Dx(<highlight><bold>0</bold></highlight> to <highlight><bold>9</bold></highlight>) is then compared sample by sample to Dy(S&plus;P to S&plus;P&plus;N&minus;1), or Dy(<highlight><bold>40</bold></highlight> to <highlight><bold>49</bold></highlight>). Since every sample point in the derivative audio stream shown in <cross-reference target="DRAWINGS">FIG. 12</cross-reference><highlight><italic>b </italic></highlight>is not an exact match with this portion of the derivative audio segment, P is incremented by <highlight><bold>1</bold></highlight> and a new portion of the derivative audio segment is compared sample by sample to the derivative audio stream, as shown in <cross-reference target="DRAWINGS">FIG. 12</cross-reference><highlight><italic>c. </italic></highlight></paragraph>
<paragraph id="P-0086" lvl="0"><number>&lsqb;0086&rsqb;</number> In <cross-reference target="DRAWINGS">FIG. 12</cross-reference><highlight><italic>c</italic></highlight>, derivative audio stream Dx(<highlight><bold>0</bold></highlight> to <highlight><bold>9</bold></highlight>) is compared sample by sample to Dy(<highlight><bold>41</bold></highlight> to <highlight><bold>50</bold></highlight>). As this portion of the derivative audio segment Dy is an exact match to the derivative audio stream Dx, the end time location for the corresponding word is set to end<highlight><subscript>3</subscript></highlight>&equals;S&plus;P&plus;N&minus;1&equals;40&plus;1&plus;10&minus;1&equals;50, and this value is inserted into the session file <highlight><bold>1210</bold></highlight>. As there are more in the session file <highlight><bold>1210</bold></highlight>, end position E would be set to 50, S would be set to <highlight><bold>50</bold></highlight>, and the process would return to step <highlight><bold>1112</bold></highlight> in <cross-reference target="DRAWINGS">FIG. 11</cross-reference>. </paragraph>
<paragraph id="P-0087" lvl="0"><number>&lsqb;0087&rsqb;</number> Returning to <cross-reference target="DRAWINGS">FIG. 2</cross-reference>, the process <highlight><bold>200</bold></highlight> may save the transcribed text &ldquo;A&rdquo; using a .txt extension at step <highlight><bold>216</bold></highlight>. At step <highlight><bold>218</bold></highlight>, the process <highlight><bold>200</bold></highlight> may save the engine session file using a .ses extension. Where the first speech engine <highlight><bold>211</bold></highlight> is the Dragon NaturallySpeaking&trade; speech engine, the engine session file may employ a .dra extension. Where the second speech engine <highlight><bold>213</bold></highlight> is an IBM Viavoice&trade; speech engine, the IBM Viavoice&trade; SDK session file employs an .isf extension. </paragraph>
<paragraph id="P-0088" lvl="0"><number>&lsqb;0088&rsqb;</number> At this stage of the process <highlight><bold>200</bold></highlight>, an engine session file may include at least one of a transcribed text, the original audio file <highlight><bold>205</bold></highlight>, and the audio tag. The engine session files for conventional speech engines are very large in size. One reason for this is the format in which the audio file <highlight><bold>205</bold></highlight> is stored. Moreover, the conventional session files are saved as combined text and audio that, as a result, cannot be compressed using standard algorithms or other techniques to achieve a desirable result. Large files are difficult to transfer between a server and a client computer or between a first client computer to a second client computer. Thus, remote processing of a conventional session file is difficult and sometimes not possible due to the large size of these files. </paragraph>
<paragraph id="P-0089" lvl="0"><number>&lsqb;0089&rsqb;</number> To overcome the above problems, the process <highlight><bold>200</bold></highlight> may save a compressed session file at step <highlight><bold>220</bold></highlight>. This compressed session file, which may employ the extension .csf, may include a transcribed text, the original audio file <highlight><bold>205</bold></highlight>, and the audio tag. However, the transcribed text, the original audio file <highlight><bold>205</bold></highlight>, and the audio tag are separated prior to being saved. Thus, the transcribed text, the original audio file <highlight><bold>205</bold></highlight>, and the audio tag are saved separately in a compressed cabinet file, which works to retain the individual identity of each of these three files. </paragraph>
<paragraph id="P-0090" lvl="0"><number>&lsqb;0090&rsqb;</number> Moreover, the transcribed text, the audio file, and the mapping file for any session of the process <highlight><bold>200</bold></highlight> may be saved separately. </paragraph>
<paragraph id="P-0091" lvl="0"><number>&lsqb;0091&rsqb;</number> Because the transcribed text, the audio file, and the audio tag or mapping file for each session may be save separately, each of these three files for any session of the process <highlight><bold>200</bold></highlight> may be compressed using standard algorithm techniques to achieve a desirable result. Thus, a text compression algorithm may be run separately on the transcribed text file and the audio tag and an audio compression algorithm may be run on the original audio file <highlight><bold>205</bold></highlight>. This is distinguished from conventional engine session files, which cannot be compressed to achieve a desirable result. </paragraph>
<paragraph id="P-0092" lvl="0"><number>&lsqb;0092&rsqb;</number> For example, the audio file <highlight><bold>205</bold></highlight> of a saved compressed session file may be converted and saved in a compressed format. Moving Picture Experts Group (MPEG)&minus;1 audio layer 3 (MP3) is a digital audio compression algorithm that achieves a compression factor of about twelve while preserving sound quality. MP3 does this by optimizing the compression according to the range of sound that people can actually hear. In one embodiment, the audio file <highlight><bold>205</bold></highlight> is converted and saved in an MP3 format as part of a compressed session file. Thus, in another embodiment, a compressed session file from the process <highlight><bold>200</bold></highlight> is transmitted from the computer <highlight><bold>120</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 1</cross-reference> onto the Internet. As is generally known, the Internet is an interconnected system of networks that connects computers around the world via a standard protocol. Accordingly, an editor or correctionist may be at location remote from the compressed session file and yet receive the compressed session file over the Internet. </paragraph>
<paragraph id="P-0093" lvl="0"><number>&lsqb;0093&rsqb;</number> Once the appropriate files are saved, the process <highlight><bold>200</bold></highlight> may proceed to step <highlight><bold>222</bold></highlight>. At step <highlight><bold>222</bold></highlight>, the process <highlight><bold>222</bold></highlight> may repeat the transcription of the audio file <highlight><bold>205</bold></highlight> using the second speech engine <highlight><bold>213</bold></highlight>. In the alternative, the process <highlight><bold>222</bold></highlight> may proceed to step <highlight><bold>224</bold></highlight>. </paragraph>
<paragraph id="P-0094" lvl="7"><number>&lsqb;0094&rsqb;</number> C. Speech Editor: Creating Files in Multiple GUI Windows </paragraph>
<paragraph id="P-0095" lvl="0"><number>&lsqb;0095&rsqb;</number> At step <highlight><bold>224</bold></highlight>, the process <highlight><bold>200</bold></highlight> may activate a speech editor <highlight><bold>225</bold></highlight> of the invention. In general, the speech editor <highlight><bold>225</bold></highlight> may be used to expedite the training of multiple speech recognition engines and/or generate a final report or document text for distribution. This may be accomplished through the simultaneous use of graphical user interface (GUI) windows to create both a verbatim text <highlight><bold>229</bold></highlight> for speech engine training and a final text <highlight><bold>231</bold></highlight> to be distributed as a document or report. The speech editor <highlight><bold>225</bold></highlight> may also permit creation of a file that maps transcribed text to verbatim text <highlight><bold>229</bold></highlight>. In turn, this mapping file may be used to facilitate a training event for a speech engine during a correction session. Here, the training event works to permit subsequent iterative correction processes to reach a higher accuracy than would be possible were this training event never to occur. Importantly, the mapping file, the verbatim text, and the final text may be created simultaneously through the use of linked GUI windows. Through use of standard scrolling techniques, these windows are not limited to the quantity of text displayed in each window. By way of distinction, the speech editor <highlight><bold>225</bold></highlight> does not directly train a speech engine. The speech editor <highlight><bold>225</bold></highlight> may be viewed as a front-end tool by which a correctionist corrects verbatim text to be submitted for speech training or corrects final text to generate a polished report or document. </paragraph>
<paragraph id="P-0096" lvl="0"><number>&lsqb;0096&rsqb;</number> After activating the speech editor <highlight><bold>225</bold></highlight> at step <highlight><bold>224</bold></highlight>, the process <highlight><bold>200</bold></highlight> may proceed to step <highlight><bold>226</bold></highlight>. At step <highlight><bold>226</bold></highlight> a compressed session file (.csf) may be open. Use of the speech editor <highlight><bold>225</bold></highlight> may require that audio be played by selecting transcribed text and depressing a play button. Although the compressed session file may be sufficient to provide the transcribed text, the audio text alignment from a compressed session file may not be as complete as the audio text alignment from an engine session file under certain circumstances. Thus, in one embodiment, the compressed session file may add an engine session file to a job by specifying an engine session file to open for audio playback purposes. In another, embodiment, the engine session file (.ses) is a Dragon NaturallySpeaking&trade; engine session file (.dra). </paragraph>
<paragraph id="P-0097" lvl="0"><number>&lsqb;0097&rsqb;</number> From step <highlight><bold>226</bold></highlight>, the process <highlight><bold>200</bold></highlight> may proceed to step <highlight><bold>228</bold></highlight>. At step <highlight><bold>228</bold></highlight>, the process <highlight><bold>200</bold></highlight> may present the decision of whether to create a verbatim text <highlight><bold>229</bold></highlight>. In either case, the process <highlight><bold>200</bold></highlight> may proceed to step <highlight><bold>230</bold></highlight>, where the process <highlight><bold>200</bold></highlight> may the decision of whether to create a final text <highlight><bold>231</bold></highlight>. Both the verbatim text <highlight><bold>229</bold></highlight> and the final text <highlight><bold>231</bold></highlight> may be displayed through graphical user interfaces (GUIs). </paragraph>
<paragraph id="P-0098" lvl="0"><number>&lsqb;0098&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 3</cross-reference> of the drawings is a view of an exemplary graphical user interface <highlight><bold>300</bold></highlight> to support the present invention. The graphical user interface (GUI) <highlight><bold>300</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 3</cross-reference> is shown in Microsoft Windows operating system version 9.x. However, the display and interactive features of the graphical user interface (GUI) <highlight><bold>300</bold></highlight> is not limited to the Microsoft Windows operating system, but may be displayed in accordance with any underlying operating system. </paragraph>
<paragraph id="P-0099" lvl="0"><number>&lsqb;0099&rsqb;</number> In previously filed, co-pending patent application PCT Application No. PCT/US01/1760, which claims the benefits of U.S. Provisional Application No. 60/208,994, the assignee of the present application discloses a system and method for comparing text generated in association with a speech recognition program. Using file comparison techniques, text generated by two speech recognition engines and the same audio file are compared. Differences are detected with each difference having a match listed before and after the difference, except for text begin and text end. In those cases, there is at least one adjacent match associated to it. By using this &ldquo;book-end&rdquo; or &ldquo;sandwich&rdquo; technique, text differences can be identified, along with the exact audio segment that was transcribed by both speech recognition engines. <cross-reference target="DRAWINGS">FIG. 3</cross-reference> of the present invention was disclosed as FIG. 7 in Serial No. 60/208,994. U.S. Serial No. 60/208,994 is incorporated by reference to the extent permitted by law. </paragraph>
<paragraph id="P-0100" lvl="0"><number>&lsqb;0100&rsqb;</number> GUI <highlight><bold>300</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 3</cross-reference> may include a source text window A <highlight><bold>302</bold></highlight>, a source text window B <highlight><bold>304</bold></highlight>, and two correction windows: a report text window <highlight><bold>306</bold></highlight> and a verbatim text window <highlight><bold>308</bold></highlight>. <cross-reference target="DRAWINGS">FIG. 4</cross-reference> illustrates a text A <highlight><bold>400</bold></highlight> and <cross-reference target="DRAWINGS">FIG. 5</cross-reference> illustrates a text B <highlight><bold>500</bold></highlight>. The text A <highlight><bold>400</bold></highlight> may be transcribed text generated from the first speech engine <highlight><bold>211</bold></highlight> and the text B <highlight><bold>500</bold></highlight> may be transcribed text generated from the second speech engine <highlight><bold>213</bold></highlight>. </paragraph>
<paragraph id="P-0101" lvl="0"><number>&lsqb;0101&rsqb;</number> The two correction windows <highlight><bold>306</bold></highlight> and <highlight><bold>308</bold></highlight> may be linked or locked together so that changes in one window may affect the corresponding text in the other window. At times, changes to the verbatim text window <highlight><bold>308</bold></highlight> need not be made in the report text window <highlight><bold>306</bold></highlight> or changes to the report text window <highlight><bold>306</bold></highlight> need not be made in the verbatim text window <highlight><bold>308</bold></highlight>. During these times, the correction windows may be unlocked from one another so that a change in one window does not affect the corresponding text in the other window. In other words, the report text window <highlight><bold>306</bold></highlight> and the verbatim text window <highlight><bold>308</bold></highlight> may be edited simultaneously or singularly as may be toggled by a correction window lock mode. </paragraph>
<paragraph id="P-0102" lvl="0"><number>&lsqb;0102&rsqb;</number> As shown in <cross-reference target="DRAWINGS">FIG. 3</cross-reference>, each text window may display utterances from the transcribed text. An utterance may be defined as a first group of words separated by a pause from a second group of words. By highlighting one of the source texts <highlight><bold>302</bold></highlight>, <highlight><bold>304</bold></highlight>, playing the associated audio, and listening to what was spoken, the report text <highlight><bold>231</bold></highlight> or the verbatim text <highlight><bold>229</bold></highlight> may be verified or changed in the case of errors. By correcting the errors in each utterance and then pressing forward to continue to the next set, both a (final) report text <highlight><bold>231</bold></highlight> and a verbatim text <highlight><bold>229</bold></highlight> may be generated simultaneously in multiple windows. Speech engines such as the IBM Viavoice&trade; SDK engine do not permit more than ten words to be corrected using a correction window. Accordingly, displaying and working with utterances works well under some circumstances. Although displaying and working with utterances works well under some circumstances, other circumstances require that the correction windows be able to correct an unlimited amount of text. </paragraph>
<paragraph id="P-0103" lvl="0"><number>&lsqb;0103&rsqb;</number> However, from the correctionist&apos;s stand-point, utterance-by-utterance display is not always the most convenient display mode. As seen in comparing <cross-reference target="DRAWINGS">FIG. 3</cross-reference> to <cross-reference target="DRAWINGS">FIG. 4</cross-reference> and <cross-reference target="DRAWINGS">FIG. 5</cross-reference>, the amount of text that is displayed in the windows <highlight><bold>302</bold></highlight>, <highlight><bold>304</bold></highlight>, <highlight><bold>306</bold></highlight> and <highlight><bold>308</bold></highlight> is less than the transcribed text from either <cross-reference target="DRAWINGS">FIG. 4</cross-reference> or <cross-reference target="DRAWINGS">FIG. 5</cross-reference>. <cross-reference target="DRAWINGS">FIG. 6</cross-reference> of the drawings is a view of an exemplary graphical user interface <highlight><bold>600</bold></highlight> to support the present invention. The speech editor <highlight><bold>225</bold></highlight> may include a front end, graphical user interface <highlight><bold>600</bold></highlight> through which a human correctionist may review and correct transcribed text, such as transcribed text &ldquo;A&rdquo; of step <highlight><bold>214</bold></highlight>. The GUI <highlight><bold>600</bold></highlight> works to make the reviewing process easy by highlighting the text that requires the correctionist&apos;s attention. Using the speech editor <highlight><bold>225</bold></highlight> navigation and audio playback methods, the correctionist may quickly and effectively review and correct a document. </paragraph>
<paragraph id="P-0104" lvl="0"><number>&lsqb;0104&rsqb;</number> The GUI <highlight><bold>600</bold></highlight> may be viewed as a multidocument user interface product that provides four windows through which the correctionist may work: a first transcribed text window <highlight><bold>602</bold></highlight>, a second transcribed text window <highlight><bold>604</bold></highlight>, and two correction windows&mdash;a verbatim text window <highlight><bold>606</bold></highlight> and a final text window <highlight><bold>608</bold></highlight>. Modifications by the correctionist may only be made in the final text window <highlight><bold>606</bold></highlight> and verbatim text window <highlight><bold>608</bold></highlight>. The contents of the first transcribed text window <highlight><bold>602</bold></highlight> and the second transcribed text window <highlight><bold>604</bold></highlight> may be fixed so that the text cannot be altered. In the current embodiment, the first transcribed text window <highlight><bold>602</bold></highlight> and the second transcribed text window <highlight><bold>604</bold></highlight> contain text that cannot be modified. </paragraph>
<paragraph id="P-0105" lvl="0"><number>&lsqb;0105&rsqb;</number> The first transcribed text window <highlight><bold>602</bold></highlight> may contain the transcribed text &ldquo;A&rdquo; of step <highlight><bold>214</bold></highlight> as the first speech engine <highlight><bold>211</bold></highlight> originally transcribed it. The second transcribed text window <highlight><bold>604</bold></highlight> may contain a transcribed text &ldquo;B&rdquo; (not shown) of step <highlight><bold>214</bold></highlight> as the second speech engine <highlight><bold>213</bold></highlight> originally transcribed it. Typically, the content of transcribed text &ldquo;A&rdquo; and transcribed text &ldquo;B&rdquo; will differ based upon the speech recognition engine used, even where both are based on the same audio file <highlight><bold>205</bold></highlight>. </paragraph>
<paragraph id="P-0106" lvl="0"><number>&lsqb;0106&rsqb;</number> A main goals of each transcribed window <highlight><bold>602</bold></highlight>, <highlight><bold>604</bold></highlight> is to provide a reference for the correctionist to always know what the original transcribed text is, to provide an avenue to play back the underlying audio file, and to provide an avenue by which the correctionist may select specific text for audio playback. The text in either the final or verbatim window <highlight><bold>606</bold></highlight>, <highlight><bold>608</bold></highlight> is not linked directly to the audio file <highlight><bold>205</bold></highlight>. The audio in each window for each match or difference may be played by selecting the text and hitting a playback button. The word or phrase played back will be the audio associated with the word or phrase where the cursor was last located. If the correctionist is in the &ldquo;All&rdquo; mode (which plays back audio for both matches and differences), audio for a phrase that crosses the boundary between a match and difference may be played by selecting and playing the phrase in the final (<highlight><bold>608</bold></highlight>) or verbatim (<highlight><bold>606</bold></highlight>) windows corresponding to the match, and then selecting and playing the phrase in the final or verbatim windows corresponding to the difference. Details concerning playback in different modes are described more fully in the Section 1 &ldquo;Navigation&rdquo; below. If the correctionist selects the entire text in the &ldquo;All&rdquo; mode and launches playback, the text will be played from the beginning to the end. Those with sufficient skill in the art the disclosure of the present invention before them will realize that playback of the audio for the selected word, phrase, or entire text could be regulated through use of a standard transcriptionist foot pedal. </paragraph>
<paragraph id="P-0107" lvl="0"><number>&lsqb;0107&rsqb;</number> The verbatim text window <highlight><bold>606</bold></highlight> may be where the correctionist modifies and corrects text to identically match what was said in the underlying dictated audio file <highlight><bold>205</bold></highlight>. A main goal of the verbatim text window <highlight><bold>606</bold></highlight> is to provide an avenue by which the correctionist may correct text for the purposes of training a speech engine. Moreover, the final text window <highlight><bold>608</bold></highlight> may be where the correctionist modifies and polishes the text to be filed away as a document product of the speaker. A main goal of the final text window <highlight><bold>608</bold></highlight> is to provide an avenue by which the correctionist may correct text for the purposes of producing a final text file for distribution. </paragraph>
<paragraph id="P-0108" lvl="0"><number>&lsqb;0108&rsqb;</number> To start a session of the speech editor <highlight><bold>225</bold></highlight>, a session file is opened at step <highlight><bold>226</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 2</cross-reference>. This may initialize three of four windows of the GUI <highlight><bold>600</bold></highlight> with transcribed text &ldquo;A&rdquo; (&ldquo;Transcribed Text,&rdquo; &ldquo;Verbatim Text,&rdquo; and &ldquo;Final Text&rdquo;). In the example, the initialization texts were generated using the IBM Viavoice&trade; SDK engine. Opening a second session file may initialize the second transcribed text window <highlight><bold>604</bold></highlight> with a different transcribed text from step <highlight><bold>214</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 2</cross-reference>. In the example, the fourth window (&ldquo;Secondary Transcribed Text) was created using the Dragon NaturallySpeaking&trade; engine. The verbatim text window is, by definition, described as being 100.00% accurate, but actual verbatim text may not be generated until corrections have been made by the editor. </paragraph>
<paragraph id="P-0109" lvl="0"><number>&lsqb;0109&rsqb;</number> The verbatim text window <highlight><bold>606</bold></highlight> and the final text window <highlight><bold>608</bold></highlight> may start off initially linked together. That is to say, whatever edits are made in one window may be propagated into the other window. In this manner, the speech editor <highlight><bold>225</bold></highlight> works to reduce the editing time required to correct two windows. The text in each of the verbatim text window <highlight><bold>606</bold></highlight> and the final text window <highlight><bold>608</bold></highlight> may be associated to the original source text located and displayed in the first transcribed text window <highlight><bold>602</bold></highlight>. Recall that the transcribed text in first transcribed text window <highlight><bold>602</bold></highlight> is aligned to the audio file <highlight><bold>205</bold></highlight>. Since the contents of each of the two modifiable windows (final and verbatim) is mapped back to the first transcribed text window <highlight><bold>602</bold></highlight>, the correctionist may select text from the first transcribed text window <highlight><bold>602</bold></highlight> and play back the audio that corresponds to the text in any of the windows <highlight><bold>602</bold></highlight>, <highlight><bold>604</bold></highlight>, <highlight><bold>606</bold></highlight>, and <highlight><bold>608</bold></highlight>. By listening to the original source audio in the audio file <highlight><bold>205</bold></highlight> the correctionist may determine how the text should read in the verbatim window (Verbatim <highlight><bold>606</bold></highlight>) and make modifications as needed in final report or document (Final <highlight><bold>608</bold></highlight>). </paragraph>
<paragraph id="P-0110" lvl="0"><number>&lsqb;0110&rsqb;</number> The text within the modifiable windows <highlight><bold>606</bold></highlight>, <highlight><bold>608</bold></highlight> conveys more information than the tangible embodiment of the spoken word. Depending upon how the four windows (Transcribed Text, Secondary Transcribed Text, VerbatimText, and Final Text) are positioned, text within the modifiable windows <highlight><bold>606</bold></highlight>, <highlight><bold>608</bold></highlight> may be aligned &ldquo;horizontally&rdquo; (side-by-side) or &ldquo;vertically&rdquo; (above or below) with the transcribed text of the transcribed text windows <highlight><bold>602</bold></highlight>, <highlight><bold>604</bold></highlight> which, in turn, is associated to the audio file <highlight><bold>205</bold></highlight>. This visual alignment permits a correctionist using the speech editor <highlight><bold>225</bold></highlight> of the invention to view the text within the final and verbatim windows <highlight><bold>606</bold></highlight>, <highlight><bold>608</bold></highlight> while audibly listening the actual words spoken by a speaker. Both audio and visual cues may be used in generating the final and verbatim text in windows <highlight><bold>606</bold></highlight>, <highlight><bold>608</bold></highlight>. </paragraph>
<paragraph id="P-0111" lvl="0"><number>&lsqb;0111&rsqb;</number> In the example, the original audio dictated, with simple formatting commands, was &ldquo;Chest and lateral &lsqb;&ldquo;new paragraph&rdquo;&rsqb; History &lsqb;&ldquo;colon&rdquo;&rsqb; pneumonia &lsqb;&ldquo;period&rdquo;&rsqb; &lsqb;&ldquo;new paragraph&rdquo;&rsqb; Referring physician&lsqb;&ldquo;colon&rdquo;&rsqb; Dr. Smith &lsqb;&ldquo;period&rdquo;&rsqb; &lsqb;&ldquo;new paragraph&rdquo;&rsqb; Heart size is mildly enlarged &lsqb;&ldquo;period&rdquo;&rsqb; There are prominent markings of the lower lung fields &lsqb;&ldquo;period&rdquo;&rsqb; The right lung is clear &lsqb;&ldquo;period&rdquo;&rsqb; There is no evidence for underlying tumor &lsqb;&ldquo;period&rdquo;&rsqb; Incidental note is made of degenerative changes of the spine and shoulders &lsqb;&ldquo;period&rdquo;&rsqb; Follow-up chest and lateral in 4 to 6 weeks is advised &lsqb;&ldquo;period&rdquo;&rsqb; &lsqb;&ldquo;new paragraph&rdquo;&rsqb; No definite evidence for active pneumonia &lsqb;&ldquo;period&rdquo;&rsqb;. </paragraph>
<paragraph id="P-0112" lvl="0"><number>&lsqb;0112&rsqb;</number> Once a transcribed file has been loaded, the first few words in each text window <highlight><bold>602</bold></highlight>, <highlight><bold>604</bold></highlight>, <highlight><bold>606</bold></highlight>, and <highlight><bold>608</bold></highlight> may be highlighted. If the correctionist clicks the mouse in a new section of text, then a new group of words may be highlighted identically in each window <highlight><bold>602</bold></highlight>, <highlight><bold>604</bold></highlight>, <highlight><bold>606</bold></highlight>, and <highlight><bold>608</bold></highlight>. As shown the verbatim text window <highlight><bold>606</bold></highlight> and the final text window <highlight><bold>608</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 6</cross-reference>, the words and &rdquo; an ammonia&rdquo; and &ldquo;doctors met&rdquo; in the IBM Viavoice&trade; -generated text have been corrected. The words &ldquo;Doctor Smith.&rdquo; are highlighted. This highlighting works to inform the correctionist which group of words they are editing. Note that in this example, the correctionist has not yet corrected the misrecognized text &ldquo;Just&rdquo;. This could be modified later. </paragraph>
<paragraph id="P-0113" lvl="0"><number>&lsqb;0113&rsqb;</number> In one embodiment, the invention may rely upon the concept of &ldquo;utterance.&rdquo; Placeholders may delineate a given text into a set of utterances and a set of phrases. In speaking or reading aloud, a pause may be viewed as a brief arrest or suspension of voice, to indicate the limits and relations of sentences and their parts. In writing and printing, a pause may be a mark indicating the place and nature of an arrest of voice in speaking. Here, an utterance may be viewed as a group of words separated by a pause from another group of words. Moreover, a phrase may be viewed as a word or a first group of words that match or are different from a word or a second group of words. A word may be text, formatting characters, a command, and the like. </paragraph>
<paragraph id="P-0114" lvl="0"><number>&lsqb;0114&rsqb;</number> By way of example, the Dragon NaturallySpeaking&trade; engine works on the basis of utterances. In one embodiment, the phrases do not overlap any utterance placeholders such that the differences are not allowed to cross the boundary from one utterance to another. However, the inventors have discovered that this makes the process of determining where utterances in an IBM Viavoice&trade; SDK speech engine generated transcribed file are located difficult and problematic. Accordingly, in another embodiment, the phrases are arranged irrespective of the utterances, even to the point of overlapping utterance placeholder characters. In a third embodiment, the given text is delineated only by phrase placeholder characters and not by utterance placeholder characters. </paragraph>
<paragraph id="P-0115" lvl="0"><number>&lsqb;0115&rsqb;</number> Conventionally, the Dragon NaturallySpeaking&trade; engine learns when training occurs by correcting text within an utterance. Here the locations of utterances between each utterance placeholder characters must be tracked. However, the inventors have noted that transcribed phrases generated by two speech recognition engines give rise to matches and differences, but there is no definite and fixed relationship between utterance boundaries and differences and matches in text generated by two speech recognition engines. Sometimes a match or difference is contained within the start and end points of an utterance. Sometimes it is not. Furthermore, errors made by the engine may cross from one Dragon NaturallySpeaking&trade;-defined utterance to the next. Accordingly, speech engines may be trained more efficiently when text is corrected using phrases (where a phrase may represent a group of words, or a single word and associated formatting or punctuation (e.g., &ldquo;new paragraph&rdquo; &lsqb;double carriage return&rsqb; or &ldquo;period&rdquo; &lsqb;.&rsqb; or &ldquo;colon&rdquo; &lsqb;.&rsqb;). In other words, where the given text is delineated only by phrase placeholder characters, the speech editor <highlight><bold>225</bold></highlight> need not track the locations of utterances with utterance placeholder character. Moreover, as discussed below, the use of phrases permit the process <highlight><bold>200</bold></highlight> to develop statistics regarding the match text and use this information to make the correction process more efficient. </paragraph>
<paragraph id="P-0116" lvl="7"><number>&lsqb;0116&rsqb;</number> 1. Efficient Navigation </paragraph>
<paragraph id="P-0117" lvl="0"><number>&lsqb;0117&rsqb;</number> The speech editor <highlight><bold>225</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 2</cross-reference> becomes a powerful tool when the correctionist opens up the transcribed file from the second speech engine <highlight><bold>213</bold></highlight>. One reason for this is that the transcribed file from the second speech engine <highlight><bold>213</bold></highlight> provides a comparison text from which the transcribed file &ldquo;A&rdquo; from the first speech engine <highlight><bold>211</bold></highlight> may be compared and the differences highlighted. In other words, the speech editor <highlight><bold>225</bold></highlight> may track the individual differences and matches between the two transcribed texts and display both of these files, complete with highlighted differences and unhighlighted matches to the correctionist. </paragraph>
<paragraph id="P-0118" lvl="0"><number>&lsqb;0118&rsqb;</number> GNU is a project by The Free Software Foundation of Cambridge, Mass. to provide a freely distributable replacement for Unix. The speech editor <highlight><bold>225</bold></highlight> may employ, for example, a GNU file difference compare method or a Windows FC File Compare utility to generate the desired difference. </paragraph>
<paragraph id="P-0119" lvl="0"><number>&lsqb;0119&rsqb;</number> The matched phrases and difference phrases are interwoven with one another. That is, between two matched phrases may be a difference phrase and between two difference phrases may be a match phrase. The match phrases and the difference phrases permit a correctionist to evaluate and correct the text in a the final and verbatim windows <highlight><bold>606</bold></highlight>, <highlight><bold>608</bold></highlight> by selecting just differences, just matches, or both and playing back the audio for each selected match or phrase. When in the &ldquo;differences&rdquo; mode, the correctionist can quickly find differences between computer transcribed texts and the likely site of errors in any given transcribed text. </paragraph>
<paragraph id="P-0120" lvl="0"><number>&lsqb;0120&rsqb;</number> In editing text in the modifiable windows <highlight><bold>606</bold></highlight>, <highlight><bold>608</bold></highlight>, the correctionist may automatically and quickly navigate from match phrase to match phrase, difference phrase to difference phrase, or match phrase to contiguous difference phrase, each defined by the transcribed text windows <highlight><bold>602</bold></highlight>, <highlight><bold>604</bold></highlight>. Jumping from one difference phrase to the next difference phrase relieves the correctionist from having to evaluate a significant amount of text. Consequently, a transcriptionist need not listen to all the audio to determine where the probable errors are located. Depending upon the reliability of the transcription for the matches by both engines, the correctionist may not need to listen to any of the associated audio for the matched phrases. By reducing the time required to review text and audio, a correctionist can more quickly produce a verbatim text or final report. </paragraph>
<paragraph id="P-0121" lvl="7"><number>&lsqb;0121&rsqb;</number> 2. Reliability Index </paragraph>
<paragraph id="P-0122" lvl="0"><number>&lsqb;0122&rsqb;</number> &ldquo;Matches&rdquo; may be viewed as a word or a set of words for which two or more speech engines have transcribed the same audio file in the same way. As noted above, it was presumed that if two speech recognition programs manufactured by two different corporations are employed in the process <highlight><bold>200</bold></highlight> and both produces transcribed text phrases that match, then it is likely that such a match phrase is correct and consideration of it by the correctionist may be skipped. However, if two speech recognition programs manufactured by two different corporations are employed in the process and both produces transcribed text phrases that match, there still is a possibility that both speech recognition programs may have made a mistake. For example, in the screen shots accompanying <cross-reference target="DRAWINGS">FIG. 6</cross-reference>, both engines have misrecognized the spoken word &ldquo;underlying&rdquo; and transcribed &ldquo;underlining&rdquo;. The engines similarly misrecognized the spoken word &ldquo;of&rdquo; and transcribed &ldquo;are&rdquo; (in the phrase &ldquo;are the spine&rdquo;). While the evaluation of differences may reveal most, if not all, of the errors made by a speech recognition engine, there is the possibility that the same mistake has been made by both speech recognition engines <highlight><bold>211</bold></highlight>, <highlight><bold>213</bold></highlight> and will be overlooked. Accordingly, the speech editor <highlight><bold>225</bold></highlight> may include instructions to determine the reliability of transcribed text matches using data generated by the correctionist. This data may be used to create a reliability index for transcribed text matches. </paragraph>
<paragraph id="P-0123" lvl="0"><number>&lsqb;0123&rsqb;</number> In one embodiment, the correctionist navigates difference phrase by difference phrase. Assume that on completing preparation of the final and verbatim text for the differences in windows <highlight><bold>606</bold></highlight>, <highlight><bold>608</bold></highlight>, the correctionist decides to review the matches from text in windows <highlight><bold>602</bold></highlight>, <highlight><bold>604</bold></highlight>. The correctionist would go into &ldquo;matches&rdquo; mode and review the matched phrases. The correctionist selects the matched phrase in the transcribed text window <highlight><bold>602</bold></highlight>, <highlight><bold>604</bold></highlight>, listens to the audio, then corrects the match phrase in the modifiable windows <highlight><bold>606</bold></highlight>, <highlight><bold>608</bold></highlight>. This correction information, including the noted difference and the change made, is stored as data in the reliability index. Over time, this reliability index may build up with further data as additional mapping is performed using the word mapping function. </paragraph>
<paragraph id="P-0124" lvl="0"><number>&lsqb;0124&rsqb;</number> Using this data of the reliability index, it is possible to formulate a statistical reliability of the matched phrases and, based on this statistical reliability, have the speech editor <highlight><bold>225</bold></highlight> automatically judge the need for a correctionist to evaluate correct a matched phrase. As an example of skipping a matched phrase based on statistical reliability, assume that the Dragon NaturallySpeaking&trade; engine and the IBM Viavoice&trade; engine are used as speech engines <highlight><bold>211</bold></highlight>, <highlight><bold>213</bold></highlight> to transcribe the same audio file <highlight><bold>205</bold></highlight> (<cross-reference target="DRAWINGS">FIG. 2</cross-reference>). Here both speech engines <highlight><bold>211</bold></highlight>, <highlight><bold>213</bold></highlight> may have previously transcribed the matched word &ldquo;house&rdquo; many times for a particular speaker. Stored data may indicate that neither engine <highlight><bold>211</bold></highlight>, <highlight><bold>213</bold></highlight> had ever misrecognized and transcribed &ldquo;house&rdquo; for any other word or phrase uttered by the speaker. In that case, the statistical reliability index would be high. However, past recognition for a particular word or phrase would not necessarily preclude a future mistake. The program of the speech editor <highlight><bold>225</bold></highlight> may thus confidently permit the correctionist to skip the match phrase &ldquo;house&rdquo; in the correction window <highlight><bold>606</bold></highlight>, <highlight><bold>608</bold></highlight> with a very low probability that either speech engine <highlight><bold>211</bold></highlight>, <highlight><bold>213</bold></highlight> had made an error. </paragraph>
<paragraph id="P-0125" lvl="0"><number>&lsqb;0125&rsqb;</number> On the other hand, the transcription information might indicate that both speech engines <highlight><bold>211</bold></highlight>, <highlight><bold>213</bold></highlight> had frequently mistranscribed &ldquo;house&rdquo; when another word was spoken, such as &ldquo;mouse&rdquo; or &ldquo;spouse&rdquo;. Statistics may deem the transcription of this particular spoken word as having a low reliability. With a low reliability index, there would be a higher risk that both speech engines <highlight><bold>211</bold></highlight>, <highlight><bold>213</bold></highlight> had made the same mistake. The correctionist would more likely be inclined to select the match phrase in the correction window <highlight><bold>606</bold></highlight>, <highlight><bold>608</bold></highlight> and playback the associated audio with a view towards possible correction. Here the correctionist may preset one or more reliability index levels in the program of the speech editor <highlight><bold>225</bold></highlight> to permit the process <highlight><bold>200</bold></highlight> to skip over some match phrases and address other match phrases. The reliability index in the current application may reflect the previous transcription history of a word by at least two speech engines <highlight><bold>211</bold></highlight>, <highlight><bold>213</bold></highlight>. Moreover, the reliability index may be constructed in different ways with the available data, such as a reliability point and one or more reliability ranges. </paragraph>
<paragraph id="P-0126" lvl="7"><number>&lsqb;0126&rsqb;</number> <highlight><bold>3</bold></highlight>. Pasting </paragraph>
<paragraph id="P-0127" lvl="0"><number>&lsqb;0127&rsqb;</number> Word processors freely permit the pasting of text, figures, control characters, &ldquo;replacement&rdquo; pasting, and the like in a work document. Conventionally, this may be achieved through control-v &ldquo;pasting.&rdquo; However, such free pasting would throw off all text tracking of text within the modifiable windows <highlight><bold>606</bold></highlight>, <highlight><bold>608</bold></highlight>. In one embodiment, each of the transcribed text windows <highlight><bold>602</bold></highlight>, <highlight><bold>604</bold></highlight> may include a paste button <highlight><bold>610</bold></highlight>. In the dual speech engine mode where different transcribed text fills the first transcribed text window <highlight><bold>602</bold></highlight> and the second transcribed text window <highlight><bold>604</bold></highlight>, the paste button <highlight><bold>610</bold></highlight> saves the correctionist from having to type in the correction window <highlight><bold>606</bold></highlight>, <highlight><bold>608</bold></highlight> under certain circumstances. For example, assume that the second speech engine <highlight><bold>213</bold></highlight> is better trained than the first speech engine <highlight><bold>211</bold></highlight> and that the transcribed text from the first speech engine <highlight><bold>211</bold></highlight> fills the windows <highlight><bold>602</bold></highlight>, <highlight><bold>606</bold></highlight>, and <highlight><bold>608</bold></highlight>. Here the text from the second speech engine <highlight><bold>213</bold></highlight> may be pasted directly into the correction window <highlight><bold>606</bold></highlight>, <highlight><bold>608</bold></highlight>. </paragraph>
<paragraph id="P-0128" lvl="7"><number>&lsqb;0128&rsqb;</number> <highlight><bold>4</bold></highlight>. Deleting </paragraph>
<paragraph id="P-0129" lvl="0"><number>&lsqb;0129&rsqb;</number> Under certain circumstances, deleting words from one of the two modifiable windows <highlight><bold>606</bold></highlight>, <highlight><bold>608</bold></highlight> may result in a loss its associated audio. Without the associated audio, a human correctionist cannot determine whether the verbatim text words or the final report text words matches what was spoken by the human speaker. In particular, where an entire phrase or an entire utterance is deleted in the correction window <highlight><bold>606</bold></highlight>, <highlight><bold>608</bold></highlight>, its position among the remaining text may be lost. To indicate where the missing text was located, a visible &ldquo;yen&rdquo; (&ldquo;&yen;&rdquo;) character is placed so that the user can select this character and play back the audio for the deleted text. In addition, a repeated integral sign (&ldquo;&sect;&rdquo;) may be used as a marker for the end point of a match or difference within the body of a text. This sign may be hidden or viewed by the user, depending upon the option selected by the correctionist. </paragraph>
<paragraph id="P-0130" lvl="0"><number>&lsqb;0130&rsqb;</number> For example, assume that the text and invisible character phrase placeholders &ldquo;&sect;&rdquo; appeared as follows:</paragraph>
<paragraph lvl="0"><in-line-formula>&sect;1111111&sect;&sect;2222222&sect;&sect;33333333333&sect;&sect;4444444&sect;&sect;55555555&sect;</in-line-formula></paragraph>
<paragraph id="P-0131" lvl="0"><number>&lsqb;0131&rsqb;</number> If the phrase &ldquo;33333333333&rdquo; were deleted, the inventors discovered that the text and phrase placeholders &ldquo;&sect;&rdquo; would appeared as follows:</paragraph>
<paragraph lvl="0"><in-line-formula>&sect;<highlight><bold>1111111</bold></highlight>&sect;&sect;<highlight><bold>2222222</bold></highlight>&sect;&sect;&sect;&sect;<highlight><bold>4444444</bold></highlight>&sect;&sect;<highlight><bold>55555555</bold></highlight>&sect;</in-line-formula></paragraph>
<paragraph id="P-0132" lvl="0"><number>&lsqb;0132&rsqb;</number> Here four placeholders &ldquo;&sect;&rdquo; now appear adjacent to one another. If a phrase placeholder was represented by two invisible characters, and a bolding placeholder was represented by four invisible placeholders, and the correctionist deleted an entire phrase, the four invisible characters which would be misinterpreted as a bolding placeholder. </paragraph>
<paragraph id="P-0133" lvl="0"><number>&lsqb;0133&rsqb;</number> One solution to this problem is as follows. If an utterance or phrase is reduced to zero contents, the speech editor <highlight><bold>225</bold></highlight> may automatically insert a visible placeholder character such as &ldquo;&yen;&rdquo; so that the text and phrase placeholders &ldquo;&sect;&rdquo; may appeared as follows:</paragraph>
<paragraph lvl="0"><in-line-formula>&sect;<highlight><bold>1111111</bold></highlight>&sect;&sect;<highlight><bold>2222222</bold></highlight>&sect;&sect;&yen;&sect;&sect;<highlight><bold>4444444</bold></highlight>&sect;&sect;<highlight><bold>55555555</bold></highlight>&sect;</in-line-formula></paragraph>
<paragraph id="P-0134" lvl="0"><number>&lsqb;0134&rsqb;</number> This above method works to prevent characters from having two identical types appear contiguously in a row. Preferably, the correctionist would not be able to manually delete this character. Moreover, if the correctionist started adding text to the space in which the visible placeholder character &ldquo;&yen;&rdquo; appears, the speech editor <highlight><bold>225</bold></highlight> may automatically remove the visible placeholder character &ldquo;&yen;&rdquo;. </paragraph>
<paragraph id="P-0135" lvl="7"><number>&lsqb;0135&rsqb;</number> D. Speech Editor having Word Mapping Tool </paragraph>
<paragraph id="P-0136" lvl="0"><number>&lsqb;0136&rsqb;</number> Returning to <cross-reference target="DRAWINGS">FIG. 2</cross-reference>, after the decision to create verbatim text <highlight><bold>229</bold></highlight> at step <highlight><bold>228</bold></highlight> and the decision to create final text <highlight><bold>231</bold></highlight> at step <highlight><bold>230</bold></highlight>, the process <highlight><bold>200</bold></highlight> may proceed to step <highlight><bold>232</bold></highlight>. At step <highlight><bold>232</bold></highlight>, the process <highlight><bold>200</bold></highlight> may determine whether to do word mapping. If no, the process <highlight><bold>200</bold></highlight> may proceed to step <highlight><bold>234</bold></highlight> where the verbatim text <highlight><bold>229</bold></highlight> may be saved as a training file. If yes, the process <highlight><bold>200</bold></highlight> may encounter a word mapping tool <highlight><bold>235</bold></highlight> at step <highlight><bold>236</bold></highlight>. For instance, when the accuracy of the transcribed text is poor, mapping may be too difficult. Accordingly, a correctionist may manually indicate that no mapping is desired. </paragraph>
<paragraph id="P-0137" lvl="0"><number>&lsqb;0137&rsqb;</number> The word mapping tool <highlight><bold>235</bold></highlight> of the invention provides a graphical user interface window within which an editor may align or map the transcribed text &ldquo;A&rdquo; to the verbatim text <highlight><bold>229</bold></highlight> to create a word mapping file. Since the transcribed text &ldquo;A&rdquo; is already aligned to the audio file <highlight><bold>205</bold></highlight> through audio tags, mapping the transcribed text &ldquo;A&rdquo; to the verbatim text <highlight><bold>229</bold></highlight> creates an chain of alignment between the verbatim text <highlight><bold>229</bold></highlight> and the audio file <highlight><bold>205</bold></highlight>. Essentially, this mapping between the verbatim text <highlight><bold>229</bold></highlight> and the audio file <highlight><bold>205</bold></highlight> provides speaker acoustic information and a speaker language model. The word mapping tool <highlight><bold>235</bold></highlight> provides at least the following advantages. </paragraph>
<paragraph id="P-0138" lvl="0"><number>&lsqb;0138&rsqb;</number> First, the word mapping tool <highlight><bold>235</bold></highlight> may be used to reduce the number of transcribed words to be corrected in a correction window. Under certain circumstances, it may be desirable to reduce the number of transcribed words to be corrected in a correction window. For example, as a speech engine, Dragon NaturallySpeaking&trade; permits an unlimited number of transcribed words to be corrected in the correction window. However, the correction window for the speech engine by IBM Viavoice&trade; SDK can substitute no more than ten words (and the corrected text itself cannot be longer than ten words). The correction windows <highlight><bold>306</bold></highlight>, <highlight><bold>308</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 3</cross-reference> in comparison with <cross-reference target="DRAWINGS">FIG. 4</cross-reference> or <cross-reference target="DRAWINGS">FIG. 5</cross-reference> illustrates drawbacks of limiting the correction windows <highlight><bold>306</bold></highlight>, <highlight><bold>308</bold></highlight> to no more than ten words. If there were a substantial number of errors in the transcribed text &ldquo;A&rdquo; where some of those errors comprised more than ten words, these errors could not be corrected using the IBM Viavoice&trade; SDK speech engine, for example. Thus, it may be desirable to reduce the number of transcribed words to be corrected in a correction window to less than eleven. </paragraph>
<paragraph id="P-0139" lvl="0"><number>&lsqb;0139&rsqb;</number> Second, because the mapping file represents an alignment between the transcribed text &ldquo;A&rdquo; and the verbatim text <highlight><bold>229</bold></highlight>, the mapping file may be used to automatically correct the transcribed text &ldquo;A&rdquo; during an automated correction session. Here, automatically correcting the transcribed text &ldquo;A&rdquo; during the correction session provides a training event from which the user speech files may be updated in advance correcting the speech engine. The inventors have found that this initial boost to the user speech files of a speech engine works to achieve a greater accuracy for the speech engine as compared to those situations where no word mapping file exists. </paragraph>
<paragraph id="P-0140" lvl="0"><number>&lsqb;0140&rsqb;</number> And third, the process of enrollment&mdash;creating speaker acoustic information and a speaker language model&mdash;and continuing training may be removed from the human speaker so as to make the speech engine a more desirable product to the speaker. One of the most discouraging aspects of conventional speech recognition programs is the enrollment process. The idea of reading from a prepared text for fifteen to thirty minutes and then manually correcting the speech engine merely to begin using the speech engine could hardly appeal to any speaker. Eliminating the need for a speaker to enroll in a speech program may make each speech engine more significantly desirable to consumers. </paragraph>
<paragraph id="P-0141" lvl="0"><number>&lsqb;0141&rsqb;</number> On encountering the word mapping tool <highlight><bold>235</bold></highlight> at step <highlight><bold>236</bold></highlight>, the process <highlight><bold>200</bold></highlight> may open a mapping window <highlight><bold>700</bold></highlight>. <cross-reference target="DRAWINGS">FIG. 7</cross-reference> illustrates an example of a mapping window <highlight><bold>700</bold></highlight>. The mapping window <highlight><bold>700</bold></highlight> may appear, for example, on the video monitor <highlight><bold>110</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 1</cross-reference> as a graphical user interface based on instructions executed by the computer <highlight><bold>120</bold></highlight> that are associated as a program with the word mapping tool <highlight><bold>235</bold></highlight> of the invention. </paragraph>
<paragraph id="P-0142" lvl="0"><number>&lsqb;0142&rsqb;</number> As seen in <cross-reference target="DRAWINGS">FIG. 7</cross-reference>, the mapping window <highlight><bold>700</bold></highlight> may include a verbatim text window <highlight><bold>702</bold></highlight> and a transcribed text window <highlight><bold>704</bold></highlight>. Verbatim text <highlight><bold>229</bold></highlight> may appear in the verbatim text window <highlight><bold>702</bold></highlight> and transcribed text &ldquo;A&rdquo; may appear in the transcribed text window <highlight><bold>704</bold></highlight>. </paragraph>
<paragraph id="P-0143" lvl="0"><number>&lsqb;0143&rsqb;</number> The verbatim window <highlight><bold>702</bold></highlight> may display the verbatim text <highlight><bold>229</bold></highlight> in a column, word by word. As set of words, the verbatim text <highlight><bold>229</bold></highlight> may be grouped together based on match/difference phrases <highlight><bold>706</bold></highlight> by running a difference program (such as DIFF available in GNU and MICROSOFT) between the transcribed text &ldquo;A&rdquo; (produced by the first speech engine <highlight><bold>211</bold></highlight>) and a transcribed text &ldquo;B&rdquo; produced by the second speech engine <highlight><bold>213</bold></highlight>. Within each phrase <highlight><bold>706</bold></highlight>, the number of verbatim words <highlight><bold>708</bold></highlight> may be sequentially numbered. For example, for the third phrase &ldquo;pneumonia.&rdquo;, there are two words: &ldquo;pneumonia&rdquo; and the punctuation mark &ldquo;period&rdquo; (seen as in <cross-reference target="DRAWINGS">FIG. 7</cross-reference>). Accordingly, &ldquo;pneumonia&rdquo; of the verbatim text <highlight><bold>229</bold></highlight> may be designated as phrase three, word one (&ldquo;3-1&rdquo;) and &ldquo;.&rdquo; may be designated as phrase three, word <highlight><bold>2</bold></highlight> (&ldquo;3-2&rdquo;). In comparing the transcribed text &ldquo;A&rdquo; produced by the first speech engine <highlight><bold>211</bold></highlight> and the transcribed text produced by the second speech engine <highlight><bold>213</bold></highlight>, consideration must be given to commands such as &ldquo;new paragraph.&rdquo; For example, in the fourth phrase of the transcribed text &ldquo;A&rdquo;, the first word is a new paragraph command (seen as &ldquo;&sub;&sub;&rdquo;) that resulted in two carriage returns. </paragraph>
<paragraph id="P-0144" lvl="0"><number>&lsqb;0144&rsqb;</number> At step <highlight><bold>238</bold></highlight>, the process <highlight><bold>200</bold></highlight> may determine whether to do word mapping for the first speech engine <highlight><bold>211</bold></highlight>. If yes, the transcribed text window <highlight><bold>704</bold></highlight> may display the transcribed text &ldquo;A&rdquo; in a column, word by word. A set of words in the transcribed text &ldquo;A&rdquo; also may be grouped together based on the match/difference phrases <highlight><bold>706</bold></highlight>. Within each phrase <highlight><bold>706</bold></highlight> of the transcribed text &ldquo;A&rdquo;, the number of transcribed words <highlight><bold>710</bold></highlight> may be sequentially numbered. </paragraph>
<paragraph id="P-0145" lvl="0"><number>&lsqb;0145&rsqb;</number> In the example shown in <cross-reference target="DRAWINGS">FIG. 7</cross-reference>, the transcribed text &ldquo;A&rdquo; resulting from a sample audio file <highlight><bold>205</bold></highlight> transcribed by the first speech engine <highlight><bold>211</bold></highlight> is illustrated. Alternatively, a correctionist may have selected the second speech engine <highlight><bold>213</bold></highlight> to be used and shown in the transcribed text window <highlight><bold>704</bold></highlight>. As seen in transcribed text window <highlight><bold>704</bold></highlight>, passing the audio file <highlight><bold>205</bold></highlight> through the first speech engine <highlight><bold>211</bold></highlight> resulted in the audio phrase &ldquo;pneumonia.&rdquo; being translated into the transcribed text &ldquo;A&rdquo; as &ldquo;an ammonia.&rdquo; by the first speech engine <highlight><bold>211</bold></highlight> (here, the IBM Viavoice&trade; SDK speech engine). Thus, for the third phrase &ldquo;an ammonia.&rdquo;, there are three words: &ldquo;an&rdquo;, &ldquo;ammonia&rdquo; and the punctuation mark &ldquo;period&rdquo; (seen as &ldquo;.&rdquo; in <cross-reference target="DRAWINGS">FIG. 7</cross-reference>, transcribed text window <highlight><bold>704</bold></highlight>). Accordingly, the word &ldquo;an&rdquo; may be designated 3-1, the word &ldquo;ammonia&rdquo; may be designated 3-2, and the word &ldquo;. &rdquo; may be designated as 3-3. </paragraph>
<paragraph id="P-0146" lvl="0"><number>&lsqb;0146&rsqb;</number> In the example shown in <cross-reference target="DRAWINGS">FIG. 7</cross-reference>, the verbatim text <highlight><bold>229</bold></highlight> and the transcribed text &ldquo;A&rdquo; were parsed into twenty seven phrases based on the difference between the transcribed text &ldquo;A&rdquo; produced by the first speech engine <highlight><bold>211</bold></highlight> and the transcribed text produced by the second speech engine <highlight><bold>213</bold></highlight>. The number of phrases may be displayed in the GUI and is identified as element <highlight><bold>712</bold></highlight> in <cross-reference target="DRAWINGS">FIG. 7</cross-reference>. The first phrase (not shown) was not matched; that is the first speech engine <highlight><bold>211</bold></highlight> translated the audio file <highlight><bold>205</bold></highlight> into the first phrase differently from the second speech engine <highlight><bold>213</bold></highlight>. The second phrase (partially seen in <cross-reference target="DRAWINGS">FIG. 7</cross-reference>) was a match. The first speech engine <highlight><bold>211</bold></highlight> (here, IBM Viavoice&trade; SDK), translated the third phrase &ldquo;pneumonia.&rdquo; of the audio file <highlight><bold>205</bold></highlight> as &ldquo;an ammonia.&rdquo; In a view not shown, the second speech engine <highlight><bold>213</bold></highlight> (here, Dragon NaturallySpeaking&trade; ) translated &ldquo;pneumonia.&rdquo; as &ldquo;Himalayan.&rdquo; Since &ldquo;an ammonia.&rdquo; is different from &ldquo;Himalayan.&rdquo;, the third phrase within the phrases <highlight><bold>706</bold></highlight> was automatically characterized as a difference phrase by the process <highlight><bold>200</bold></highlight>. </paragraph>
<paragraph id="P-0147" lvl="0"><number>&lsqb;0147&rsqb;</number> Since the verbatim text <highlight><bold>229</bold></highlight> represents exactly what was spoken at the third phrase within the phrases <highlight><bold>706</bold></highlight>, it is known that the verbatim text at this phrase is &ldquo;pneumonia.&rdquo; Thus, &ldquo;an ammonia.&rdquo; must somehow map to the phrase &ldquo;pneumonia.&rdquo;. Within the transcribed text window <highlight><bold>704</bold></highlight> of the example of <cross-reference target="DRAWINGS">FIG. 7</cross-reference>, the editor may select the box next to phrase three, word one (3-1) &ldquo;an&rdquo;, the box next to 3-2 &ldquo;ammonia&rdquo;. Within the verbatim window <highlight><bold>702</bold></highlight>, the editor may select the box next to 3-1 &ldquo;pneumonia&rdquo;. The editor then may select &ldquo;map&rdquo; from buttons <highlight><bold>714</bold></highlight>. This process may be repeated for each word in the transcribed text &ldquo;A&rdquo; to obtain a first mapping file at step <highlight><bold>240</bold></highlight> (see <cross-reference target="DRAWINGS">FIG. 2</cross-reference>). In making the mapping decisions, the computer may limit an editor or self-limit the number of verbatim words and transcribed words mapped to one another to less than eleven. Once phrases are mapped, they may be removed from the view of the mapping window <highlight><bold>700</bold></highlight>. </paragraph>
<paragraph id="P-0148" lvl="0"><number>&lsqb;0148&rsqb;</number> At step <highlight><bold>202</bold></highlight>, the mapping may be saved ads a first training file and the process <highlight><bold>200</bold></highlight> advanced to step <highlight><bold>244</bold></highlight>. Alternatively, if at step <highlight><bold>238</bold></highlight> the decision is made to forgo doing word mapping for the first speech engine <highlight><bold>211</bold></highlight>, the process advances to step <highlight><bold>244</bold></highlight>. At step <highlight><bold>244</bold></highlight>, a decision is made as to whether to do word mapping for the second speech engine <highlight><bold>213</bold></highlight>. If yes, a second mapping file may be created at step <highlight><bold>246</bold></highlight>, saved as a second training file at step <highlight><bold>248</bold></highlight>, and the process <highlight><bold>200</bold></highlight> may proceed to step <highlight><bold>250</bold></highlight> to encounter a correction session <highlight><bold>251</bold></highlight>. If the decision is made to forgo word mapping of the second speech engine <highlight><bold>213</bold></highlight>, the process <highlight><bold>200</bold></highlight> may proceed to step <highlight><bold>250</bold></highlight> to encounter the correction session <highlight><bold>251</bold></highlight> </paragraph>
<paragraph id="P-0149" lvl="7"><number>&lsqb;0149&rsqb;</number> 1. Efficient Navigation </paragraph>
<paragraph id="P-0150" lvl="0"><number>&lsqb;0150&rsqb;</number> Although mapping each word of the transcribed text may work to create a mapping file, it is desirable to permit an editor to efficiently navigate though the transcribed text in the mapping window <highlight><bold>700</bold></highlight>. Some rules may be developed to make the mapping window <highlight><bold>700</bold></highlight> a more efficient navigation environment. </paragraph>
<paragraph id="P-0151" lvl="0"><number>&lsqb;0151&rsqb;</number> If two speech engines manufactured by two different corporations are employed with both producing various transcribed text phrases at step <highlight><bold>214</bold></highlight> (<cross-reference target="DRAWINGS">FIG. 2</cross-reference>) that match, then it is likely that such matched phrases of the transcribed text and their associated verbatim text phrases can be aligned automatically by the word mapping tool <highlight><bold>235</bold></highlight> of the invention. As another example, for a given phrase, if the number of the verbatim words <highlight><bold>708</bold></highlight> is one, then all the transcribed words <highlight><bold>710</bold></highlight> of that same phrase could only be mapped to this one word of the verbatim words <highlight><bold>708</bold></highlight>, no matter how many number of the words X are in the transcribed words <highlight><bold>710</bold></highlight> for this phrase. The converse is also true. If the number of the transcribed words <highlight><bold>710</bold></highlight> for a give phrase is one, then all the verbatim words <highlight><bold>708</bold></highlight> of that same phrase could only be mapped to this one word of the transcribed words <highlight><bold>710</bold></highlight>. As another example of automatic mapping, if the number of the words X of the verbatim words <highlight><bold>708</bold></highlight> for a given phrase equals the number of the words X of the transcribed words <highlight><bold>710</bold></highlight>, then all of the verbatim words <highlight><bold>708</bold></highlight> of this phrase may be automatically mapped to all of the transcribed words <highlight><bold>710</bold></highlight> for this same phrase. After this automatic mapping is done, the mapped phrases are no longer displayed in the mapping window <highlight><bold>700</bold></highlight>. Thus, navigation may be improved. </paragraph>
<paragraph id="P-0152" lvl="0"><number>&lsqb;0152&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 8</cross-reference> illustrates options <highlight><bold>800</bold></highlight> having automatic mapping options for the word mapping tool <highlight><bold>235</bold></highlight> of the invention. The automatic mapping option Map X to X <highlight><bold>802</bold></highlight> represents the situation where the number of the words X of the verbatim words <highlight><bold>708</bold></highlight> for a given phrase equals the number of the words X of the transcribed words <highlight><bold>710</bold></highlight>. The automatic mapping option Map X to <highlight><bold>1</bold></highlight> <highlight><bold>804</bold></highlight> represents the situation where the number of words in the transcribed words <highlight><bold>710</bold></highlight> for a given phrase is equal to one. Moreover, the automatic mapping option Map <highlight><bold>1</bold></highlight> to X <highlight><bold>806</bold></highlight> represents the situation where the number of words in the verbatim words <highlight><bold>708</bold></highlight> for a given phrase is equal to one. As shown, each of these options may be selected individually in various manners known in the user interface art. </paragraph>
<paragraph id="P-0153" lvl="0"><number>&lsqb;0153&rsqb;</number> Returning to <cross-reference target="DRAWINGS">FIG. 7</cross-reference> with the automatic mapping options selected and an auto advance feature activated as indicated by a check <highlight><bold>716</bold></highlight>, the word mapping tool <highlight><bold>235</bold></highlight> automatically mapped the first phrase and the second phrase so as to present the third phrase at the beginning of the subpanels <highlight><bold>702</bold></highlight> and <highlight><bold>704</bold></highlight> such that the editor may evaluate and map the particular verbatim words <highlight><bold>708</bold></highlight> and the particular transcribed words <highlight><bold>710</bold></highlight>. As may be seen <cross-reference target="DRAWINGS">FIG. 7, a</cross-reference> &ldquo;&num; complete&rdquo; label <highlight><bold>718</bold></highlight> indicates that the number of verbatim and transcribed phrases already mapped by the word mapping tool <highlight><bold>235</bold></highlight> (in this example, nineteen). This means that the editor need only evaluate and map eight phrases as opposed to manually evaluating and mapping all twenty seven phrases. </paragraph>
<paragraph id="P-0154" lvl="0"><number>&lsqb;0154&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 9</cross-reference> of the drawings is a view of an exemplary graphical user interface <highlight><bold>900</bold></highlight> to support the present invention. As seen, GUI <highlight><bold>900</bold></highlight> may include multiple windows, including the first transcribed text window <highlight><bold>602</bold></highlight>, the second transcribed text window <highlight><bold>604</bold></highlight>, and two correction windows&mdash;the verbatim text window <highlight><bold>606</bold></highlight> and the final text window <highlight><bold>608</bold></highlight>. Moreover, GUI <highlight><bold>900</bold></highlight> may include the verbatim text window <highlight><bold>702</bold></highlight> and the transcribed text window <highlight><bold>704</bold></highlight>. As known, the location, size, and shape of the various windows displayed in <cross-reference target="DRAWINGS">FIG. 9</cross-reference> may be modified to a correctionist&apos;s taste. </paragraph>
<paragraph id="P-0155" lvl="7"><number>&lsqb;0155&rsqb;</number> 2. Reliability Index </paragraph>
<paragraph id="P-0156" lvl="0"><number>&lsqb;0156&rsqb;</number> Above, it was presumed that if two different speech engines (e.g., manufactured by two different corporations or one engine run twice with different settings) are employed with both producing transcribed text phrases that match, then it is likely that such a match phrase and its associated verbatim text phrase can be aligned automatically by the word mapping tool <highlight><bold>235</bold></highlight>. However, even if two different speech engines are employed and both produce matching phrases, there still is a possibility that both speech engines may have made the same mistake. Thus, this presumption or automatic mapping rule raises reliability issues. </paragraph>
<paragraph id="P-0157" lvl="0"><number>&lsqb;0157&rsqb;</number> If only different phrases of the phrases <highlight><bold>706</bold></highlight> are reviewed by the editor, the possibility that the same mistake made by both speech engines <highlight><bold>211</bold></highlight>, <highlight><bold>213</bold></highlight> will be overlooked. Accordingly, the word mapping tool <highlight><bold>235</bold></highlight> may facilitate the review of the reliability of transcribed text matches using data generated by the word mapping tool <highlight><bold>235</bold></highlight>. This data may be used to create a reliability index for transcribed text matches similar to that used in <cross-reference target="DRAWINGS">FIG. 6</cross-reference>. This reliability index may be used to create a &ldquo;stop word&rdquo; list. The stop word list may be selectively used to override automatic mapping and determine various reliability trends. </paragraph>
<paragraph id="P-0158" lvl="7"><number>&lsqb;0158&rsqb;</number> E. The Correction Session <highlight><bold>251</bold></highlight> </paragraph>
<paragraph id="P-0159" lvl="0"><number>&lsqb;0159&rsqb;</number> With a training file saved at either step <highlight><bold>234</bold></highlight>, <highlight><bold>242</bold></highlight>, or <highlight><bold>248</bold></highlight>, the process <highlight><bold>200</bold></highlight> may proceed to the step <highlight><bold>250</bold></highlight> to encounter the correction session <highlight><bold>251</bold></highlight>. The correction session <highlight><bold>251</bold></highlight> involves automatically correcting a text file. The lesson learned may be input into a speech engine by updating the user speech files. </paragraph>
<paragraph id="P-0160" lvl="0"><number>&lsqb;0160&rsqb;</number> At step <highlight><bold>252</bold></highlight>, the first speech engine <highlight><bold>211</bold></highlight> may be selected for automatic correction. At step <highlight><bold>254</bold></highlight>, the appropriate training file may be loaded. Recall that the training files may have been saved at steps <highlight><bold>234</bold></highlight>, <highlight><bold>242</bold></highlight>, and <highlight><bold>248</bold></highlight>. At step <highlight><bold>256</bold></highlight>, the process <highlight><bold>200</bold></highlight> may determine whether a mapping file exists for the selected speech engine, here the first speech engine <highlight><bold>211</bold></highlight>. If yes, the appropriate session file (such as an engine session file (.ses)) may be read in at step <highlight><bold>258</bold></highlight> from the location in which it was saved during the step <highlight><bold>218</bold></highlight>. </paragraph>
<paragraph id="P-0161" lvl="0"><number>&lsqb;0161&rsqb;</number> At step <highlight><bold>260</bold></highlight>, the mapping file may be processed. At step <highlight><bold>262</bold></highlight> the transcribed text &ldquo;A&rdquo; from the step <highlight><bold>214</bold></highlight> may automatically be corrected according to the mapping file. Using the preexisting speech engine, this automatic correction works to create speaker acoustic information and a speaker language model for that speaker on that particular speech engine. At step <highlight><bold>264</bold></highlight>, an incremental value &ldquo;N&rdquo; is assigned equal to zero. At step <highlight><bold>266</bold></highlight>, the user speech files may be updated with the speaker acoustic information and the speaker language model created at step <highlight><bold>262</bold></highlight>. Updating the user speech files with this speaker acoustic information and speaker language model achieves a greater accuracy for the speech engine as compared to those situations where no word mapping file exists. </paragraph>
<paragraph id="P-0162" lvl="0"><number>&lsqb;0162&rsqb;</number> If no mapping file exists at step <highlight><bold>256</bold></highlight> for the engine selected in step <highlight><bold>252</bold></highlight>, the process <highlight><bold>200</bold></highlight> proceeds to step <highlight><bold>268</bold></highlight>. At step <highlight><bold>268</bold></highlight>, a difference is created between the transcribed text &ldquo;A&rdquo; of the step <highlight><bold>214</bold></highlight> and the verbatim text <highlight><bold>229</bold></highlight>. At step <highlight><bold>270</bold></highlight>, an incremental value &ldquo;N&rdquo; is assigned equal to zero. At step <highlight><bold>272</bold></highlight>, the differences between the transcribed text &ldquo;A&rdquo; of the step <highlight><bold>214</bold></highlight> and the verbatim text <highlight><bold>229</bold></highlight> are automatically corrected based on the user speech files in existence at that time in the process <highlight><bold>200</bold></highlight>. This automatic correction works to create speaker acoustic information and a speaker language model with which the user speech files may be updated at step <highlight><bold>266</bold></highlight>. </paragraph>
<paragraph id="P-0163" lvl="0"><number>&lsqb;0163&rsqb;</number> In an embodiment of the invention, the matches between the transcribed text &ldquo;A&rdquo; of the step <highlight><bold>214</bold></highlight> and the verbatim text <highlight><bold>229</bold></highlight> are automatically corrected in addition to or in the alternate from the differences. As disclosed more fully in co-pending U.S. Non-Provisional application Ser. No. 09/362,255, the assignees of the present patent disclosed a system in which automatically correcting matches worked to improve the accuracy of a speech engine. From step <highlight><bold>266</bold></highlight>, the process <highlight><bold>200</bold></highlight> may proceed to the step <highlight><bold>274</bold></highlight>. </paragraph>
<paragraph id="P-0164" lvl="0"><number>&lsqb;0164&rsqb;</number> At the step <highlight><bold>274</bold></highlight>, the correction session <highlight><bold>251</bold></highlight> may determine the accuracy percentage of either the automatic correction <highlight><bold>262</bold></highlight> or the automatic correction at step <highlight><bold>272</bold></highlight>. This accuracy percentage is calculated by the simple formula: Correct Word Count/Total Word Count. At step <highlight><bold>276</bold></highlight>, the process <highlight><bold>200</bold></highlight> may determine whether a predetermined target accuracy has been reached. An example of a predetermined target accuracy is 95%. </paragraph>
<paragraph id="P-0165" lvl="0"><number>&lsqb;0165&rsqb;</number> If the target accuracy has not been reached, then the process <highlight><bold>200</bold></highlight> may determine at step <highlight><bold>278</bold></highlight> whether the value of the increment N is greater than a predetermined number of maximum iterations, which is a value that may be manually selected or other wise predetermined. Step <highlight><bold>278</bold></highlight> works to prevent the correction session <highlight><bold>251</bold></highlight> from continuing forever. </paragraph>
<paragraph id="P-0166" lvl="0"><number>&lsqb;0166&rsqb;</number> If the value of the increment N is not greater than the predetermined number of maximum iterations, then the increment N is increased by one at step <highlight><bold>280</bold></highlight> (so that now N&minus;1) and the process <highlight><bold>200</bold></highlight> proceeds to step <highlight><bold>282</bold></highlight>. At step <highlight><bold>282</bold></highlight>, the audio file <highlight><bold>205</bold></highlight> is transcribed into a transcribed text <highlight><bold>1</bold></highlight>. At step <highlight><bold>284</bold></highlight>, differences are created between the transcribed text <highlight><bold>1</bold></highlight> and the verbatim text <highlight><bold>229</bold></highlight>. These differences may be corrected at step <highlight><bold>272</bold></highlight>, from which the first speech engine <highlight><bold>211</bold></highlight> may learn at step <highlight><bold>266</bold></highlight>. Recall that at step <highlight><bold>266</bold></highlight>, the user speech files may be updated with the speaker acoustic information and the speaker language model. </paragraph>
<paragraph id="P-0167" lvl="0"><number>&lsqb;0167&rsqb;</number> This iterative process continues until either the target accuracy is reached at step <highlight><bold>276</bold></highlight> or the value of the increment N is greater than the predetermined number of maximum iterations at step <highlight><bold>278</bold></highlight>. At the occurrence of either situation, the process <highlight><bold>200</bold></highlight> proceeds to step <highlight><bold>286</bold></highlight>. At step <highlight><bold>286</bold></highlight>, the process may determine whether to do word mapping at this juncture (such as in the situation of an non-enrolled user profile as discussed below). If yes, the process <highlight><bold>200</bold></highlight> proceeds to the word mapping tool <highlight><bold>235</bold></highlight>. If no, the process <highlight><bold>200</bold></highlight> may proceed to step <highlight><bold>288</bold></highlight>. </paragraph>
<paragraph id="P-0168" lvl="0"><number>&lsqb;0168&rsqb;</number> At step <highlight><bold>288</bold></highlight>, the process <highlight><bold>200</bold></highlight> may determine whether to repeat the correction session, such as for the second speech engine <highlight><bold>213</bold></highlight>. If yes, the process <highlight><bold>200</bold></highlight> may proceed to the step <highlight><bold>250</bold></highlight> to encounter the correction session. If no the process <highlight><bold>200</bold></highlight> may end. </paragraph>
<paragraph id="P-0169" lvl="7"><number>&lsqb;0169&rsqb;</number> F. Non-Enrolled User Profile cont. </paragraph>
<paragraph id="P-0170" lvl="0"><number>&lsqb;0170&rsqb;</number> As discussed above, the inventors have discovered that iteratively processing the audio file <highlight><bold>205</bold></highlight> with a non-enrolled user profile through the correction session <highlight><bold>251</bold></highlight> of the invention surprisingly resulted in growing the accuracy of a speech engine to a point at which the speaker may be presented with a speech product from which the accuracy reasonably may be grown. Increasing the accuracy of a speech engine with a non-enrolled user profile may occur as follows. </paragraph>
<paragraph id="P-0171" lvl="0"><number>&lsqb;0171&rsqb;</number> At step <highlight><bold>208</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 2, a</cross-reference> non-enrolled user profile may be created. The transcribed text &ldquo;A&rdquo; may be obtained at the step <highlight><bold>214</bold></highlight> and the verbatim text <highlight><bold>229</bold></highlight> may be created at the step <highlight><bold>228</bold></highlight>. Creating the final text at step <highlight><bold>230</bold></highlight> and the word mapping process as step <highlight><bold>232</bold></highlight> may be bypassed so that the verbatim text <highlight><bold>229</bold></highlight> may be saved at step <highlight><bold>234</bold></highlight>. </paragraph>
<paragraph id="P-0172" lvl="0"><number>&lsqb;0172&rsqb;</number> At step <highlight><bold>252</bold></highlight>, the first speech engine <highlight><bold>211</bold></highlight> may be selected and the training file from step <highlight><bold>234</bold></highlight> may be loaded at step <highlight><bold>254</bold></highlight>. With no mapping file, the process <highlight><bold>200</bold></highlight> may create a difference between the transcribed text &ldquo;A&rdquo; and the verbatim text <highlight><bold>229</bold></highlight> at step <highlight><bold>268</bold></highlight>. When the user files <highlight><bold>266</bold></highlight> are updated at step <highlight><bold>266</bold></highlight>, the correction of any differences at step <highlight><bold>272</bold></highlight> effectively may teach the first speech engine <highlight><bold>211</bold></highlight> about what verbatim text should go with what audio for a given audio file <highlight><bold>205</bold></highlight>. By iteratively muscling this automatic correction process around the correction cycle, the accuracy percentage of the first session engine <highlight><bold>211</bold></highlight> increases. </paragraph>
<paragraph id="P-0173" lvl="0"><number>&lsqb;0173&rsqb;</number> Under these specialized circumstances (among others), the target accuracy at step <highlight><bold>276</bold></highlight> may be set low (say, approximately 45%) relative to a desired accuracy level (say, approximately 95%). In this context, the process of increasing the accuracy of a speech engine with a non-enrolled user profile may be a precursor process to performing word mapping. Thus, if the lower target accuracy is reached at step <highlight><bold>276</bold></highlight>, the process <highlight><bold>200</bold></highlight> may proceed to the word mapping tool <highlight><bold>235</bold></highlight> through step <highlight><bold>286</bold></highlight>. Alternatively, in the event the lowered target accuracy may not be reached with the initial model and the audio file <highlight><bold>205</bold></highlight>, the maximum iterations may cause the process <highlight><bold>200</bold></highlight> to continue to step <highlight><bold>286</bold></highlight>. Thus, if the target accuracy has not been reached at step <highlight><bold>276</bold></highlight> and the value of the increment N is greater than the predetermined number of maximum iterations at step <highlight><bold>278</bold></highlight>, it may be necessary to engage in word mapping to give the accuracy a leg up. Here, step <highlight><bold>286</bold></highlight> may be reached from step <highlight><bold>278</bold></highlight>. At step <highlight><bold>278</bold></highlight>, the process <highlight><bold>200</bold></highlight> may proceed to the word mapping tool <highlight><bold>235</bold></highlight>. </paragraph>
<paragraph id="P-0174" lvl="0"><number>&lsqb;0174&rsqb;</number> In the alternative, the target accuracy at step <highlight><bold>276</bold></highlight> may be set equal to the desired accuracy. In this context, the process of increasing the accuracy of a speech engine with a non-enrolled user profile may in and of itself be sufficient to boost the accuracy to the desired accuracy of, for example, approximately 95% accuracy. Here, the process <highlight><bold>200</bold></highlight> may advance to step <highlight><bold>290</bold></highlight> where the process <highlight><bold>200</bold></highlight> may end. </paragraph>
<paragraph id="P-0175" lvl="7"><number>&lsqb;0175&rsqb;</number> G. Conclusion </paragraph>
<paragraph id="P-0176" lvl="0"><number>&lsqb;0176&rsqb;</number> The present invention relates to speech recognition and to methods for avoiding the enrollment process and minimizing the intrusive training required to achieve a commercially acceptable speech to text converter. The invention may achieve this by transcribing dictated audio by two speech recognition engines (e.g., Dragon NaturallySpeaking&trade; and IBM Viavoice&trade; SDK), saving a session file and text produced by each engine, creating a new session file with compressed audio for each transcription for transfer to a remote client or server, preparation of a verbatim text and a final text at the client, and creation of a word map between verbatim text and transcribed text by a correctionist for improved automated, repetitive corrective adaptation of each engine. </paragraph>
<paragraph id="P-0177" lvl="0"><number>&lsqb;0177&rsqb;</number> The Dragon NaturallySpeaking&trade; software development kit does not provide the exact location of the audio for a given word in the audio stream. Without the exact start point and stop point for the audio, the audio for any given word or phrase may be obtained indirectly by selecting the word or phrase and playing back the audio in the Dragon NaturallySpeaking&trade; text processor window. However, the above described word mapping technique permits each word of the Dragon NaturallySpeaking&trade; transcribed text to be associated to the word(s) of the verbatim text and automated corrective adaptation to be performed. </paragraph>
<paragraph id="P-0178" lvl="0"><number>&lsqb;0178&rsqb;</number> Moreover, the IBM Viavoice&trade; SDK software development kit permits an application to be created that lists audio files and the start point and stop point of each file in the audio stream corresponding to each separate word, character, or punctuation. This feature can be used to associate and save the audio in a compressed format for each word in the transcribed text. In this way, a session file can be created for the dictated text and distributed to remote speakers with text processor software that will open the session file. </paragraph>
<paragraph id="P-0179" lvl="0"><number>&lsqb;0179&rsqb;</number> The foregoing description and drawings merely explain and illustrate the invention and the invention is not limited thereto. While the specification in this invention is described in relation to certain implementation or embodiments, many details are set forth for the purpose of illustration. Thus, the foregoing merely illustrates the principles of the invention. For example, the invention may have other specific forms without departing for its spirit or essential characteristic. The described arrangements are illustrative and not restrictive. To those skilled in the art, the invention is susceptible to additional implementations or embodiments and certain of these details described in this application may be varied considerably without departing from the basic principles of the invention. It will thus be appreciated that those skilled in the art will be able to devise various arrangements which, although not explicitly described or shown herein, embody the principles of the invention and, thus, within its scope and spirit. </paragraph>
</section>
</detailed-description>
</subdoc-description>
<subdoc-claims>
<heading lvl="1">What is claimed is: </heading>
<claim id="CLM-00001">
<claim-text><highlight><bold>1</bold></highlight>. A method to determine time location of at least one audio segment in an original audio file comprising: 
<claim-text>(a) receiving the original audio file; </claim-text>
<claim-text>(b) transcribing a current audio segment from the original audio file using speech recognition software; </claim-text>
<claim-text>(c) extracting a transcribed element and a binary audio stream corresponding to the transcribed element from the speech recognition software; </claim-text>
<claim-text>(d) saving an association between the transcribed element and the corresponding binary audio stream; </claim-text>
<claim-text>(e) repeating (b) through (d) for each audio segment in the original audio file; </claim-text>
<claim-text>(f) for each transcribed element, searching for the associated binary audio stream in the original audio file, while tracking an end time location of that search within the original audio file; and </claim-text>
<claim-text>(g) inserting the end time location for each binary audio stream into the transcribed element-corresponding binary audio stream association. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00002">
<claim-text><highlight><bold>2</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference> wherein searching includes removing any DC offset from the corresponding binary audio stream. </claim-text>
</claim>
<claim id="CLM-00003">
<claim-text><highlight><bold>3</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00002">claim 2</dependent-claim-reference>, wherein removing any DC offset includes taking a derivative of the corresponding binary audio stream to produce a derivative binary audio stream. </claim-text>
</claim>
<claim id="CLM-00004">
<claim-text><highlight><bold>4</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00003">claim 3</dependent-claim-reference> wherein searching includes 
<claim-text>taking a derivative of a segment of the original audio file to produce a derivative audio segment; and </claim-text>
<claim-text>searching for the derivative binary audio stream in the derivative audio segment. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00005">
<claim-text><highlight><bold>5</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference> further including saving each transcribed element-corresponding binary audio stream association in a single file. </claim-text>
</claim>
<claim id="CLM-00006">
<claim-text><highlight><bold>6</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00005">claim 5</dependent-claim-reference> where the single file includes, for each word saved, a text for the transcribed element and a pointer to the binary audio stream. </claim-text>
</claim>
<claim id="CLM-00007">
<claim-text><highlight><bold>7</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00005">claim 5</dependent-claim-reference> wherein extracting is performed by using the Microsoft Speech API as an interface to the speech recognition software, wherein the speech recognition software does not return a word with a corresponding audio stream. </claim-text>
</claim>
<claim id="CLM-00008">
<claim-text><highlight><bold>8</bold></highlight>. A system for determining a time location of at least one audio segment in an original audio file comprising: 
<claim-text>means for receiving the original audio file; </claim-text>
<claim-text>means for transcribing a current audio segment from the original audio file using speech recognition software; </claim-text>
<claim-text>means for extracting a transcribed element and a binary audio stream corresponding to the transcribed element from the speech recognition software; </claim-text>
<claim-text>means for saving an association between the transcribed element and the corresponding binary audio stream; </claim-text>
<claim-text>means for searching for the associated binary audio stream in the original audio file, while tracking an end time location of that search within the original audio file; and </claim-text>
<claim-text>means for inserting the end time location for the binary audio stream into the transcribed element-corresponding binary audio stream association. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00009">
<claim-text><highlight><bold>9</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00008">claim 8</dependent-claim-reference> wherein the means for searching include means for removing any DC offset from the corresponding binary audio stream. </claim-text>
</claim>
<claim id="CLM-00010">
<claim-text><highlight><bold>10</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00009">claim 9</dependent-claim-reference>, wherein the means for removing any DC offset include means for taking a derivative of the corresponding binary audio stream to produce a derivative binary audio stream. </claim-text>
</claim>
<claim id="CLM-00011">
<claim-text><highlight><bold>11</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00011">claim 10</dependent-claim-reference> wherein means for searching include means for taking a derivative of a segment of the original audio file to produce a derivative audio segment; and means for searching for the derivative binary audio stream in the derivative audio segment. </claim-text>
</claim>
<claim id="CLM-00012">
<claim-text><highlight><bold>12</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00008">claim 8</dependent-claim-reference> further including means for saving each word-corresponding binary audio stream association in a single file. </claim-text>
</claim>
<claim id="CLM-00013">
<claim-text><highlight><bold>13</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00011">claim 12</dependent-claim-reference> where the single file includes, for each word saved, a text for the word and a pointer to the binary audio stream. </claim-text>
</claim>
<claim id="CLM-00014">
<claim-text><highlight><bold>14</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00005">claim 5</dependent-claim-reference> wherein the means for extracting is performed by using the Microsoft Speech API as an interface to the speech recognition software, wherein the speech recognition software does not return a word with a corresponding audio stream. </claim-text>
</claim>
<claim id="CLM-00015">
<claim-text><highlight><bold>15</bold></highlight>. A system for determining a time location of at least one audio segment in an original audio file comprising: 
<claim-text>a storage device for storing the original audio file; </claim-text>
<claim-text>a speech recognition engine to transcribe a current audio segment from the original audio file; </claim-text>
<claim-text>a program that extracts a transcribed element and a binary audio stream corresponding to the transcribed element from the speech recognition software; saves an association between the transcribed element and the corresponding binary audio stream into a session file; searches for the binary audio stream audio stream in the original audio file; and inserts the end time location for each binary audio stream into the transcribed element-corresponding binary audio stream association. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00016">
<claim-text><highlight><bold>16</bold></highlight>. The system of <dependent-claim-reference depends_on="CLM-00011">claim 15</dependent-claim-reference> wherein the program uses a Microsoft Speech API.</claim-text>
</claim>
</subdoc-claims>
<subdoc-drawings id="DRAWINGS">
<heading lvl="0" align="CENTER">Drawings</heading>
<representative-figure>2</representative-figure>
<figure id="figure-D00000">
<image id="EMI-D00000" file="US20030004724A1-20030102-D00000.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00001">
<image id="EMI-D00001" file="US20030004724A1-20030102-D00001.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00002">
<image id="EMI-D00002" file="US20030004724A1-20030102-D00002.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00003">
<image id="EMI-D00003" file="US20030004724A1-20030102-D00003.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00004">
<image id="EMI-D00004" file="US20030004724A1-20030102-D00004.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00005">
<image id="EMI-D00005" file="US20030004724A1-20030102-D00005.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00006">
<image id="EMI-D00006" file="US20030004724A1-20030102-D00006.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00007">
<image id="EMI-D00007" file="US20030004724A1-20030102-D00007.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00008">
<image id="EMI-D00008" file="US20030004724A1-20030102-D00008.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00009">
<image id="EMI-D00009" file="US20030004724A1-20030102-D00009.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00010">
<image id="EMI-D00010" file="US20030004724A1-20030102-D00010.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00011">
<image id="EMI-D00011" file="US20030004724A1-20030102-D00011.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00012">
<image id="EMI-D00012" file="US20030004724A1-20030102-D00012.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00013">
<image id="EMI-D00013" file="US20030004724A1-20030102-D00013.TIF" imf="TIFF" ti="DR"/>
</figure>
</subdoc-drawings>
</patent-application-publication>
