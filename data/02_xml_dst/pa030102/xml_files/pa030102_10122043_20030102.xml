<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE patent-application-publication SYSTEM "pap-v16-2002-01-01.dtd" [
<!ENTITY US20030005070A1-20030102-P00001.TIF SYSTEM "US20030005070A1-20030102-P00001.TIF" NDATA TIF>
<!ENTITY US20030005070A1-20030102-P00002.TIF SYSTEM "US20030005070A1-20030102-P00002.TIF" NDATA TIF>
<!ENTITY US20030005070A1-20030102-P00003.TIF SYSTEM "US20030005070A1-20030102-P00003.TIF" NDATA TIF>
<!ENTITY US20030005070A1-20030102-P00004.TIF SYSTEM "US20030005070A1-20030102-P00004.TIF" NDATA TIF>
<!ENTITY US20030005070A1-20030102-P00005.TIF SYSTEM "US20030005070A1-20030102-P00005.TIF" NDATA TIF>
<!ENTITY US20030005070A1-20030102-D00000.TIF SYSTEM "US20030005070A1-20030102-D00000.TIF" NDATA TIF>
<!ENTITY US20030005070A1-20030102-D00001.TIF SYSTEM "US20030005070A1-20030102-D00001.TIF" NDATA TIF>
<!ENTITY US20030005070A1-20030102-D00002.TIF SYSTEM "US20030005070A1-20030102-D00002.TIF" NDATA TIF>
<!ENTITY US20030005070A1-20030102-D00003.TIF SYSTEM "US20030005070A1-20030102-D00003.TIF" NDATA TIF>
<!ENTITY US20030005070A1-20030102-D00004.TIF SYSTEM "US20030005070A1-20030102-D00004.TIF" NDATA TIF>
]>
<patent-application-publication>
<subdoc-bibliographic-information>
<document-id>
<doc-number>20030005070</doc-number>
<kind-code>A1</kind-code>
<document-date>20030102</document-date>
</document-id>
<publication-filing-type>new</publication-filing-type>
<domestic-filing-data>
<application-number>
<doc-number>10122043</doc-number>
</application-number>
<application-number-series-code>10</application-number-series-code>
<filing-date>20020411</filing-date>
</domestic-filing-data>
<technical-information>
<classification-ipc>
<classification-ipc-primary>
<ipc>G06F015/167</ipc>
</classification-ipc-primary>
<classification-ipc-secondary>
<ipc>G06F015/177</ipc>
</classification-ipc-secondary>
<classification-ipc-edition>07</classification-ipc-edition>
</classification-ipc>
<classification-us>
<classification-us-primary>
<uspc>
<class>709</class>
<subclass>213000</subclass>
</uspc>
</classification-us-primary>
<classification-us-secondary>
<uspc>
<class>709</class>
<subclass>220000</subclass>
</uspc>
</classification-us-secondary>
</classification-us>
<title-of-invention>Dynamic determination of memory mapped input output range granularity for multi-node computer system</title-of-invention>
</technical-information>
<continuity-data>
<non-provisional-of-provisional>
<document-id>
<doc-number>60301955</doc-number>
<document-date>20010629</document-date>
<country-code>US</country-code>
</document-id>
</non-provisional-of-provisional>
</continuity-data>
<inventors>
<first-named-inventor>
<name>
<given-name>Prabhunandan</given-name>
<middle-name>B.</middle-name>
<family-name>Narasimhamurthy</family-name>
</name>
<residence>
<residence-us>
<city>San Jose</city>
<state>CA</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</first-named-inventor>
<inventor>
<name>
<given-name>Yukio</given-name>
<family-name>Nishimura</family-name>
</name>
<residence>
<residence-non-us>
<city>Ishikawa-ken</city>
<country-code>JP</country-code>
</residence-non-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
<inventor>
<name>
<given-name>Sudheer</given-name>
<family-name>Miryala</family-name>
</name>
<residence>
<residence-us>
<city>San Jose</city>
<state>CA</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
<inventor>
<name>
<given-name>Kazunori</given-name>
<family-name>Masuyama</family-name>
</name>
<residence>
<residence-non-us>
<city>Kanagawa</city>
<country-code>JP</country-code>
</residence-non-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
</inventors>
<correspondence-address>
<name-1>FENWICK &amp; WEST LLP</name-1>
<name-2></name-2>
<address>
<address-1>TWO PALO ALTO SQUARE</address-1>
<city>PALO ALTO</city>
<state>CA</state>
<postalcode>94306</postalcode>
<country>
<country-code>US</country-code>
</country>
</address>
</correspondence-address>
</subdoc-bibliographic-information>
<subdoc-abstract>
<paragraph id="A-0001" lvl="0">A method and system enables dynamic support of memory mapping devices in a multi-node computer system. One of central process unit (CPU) nodes determines a total amount of MMIO address spaces that are needed for all MMIO devices and generates an optimized granularity to support the total amount of MMIO address spaces. Based on the granularity, a CPU node controller configures MMIO range registers of the interconnect and other MMIO registers in IO nodes and CPU node controllers to support dynamic changes of MMIO address space requirements of the system. </paragraph>
</subdoc-abstract>
<subdoc-description>
<cross-reference-to-related-applications>
<heading lvl="1">RELATED APPLICATION </heading>
<paragraph id="P-0001" lvl="0"><number>&lsqb;0001&rsqb;</number> This application claims priority under 35 U.S.C. &sect;119(e) from co-pending U.S. Provisional Patent Application serial No. 60/301,955, entitled &ldquo;Algorithm For Dynamically Determining The Memory Mapped Input Output Range Granularity For A Multi-Node Computer System,&rdquo; filed on Jun. 29, 2001, by Prabhunandan B. Narasimhamurthy, et al, which is incorporated by reference in its entirety herein.</paragraph>
</cross-reference-to-related-applications>
<summary-of-invention>
<section>
<heading lvl="1">FIELD OF THE INVENTION </heading>
<paragraph id="P-0002" lvl="0"><number>&lsqb;0002&rsqb;</number> The present invention relates generally to an interconnect-based multi-node computer system, in particular, to supporting memory mapped Input Output (MMIO) processing for a multi-node computer system. </paragraph>
</section>
<section>
<heading lvl="1">BACKGROUND OF THE INVENTION </heading>
<paragraph id="P-0003" lvl="0"><number>&lsqb;0003&rsqb;</number> A multi-node computer system typically contains a plurality of central processing units (CPU) node, a plurality of interconnects, and a plurality of input output (IO) nodes. Each IO node is coupled to multiple IO devices, which may be conventional peripheral devices, such as peripheral component interconnect (PCI), small computer system interface (SCSI) type devices. Such multi-node computer system may perform complex computing tasks such as interacting simultaneously with a large number of IO devices. </paragraph>
<paragraph id="P-0004" lvl="0"><number>&lsqb;0004&rsqb;</number> In a multi-node computer system, some or all of the IO devices may be memory-mapped IO (MMIO) devices. The memory-mapped IO devices are connected to address and data lines within the IO nodes, and the CPU nodes in a manner similar to the connection of memory devices. Whenever the IO nodes, or the CPU nodes read or write the addresses associated with the IO devices, the IO nodes or the CPU nodes can transfer data to or from the IO devices. One of the advantages of such memory-mapped IO devices is that processors in the CPU nodes or the interconnect can use any single instruction that accesses their memory spaces to operate upon data that are transmitted at IO device ports rather than first moving the data into processors, manipulating the data and then writing the data back to the IO device port. By doing so, memory-mapped IO devices typically reduce computation burdens on the processors. </paragraph>
<paragraph id="P-0005" lvl="0"><number>&lsqb;0005&rsqb;</number> To support these memory-mapped IO devices, components in the multi-node computer system, including the CPU nodes and the IO nodes, need to allocate large amount of physical address space in their memory units. In a conventional multi-node computer system, the interconnect has only a fixed number of MMIO range registers to specify the range of address spaces that are allocated for memory-mapped IO devices coupled to a given IO node. Such interconnect registers are typically &ldquo;base&rdquo; and &ldquo;size&rdquo; types of registers, which requires a base and size declaration for each CPU node and IO node coupled to the interconnect. If there are n nodes in the computer system, it will require n Base registers and n Size registers. The conventional &ldquo;base&rdquo; and &ldquo;size&rdquo; type registers thus consume substantial resources. </paragraph>
<paragraph id="P-0006" lvl="0"><number>&lsqb;0006&rsqb;</number> Moreover, the conventional &ldquo;base&rdquo; and &ldquo;size&rdquo; type registers fail to provide scalability for memory-mapping in a multi-node computer system. When the IO nodes connect to a large number of IO devices, the MMIO address space requirement for the multi-node computer system can be an arbitrarily large number. It would be very inefficient for a programmer to configure every MMIO range register to specify the base and the size in the interconnect for each IO device. </paragraph>
<paragraph id="P-0007" lvl="0"><number>&lsqb;0007&rsqb;</number> Therefore, it is desirable to provide an efficient and scalable method and system to dynamically support MMIO devices in an interconnect-based multi-node computer system. </paragraph>
</section>
<section>
<heading lvl="1">SUMMARY OF THE INVENTION </heading>
<paragraph id="P-0008" lvl="0"><number>&lsqb;0008&rsqb;</number> In accordance with the present invention, there is provided a method and system for dynamically supporting memory mapped IO (MMIO) devices in an interconnect-based multi-node computer system. In particular, the method and system support arbitrarily large amounts of MMIO address space with a fixed number of interconnect range registers and efficiently uses allocated MMIO memory space in a multi-node computer system. In one embodiment, the method includes: (1) dynamically determining a total amount of MMIO address space requirement for all MMIO devices; (2) calculating an optimized value of a MMIO range granularity to support the total MMIO address space requirement; (3) programming MMIO registers of the interconnect based on the determined granularity; and (4) programming IO node controllers and CPU node controllers based on the determined granularity to support all MMIO devices. </paragraph>
</section>
</summary-of-invention>
<brief-description-of-drawings>
<section>
<heading lvl="1">BRIEF DESCRIPTION OF THE DRAWINGS </heading>
<paragraph id="P-0009" lvl="0"><number>&lsqb;0009&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 1</cross-reference> is a schematic diagram illustrating an interconnect-based multi-node computer system in accordance with an embodiment of the present invention; </paragraph>
<paragraph id="P-0010" lvl="0"><number>&lsqb;0010&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 2</cross-reference> is a flow chart illustrating a method for supporting MMIO address space requirement according to an embodiment of the present invention; </paragraph>
<paragraph id="P-0011" lvl="0"><number>&lsqb;0011&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 3</cross-reference> is a flow chart illustrating a method for determining the total amount of MMIO address space requirement of a multi-node computer system; and </paragraph>
<paragraph id="P-0012" lvl="0"><number>&lsqb;0012&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 4</cross-reference> is a flow chart illustrating a method for determining an optimized MMIO range granularity according to an embodiment of the present invention.</paragraph>
</section>
</brief-description-of-drawings>
<detailed-description>
<section>
<heading lvl="1">DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENT </heading>
<paragraph id="P-0013" lvl="0"><number>&lsqb;0013&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 1</cross-reference> illustrates a multi-node computer system <highlight><bold>100</bold></highlight> that typically contains a plurality of central processing units (CPU) node controllers <highlight><bold>103</bold></highlight> (&num;1, &num;2, &num;3, . . . &num;n), an interconnect <highlight><bold>101</bold></highlight>, and a plurality of Input/Output (IO) nodes <highlight><bold>107</bold></highlight>. The CPU nodes controllers <highlight><bold>103</bold></highlight> and the IO nodes <highlight><bold>102</bold></highlight> are all coupled to the interconnect <highlight><bold>101</bold></highlight>. Typically, each of the CPU node controllers <highlight><bold>103</bold></highlight> is coupled to the interconnect <highlight><bold>101</bold></highlight> through a scalability port <highlight><bold>105</bold></highlight>. A multi-node computer system <highlight><bold>100</bold></highlight> may also include multiple interconnects <highlight><bold>101</bold></highlight>. Each of the interconnects <highlight><bold>101</bold></highlight> routes commands and data among different nodes and devices in the system <highlight><bold>100</bold></highlight>. The details of an interconnect-based multi-node computer system are well known in the art, and are not included here to avoid obscuring a clear description of the present invention. </paragraph>
<paragraph id="P-0014" lvl="0"><number>&lsqb;0014&rsqb;</number> Within each IO node <highlight><bold>102</bold></highlight>, there is typically included an IO node controller <highlight><bold>107</bold></highlight>, and bridges <highlight><bold>113</bold></highlight> and IO devices <highlight><bold>111</bold></highlight>. Different groups of IO devices <highlight><bold>111</bold></highlight> are respectively coupled to each of the bridges <highlight><bold>113</bold></highlight> (e.g., bridge &num;1, &num;2, . . . &num;b) and the bridges <highlight><bold>113</bold></highlight> are coupled to the IO node controller <highlight><bold>107</bold></highlight> through corresponding hub links <highlight><bold>109</bold></highlight>. The IO devices <highlight><bold>111</bold></highlight> are conventional peripheral devices, such as PCI, SCSI type devices. In one embodiment, IO node <highlight><bold>102</bold></highlight> contains at least one MMIO device. IO nodes <highlight><bold>102</bold></highlight> may also be coupled to IO mapped IO devices depending on the operational needs of the system <highlight><bold>100</bold></highlight>. </paragraph>
<paragraph id="P-0015" lvl="0"><number>&lsqb;0015&rsqb;</number> To support all MMIO devices <highlight><bold>111</bold></highlight> or other MMIO components in the system <highlight><bold>100</bold></highlight>, the multi-node computer system <highlight><bold>100</bold></highlight> contains memory units, which may be distributed across the system <highlight><bold>100</bold></highlight> and globally accessible by all components <highlight><bold>102</bold></highlight>, <highlight><bold>103</bold></highlight>. To manage the address spaces provided by these memory units in the system <highlight><bold>100</bold></highlight>, the interconnect <highlight><bold>101</bold></highlight> contains an address decoder <highlight><bold>107</bold></highlight> and a plurality of MMIO address range registers <highlight><bold>115</bold></highlight> as shown in <cross-reference target="DRAWINGS">FIG. 1</cross-reference>. For example, with respect to MMIO devices <highlight><bold>111</bold></highlight>, the MMIO address range registers <highlight><bold>115</bold></highlight> store the ranges of address space provided for MMIO devices <highlight><bold>111</bold></highlight> based on specific functions and ports. </paragraph>
<paragraph id="P-0016" lvl="0"><number>&lsqb;0016&rsqb;</number> The address decoder <highlight><bold>107</bold></highlight> typically receives read or write (r/w) requests from each IO node <highlight><bold>102</bold></highlight> during interaction between MMIO devices <highlight><bold>111</bold></highlight> and CPU node controllers <highlight><bold>103</bold></highlight>. As a part of memory mapping process, the address decoder <highlight><bold>107</bold></highlight> determines from which MMIO device <highlight><bold>111</bold></highlight> the request comes based on the address range information stored in the MMIO address range registers <highlight><bold>115</bold></highlight>, e.g., the range registers 0, 1, . . . r. After the determination, the address decoder <highlight><bold>107</bold></highlight> can direct such request to corresponding CPU node controllers <highlight><bold>103</bold></highlight> for memory mapping processing. </paragraph>
<paragraph id="P-0017" lvl="0"><number>&lsqb;0017&rsqb;</number> One embodiment of the present invention provides a programmable range granularity value to manage the address spaces needed by the MMIO devices <highlight><bold>111</bold></highlight>. The programmable granularity enables the system <highlight><bold>100</bold></highlight> to manage its memory space in various components <highlight><bold>101</bold></highlight>, <highlight><bold>102</bold></highlight>, <highlight><bold>103</bold></highlight> in a more efficient manner. For example, processors in the interconnect <highlight><bold>101</bold></highlight> are capable of writing to range registers <highlight><bold>115</bold></highlight> to adjust to a new granularity using reduced clock cycle time to improve the performance of the multi-node computer system <highlight><bold>100</bold></highlight>. </paragraph>
<paragraph id="P-0018" lvl="0"><number>&lsqb;0018&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 2</cross-reference> is a flow chart illustrating a method for dynamically determining MMIO range granularity according to an embodiment of the present invention. The method can be performed by a bootstrap processor (BSP) in the system <highlight><bold>100</bold></highlight>. During the initialization process of the system <highlight><bold>100</bold></highlight>, one of the CPU node controllers <highlight><bold>103</bold></highlight> is designated as the BSP to execute the MMIO granularity determination process. When there are any changes of the number of MMIO devices <highlight><bold>111</bold></highlight> in the system <highlight><bold>100</bold></highlight>, the BSP communicates with the interconnect <highlight><bold>107</bold></highlight> and IO node controllers <highlight><bold>102</bold></highlight> and other non-BSP CPU node controllers <highlight><bold>103</bold></highlight> to perform MMIO granularity determination routines as described herein through the scalability port <highlight><bold>105</bold></highlight>. As described below in detail, the CPU node controller <highlight><bold>103</bold></highlight>, acting as a BSP, scans IO nodes <highlight><bold>102</bold></highlight>, determines the granularity of range registers <highlight><bold>115</bold></highlight> in the interconnect <highlight><bold>101</bold></highlight> and programs IO nodes <highlight><bold>102</bold></highlight> and CPU node controllers <highlight><bold>103</bold></highlight> to adjust to the new granularity to support MMIO devices. </paragraph>
<paragraph id="P-0019" lvl="0"><number>&lsqb;0019&rsqb;</number> According to one embodiment of the present invention, the BSP determines <highlight><bold>201</bold></highlight> the amount of MMIO address space required for all IO nodes <highlight><bold>102</bold></highlight>. As will be further described with reference to <cross-reference target="DRAWINGS">FIG. 3</cross-reference>, the amount of MMIO address spaces required for memory-mapped IO devices <highlight><bold>111</bold></highlight> within each IO node <highlight><bold>102</bold></highlight> is aggregated to generate the total amount of MMIO address space for the entire system <highlight><bold>100</bold></highlight>. After the total amount of MMIO address spaces is determined, the BSP calculates <highlight><bold>203</bold></highlight> a proper granularity of the MMIO address space managed by the address range registers <highlight><bold>115</bold></highlight>. In a preferred embodiment, the granularity may be an optimized value of the MMIO range granularity to support the found total MMIO address space requirement. The BSP <highlight><bold>205</bold></highlight> programs the range registers <highlight><bold>115</bold></highlight> in the interconnect <highlight><bold>101</bold></highlight> based on the granularity. The BSP proceeds to program <highlight><bold>207</bold></highlight> the MMIO registers used by the IO nodes <highlight><bold>102</bold></highlight> and to program <highlight><bold>209</bold></highlight> MMIO registers in CPU node controllers <highlight><bold>103</bold></highlight> based on the granularity. </paragraph>
<paragraph id="P-0020" lvl="0"><number>&lsqb;0020&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 3</cross-reference> illustrates a method for determining the total amount of MMIO address space for the system <highlight><bold>100</bold></highlight>. In one embodiment, the BSP scans all the MMIO devices <highlight><bold>111</bold></highlight> in the system <highlight><bold>100</bold></highlight> and aggregates the amount of MMIO address spaces that are needed for every MMIO device <highlight><bold>111</bold></highlight> within each IO node <highlight><bold>102</bold></highlight>. The BSP starts this process by scanning <highlight><bold>301</bold></highlight> from any of IO node <highlight><bold>102</bold></highlight><highlight><italic>i </italic></highlight>e.g., i&equals;1. For each IO node <highlight><bold>102</bold></highlight><highlight><italic>i</italic></highlight>, the BSP scans <highlight><bold>303</bold></highlight> each hub link <highlight><bold>109</bold></highlight><highlight><italic>l </italic></highlight>(e.g., l&equals;1, . . . n) that is coupled to an IO node controller <highlight><bold>107</bold></highlight><highlight><italic>i</italic></highlight>. For each hub link <highlight><bold>109</bold></highlight><highlight><italic>l</italic></highlight>, there is typically a bridge <highlight><bold>113</bold></highlight> coupling the IO devices <highlight><bold>111</bold></highlight> within the IO node <highlight><bold>102</bold></highlight><highlight><italic>i </italic></highlight>to the IO node controller <highlight><bold>107</bold></highlight><highlight><italic>i</italic></highlight>. IO devices <highlight><bold>111</bold></highlight> are coupled to corresponding bridge <highlight><bold>113</bold></highlight><highlight><italic>b </italic></highlight>(b&equals;1, . . . m). Thus, for each hub link <highlight><bold>109</bold></highlight>, the BSP scans <highlight><bold>305</bold></highlight> each bridge <highlight><bold>113</bold></highlight><highlight><italic>b </italic></highlight>in the IO node <highlight><bold>102</bold></highlight><highlight><italic>i </italic></highlight>and then determines <highlight><bold>307</bold></highlight> the MMIO address space that are used by all MMIO devices <highlight><bold>111</bold></highlight> that are coupled to the bridge <highlight><bold>113</bold></highlight><highlight><italic>b</italic></highlight>. The BSP accumulates <highlight><bold>309</bold></highlight> the amount of MMIO address space for each IO node <highlight><bold>102</bold></highlight><highlight><italic>i</italic></highlight>. After all the devices <highlight><bold>111</bold></highlight> within an IO node <highlight><bold>102</bold></highlight><highlight><italic>i </italic></highlight>are scanned, the BSP will continue to scan <highlight><bold>315</bold></highlight> next IO node <highlight><bold>102</bold></highlight><highlight><italic>i </italic></highlight>to accumulate the MMIO size until it reaches the last MMIO device <highlight><bold>111</bold></highlight> at the last IO nodes <highlight><bold>102</bold></highlight><highlight><italic>i</italic></highlight>. As an implementation, the BSP stores the MMIO address space size for each IO node <highlight><bold>102</bold></highlight><highlight><italic>i </italic></highlight>in an array IO_MMIOreqArray&lsqb;i&rsqb;. </paragraph>
<paragraph id="P-0021" lvl="0"><number>&lsqb;0021&rsqb;</number> An example of the total MMIO amount determination is set forth in Appendix 1. </paragraph>
<paragraph id="P-0022" lvl="0"><number>&lsqb;0022&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 4</cross-reference> illustrates a method for determining a granularity of the MMIO address spaces that are defined by range registers <highlight><bold>115</bold></highlight> in the interconnect <highlight><bold>101</bold></highlight>. Upon determining the total amount of MMIO address space required by all MMIO devices <highlight><bold>111</bold></highlight>, the BSP performs the steps as described below to determine a granularity of the total MMIO address spaces. </paragraph>
<paragraph id="P-0023" lvl="0"><number>&lsqb;0023&rsqb;</number> In one embodiment, the BSP stores a variable granfield and sets <highlight><bold>401</bold></highlight> the preliminary value of such granfield to be zero. As described below, the variable granfield will have a maximum value depending on the hardware capability of the system <highlight><bold>100</bold></highlight>. The BSP defines a granularity size (Gran) of the MMIO address spaces as below: </paragraph>
<paragraph lvl="0"><in-line-formula>Gran&equals;16 megabytes&times;2&circ; granfield&emsp;&emsp;Equation (1) </in-line-formula></paragraph>
<paragraph id="P-0024" lvl="0"><number>&lsqb;0024&rsqb;</number> In one implementation, 16 megabytes in Equation (1) is a minimum address space size that is normally used by an MMIO device <highlight><bold>11</bold></highlight>. In alternative embodiments, other values of address space sizes may be set depending upon the actual needs of the system <highlight><bold>100</bold></highlight>. Based on the granularity value Gran determined by Equation (1), the BSP calculates <highlight><bold>405</bold></highlight> the number of the range registers that are needed to satisfy the total amount of MMIO address space requirement. In one implementation, the number of range registers that are needed for each IO node <highlight><bold>102</bold></highlight><highlight><italic>i </italic></highlight>is calculated by: </paragraph>
<paragraph lvl="0"><in-line-formula>Number of range registers&lsqb;i&rsqb;&equals;IO_MMIOreqArray&lsqb;i&rsqb;/Gran&emsp;&emsp;Equation (2) </in-line-formula></paragraph>
<paragraph id="P-0025" lvl="0"><number>&lsqb;0025&rsqb;</number> Correspondingly, the total number of the range registers needed for all IO nodes <highlight><bold>102</bold></highlight> can be obtained by aggregating the results in Equation (2) for each IO node <highlight><bold>102</bold></highlight><highlight><italic>i</italic></highlight>. The total number of the range registers is subsequently compared <highlight><bold>407</bold></highlight> with the maximum number of the range registers that are currently provided by the interconnect <highlight><bold>101</bold></highlight>. If it exceeds the maximum capability of the system <highlight><bold>100</bold></highlight>, the BSP increases <highlight><bold>409</bold></highlight> the variable granfield by 1 and recalculates the new granularity value according to Equation (1). After the new granularity value is generated, the steps <highlight><bold>403</bold></highlight>-<highlight><bold>407</bold></highlight> are repeated until the number of range registers that are needed does not exceed the maximum hardware capability offered by the interconnect <highlight><bold>101</bold></highlight>. As a result, the corresponding granularity value (Gran) is a preferred value of the granularity of the range size. Such granularity value will then be used to program the related MMIO firmware in the interconnect <highlight><bold>101</bold></highlight>, CPU node controllers <highlight><bold>103</bold></highlight> and IO nodes <highlight><bold>102</bold></highlight>. </paragraph>
<paragraph id="P-0026" lvl="0"><number>&lsqb;0026&rsqb;</number> An example of the granularity determination process is set forth in Appendix 2. </paragraph>
<paragraph id="P-0027" lvl="0"><number>&lsqb;0027&rsqb;</number> Referring back to <cross-reference target="DRAWINGS">FIG. 2</cross-reference>, in a preferred embodiment, the BSP first programs the MMIO range registers in the interconnect <highlight><bold>101</bold></highlight> based on the granularity value derived at step <highlight><bold>203</bold></highlight>. By way of example, the interconnect <highlight><bold>101</bold></highlight> typically contains MMIO registers specifying the starting address for memory-mapping a specific MMIO device <highlight><bold>111</bold></highlight> at a specific IO node <highlight><bold>102</bold></highlight><highlight><italic>i </italic></highlight>and the limit of MMIO address space that can be allocated for such MMIO device <highlight><bold>111</bold></highlight>. The granularity determined from step <highlight><bold>203</bold></highlight> can then be used by the MMIO registers to define the starting address and corresponding length of address space for each MMIO device <highlight><bold>111</bold></highlight>. </paragraph>
<paragraph id="P-0028" lvl="0"><number>&lsqb;0028&rsqb;</number> An example of programming the MMIO range registers in the interconnect <highlight><bold>101</bold></highlight> is set forth in Appendix 3. Note that since the system <highlight><bold>100</bold></highlight> can contain multiple interconnects, some of which can be non-default interconnects, the BSP can also program MMIO range registers in all of the interconnects included in the system <highlight><bold>100</bold></highlight>. </paragraph>
<paragraph id="P-0029" lvl="0"><number>&lsqb;0029&rsqb;</number> Still referring to <cross-reference target="DRAWINGS">FIG. 2</cross-reference>, having programmed the MMIO registers in the interconnect <highlight><bold>101</bold></highlight>, the BSP also programs the MMIO registers in each IO node controllers <highlight><bold>107</bold></highlight><highlight><italic>i </italic></highlight>based on the determined granularity value. In one preferred embodiment, at each IO node controller <highlight><bold>107</bold></highlight><highlight><italic>i</italic></highlight>, there are MMIO registers, e.g., MMIOBL specifying the base address of MMIO address space, and MMIOLL, specifying the limit of the MMIO address space allocated by such IO node <highlight><bold>102</bold></highlight> for memory mapping. These MMIO registers can define the address spaces that are used for each specific MMIO device <highlight><bold>111</bold></highlight> that are coupled to specific hub links <highlight><bold>109</bold></highlight><highlight><italic>l</italic></highlight>. Based on the granularity value, the BSP can configure these MMIO registers in a uniform manner without the need of programming individually for each specific MMIO device. The determined granularity ensures that each MMIO device has sufficient address spaces that are allocated for memory mapping. </paragraph>
<paragraph id="P-0030" lvl="0"><number>&lsqb;0030&rsqb;</number> Correspondingly, the BSP also programs the MMIO registers in the CPU node controllers <highlight><bold>103</bold></highlight>. Conventional CPU node controllers <highlight><bold>103</bold></highlight> also contain MMIO registers to define the starting address and size limits for address spaces that are used for memory mapping. The granularity value provides the limits for MMIO address space that are used by all MMIO devices. Thus, CPU node controllers <highlight><bold>103</bold></highlight> can use this information to configure its MMIO registers. </paragraph>
<paragraph id="P-0031" lvl="0"><number>&lsqb;0031&rsqb;</number> An example of programming the MMIO registers in IO node controllers <highlight><bold>107</bold></highlight> and CPU node controllers <highlight><bold>103</bold></highlight> is set forth in Appendix 4. </paragraph>
<paragraph id="P-0032" lvl="0"><number>&lsqb;0032&rsqb;</number> In summary, the present invention provides a method and system to support dynamic changes in the MMIO address spaces that are used by MMIO devices in a multi-node computer system. The present invention determines a proper granularity value of the total amount of MMIO address spaces and uses the granularity value to program corresponding MMIO registers in the various components of the computer system. In doing so, the present invention avoids the complexity and inefficiency of configuring MMIO registers for specific MMIO devices as the number of MMIO devices changes. Further, the determination of granularity of MMIO address spaces enables an efficient use of the memories of a multi-node computer system. 
<image file="US20030005070A1-20030102-P00001.TIF" id="EMI-00001"></image>
<image file="US20030005070A1-20030102-P00002.TIF" id="EMI-00002"></image>
<image file="US20030005070A1-20030102-P00003.TIF" id="EMI-00003"></image>
<image file="US20030005070A1-20030102-P00004.TIF" id="EMI-00004"></image>
<image file="US20030005070A1-20030102-P00005.TIF" id="EMI-00005"></image>
</paragraph>
</section>
</detailed-description>
</subdoc-description>
<subdoc-claims>
<heading lvl="1">What is claimed is: </heading>
<claim id="CLM-00001">
<claim-text><highlight><bold>1</bold></highlight>. A method for supporting memory mapped input output (MMIO) devices in a multi-node computer system, the multi-node computer system comprising an interconnect having a plurality of MMIO address space range registers, a plurality of central processor unit (CPU) nodes, a plurality of input output (<highlight><bold>10</bold></highlight>) nodes, each IO node including at least an MMIO device, the method comprising: 
<claim-text>determining a total amount of MMIO address spaces that are used by the multi-node computer system; </claim-text>
<claim-text>determining a granularity of the MMIO address spaces based on the number of the MMIO address space range registers; and </claim-text>
<claim-text>configuring the range registers of the interconnect based on the determined granularity. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00002">
<claim-text><highlight><bold>2</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, further comprising: 
<claim-text>configuring the IO nodes based on the determined granularity to support the MMIO device. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00003">
<claim-text><highlight><bold>3</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00002">claim 2</dependent-claim-reference>, further comprising: 
<claim-text>configuring the CPU node based on the determined granularity to support the MMIO device. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00004">
<claim-text><highlight><bold>4</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein determining a total amount of MMIO address spaces comprises: 
<claim-text>scanning each of MMIO devices that are included in each IO node; </claim-text>
<claim-text>determining the amount of MMIO address spaces that are needed by the MMIO device; and </claim-text>
<claim-text>generating the amount of MMIO address spaces that are needed by all the MMIO devices that are included in the computer system. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00005">
<claim-text><highlight><bold>5</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein determining a granularity of the MMIO address spaces comprises: 
<claim-text>generating a preliminary granularity value; </claim-text>
<claim-text>generating the number of range registers based on the preliminary granularity value; </claim-text>
<claim-text>comparing the generated number of range registers with the maximum number of the range registers that is provided by the system; and </claim-text>
<claim-text>generating a proper granularity of the MMIO address spaces based on the maximum number of the range registers. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00006">
<claim-text><highlight><bold>6</bold></highlight>. A computer system comprising: 
<claim-text>an interconnect having a plurality of memory mapping input output (MMIO) address space range registers; </claim-text>
<claim-text>a plurality of input output (IO) nodes, each IO node including at least one MMIO device; and </claim-text>
<claim-text>a plurality of central processor unit (CPU) nodes with at least one of the CPU nodes being configured to determine a granularity of MMIO address spaces that are needed to support all MMIO devices of the IO nodes. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00007">
<claim-text><highlight><bold>7</bold></highlight>. The system of <dependent-claim-reference depends_on="CLM-00006">claim 6</dependent-claim-reference>, wherein the at least one of the CPU nodes is configured to program the range registers of the interconnect based on the determined granularity to support the memory mapping process of the MMIO devices. </claim-text>
</claim>
<claim id="CLM-00008">
<claim-text><highlight><bold>8</bold></highlight>. The system of <dependent-claim-reference depends_on="CLM-00007">claim 7</dependent-claim-reference>, wherein the at least one of the CPU nodes is configured to program the IO nodes based on the determined granularity to support the MMIO device. </claim-text>
</claim>
<claim id="CLM-00009">
<claim-text><highlight><bold>9</bold></highlight>. The system of <dependent-claim-reference depends_on="CLM-00008">claim 8</dependent-claim-reference>, wherein the at least one of the CPU nodes is configured to program the CPU nodes based on the determined granularity to support the MMIO device. </claim-text>
</claim>
<claim id="CLM-00010">
<claim-text><highlight><bold>10</bold></highlight>. A computer readable medium containing a computer program for enabling the support of memory mapped input output (MMIO) devices in a multi-node computer system, the multi-node computer system comprising an interconnect having a plurality of MMIO address space range registers, a plurality of central processor unit (CPU) nodes, a plurality of input output (IO) nodes, each IO node including at least an MMIO device, said computer program, when executed by one of the plurality of CPU nodes, performing the method of: 
<claim-text>determining a total amount of MMIO address spaces that are used in the multi-node computer system; </claim-text>
<claim-text>determining a granularity of the MMIO address spaces based on the number of the MMIO address space range registers; and </claim-text>
<claim-text>configuring the range registers of the interconnect based on the determined granularity; </claim-text>
<claim-text>configuring the IO nodes based on the determined granularity to support the MMIO device; and </claim-text>
<claim-text>configuring the CPU node based on the determined granularity to support the MMIO device. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00011">
<claim-text><highlight><bold>11</bold></highlight>. A computer system comprising: 
<claim-text>an interconnect having a plurality of memory mapping input output (MMIO) address space range registers; </claim-text>
<claim-text>a plurality of input output (IO) nodes, coupled to the interconnect, each IO node including at least an MMIO device; </claim-text>
<claim-text>a plurality of central processor unit (CPU) nodes, coupled to the interconnect; and </claim-text>
<claim-text>means for determining a granularity of MMIO address spaces that are needed to support all MMIO devices of the system based on the number of the MMIO address space range registers. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00012">
<claim-text><highlight><bold>12</bold></highlight>. The system of <dependent-claim-reference depends_on="CLM-00011">claim 11</dependent-claim-reference>, further comprising: 
<claim-text>means for determining a total amount of MMIO address spaces that are used in the multi-node computer system. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00013">
<claim-text><highlight><bold>13</bold></highlight>. The system of <dependent-claim-reference depends_on="CLM-00011">claim 12</dependent-claim-reference>, further comprising: 
<claim-text>means for configuring the range registers of the interconnect based on the determined granularity. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00014">
<claim-text><highlight><bold>14</bold></highlight>. The system of <dependent-claim-reference depends_on="CLM-00011">claim 13</dependent-claim-reference>, further comprising: 
<claim-text>means for configuring the IO nodes based on the determined granularity to support the MMIO device. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00015">
<claim-text><highlight><bold>15</bold></highlight>. The system of <dependent-claim-reference depends_on="CLM-00011">claim 14</dependent-claim-reference>, further comprising: 
<claim-text>means for configuring the CPU node based on the determined granularity to support the MMIO device.</claim-text>
</claim-text>
</claim>
</subdoc-claims>
<subdoc-drawings id="DRAWINGS">
<heading lvl="0" align="CENTER">Drawings</heading>
<representative-figure>2</representative-figure>
<figure id="figure-D00000">
<image id="EMI-D00000" file="US20030005070A1-20030102-D00000.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00001">
<image id="EMI-D00001" file="US20030005070A1-20030102-D00001.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00002">
<image id="EMI-D00002" file="US20030005070A1-20030102-D00002.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00003">
<image id="EMI-D00003" file="US20030005070A1-20030102-D00003.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00004">
<image id="EMI-D00004" file="US20030005070A1-20030102-D00004.TIF" imf="TIFF" ti="DR"/>
</figure>
</subdoc-drawings>
</patent-application-publication>
