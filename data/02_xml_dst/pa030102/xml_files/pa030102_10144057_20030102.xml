<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE patent-application-publication SYSTEM "pap-v16-2002-01-01.dtd" [
<!ENTITY US20030001117A1-20030102-D00000.TIF SYSTEM "US20030001117A1-20030102-D00000.TIF" NDATA TIF>
<!ENTITY US20030001117A1-20030102-D00001.TIF SYSTEM "US20030001117A1-20030102-D00001.TIF" NDATA TIF>
<!ENTITY US20030001117A1-20030102-D00002.TIF SYSTEM "US20030001117A1-20030102-D00002.TIF" NDATA TIF>
<!ENTITY US20030001117A1-20030102-D00003.TIF SYSTEM "US20030001117A1-20030102-D00003.TIF" NDATA TIF>
<!ENTITY US20030001117A1-20030102-D00004.TIF SYSTEM "US20030001117A1-20030102-D00004.TIF" NDATA TIF>
<!ENTITY US20030001117A1-20030102-D00005.TIF SYSTEM "US20030001117A1-20030102-D00005.TIF" NDATA TIF>
<!ENTITY US20030001117A1-20030102-D00006.TIF SYSTEM "US20030001117A1-20030102-D00006.TIF" NDATA TIF>
<!ENTITY US20030001117A1-20030102-D00007.TIF SYSTEM "US20030001117A1-20030102-D00007.TIF" NDATA TIF>
<!ENTITY US20030001117A1-20030102-D00008.TIF SYSTEM "US20030001117A1-20030102-D00008.TIF" NDATA TIF>
<!ENTITY US20030001117A1-20030102-D00009.TIF SYSTEM "US20030001117A1-20030102-D00009.TIF" NDATA TIF>
<!ENTITY US20030001117A1-20030102-D00010.TIF SYSTEM "US20030001117A1-20030102-D00010.TIF" NDATA TIF>
]>
<patent-application-publication>
<subdoc-bibliographic-information>
<document-id>
<doc-number>20030001117</doc-number>
<kind-code>A1</kind-code>
<document-date>20030102</document-date>
</document-id>
<publication-filing-type>new</publication-filing-type>
<domestic-filing-data>
<application-number>
<doc-number>10144057</doc-number>
</application-number>
<application-number-series-code>10</application-number-series-code>
<filing-date>20020510</filing-date>
</domestic-filing-data>
<technical-information>
<classification-ipc>
<classification-ipc-primary>
<ipc>G01N021/86</ipc>
</classification-ipc-primary>
<classification-ipc-secondary>
<ipc>G01V008/00</ipc>
</classification-ipc-secondary>
<classification-ipc-edition>07</classification-ipc-edition>
</classification-ipc>
<classification-us>
<classification-us-primary>
<uspc>
<class>250</class>
<subclass>559190</subclass>
</uspc>
</classification-us-primary>
</classification-us>
<title-of-invention>Dimensional measurement apparatus for object features</title-of-invention>
</technical-information>
<continuity-data>
<non-provisional-of-provisional>
<document-id>
<doc-number>60291070</doc-number>
<document-date>20010515</document-date>
<country-code>US</country-code>
</document-id>
</non-provisional-of-provisional>
</continuity-data>
<inventors>
<first-named-inventor>
<name>
<given-name>Kwangik</given-name>
<family-name>Hyun</family-name>
</name>
<residence>
<residence-us>
<city>Gilroy</city>
<state>CA</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</first-named-inventor>
</inventors>
<correspondence-address>
<name-1>Kwangik Hyun</name-1>
<name-2></name-2>
<address>
<address-1>1431 Briarberry Lane</address-1>
<city>Gilroy</city>
<state>CA</state>
<postalcode>95020</postalcode>
<country>
<country-code>US</country-code>
</country>
</address>
</correspondence-address>
</subdoc-bibliographic-information>
<subdoc-abstract>
<paragraph id="A-0001" lvl="0">A dimensional measurement apparatus comprises one photographic device with plural lighting devices. Properly disposed devices enable dimensional measurements of object features in two- and three-dimensional spaces. To achieve the measurements, proper device calibrations are required. After defining the disposition of device setups and their calibrations, the devices can be integrated with additional electronic hardware to obtain object feature data from the integrated devices. The obtained object feature information will be processed into three-dimensional world coordinates by utilizing the devices calibration data. Using the resultant data after processing, object feature inspections and volumetric representations could be realized. The apparatus provides dual line-scanning capability with opposite directional incident angle projections for the illuminations. The dual line-scanning method provides advantages that it reduces data gathering time compare to a single scanning method in a fixed resolution, and it also enhances measurement accuracies since the dual line-scanning method reduces object occlusion problem and errors from the width of the illuminator especially for the curved shaped object. </paragraph>
</subdoc-abstract>
<subdoc-description>
<cross-reference-to-related-applications>
<heading lvl="1">CROSS-REFERENCE TO RELATED APPLICATION </heading>
<paragraph id="P-0001" lvl="0"><number>&lsqb;0001&rsqb;</number> This application claims the benefit of Provisional Patent Application Ser. No. 60/291,070 filed May 15, 2001.</paragraph>
</cross-reference-to-related-applications>
<summary-of-invention>
<section>
<heading lvl="1">FEDERALLY SPONSERED RESEARCH </heading>
<paragraph id="P-0002" lvl="0"><number>&lsqb;0002&rsqb;</number> Not Applicable </paragraph>
<paragraph id="P-0003" lvl="7"><number>&lsqb;0003&rsqb;</number> SEQUENCE LISTING OR PROGRAM </paragraph>
<paragraph id="P-0004" lvl="0"><number>&lsqb;0004&rsqb;</number> Not Applicable </paragraph>
</section>
<section>
<heading lvl="1">FIELD OF INVENTION </heading>
<paragraph id="P-0005" lvl="0"><number>&lsqb;0005&rsqb;</number> This invention relates to a dimensional measurement apparatus of two- and three-dimensional object features. In particular, the present invention relates to object feature representation apparatus as well as inspection apparatus utilizing the measured two- and three-dimensional object feature information. </paragraph>
</section>
<section>
<heading lvl="1">BACKGROUND OF THE INVENTION </heading>
<paragraph id="P-0006" lvl="0"><number>&lsqb;0006&rsqb;</number> PCB manufacturing industry faces to an innovation of technology trends that electronic devices are getting smaller and more complicate than previous industry trend when information technology is growing with hardware such as Personal Digital Assistances (PDAs), palm top computer as well as several Personal Communication Systems (PCS) (i.e., cell phone). By emerging these small-size devices, Print Circuit Board (PCB) manufacturing industry needs to provide such a small-size compact electronic devices that are composed of many small electronic parts mounted. To produce such devices, manufacturing processes needs high precision technologies as well as high precision tools for inspection. One of bottlenecks for the manufacturing process is a requirement of three-dimensional inspection. Since increase of product yield is one of the important issues for PCB manufacturing industry, proper equipments are required to minimize defective products at the end of the manufacturing process. However, several types of inspections for intermediate processes are required before completing manufacturing processes to reduce defective product scraps at the final manufacturing process. </paragraph>
<paragraph id="P-0007" lvl="0"><number>&lsqb;0007&rsqb;</number> The followings are brief intermediate inspection processes for the PCB manufacturing processes. Bare PCB itself needs to be inspected whether there are no defects by checking its flatness, hole size, hole location and hole existence for preparation of actual electronic parts assembly. Also, etching lines need to be inspected whether there are any undesired shorts or opens in the circuit using Automatic Optical Inspection (AOI) equipment as an example. After these inspections are carried out, solder pastes are applied to pads for electronic parts mounting and interconnection of the circuits. Before mounting the parts, solder paste inspection is carried out to make sure that proper solder pastes have desired amount of pastes as well as if the pastes are applied at the correct position of the pads. After mounting the parts on the pads, also, existence of the parts as well as parts mounting status in position-wise needs to be checked. X-ray inspection could be used to take a picture of internal thru-hole soldering status between layers. These serial inspections mostly need specially designed equipments to carry out inspections of the specific defect types. </paragraph>
<paragraph id="P-0008" lvl="0"><number>&lsqb;0008&rsqb;</number> Some of the most complicate and accuracy-requiring inspections are solder pastes as well as chip carriers (such as Ball Grid Arrays (BGAs)) inspections. </paragraph>
<paragraph id="P-0009" lvl="0"><number>&lsqb;0009&rsqb;</number> Due to the technology trend of electronic devices such as PDA, portable computer and small-size personal communication devices such as PCS, manufacturing processes require high precision manufacturing technologies to deal with compact size, densely populated print circuit boards. To facilitate the size constraint, there are several types of chip carriers of semiconductor packages such as PGA (Pin Grid Array), QFP (Quad Flat Package), BGA (Ball Grid Array) etc. These semiconductor packages are to be mounted on the PCB that has solder pastes deposited on the pads. However, once the packages are loaded on the PCB, the PCB will carry out the reflow process. During reflow process with high temperature application to the PCB, the amount of solder paste deposition will affect to the product and may cause short or open defects as well. Additionally, BGA has its own solder balls on the package so that they will be molten to be interconnected to the PCB conductive pads mechanically and electrically. If the solder balls on the package are too small, too much or missing, these defective packages cause mal-interconnections as well as misplacements of the package on the PCB, which finally cause electronic functional defect mal-interconnections. as well as misplacements of the package on the PCB, which finally cause electronic functional defect. </paragraph>
<paragraph id="P-0010" lvl="0"><number>&lsqb;0010&rsqb;</number> To reduce the product defects, some of the defects are required electrical tests for the inspection; others need optical inspection such as cosmetic defects (i.e., pattern missing, foreign materials, character or mark imprint missing or distortions as well). However, current technology could mostly cover these cosmetic defects. Moreover, these defects were existed in the previous time so that the required technologies already provide solutions to resolve them. Since the electronic parts are getting smaller and the PCB size is getting smaller and compact, inspection metrologies are changed toward complicate and precise with shorter throughput. Especially, to accommodate the smaller and high functional electronic parts, manufacturing processes need to be changed to provide solutions for the changing trends. Some of the defect types require a volumetric inspection for accurate and efficient defect analyses. To carry out inspections for these defect types, three-dimensional measurement apparatus can be utilized. </paragraph>
<paragraph id="P-0011" lvl="0"><number>&lsqb;0011&rsqb;</number> Additionally, three-dimensional inspection can be utilized for a solder paste inspection. The inspection controls solder paste volume applied to conductive pads on the PCB as well as accurate paste application positions. After deposition of the solder paste, Surface Mount Device (SMD) will put all the electronic parts. The solder paste will hold the parts until the electronic parts mountings are done. The following manufacturing process is reflow process that a certain temperature will be applied so that solders are supposed to be molten. This reflow process actually accomplishes electrical and mechanical interconnection between electronic part pads and the conductive pads on the surface of the PCB. However, if the solder paste deposition is too small, it may cause a circuit open with unstable electrical as well as mechanical interconnections during the reflow process. If the solder paste deposition is too much, the circuit may be short to the adjacent conductive pads. </paragraph>
<paragraph id="P-0012" lvl="0"><number>&lsqb;0012&rsqb;</number> As described above on the needs of complicate and accuracy-requiring inspections for the solder pastes as well as chip carriers (such as Ball Grid Arrays (BGAs)) in the PCB manufacturing industry, dimensional measurement methodologies and equipments are required to increase a production yield and for a better product quality. </paragraph>
<paragraph id="P-0013" lvl="0"><number>&lsqb;0013&rsqb;</number> U.S. Pat. No. 4,733,969 issued to Steven K. Case et. al. discloses a sensor system including a camera and an illuminator disposed properly to measure a three-dimensional object. The illuminator is located vertically to a measurement surface with a photo detector disposed at an angle. Generally three-dimensional measurement system with a use of illuminator as a light source has a shadow effect due to an object height that blocks the illuminator. Also if an illuminator is projected to an object vertically, a reflected light from the object may show reflections from an object as well as from a lower surface. </paragraph>
<paragraph id="P-0014" lvl="0"><number>&lsqb;0014&rsqb;</number> U.S. Pat. No. 5,859,924 issued to Kuo-Ching Liu et. al. described three-dimensional vision system with two position sensing detectors. To minimize a shadow effect, two photo diode arrays were employed. Additionally, another photo diode array is attached so as to get a two dimensional image data. The system can obtain 3D information using simple optical triangulation method. However, since the illuminator is projected from the top and the system measures reflected image from an object, it&apos;s difficult for the system to measure edge portions of a steeped curved shape such as ball shape. Also the measuring points have a two dimensionally projected points distribution, in other word, a uniformly distributed points which is not proper to describe a three dimensional object. </paragraph>
<paragraph id="P-0015" lvl="0"><number>&lsqb;0015&rsqb;</number> U.S. Pat. No. 6,072,898 issued to Elwin M. Beaty et. al. described a system to measure three-dimensional data by utilizing shadows of illuminations. By measuring the shadow size of an object, three-dimensional data is calculated. This method is good for pass-fail inspection since the method simply provides a maximum height of the object. However, it has difficulties to measure dimensional properties such as volume as well as height of fine curved-surfaces such as solder paste as well as file BGA balls. </paragraph>
<paragraph id="P-0016" lvl="7"><number>&lsqb;0016&rsqb;</number> Objects and Advantages </paragraph>
<paragraph id="P-0017" lvl="0"><number>&lsqb;0017&rsqb;</number> Comparing to the previous arts, the presented invention advantages are: </paragraph>
<paragraph id="P-0018" lvl="2"><number>&lsqb;0018&rsqb;</number> (a) to provide an apparatus to measure three-dimensional object by utilizing plural illuminators for faster measurement simultaneously; </paragraph>
<paragraph id="P-0019" lvl="2"><number>&lsqb;0019&rsqb;</number> (b) to provide an apparatus for precise and accurate measurement of a curved shape; </paragraph>
<paragraph id="P-0020" lvl="2"><number>&lsqb;0020&rsqb;</number> (c) to provide an apparatus for occlusion-minimized measurement; </paragraph>
<paragraph id="P-0021" lvl="2"><number>&lsqb;0021&rsqb;</number> (d) to provide an apparatus for two and three-dimensional measurement simultaneously. </paragraph>
</section>
<section>
<heading lvl="1">SUMMARY OF THE INVENTION </heading>
<paragraph id="P-0022" lvl="0"><number>&lsqb;0022&rsqb;</number> In the present invention, a dimensional measurement method provides a way of measuring two- and three-dimensional object features within photographic device field of view with two properly disposed lighting devices (i.e., lasers). Utilizing this method, three-dimensional object feature representation and inspection can be carried out by the presented dimensional measurement apparatus. </paragraph>
<paragraph id="P-0023" lvl="0"><number>&lsqb;0023&rsqb;</number> The dimensional measurement apparatus comprises one photographic device with plural lighting devices. Properly disposed devices enable dimensional measurements of object features in two- and three-dimensional spaces. To achieve the measurements, proper device calibrations are required. After defining the disposition of device setups and their calibrations, the devices can be integrated with additional electronic hardware to obtain object feature data from the integrated devices. The obtained measured object feature information will be processed into three-dimensional world coordinates by utilizing the devices calibration data. Using the resultant data, object feature inspections and volumetric representations could be realized. The apparatus provides dual line-scanning capability with opposite directional incident angle projections for the illuminations. The dual line-scanning method provides advantages that it reduces data gathering time compare to a single scanning method in a fixed resolution, and it also enhances measurement accuracies since the dual line-scanning method reduces object occlusion problem and errors from the width of the illuminator especially for the curved shaped object. </paragraph>
<paragraph id="P-0024" lvl="0"><number>&lsqb;0024&rsqb;</number> The measurement hardware is consisted of two lighting devices that generate lines of light disposed opposite directions each other, and the photographic device is located so as to view the reflections of the two lightings from the defined object feature surface, that are interfaced with a processor. To do interface of the devices for measurement, the photographic device needs frame grabber to grab the photographic device image. Input/output controller in conjunction with the processor controls the lighting devices (i.e., lasers and illuminator). To view a real object features and to define the inspection area for the features, an illuminator is attached under the photographic device. The lens system attached to the photographic device provides capabilities to view the lines of light reflected from the surface of the object features as well as the image reflected from the surface of the PCB by illumination. </paragraph>
<paragraph id="P-0025" lvl="0"><number>&lsqb;0025&rsqb;</number> The photographic device (i.e., CCD (charge coupled device) and CMOS (complementary metal oxide semiconductor) cameras) is to be selected to image a certain wavelength (i.e., 670 nm wavelength) of the lighting sources. By adjusting the light sources with opposite incident angles toward an object feature and the selected photographic device position, the photographic device grabs the two reflected line images at the same time. To convert the reflected line images into two- and three-dimensional world coordinates, optical calibrations need to be performed in advance. The optical calibrations include two-dimensional photographic device calibration and three-dimensional optical geometric calibration using standard optical triangulation principals. The grabbed images will be processed and machined using image processing algorithms such as model-based image filtering, feature segmentation and feature extraction algorithms to extract useful object feature height information in the image space. Using the optical calibration results, all the obtained object feature information can be interpreted and represented into two- and three-dimensional world coordinate space. Based on the inspection or the representation algorithms, the extracted image space information of the object features will be visualized and stored in respectively desired formats. </paragraph>
<paragraph id="P-0026" lvl="0"><number>&lsqb;0026&rsqb;</number> To perform the dimensional measurement for a desired inspection area, additional traversing mechanism needs to be integrated. The measurement apparatus that measures a predefined area consists of the optical dispositions (such as a photographic device, lighting devices and illuminator) and X-Y-Z axis traversing mechanism integrated with control hardware and software algorithms. The apparatus also has input/output devices such as monitor and keyboard, and hardware such as frame grabber for interface between the processor and optical arrangements.</paragraph>
</section>
</summary-of-invention>
<brief-description-of-drawings>
<section>
<heading lvl="1">BRIEF DESCRIPTION OF THE DRAWINGS </heading>
<paragraph id="P-0027" lvl="0"><number>&lsqb;0027&rsqb;</number> The present invention will be readily apparent from the following more detailed description of exemplary embodiments and accompanying drawings wherein: </paragraph>
<paragraph id="P-0028" lvl="0"><number>&lsqb;0028&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 1</cross-reference> is a block diagram of measurement head of a first exemplary representative embodiment of the present invention; </paragraph>
<paragraph id="P-0029" lvl="0"><number>&lsqb;0029&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 2</cross-reference>(<highlight><italic>a</italic></highlight>) and <cross-reference target="DRAWINGS">FIG. 2</cross-reference>(<highlight><italic>b</italic></highlight>) are detailed schematic diagrams for dimensional measurement method (for left-half image analysis) according to the present invention; </paragraph>
<paragraph id="P-0030" lvl="0"><number>&lsqb;0030&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 3</cross-reference>(<highlight><italic>a</italic></highlight>) and <cross-reference target="DRAWINGS">FIG. 3</cross-reference>(<highlight><italic>b</italic></highlight>) are detailed schematic diagrams for dimensional measurement method (for right-half image analysis) according to the present invention; </paragraph>
<paragraph id="P-0031" lvl="0"><number>&lsqb;0031&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 4</cross-reference>(<highlight><italic>a</italic></highlight>), <cross-reference target="DRAWINGS">FIG. 4</cross-reference>(<highlight><italic>b</italic></highlight>), <cross-reference target="DRAWINGS">FIG. 4</cross-reference>(<highlight><italic>c</italic></highlight>) and <cross-reference target="DRAWINGS">FIG. 4</cross-reference>(<highlight><italic>d</italic></highlight>) are illustrations of photographic image samples corresponding to the various object features; </paragraph>
<paragraph id="P-0032" lvl="0"><number>&lsqb;0032&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 5</cross-reference>(<highlight><italic>a</italic></highlight>), <cross-reference target="DRAWINGS">FIG. 5</cross-reference>(<highlight><italic>b</italic></highlight>) and <cross-reference target="DRAWINGS">FIG. 5</cross-reference>(<highlight><italic>c</italic></highlight>) illustrate dual-scanning method in the content of measuring points; </paragraph>
<paragraph id="P-0033" lvl="0"><number>&lsqb;0033&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 6</cross-reference>(<highlight><italic>a</italic></highlight>) and <cross-reference target="DRAWINGS">FIG. 6</cross-reference>(<highlight><italic>b</italic></highlight>) are calibration target samples that can be used for optical calibration according to the present invention; </paragraph>
<paragraph id="P-0034" lvl="0"><number>&lsqb;0034&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 7</cross-reference> is a flowchart of the dimensional measurement procedure; </paragraph>
<paragraph id="P-0035" lvl="0"><number>&lsqb;0035&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 8</cross-reference> is a flowchart of the photographic device calibration procedure for the measurement according to the present invention; </paragraph>
<paragraph id="P-0036" lvl="0"><number>&lsqb;0036&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 9</cross-reference> is a dimensional measurement apparatus block diagram of a second exemplary representative embodiment of the present invention; </paragraph>
<paragraph id="P-0037" lvl="0"><number>&lsqb;0037&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 10</cross-reference> is coordinate systems to obtain the three-dimensional information using dimensional measurement apparatus using X-Y-Z traversing mechanism.</paragraph>
</section>
</brief-description-of-drawings>
<detailed-description>
<paragraph id="P-0038" lvl="0"><number>&lsqb;0038&rsqb;</number>  
<table-cwu id="TABLE-US-00001">
<number>1</number>
<table frame="none" colsep="0" rowsep="0">
<tgroup align="left" colsep="0" rowsep="0" cols="1">
<colspec colname="1" colwidth="217PT" align="center"/>
<thead>
<row>
<entry></entry>
</row>
<row><entry namest="1" nameend="1" align="center" rowsep="1"></entry>
</row>
<row>
<entry>Reference Numerals In Drawings</entry>
</row>
<row><entry namest="1" nameend="1" align="center" rowsep="1"></entry>
</row>
</thead>
<tbody valign="top">
<row>
<entry></entry>
</row>
</tbody>
</tgroup>
<tgroup align="left" colsep="0" rowsep="0" cols="4">
<colspec colname="1" colwidth="21PT" align="right"/>
<colspec colname="2" colwidth="98PT" align="left"/>
<colspec colname="3" colwidth="21PT" align="right"/>
<colspec colname="4" colwidth="77PT" align="left"/>
<tbody valign="top">
<row>
<entry>101</entry>
<entry>measurement head</entry>
<entry>102</entry>
<entry>laser</entry>
</row>
<row>
<entry>103</entry>
<entry>laser</entry>
<entry>104</entry>
<entry>photographic device</entry>
</row>
<row>
<entry>105</entry>
<entry>lens system</entry>
<entry>106</entry>
<entry>optical lens system</entry>
</row>
<row>
<entry>107</entry>
<entry>illuminator</entry>
<entry>108</entry>
<entry>line of light</entry>
</row>
<row>
<entry>109</entry>
<entry>mirror</entry>
<entry>111</entry>
<entry>mirror</entry>
</row>
<row>
<entry>112</entry>
<entry>object feature</entry>
<entry>113</entry>
<entry>frame grabber</entry>
</row>
<row>
<entry>114</entry>
<entry>Laser/illuminator controller</entry>
<entry>115</entry>
<entry>display device</entry>
</row>
<row>
<entry>116</entry>
<entry>processor</entry>
<entry>117</entry>
<entry>memory</entry>
</row>
<row>
<entry>118</entry>
<entry>line of light</entry>
<entry>119</entry>
<entry>mirror</entry>
</row>
<row>
<entry>120</entry>
<entry>reflected lines of light</entry>
<entry>121</entry>
<entry>reflected lines</entry>
</row>
<row>
<entry>201</entry>
<entry>image</entry>
<entry>202</entry>
<entry>image centerline</entry>
</row>
<row>
<entry>203</entry>
<entry>line of light</entry>
<entry>204</entry>
<entry>photographic device</entry>
</row>
<row>
<entry>205</entry>
<entry>viewing angle</entry>
<entry>206</entry>
<entry>laser</entry>
</row>
<row>
<entry>207</entry>
<entry>line of light</entry>
<entry>208</entry>
<entry>laser project angle</entry>
</row>
<row>
<entry>209</entry>
<entry>object</entry>
<entry>210</entry>
<entry>reflected line</entry>
</row>
<row>
<entry>211</entry>
<entry>photographic device image</entry>
<entry>212</entry>
<entry>left half size</entry>
</row>
<row>
<entry>213</entry>
<entry>calibration plane</entry>
<entry>301</entry>
<entry>image</entry>
</row>
<row>
<entry>303</entry>
<entry>reflected line of light</entry>
<entry>306</entry>
<entry>laser</entry>
</row>
<row>
<entry>307</entry>
<entry>line of light</entry>
<entry>308</entry>
<entry>laser project angle</entry>
</row>
<row>
<entry>309</entry>
<entry>object</entry>
<entry>310</entry>
<entry>reflected line</entry>
</row>
<row>
<entry>312</entry>
<entry>right half size</entry>
<entry>402</entry>
<entry>line</entry>
</row>
<row>
<entry>403</entry>
<entry>line</entry>
<entry>404</entry>
<entry>surface</entry>
</row>
<row>
<entry>405</entry>
<entry>projected lines of light</entry>
<entry>406</entry>
<entry>object feature</entry>
</row>
<row>
<entry>407</entry>
<entry>distorted line</entry>
<entry>408</entry>
<entry>distorted line</entry>
</row>
<row>
<entry>412</entry>
<entry>object feature</entry>
<entry>410</entry>
<entry>projected line</entry>
</row>
<row>
<entry>411</entry>
<entry>projected line of light</entry>
</row>
<row>
<entry>412</entry>
<entry>previous measured point</entry>
</row>
<row>
<entry>503</entry>
<entry>previous measured point</entry>
</row>
<row>
<entry>504</entry>
<entry>subsequent measurement point</entry>
</row>
<row>
<entry>505</entry>
<entry>subsequent measurement point</entry>
<entry>506</entry>
<entry>measurement point</entry>
</row>
<row>
<entry>507</entry>
<entry>measurement point</entry>
</row>
<row>
<entry>508</entry>
<entry>subsequent measurement point</entry>
</row>
<row>
<entry>509</entry>
<entry>subsequent measurement point</entry>
<entry>510</entry>
<entry>measurement points</entry>
</row>
<row>
<entry>511</entry>
<entry>line</entry>
</row>
<row>
<entry>512</entry>
<entry>subsequent measurement point</entry>
</row>
<row>
<entry>513</entry>
<entry>subsequent measurement point</entry>
<entry>514</entry>
<entry>point</entry>
</row>
<row>
<entry>601</entry>
<entry>calibration target</entry>
</row>
<row>
<entry>602</entry>
<entry>calibration target with dots</entry>
</row>
<row>
<entry>603</entry>
<entry>small dots with the same pitch</entry>
</row>
<row>
<entry>605</entry>
<entry>calibration target</entry>
<entry>606</entry>
<entry>intersection point</entry>
</row>
<row>
<entry>607</entry>
<entry>pitch</entry>
<entry>608</entry>
<entry>pitch</entry>
</row>
<row>
<entry>701</entry>
<entry>image</entry>
<entry>701</entry>
<entry>image</entry>
</row>
<row>
<entry>703</entry>
<entry>defined area</entry>
<entry>704</entry>
<entry>coordinate space</entry>
</row>
<row>
<entry>705</entry>
<entry>traversing mechanism</entry>
<entry>801</entry>
<entry>frame grabber</entry>
</row>
<row>
<entry>803</entry>
<entry>calibration target</entry>
<entry>805</entry>
<entry>apparatus design</entry>
</row>
<row>
<entry>901</entry>
<entry>memory</entry>
<entry>902</entry>
<entry>input device</entry>
</row>
<row>
<entry>903</entry>
<entry>X-Y-Z traversing mechanism</entry>
<entry>904</entry>
<entry>I/O controller</entry>
</row>
<row>
<entry>905</entry>
<entry>image data processor</entry>
<entry>909</entry>
<entry>measurement head</entry>
</row>
<row>
<entry>906</entry>
<entry>fixed frame</entry>
</row>
<row>
<entry>907</entry>
<entry>X-Y-Z traversing mechanism</entry>
</row>
<row>
<entry>908</entry>
<entry>object feature</entry>
<entry>909</entry>
<entry>measurement head</entry>
</row>
<row><entry namest="1" nameend="4" align="center" rowsep="1"></entry>
</row>
</tbody>
</tgroup>
</table>
</table-cwu>
</paragraph>
<section>
<heading lvl="1">DESCRIPTION OF THE PREFERRED EMBODIMENTS </heading>
<paragraph id="P-0039" lvl="0"><number>&lsqb;0039&rsqb;</number> The embodiments of the present invention will be described with reference to the attached drawings. </paragraph>
<paragraph id="P-0040" lvl="0"><number>&lsqb;0040&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 1</cross-reference> is a block diagram of measurement head of a first exemplary representative embodiment of the present invention. This block diagram illustrates a dimensional measurement apparatus with a present invention of measurement head <highlight><bold>101</bold></highlight>. The measurement head <highlight><bold>101</bold></highlight> consists of photographic device <highlight><bold>104</bold></highlight>, lens system <highlight><bold>105</bold></highlight>, illuminator <highlight><bold>107</bold></highlight>, two mirrors <highlight><bold>109</bold></highlight>, <highlight><bold>119</bold></highlight> and two lasers <highlight><bold>102</bold></highlight>, <highlight><bold>103</bold></highlight> with optical lens systems <highlight><bold>106</bold></highlight>, <highlight><bold>109</bold></highlight>, <highlight><bold>111</bold></highlight>. The photographic device <highlight><bold>104</bold></highlight> needs to be set up to focus a measuring object feature <highlight><bold>112</bold></highlight> for a good focused image gathering. The photographic device <highlight><bold>104</bold></highlight> field of view is predefined. The two lasers <highlight><bold>102</bold></highlight>, <highlight><bold>103</bold></highlight> generate individual single line of light <highlight><bold>108</bold></highlight>, <highlight><bold>118</bold></highlight> that project inside of the photographic device <highlight><bold>104</bold></highlight> field of view. The reflected lines of light <highlight><bold>120</bold></highlight>, <highlight><bold>121</bold></highlight> will be imaged on to the photographic device <highlight><bold>104</bold></highlight>. Due to the object feature&apos;s <highlight><bold>112</bold></highlight> height along the Z-axis, the reflected lines <highlight><bold>120</bold></highlight>, <highlight><bold>121</bold></highlight> will be imaged as distorted lines. The obtained distorted lines include the object feature&apos;s z-directional information. The lasers <highlight><bold>102</bold></highlight>, <highlight><bold>103</bold></highlight> location as well as projection angles can be varied by design. Since the photographic device <highlight><bold>104</bold></highlight> will obtain the two reflected laser lines <highlight><bold>120</bold></highlight>, <highlight><bold>121</bold></highlight> simultaneously, the laser projection angle needs to be set up properly so that the lines <highlight><bold>120</bold></highlight>, <highlight><bold>121</bold></highlight> will not be overlapped each other in the photographic device <highlight><bold>104</bold></highlight> image within the pre-designed measurable height range along the Z-axis when the photographic device <highlight><bold>104</bold></highlight> grabs the reflected laser lines <highlight><bold>120</bold></highlight>, <highlight><bold>121</bold></highlight> from a certain object feature <highlight><bold>112</bold></highlight>. To do adjust the proper laser projection angles, mirrors are used in this exemplary illustration. However, lasers <highlight><bold>102</bold></highlight>, <highlight><bold>103</bold></highlight> can be directly projected with a proper projection incident angle setup. Illuminator <highlight><bold>107</bold></highlight> is attached so that when the photographic device <highlight><bold>104</bold></highlight> needs to view an actual object feature <highlight><bold>112</bold></highlight> view, the photographic device <highlight><bold>104</bold></highlight> can obtain enough illumination for the object feature view. However, when the measurement is started, the illuminator <highlight><bold>107</bold></highlight> may need to turn off so that the photographic device <highlight><bold>104</bold></highlight> can images a certain range of light wavelength for better image processing purpose. The present invention includes variations of projection methods such as utilizing mirrors <highlight><bold>109</bold></highlight>, <highlight><bold>111</bold></highlight> for detouring the laser lights <highlight><bold>108</bold></highlight>, <highlight><bold>118</bold></highlight> or direct projection of lasers <highlight><bold>102</bold></highlight>, <highlight><bold>103</bold></highlight> with an incident angle. Also the various light sources (i.e., different wavelengths) can be used as long as the photographic device <highlight><bold>104</bold></highlight> can image the wavelengths of the projected light source. The various photographic devices can such as Photo-Sensitive Device (PSD), Charged-Coupled Device (CCD) or Complementary Metal Oxide Semiconductor (CMOS) cameras. The frame grabber <highlight><bold>113</bold></highlight> is interfaced between processor <highlight><bold>116</bold></highlight> and photographic device <highlight><bold>104</bold></highlight>. Laser/illuminator controller <highlight><bold>114</bold></highlight> controls the Illuminator <highlight><bold>107</bold></highlight> and Lasers <highlight><bold>102</bold></highlight>, <highlight><bold>103</bold></highlight>. The memory <highlight><bold>117</bold></highlight> is used to store program algorithms to process the images and control additional devices such ad laser <highlight><bold>102</bold></highlight>, <highlight><bold>103</bold></highlight> and illuminator <highlight><bold>107</bold></highlight>. With the proper processing of the image obtained by the photographic device <highlight><bold>104</bold></highlight> through frame grabber <highlight><bold>113</bold></highlight>, processor <highlight><bold>116</bold></highlight> and memory <highlight><bold>117</bold></highlight>, processed resultant data can be displayed through display device <highlight><bold>115</bold></highlight>, and also can be stored in to the memory <highlight><bold>117</bold></highlight> for further processing. The calibration plane <highlight><bold>213</bold></highlight> will be used as a reference plane for the object height setup. The photographic device active image area size can be varied as long as the device can obtain the desired reflected lines image (i.e., CMOS camera is used as a photographic device in this exemplary illustration. The image area size is 1288&times;1032, as an example) The lighting device wavelength could be any range as long as the photographic device with proper lens system can image the reflected wavelength from the surface of the object features. Number of lighting devices could be plural for the desired multiple lines generation with their line projection angles respectively. Also, the configurations for the lighting devices and the photographic device could be varied as long as the reflected lines are in the photographic divide&apos;s field of view. Other lighting device setup examples could be utilization of multiple lines projections from one lighting source or four lines projection from four different directions with <highlight><bold>90</bold></highlight>-degree incident angle distance. </paragraph>
<paragraph id="P-0041" lvl="0"><number>&lsqb;0041&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 2</cross-reference>(<highlight><italic>a</italic></highlight>) and <cross-reference target="DRAWINGS">FIG. 2</cross-reference>(<highlight><italic>b</italic></highlight>) are a detailed schematic diagram for dimensional measurement method (for left-half image analysis) according to the present invention. In <cross-reference target="DRAWINGS">FIG. 2</cross-reference>(<highlight><italic>a</italic></highlight>) shows photographic device image <highlight><bold>201</bold></highlight>. Since the invented measurement head consists of multiple light lines (The <cross-reference target="DRAWINGS">FIG. 1</cross-reference> shows two light lines as an exemplary illustration.), the photographic device image needs to be divided properly. The image centerline <highlight><bold>202</bold></highlight> is used for two light line application. When the laser <highlight><bold>206</bold></highlight> projects a line of light <highlight><bold>207</bold></highlight> with an incident projection angle <highlight><bold>208</bold></highlight> to the object <highlight><bold>209</bold></highlight>, the reflected line from the object feature <highlight><bold>209</bold></highlight> will be imaged as <highlight><bold>203</bold></highlight> for a flat surface. The photographic device <highlight><bold>204</bold></highlight> will obtain the image <highlight><bold>201</bold></highlight> with a reflected line of light <highlight><bold>203</bold></highlight> on the left half size <highlight><bold>212</bold></highlight> of the image active area <highlight><bold>201</bold></highlight>. To obtain the reflected line of light <highlight><bold>210</bold></highlight>, the incident projection angle and the laser need to be properly positioned. Also the viewing angle for the photographic device <highlight><bold>205</bold></highlight> needs to cover the reflection range of the object so that the photographic device can obtain the image <highlight><bold>201</bold></highlight>. </paragraph>
<paragraph id="P-0042" lvl="0"><number>&lsqb;0042&rsqb;</number> To obtain three-dimensional information for the object features in the photographic image obtained, a standard optical triangulation principals. Based on the <cross-reference target="DRAWINGS">FIG. 2</cross-reference>(<highlight><italic>b</italic></highlight>), the object height H<highlight><subscript>1 </subscript></highlight>could be obtained by the following equation: </paragraph>
<paragraph lvl="0"><in-line-formula><highlight><italic>H</italic></highlight><highlight><subscript>1</subscript></highlight>&equals;(<highlight><italic>B</italic></highlight><highlight><subscript>1</subscript></highlight><highlight><italic>&minus;A</italic></highlight><highlight><subscript>1</subscript></highlight>) tan (&thgr;<highlight><subscript>1</subscript></highlight>) </in-line-formula></paragraph>
<paragraph id="P-0043" lvl="7"><number>&lsqb;0043&rsqb;</number> The will be predefined and can be provided from the laser projection setup. To calculate the (B<highlight><subscript>1</subscript></highlight>&minus;A<highlight><subscript>1</subscript></highlight>), photographic device <highlight><bold>204</bold></highlight> calibration needs to be preceded. The calibration includes a relationship definition between photographic device image <highlight><bold>201</bold></highlight> coordinates and their corresponding world coordinates. To do the photographic device calibration, the calibration plane <highlight><bold>213</bold></highlight> should be defined. The world coordinate A<highlight><subscript>1 </subscript></highlight>is predefined by the laser project angle <highlight><bold>208</bold></highlight> and laser position setup. The world coordinates A<highlight><subscript>1 </subscript></highlight>and B<highlight><subscript>1 </subscript></highlight>can be obtained from the photographic device image (<highlight><bold>211</bold></highlight> for A<highlight><subscript>1 </subscript></highlight>and <highlight><bold>203</bold></highlight> for B<highlight><subscript>1</subscript></highlight>) by utilizing the calibration data. </paragraph>
<paragraph id="P-0044" lvl="0"><number>&lsqb;0044&rsqb;</number> The laser position and projection angle should be setup properly so that the photographic device <highlight><bold>204</bold></highlight> can image the reflected line of light <highlight><bold>210</bold></highlight> inside the viewing angle <highlight><bold>205</bold></highlight>. The object height H should be in the range of pre-defined range so that the reflected line of light <highlight><bold>203</bold></highlight> should be imaged within the photographic device active imaging area <highlight><bold>201</bold></highlight> (for the left-side projection (<cross-reference target="DRAWINGS">FIG. 2</cross-reference>(<highlight><italic>b</italic></highlight>), the active imaging area will be on the left-half size <highlight><bold>212</bold></highlight> of the device image <highlight><bold>201</bold></highlight>.). </paragraph>
<paragraph id="P-0045" lvl="0"><number>&lsqb;0045&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 3</cross-reference>(<highlight><italic>a</italic></highlight>) and <cross-reference target="DRAWINGS">FIG. 3</cross-reference>(<highlight><italic>b</italic></highlight>) are a detailed schematic diagram for dimensional measurement method (for right-half image analysis) according to the present invention. In <cross-reference target="DRAWINGS">FIG. 3</cross-reference>(<highlight><italic>a</italic></highlight>) shows photographic device image <highlight><bold>201</bold></highlight>. Since the invented measurement head consists of multiple light lines (The <cross-reference target="DRAWINGS">FIG. 1</cross-reference> shows two light lines as an exemplary illustration.), the photographic device image needs to be divided properly. The image centerline <highlight><bold>202</bold></highlight> is used for two light line application. When the laser <highlight><bold>306</bold></highlight> projects a line of light <highlight><bold>307</bold></highlight> with an incident projection angle <highlight><bold>308</bold></highlight> to the object <highlight><bold>309</bold></highlight>, the reflected line from the object feature <highlight><bold>309</bold></highlight> will be imaged as <highlight><bold>303</bold></highlight> for a flat surface. The photographic device <highlight><bold>204</bold></highlight> will obtain the image <highlight><bold>301</bold></highlight> with a reflected line of light <highlight><bold>203</bold></highlight> on the right half size <highlight><bold>312</bold></highlight> of the image active area <highlight><bold>201</bold></highlight>. To obtain the reflected line of light <highlight><bold>310</bold></highlight>, the incident projection angle and the laser need to be properly positioned. Also the viewing angle for the photographic device <highlight><bold>205</bold></highlight> needs to cover the reflection range of the object so that the photographic device can obtain the image <highlight><bold>201</bold></highlight>. </paragraph>
<paragraph id="P-0046" lvl="0"><number>&lsqb;0046&rsqb;</number> To obtain three-dimensional information for the object features in the photographic image obtained, a standard optical triangulation principals. Based on the <cross-reference target="DRAWINGS">FIG. 3</cross-reference>(<highlight><italic>b</italic></highlight>), the object height H<highlight><subscript>2 </subscript></highlight>could be obtained by the following equation: </paragraph>
<paragraph lvl="0"><in-line-formula><highlight><italic>H</italic></highlight><highlight><subscript>2</subscript></highlight>&equals;(<highlight><italic>B</italic></highlight><highlight><subscript>2</subscript></highlight><highlight><italic>&minus;A</italic></highlight><highlight><subscript>2</subscript></highlight>) tan (&thgr;<highlight><subscript>2</subscript></highlight>) </in-line-formula></paragraph>
<paragraph id="P-0047" lvl="0"><number>&lsqb;0047&rsqb;</number> The will be predefined and can be provided from the laser projection setup. To calculate the (B<highlight><subscript>2</subscript></highlight>&minus;A<highlight><subscript>2</subscript></highlight>), photographic device <highlight><bold>204</bold></highlight> calibration needs to be preceded. The calibration includes a relationship definition between photographic device image <highlight><bold>201</bold></highlight> coordinates and their corresponding world coordinates. To do the photographic device calibration, the calibration plane <highlight><bold>213</bold></highlight> should be defined. The world coordinate A<highlight><subscript>2 </subscript></highlight>is predefined by the laser project angle <highlight><bold>308</bold></highlight> and laser position setup. The world coordinates A<highlight><subscript>2 </subscript></highlight>and B<highlight><subscript>2 </subscript></highlight>can be obtained from the photographic device image (<highlight><bold>311</bold></highlight> for A<highlight><subscript>2 </subscript></highlight>and <highlight><bold>303</bold></highlight> for B<highlight><subscript>2</subscript></highlight>) by utilizing the calibration data. </paragraph>
<paragraph id="P-0048" lvl="0"><number>&lsqb;0048&rsqb;</number> The laser position and projection angle should be setup properly so that the photographic device <highlight><bold>204</bold></highlight> can image the reflected line of light <highlight><bold>310</bold></highlight> inside the viewing angle <highlight><bold>205</bold></highlight>. The object height H<highlight><subscript>2 </subscript></highlight>should be in the range of pre-defined range so that the reflected line of light <highlight><bold>303</bold></highlight> should be imaged within the photographic device active imaging area <highlight><bold>201</bold></highlight> (for the right-side projection (<cross-reference target="DRAWINGS">FIG. 3</cross-reference>(<highlight><italic>b</italic></highlight>), the active imaging area will be on the right-half size <highlight><bold>312</bold></highlight> of the device image <highlight><bold>201</bold></highlight>.). </paragraph>
<paragraph id="P-0049" lvl="0"><number>&lsqb;0049&rsqb;</number> As described the measurement method using <cross-reference target="DRAWINGS">FIG. 2</cross-reference>(<highlight><italic>b</italic></highlight>) and <cross-reference target="DRAWINGS">FIG. 3</cross-reference>(<highlight><italic>b</italic></highlight>), one photographic device <highlight><bold>204</bold></highlight> can obtain two distorted lines <highlight><bold>203</bold></highlight>, <highlight><bold>303</bold></highlight> of light reflected to the photographic device active image area <highlight><bold>201</bold></highlight>. For multiple line projection using light source of lines, the photographic device active image area can be divided into several areas as described above. </paragraph>
<paragraph id="P-0050" lvl="0"><number>&lsqb;0050&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 4</cross-reference>(<highlight><italic>a</italic></highlight>), <cross-reference target="DRAWINGS">FIG. 4</cross-reference>(<highlight><italic>b</italic></highlight>), <cross-reference target="DRAWINGS">FIG. 4</cross-reference>(<highlight><italic>c</italic></highlight>) and <cross-reference target="DRAWINGS">FIG. 4</cross-reference>(<highlight><italic>d</italic></highlight>) are illustrations of photographic image samples corresponding to the various object features. The lines <highlight><bold>402</bold></highlight>, <highlight><bold>403</bold></highlight> of the image <highlight><bold>201</bold></highlight> in </paragraph>
<paragraph id="P-0051" lvl="0"><number>&lsqb;0051&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 4</cross-reference>(<highlight><italic>a</italic></highlight>) represent the heights for the intersection lines between the object feature <highlight><bold>406</bold></highlight> surface and the projected lines of light <highlight><bold>404</bold></highlight> and <highlight><bold>405</bold></highlight> respectively. The distorted lines <highlight><bold>407</bold></highlight>, <highlight><bold>408</bold></highlight> of the image <highlight><bold>201</bold></highlight> in <cross-reference target="DRAWINGS">FIG. 4</cross-reference>(<highlight><italic>c</italic></highlight>) represent the heights for the intersection lines between the object feature <highlight><bold>412</bold></highlight> surface and the projected lines of light <highlight><bold>410</bold></highlight> and <highlight><bold>411</bold></highlight> respectively. Once the lines of light projection angles for both left and right projection cases are determined, the reflected lines in the photographic device for the both sides projections will be moved along a single direction (&larr; and &rarr; directions respectively) as the object feature height is getting higher. For example, left-size projection case (<cross-reference target="DRAWINGS">FIG. 2</cross-reference>(<highlight><italic>b</italic></highlight>), the line of the light <highlight><bold>211</bold></highlight> on the calibration plane <highlight><bold>213</bold></highlight> will be moved toward left (&larr;) as the object feature height is getting higher as the reflected distorted line <highlight><bold>203</bold></highlight> shown in the <cross-reference target="DRAWINGS">FIG. 2</cross-reference>(<highlight><italic>a</italic></highlight>) so that the reflected distorted image will not be in the left-side of the active imaging area <highlight><bold>212</bold></highlight>. For right-side projection case (<cross-reference target="DRAWINGS">FIG. 3</cross-reference>(<highlight><italic>b</italic></highlight>) as well, the reflected distorted line <highlight><bold>303</bold></highlight> will be only located in the right-hand side of the active imaging area <highlight><bold>312</bold></highlight> of the photographic device image <highlight><bold>201</bold></highlight> and will be moved toward right (&rarr;) as the object feature height is getting higher. However, if the object height is higher than the pre-designed value (in other words, height measurement limit) and the calibration plane <highlight><bold>213</bold></highlight>, the reflected distorted lines image <highlight><bold>203</bold></highlight>, <highlight><bold>303</bold></highlight> may not be within the photographic device imaging area <highlight><bold>201</bold></highlight> so that the apparatus cannot measure the object feature height. If the object is lower than calibration plane <highlight><bold>213</bold></highlight>, the reflected distorted lines image <highlight><bold>203</bold></highlight>, <highlight><bold>303</bold></highlight> will be moved toward the reversal direction (for left and right projection cases, reflected distorted line images will be moved toward the image centerline <highlight><bold>202</bold></highlight>, &rarr; and &larr; directions respectively.). </paragraph>
<paragraph id="P-0052" lvl="0"><number>&lsqb;0052&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 5</cross-reference>(<highlight><italic>a</italic></highlight>), <cross-reference target="DRAWINGS">FIG. 5</cross-reference>(<highlight><italic>b</italic></highlight>) and <cross-reference target="DRAWINGS">FIG. 5</cross-reference>(<highlight><italic>c</italic></highlight>) illustrate dual-scanning method in the content of measuring points. <cross-reference target="DRAWINGS">FIG. 5</cross-reference>(<highlight><italic>a</italic></highlight>) shows scanning method to increase measurement speed up to double by defining a certain step of traversing mechanism movement. For example, the two lines <highlight><bold>502</bold></highlight>, <highlight><bold>503</bold></highlight> move together at a same time and the subsequent measurement points <highlight><bold>504</bold></highlight>, <highlight><bold>505</bold></highlight> can be measured between the previous measured points <highlight><bold>502</bold></highlight>, <highlight><bold>503</bold></highlight>. The proper movement step can be calculated so that all the measured points have the same interval/step of measurements. <cross-reference target="DRAWINGS">FIG. 5</cross-reference>(<highlight><italic>b</italic></highlight>) shows measurement points measured without correct measurement step calculations. Without proper movement step calculation, the measurement points <highlight><bold>506</bold></highlight>, <highlight><bold>507</bold></highlight> and subsequent measurement points <highlight><bold>508</bold></highlight>, <highlight><bold>509</bold></highlight> may have different measurement intervals. <cross-reference target="DRAWINGS">FIG. 5</cross-reference>(<highlight><italic>c</italic></highlight>) shows a scanning method to increase measurement accuracy by measuring points twice. For example, the two lines <highlight><bold>510</bold></highlight>, <highlight><bold>511</bold></highlight> move together at a same time, the movement step for the subsequent measurement point <highlight><bold>512</bold></highlight>, <highlight><bold>513</bold></highlight> can be calculated so that all the measured points can be measured twice, once from left-side projection setup <cross-reference target="DRAWINGS">FIG. 2</cross-reference>(<highlight><italic>b</italic></highlight>) and another from right-side projection setup <cross-reference target="DRAWINGS">FIG. 3</cross-reference>(<highlight><italic>b</italic></highlight>). The point <highlight><bold>514</bold></highlight> will be measured twice, one from <highlight><bold>510</bold></highlight> and another from <highlight><bold>513</bold></highlight> as shown in the <cross-reference target="DRAWINGS">FIG. 3</cross-reference>(<highlight><italic>c</italic></highlight>). The two measurement points <highlight><bold>510</bold></highlight>, <highlight><bold>513</bold></highlight> can be post-processed (i.e., averaged) to obtain better measurement accuracy for the point <highlight><bold>514</bold></highlight>. </paragraph>
<paragraph id="P-0053" lvl="0"><number>&lsqb;0053&rsqb;</number> When the components (i.e., photographic device field of view and lighting device projection angles) of the measurement head disposition are defined, inspection resolution for X, Y and Z axes can be defined. However, based on the optical calibrations method, the resultant resolutions could be varied. When the range of Z-axis measurements range is defined, the corresponding imaging area of the photographic device can be defined. Therefore, one photographic device can process the image of multiple lines of light reflected from the object features. For example, CCD or CMOS camera can take multiple lines of image at the same time and process the lines separate based on the corresponding optical calibration results. However, since the multiple lines have their own pre-fixed projection angles, optical calibration results will be different among the lines. </paragraph>
<paragraph id="P-0054" lvl="0"><number>&lsqb;0054&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 6</cross-reference>(<highlight><italic>a</italic></highlight>) and <cross-reference target="DRAWINGS">FIG. 6</cross-reference>(<highlight><italic>b</italic></highlight>) are calibration target samples that can be used for photographic device calibration according to the present invention. The provided calibration targets <highlight><bold>601</bold></highlight>, <highlight><bold>605</bold></highlight> can be used for photographic device calibration to interpret the photographic device image pixel coordinates into world coordinates. <cross-reference target="DRAWINGS">FIG. 6</cross-reference>(<highlight><italic>a</italic></highlight>) consists of small dots with the same pitch <highlight><bold>603</bold></highlight>, <highlight><bold>604</bold></highlight> between dots along horizontal axis and vertical axis. To perform optical calibration, the centroid of the dot <highlight><bold>602</bold></highlight> in the photographic device image can be obtained using image processing algorithms. After obtaining all the centroids of the dots in the image pixel coordinates, the coordinates could be correlated to the real world coordinates for the calibration target. The photographic device calibration can be done using Least Square Error method or Bi-linear interpolation method, as examples. <cross-reference target="DRAWINGS">FIG. 6</cross-reference>(<highlight><italic>b</italic></highlight>) as well can be utilized for the photographic device calibration. To use the calibration target <highlight><bold>605</bold></highlight>, the intersection points such as the intersection point <highlight><bold>606</bold></highlight> can be extracted using image processing algorithms. The pitch <highlight><bold>607</bold></highlight>, <highlight><bold>608</bold></highlight> can be the same. The extracted intersection points in the image pixel coordinates can be correlated to the intersection points in the world coordinates. The calibration mathematics can be the same as the calibration target with dots <highlight><bold>602</bold></highlight> once the image pixel coordinates and the world coordinates for the intersection points for the calibration target are obtained. </paragraph>
<paragraph id="P-0055" lvl="0"><number>&lsqb;0055&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 7</cross-reference> is a flowchart of the dimensional measurement procedures To carry out the dimensional measurement, the photographic device <highlight><bold>104</bold></highlight> needs to grab the image <highlight><bold>701</bold></highlight> to obtain the distorted contour lines of light from the object feature surface. The frame grabber <highlight><bold>113</bold></highlight> is used to obtain the photographic device image to transfer the data to the processor <highlight><bold>116</bold></highlight>. Once the processor receives the image data from the frame grabber, software algorithms will be used to process the image <highlight><bold>702</bold></highlight> to extract the object feature height information. Scanning will be carried out till the defined area is completely scanned <highlight><bold>703</bold></highlight>. Using photographic device <highlight><bold>104</bold></highlight> calibration data and optical setup data (i.e., projection angles <highlight><bold>208</bold></highlight>, <highlight><bold>308</bold></highlight>), the obtained reflected contour for the object feature could be converted into world coordinate space <highlight><bold>704</bold></highlight>. Since the scanning utilize traversing mechanism to scan the desired areas, the converted world coordinates and the traversing mechanism coordinates need to be added together <highlight><bold>705</bold></highlight>, which finally can represent the three-dimensional representation of the desired object feature. </paragraph>
<paragraph id="P-0056" lvl="0"><number>&lsqb;0056&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 8</cross-reference> is a flowchart of the photographic device calibration procedure for the measurement according to the present invention. The proposed calibration target <highlight><bold>601</bold></highlight> or <highlight><bold>605</bold></highlight> can be used for the photographic device calibration. Using the photographic device <highlight><bold>104</bold></highlight>, the calibration target image can be grabbed through the frame grabber <highlight><bold>801</bold></highlight>. The centroids for the dots target or the intersections of the grid lines can be extracted <highlight><bold>802</bold></highlight>. Using Least square Error Method or Bi-sectional Interpolation Method, the obtained calibration target information such as centroids or intersections in the content of image pixel coordinates can be correlated on to the world coordinates for the centroids or intersections for the calibration target <highlight><bold>803</bold></highlight>. The results of the correlation will be used for the apparatus optical calibration for object feature height information conversion. The laser projection angles <highlight><bold>208</bold></highlight>, <highlight><bold>308</bold></highlight> needs to be defined based on the apparatus design <highlight><bold>805</bold></highlight>, and the defined angles will be utilized for the apparatus optical calibration for height measurement. </paragraph>
<paragraph id="P-0057" lvl="0"><number>&lsqb;0057&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 9</cross-reference> is a dimensional measurement apparatus block diagram of a second exemplary representative embodiment of the present invention. The block diagram shows the dimensional measurement apparatus integrated with necessary additional devices such as processor and memories <highlight><bold>901</bold></highlight> for image processing and algorithms for obtained data handling to extract the information for the object feature height as well as representation of the object features, display device with input devices <highlight><bold>902</bold></highlight> for resultant data display. The measurement head will be attached to the traversing mechanism, or the measurement head will be fixed and traversing mechanism can be located at the lower of the measurement head so that the object features can be scanned using the X-Y traversing mechanism. The Z-axis will be used to adjust the calibration plane <highlight><bold>213</bold></highlight> as a reference. Therefore the system equips the X-Y-Z traversing mechanism <highlight><bold>903</bold></highlight>. I/O controller such as illuminator and lasers will be controller by the I/O controller <highlight><bold>904</bold></highlight>. The frame grabber and image data processor <highlight><bold>905</bold></highlight> will be integrated to process the photographic device image. In <cross-reference target="DRAWINGS">FIG. 9</cross-reference>, the measurement head <highlight><bold>909</bold></highlight> is attached to the fixed frame <highlight><bold>906</bold></highlight> to hole the head, and X-Y-Z traversing mechanism <highlight><bold>907</bold></highlight> is located at the below of the measurement head. To measure the object feature <highlight><bold>908</bold></highlight>, the feature needs to be located below the measurement head always in this setup. However, the present invention includes that the measurement head can be attached to the traversing mechanism so that the object feature can be located at the fixed location on the calibration plane. </paragraph>
<paragraph id="P-0058" lvl="0"><number>&lsqb;0058&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 10</cross-reference> is coordinate systems to obtain the three-dimensional information using dimensional measurement apparatus using X-Y-Z traversing mechanism. The photographic device <highlight><bold>104</bold></highlight> needs to be calibrated to setup the relationship between photographic device pixel coordinates and the world coordinates (RW) of the corresponding calibration targets (i.e., centriods of circles or intersections of the line grids). Utilizing the photographic device <highlight><bold>104</bold></highlight> calibration results and the precisely adjusted lighting device projection angles <highlight><bold>208</bold></highlight>, <highlight><bold>308</bold></highlight>, standard optical used to obtain the geometric and optical relationships for the measurement head assembly <highlight><bold>909</bold></highlight>. When the images are being grabbed, the traversing mechanism <highlight><bold>907</bold></highlight> signal will be utilized to synchronize the traversing mechanism locations and the measurement data obtained through the measurement head <highlight><bold>909</bold></highlight>. </paragraph>
<paragraph lvl="0"><in-line-formula><highlight><italic>R&equals;I&plus;S </italic></highlight></in-line-formula></paragraph>
<paragraph id="P-0059" lvl="7"><number>&lsqb;0059&rsqb;</number> Where, R is an actual measured point in the world coordinate system (RW), I is a fixed vector to represent the geometrical relationship between world coordinates and the measurement head coordinates and S is a measured point coordinates in the measurement head coordinate system (SW). The RWX, RWY and RWZ are for the world coordinates along the X-, Y- and Z-axes. The SWX, SWY and SWZ are for the sensor coordinates. </paragraph>
<paragraph id="P-0060" lvl="0"><number>&lsqb;0060&rsqb;</number> As is described in considerable detail from the foregoing, the present invention provides a means of two- and three-dimensional measurement method and process for the object features. Also utilizing the present invention of the process, two- and three-dimensional measurement apparatus is presented, which include in present invention. Although the embodiments are described for solder paste inspection as well as BGA inspection, the present invention can also be applied to many different types semiconductor chip carriers (packages) such as PGAs (Pin Grid Arrays), QFPs (Quad Flat Packages), Flip Chips and several types of J-leaded packages. The present invention can be applied to the object feature representation and reconstruction as well. However, the present invention can be achieved through various specifications of the devices and apparatus, and that various modifications, both as to the apparatus details and operating procedures, without departing from the sprit and the scope of the invention. </paragraph>
</section>
</detailed-description>
</subdoc-description>
<subdoc-claims>
<heading lvl="1">What is claimed is: </heading>
<claim id="CLM-00001">
<claim-text><highlight><bold>1</bold></highlight>. Dimensional measurement apparatus determining at least one dimension of at least a portion of an object feature comprising: 
<claim-text>a) Single photographic means disposed above the object to be measured comprising dual imaging area divisions for dual incident light projections processing; </claim-text>
<claim-text>b) Dual illumination projection means disposed at the opposite directions each other; </claim-text>
<claim-text>c) Measurement head means comprising single photographic means a) and dual illumination projection means b); </claim-text>
<claim-text>d) A processor, interfaced with the measurement head to obtain the scanned image and process the image, convert it to three-dimensional information using processing algorithms and calibration data. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00002">
<claim-text><highlight><bold>2</bold></highlight>. The apparatus of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference> further comprising: 
<claim-text>a) calibration means for dual illumination projections; </claim-text>
<claim-text>b) height calculations means for dual illumination projections; </claim-text>
<claim-text>c) photographic image processing means for dual illumination projections; </claim-text>
<claim-text>d) scanning means with dual illumination projections; </claim-text>
<claim-text>e) photographic device calibration means with dual image area divisions.</claim-text>
</claim-text>
</claim>
</subdoc-claims>
<subdoc-drawings id="DRAWINGS">
<heading lvl="0" align="CENTER">Drawings</heading>
<representative-figure>1</representative-figure>
<figure id="figure-D00000">
<image id="EMI-D00000" file="US20030001117A1-20030102-D00000.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00001">
<image id="EMI-D00001" file="US20030001117A1-20030102-D00001.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00002">
<image id="EMI-D00002" file="US20030001117A1-20030102-D00002.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00003">
<image id="EMI-D00003" file="US20030001117A1-20030102-D00003.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00004">
<image id="EMI-D00004" file="US20030001117A1-20030102-D00004.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00005">
<image id="EMI-D00005" file="US20030001117A1-20030102-D00005.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00006">
<image id="EMI-D00006" file="US20030001117A1-20030102-D00006.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00007">
<image id="EMI-D00007" file="US20030001117A1-20030102-D00007.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00008">
<image id="EMI-D00008" file="US20030001117A1-20030102-D00008.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00009">
<image id="EMI-D00009" file="US20030001117A1-20030102-D00009.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00010">
<image id="EMI-D00010" file="US20030001117A1-20030102-D00010.TIF" imf="TIFF" ti="DR"/>
</figure>
</subdoc-drawings>
</patent-application-publication>
