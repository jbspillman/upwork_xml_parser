<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE patent-application-publication SYSTEM "pap-v16-2002-01-01.dtd" [
<!ENTITY US20030001837A1-20030102-M00001.NB SYSTEM "US20030001837A1-20030102-M00001.NB" NDATA NB>
<!ENTITY US20030001837A1-20030102-M00001.TIF SYSTEM "US20030001837A1-20030102-M00001.TIF" NDATA TIF>
<!ENTITY US20030001837A1-20030102-M00002.NB SYSTEM "US20030001837A1-20030102-M00002.NB" NDATA NB>
<!ENTITY US20030001837A1-20030102-M00002.TIF SYSTEM "US20030001837A1-20030102-M00002.TIF" NDATA TIF>
<!ENTITY US20030001837A1-20030102-M00003.NB SYSTEM "US20030001837A1-20030102-M00003.NB" NDATA NB>
<!ENTITY US20030001837A1-20030102-M00003.TIF SYSTEM "US20030001837A1-20030102-M00003.TIF" NDATA TIF>
<!ENTITY US20030001837A1-20030102-M00004.NB SYSTEM "US20030001837A1-20030102-M00004.NB" NDATA NB>
<!ENTITY US20030001837A1-20030102-M00004.TIF SYSTEM "US20030001837A1-20030102-M00004.TIF" NDATA TIF>
<!ENTITY US20030001837A1-20030102-M00005.NB SYSTEM "US20030001837A1-20030102-M00005.NB" NDATA NB>
<!ENTITY US20030001837A1-20030102-M00005.TIF SYSTEM "US20030001837A1-20030102-M00005.TIF" NDATA TIF>
<!ENTITY US20030001837A1-20030102-M00006.NB SYSTEM "US20030001837A1-20030102-M00006.NB" NDATA NB>
<!ENTITY US20030001837A1-20030102-M00006.TIF SYSTEM "US20030001837A1-20030102-M00006.TIF" NDATA TIF>
<!ENTITY US20030001837A1-20030102-M00007.NB SYSTEM "US20030001837A1-20030102-M00007.NB" NDATA NB>
<!ENTITY US20030001837A1-20030102-M00007.TIF SYSTEM "US20030001837A1-20030102-M00007.TIF" NDATA TIF>
<!ENTITY US20030001837A1-20030102-D00000.TIF SYSTEM "US20030001837A1-20030102-D00000.TIF" NDATA TIF>
<!ENTITY US20030001837A1-20030102-D00001.TIF SYSTEM "US20030001837A1-20030102-D00001.TIF" NDATA TIF>
<!ENTITY US20030001837A1-20030102-D00002.TIF SYSTEM "US20030001837A1-20030102-D00002.TIF" NDATA TIF>
<!ENTITY US20030001837A1-20030102-D00003.TIF SYSTEM "US20030001837A1-20030102-D00003.TIF" NDATA TIF>
<!ENTITY US20030001837A1-20030102-D00004.TIF SYSTEM "US20030001837A1-20030102-D00004.TIF" NDATA TIF>
<!ENTITY US20030001837A1-20030102-D00005.TIF SYSTEM "US20030001837A1-20030102-D00005.TIF" NDATA TIF>
<!ENTITY US20030001837A1-20030102-D00006.TIF SYSTEM "US20030001837A1-20030102-D00006.TIF" NDATA TIF>
<!ENTITY US20030001837A1-20030102-D00007.TIF SYSTEM "US20030001837A1-20030102-D00007.TIF" NDATA TIF>
<!ENTITY US20030001837A1-20030102-D00008.TIF SYSTEM "US20030001837A1-20030102-D00008.TIF" NDATA TIF>
<!ENTITY US20030001837A1-20030102-D00009.TIF SYSTEM "US20030001837A1-20030102-D00009.TIF" NDATA TIF>
<!ENTITY US20030001837A1-20030102-D00010.TIF SYSTEM "US20030001837A1-20030102-D00010.TIF" NDATA TIF>
<!ENTITY US20030001837A1-20030102-D00011.TIF SYSTEM "US20030001837A1-20030102-D00011.TIF" NDATA TIF>
<!ENTITY US20030001837A1-20030102-D00012.TIF SYSTEM "US20030001837A1-20030102-D00012.TIF" NDATA TIF>
<!ENTITY US20030001837A1-20030102-D00013.TIF SYSTEM "US20030001837A1-20030102-D00013.TIF" NDATA TIF>
<!ENTITY US20030001837A1-20030102-D00014.TIF SYSTEM "US20030001837A1-20030102-D00014.TIF" NDATA TIF>
]>
<patent-application-publication>
<subdoc-bibliographic-information>
<document-id>
<doc-number>20030001837</doc-number>
<kind-code>A1</kind-code>
<document-date>20030102</document-date>
</document-id>
<publication-filing-type>new</publication-filing-type>
<domestic-filing-data>
<application-number>
<doc-number>10150840</doc-number>
</application-number>
<application-number-series-code>10</application-number-series-code>
<filing-date>20020520</filing-date>
</domestic-filing-data>
<foreign-priority-data>
<priority-application-number>
<doc-number>0112203.5</doc-number>
</priority-application-number>
<filing-date>20010518</filing-date>
<country-code>GB</country-code>
</foreign-priority-data>
<technical-information>
<classification-ipc>
<classification-ipc-primary>
<ipc>G06T015/00</ipc>
</classification-ipc-primary>
<classification-ipc-edition>07</classification-ipc-edition>
</classification-ipc>
<classification-us>
<classification-us-primary>
<uspc>
<class>345</class>
<subclass>419000</subclass>
</uspc>
</classification-us-primary>
</classification-us>
<title-of-invention>Method and apparatus for generating confidence data</title-of-invention>
</technical-information>
<inventors>
<first-named-inventor>
<name>
<given-name>Adam</given-name>
<middle-name>Michael</middle-name>
<family-name>Baumberg</family-name>
</name>
<residence>
<residence-non-us>
<city>Bracknell</city>
<country-code>GB</country-code>
</residence-non-us>
</residence>
<authority-applicant>INV</authority-applicant>
</first-named-inventor>
</inventors>
<correspondence-address>
<name-1>FITZPATRICK CELLA HARPER &amp; SCINTO</name-1>
<name-2></name-2>
<address>
<address-1>30 ROCKEFELLER PLAZA</address-1>
<city>NEW YORK</city>
<state>NY</state>
<postalcode>10112</postalcode>
<country>
<country-code>US</country-code>
</country>
</address>
</correspondence-address>
</subdoc-bibliographic-information>
<subdoc-abstract>
<paragraph id="A-0001" lvl="0">An image processing apparatus (<highlight><bold>2</bold></highlight>) is disclosed in which input images are processed to generate texture map data for texture rendering a generated three-dimensional computer model of object(s) appearing in the images. In order to select the portions of the images utilized, confidence data is generated indicative of the extent portions of the surface of a model are visible in each of the images. The images are then combined utilizing this confidence data, where image data representative of different spatial frequencies are blended in different ways utilizing the confidence data. </paragraph>
</subdoc-abstract>
<subdoc-description>
<summary-of-invention>
<paragraph id="P-0001" lvl="0"><number>&lsqb;0001&rsqb;</number> The present invention concerns method and apparatus for generating confidence data. In particular the present invention concerns method and apparatus for generating data indicative of the reliability of image data from specified viewpoints of an object to be modelled for generating texture data for portions of the model of an object appearing in the image. </paragraph>
<paragraph id="P-0002" lvl="0"><number>&lsqb;0002&rsqb;</number> When a large number of images of an object from different viewpoints are available, this provides a large amount of information about the appearance of the surface of an object. Texture data for texture rendering a model of an object appearing in images can then be generated from the images. </paragraph>
<paragraph id="P-0003" lvl="0"><number>&lsqb;0003&rsqb;</number> However, as for any three-dimensional object not all of the surface of any object will be visible in any particular view, it is necessary to combine image data from different images to generate texture rendering data. A means for determining which images are utilised for generating which portions of texture render data is therefore required. </paragraph>
<paragraph id="P-0004" lvl="0"><number>&lsqb;0004&rsqb;</number> In accordance with one aspect of the present invention there is provided a method of generating reliability data indicative of the reliability of portions of an image of an object from a first viewpoint for generating image data for a computer model of said object appearing in said image viewed from a second viewpoint comprising the steps of: </paragraph>
<paragraph id="P-0005" lvl="2"><number>&lsqb;0005&rsqb;</number> generating an image of a said object viewed from a first viewpoint rendered utilizing data representative of the reliability of utilizing portions of an image of an object from said first viewpoint for generating data representative of the surface of said object; and </paragraph>
<paragraph id="P-0006" lvl="2"><number>&lsqb;0006&rsqb;</number> generating reliability data for said object as viewed from a second viewpoint utilizing said generated image to generate reliability data for portions of a said object visible from both said first and second viewpoints and default data indicative of low reliability for the remainder of the surface of said object visible from said second viewpoint. </paragraph>
</summary-of-invention>
<brief-description-of-drawings>
<paragraph id="P-0007" lvl="0"><number>&lsqb;0007&rsqb;</number> Further aspects and embodiments of the present invention will become apparent with reference to the following description and drawings in which: </paragraph>
<paragraph id="P-0008" lvl="0"><number>&lsqb;0008&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 1</cross-reference> is a schematic block diagram of a first embodiment of the present invention; </paragraph>
<paragraph id="P-0009" lvl="0"><number>&lsqb;0009&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 2</cross-reference> is an exemplary illustration of the position and orientation of six texture maps bounding an exemplary subject object; </paragraph>
<paragraph id="P-0010" lvl="0"><number>&lsqb;0010&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 3</cross-reference> is a schematic block diagram of the surface texturer of <cross-reference target="DRAWINGS">FIG. 1</cross-reference>; </paragraph>
<paragraph id="P-0011" lvl="0"><number>&lsqb;0011&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 4</cross-reference> is a f low diagram of the processing of the weight determination module of the surf ace texturer of <cross-reference target="DRAWINGS">FIG. 3</cross-reference>; </paragraph>
<paragraph id="P-0012" lvl="0"><number>&lsqb;0012&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 5</cross-reference> is a f low diagram of the processing of the weight determination module to generate a weight map image; </paragraph>
<paragraph id="P-0013" lvl="0"><number>&lsqb;0013&rsqb;</number> <cross-reference target="DRAWINGS">FIGS. 6A, 6B</cross-reference>, <highlight><bold>7</bold></highlight>A and <highlight><bold>7</bold></highlight>B are exemplary illustrations of data generated by the weight determination module in order to generate a weight map image for an exemplary object; </paragraph>
<paragraph id="P-0014" lvl="0"><number>&lsqb;0014&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 8</cross-reference> is a weight map image corresponding to the data represented by <cross-reference target="DRAWINGS">FIGS. 6A, 6B</cross-reference>, <highlight><bold>7</bold></highlight>A and <highlight><bold>7</bold></highlight>B; </paragraph>
<paragraph id="P-0015" lvl="0"><number>&lsqb;0015&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 9</cross-reference> is a flow diagram of the processing of the weight determination module to generate weight function data utilizing a weight map image; </paragraph>
<paragraph id="P-0016" lvl="0"><number>&lsqb;0016&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 10</cross-reference> is a schematic block diagram of the texture map determination module of <cross-reference target="DRAWINGS">FIG. 3</cross-reference>; </paragraph>
<paragraph id="P-0017" lvl="0"><number>&lsqb;0017&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 11</cross-reference> is a flow diagram of the processing of low frequency canonical projections; </paragraph>
<paragraph id="P-0018" lvl="0"><number>&lsqb;0018&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 12</cross-reference> is a flow diagram of the processing of high frequency canonical projections; </paragraph>
<paragraph id="P-0019" lvl="0"><number>&lsqb;0019&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 13</cross-reference> is a schematic block diagram of a surface texturer in accordance with a second embodiment of the present invention; </paragraph>
<paragraph id="P-0020" lvl="0"><number>&lsqb;0020&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 14</cross-reference> is a flow diagram of the processing of the canonical view determination module the surface texturer of <cross-reference target="DRAWINGS">FIG. 13</cross-reference>; </paragraph>
<paragraph id="P-0021" lvl="0"><number>&lsqb;0021&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 15</cross-reference> is a f low diagram of the processing of high frequency canonical projections in accordance with a third embodiment of the present invention; and </paragraph>
<paragraph id="P-0022" lvl="0"><number>&lsqb;0022&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 16</cross-reference> is a graph illustrating generated blend functions.</paragraph>
</brief-description-of-drawings>
<detailed-description>
<paragraph id="P-0023" lvl="0"><number>&lsqb;0023&rsqb;</number> A first embodiment of will now be described in which a number of generated projected images corresponding to the same viewpoints are combined to generate composite texture map data for texture rendering a 3D computer model of object(s) appearing in the images. In the embodiment projected images corresponding to the same view points are combined utilizing generated weight function data indicative of the relative reliability of corresponding portions of the projected images. </paragraph>
<paragraph id="P-0024" lvl="7"><number>&lsqb;0024&rsqb;</number> First Embodiment </paragraph>
<paragraph id="P-0025" lvl="0"><number>&lsqb;0025&rsqb;</number> Referring to <cross-reference target="DRAWINGS">FIG. 1</cross-reference>, an embodiment of the invention comprises a processing apparatus <highlight><bold>2</bold></highlight>, such as a personal computer, containing, in a conventional manner, one or more processors, memories, graphics cards etc., together with a display device <highlight><bold>4</bold></highlight>, such as a conventional personal computer monitor, user input devices <highlight><bold>6</bold></highlight>, such as a keyboard, mouse etc. and a printer <highlight><bold>8</bold></highlight>. </paragraph>
<paragraph id="P-0026" lvl="0"><number>&lsqb;0026&rsqb;</number> The processing apparatus <highlight><bold>2</bold></highlight> is programmed to operate in accordance with programming instructions input, for example, as data stored on a data storage medium, such as disk <highlight><bold>12</bold></highlight>, and/or as a signal <highlight><bold>14</bold></highlight> input to the processing apparatus <highlight><bold>2</bold></highlight>, for example from a remote database, by transmission over a communication network (not shown) such as the Internet or by transmission through the atmosphere, and/or entered by a user via a user input device <highlight><bold>6</bold></highlight> such as a keyboard. </paragraph>
<paragraph id="P-0027" lvl="0"><number>&lsqb;0027&rsqb;</number> As will be described in more detail below, the programming instructions comprise instructions to cause the processing apparatus <highlight><bold>2</bold></highlight> to become configured to process input data defining a plurality of images of a subject object recorded from different view points. The input data then is processed to generate data identifying the positions and orientations at which the input images were recorded. These calculated positions and orientations and the image data are then used to generate data defining a three-dimensional computer model of the subject object. </paragraph>
<paragraph id="P-0028" lvl="0"><number>&lsqb;0028&rsqb;</number> When programmed by the programming instructions, processing apparatus <highlight><bold>2</bold></highlight> effectively becomes configured into a number of functional units for performing processing operations. Examples of such functional units and their interconnections are shown in <cross-reference target="DRAWINGS">FIG. 1</cross-reference>. The units and interconnections illustrated in <cross-reference target="DRAWINGS">FIG. 1</cross-reference> are, however, notional and are shown for illustration purposes only to assist understanding; they do not necessarily represent the exact units and connections into which the processor, memory etc. of the processing apparatus <highlight><bold>2</bold></highlight> become configured. </paragraph>
<paragraph id="P-0029" lvl="0"><number>&lsqb;0029&rsqb;</number> Referring to the functional units shown in <cross-reference target="DRAWINGS">FIG. 1, a</cross-reference> central controller <highlight><bold>20</bold></highlight> processes inputs from the user input devices <highlight><bold>6</bold></highlight>, and also provides control and processing for the other functional units. Memory <highlight><bold>24</bold></highlight> is provided for use by central controller <highlight><bold>20</bold></highlight> and the other functional units. </paragraph>
<paragraph id="P-0030" lvl="0"><number>&lsqb;0030&rsqb;</number> Input data store <highlight><bold>26</bold></highlight> stores input data input to the processing apparatus <highlight><bold>2</bold></highlight> for example as data stored on a storage device, such as disk <highlight><bold>28</bold></highlight>, as a signal <highlight><bold>30</bold></highlight> transmitted to the processing apparatus <highlight><bold>2</bold></highlight>, or using a user input device <highlight><bold>6</bold></highlight>. The input data defines a plurality of colour images of one or more objects recorded at different positions and orientations. In addition, in this embodiment, the input data also includes data defining the intrinsic parameters of the camera which recorded the images, that is, the aspect ratio, focal length, principal point (the point at which the optical axis intersects the imaging plane), first order radial distortion coefficient, and skew angle (the angle between the axes on the pixel grid; because the axes may not be exactly orthogonal). </paragraph>
<paragraph id="P-0031" lvl="0"><number>&lsqb;0031&rsqb;</number> The input data defining the input images may be generated for example by downloading pixel data from a digital camera which recorded the images, or by scanning photographs using a scanner (not shown). The input data defining the intrinsic camera parameters may be input by a user using a user input device <highlight><bold>6</bold></highlight>. </paragraph>
<paragraph id="P-0032" lvl="0"><number>&lsqb;0032&rsqb;</number> Position determination module <highlight><bold>32</bold></highlight> processes the input images received by the input data store <highlight><bold>26</bold></highlight> to determine the relative positions and orientations of camera view points from which image data of an object represented by the image data have been obtained. In this embodiment, this is achieved in a conventional manner by identifying and matching features present in the input images and calculating relative positions of camera views utilising these matches. </paragraph>
<paragraph id="P-0033" lvl="0"><number>&lsqb;0033&rsqb;</number> Surface modeller <highlight><bold>34</bold></highlight> processes the data defining the input images and the data defining the positions and orientations at which the images were recorded to generate data defining a 3D computer wire mesh model representing the actual surface(s) of the object(s) in the images. In this embodiment this 3D model defines a plurality of triangles representing the surface of the subject object modelled. </paragraph>
<paragraph id="P-0034" lvl="0"><number>&lsqb;0034&rsqb;</number> Surface texturer <highlight><bold>36</bold></highlight> generates texture data from the input image data for rendering onto the surface model produced by surface modeller <highlight><bold>34</bold></highlight>. In particular, in this embodiment the surface texturer <highlight><bold>36</bold></highlight> processes the input image data and the data defining the positions and orientations at which the images were recorded to generate six texture maps comprising six views of a subject object as viewed from a box bounding the subject object. These generated texture maps are then utilized to texture render the surface model so that images of a modelled subject object from any viewpoint may be generated. </paragraph>
<paragraph id="P-0035" lvl="0"><number>&lsqb;0035&rsqb;</number> As part of generation of the texture maps, the surface texturer <highlight><bold>36</bold></highlight> initially determines weight function data indicative of the relative reliability of different input images for generating texture maps for texture rendering different portions of a modelled object. The processing of the surface texturer <highlight><bold>36</bold></highlight> to generate this weight function data and the subsequent generation of texture maps from the input image data will be described in detail later. </paragraph>
<paragraph id="P-0036" lvl="0"><number>&lsqb;0036&rsqb;</number> Display processor <highlight><bold>40</bold></highlight>, under the control of central controller <highlight><bold>20</bold></highlight>, displays instructions to a user via display device <highlight><bold>4</bold></highlight>. In addition, under the control of central controller <highlight><bold>20</bold></highlight>, display processor <highlight><bold>40</bold></highlight> also displays images of the 3D computer model of the object from a user-selected viewpoint by processing the surface model data generated by surface modeller <highlight><bold>34</bold></highlight> and rendering texture data produced by surface texturer <highlight><bold>36</bold></highlight> onto the surface model. </paragraph>
<paragraph id="P-0037" lvl="0"><number>&lsqb;0037&rsqb;</number> Printer controller <highlight><bold>42</bold></highlight>, under the control of central controller <highlight><bold>30</bold></highlight> causes hard copies of images of the 3D computer model of the object selected and displayed on the display device <highlight><bold>4</bold></highlight> to be printed by the printer <highlight><bold>8</bold></highlight>. </paragraph>
<paragraph id="P-0038" lvl="0"><number>&lsqb;0038&rsqb;</number> Output data store <highlight><bold>44</bold></highlight> stores the surface model and texture data therefor generated by surface modeller <highlight><bold>34</bold></highlight> and surface texturer <highlight><bold>36</bold></highlight>. Central controller <highlight><bold>20</bold></highlight> controls the output of data from output data store <highlight><bold>44</bold></highlight>, for example as data on a storage device, such as disk <highlight><bold>46</bold></highlight>, or as a signal <highlight><bold>48</bold></highlight>. </paragraph>
<paragraph id="P-0039" lvl="0"><number>&lsqb;0039&rsqb;</number> An outline of the processing of the surface texturer <highlight><bold>36</bold></highlight> for generating texture data for rendering onto a surface model produced by the surface modeller <highlight><bold>34</bold></highlight> will now be described. The structure and processing of the surface texturer <highlight><bold>36</bold></highlight> will then be described in detail. </paragraph>
<paragraph id="P-0040" lvl="7"><number>&lsqb;0040&rsqb;</number> Outline of Processing to Generate Canonical Texture Maps </paragraph>
<paragraph id="P-0041" lvl="0"><number>&lsqb;0041&rsqb;</number> When a plurality of images of a subject object recorded from different viewpoints are available, this provides a large amount of data about the outward appearance of the subject object. Where images are recorded from different viewpoints, these images provide varying amounts of data for the different portions of the subject object as those portions are visible to a lesser or greater amount within the images. In order to create a model of the appearance of an object, it is necessary to process these images to generate texture data so that a consistent texture model of a subject object can be created. </paragraph>
<paragraph id="P-0042" lvl="0"><number>&lsqb;0042&rsqb;</number> In this embodiment this is achieved by the surface texturer <highlight><bold>36</bold></highlight> which processes the input image data of a subject object recorded from different viewpoints in order to generate texture data for rendering the surface model produced by the surface modeller <highlight><bold>34</bold></highlight>. In this embodiment this texture data comprises six texture maps, the six texture maps comprising views of subject object from the six faces of a cuboid centred on the subject object. </paragraph>
<paragraph id="P-0043" lvl="0"><number>&lsqb;0043&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 2</cross-reference> is an exemplary illustration of the position and orientation of six texture maps <highlight><bold>50</bold></highlight>-<highlight><bold>55</bold></highlight> bounding an exemplary subject object <highlight><bold>56</bold></highlight>. In this embodiment the six texture maps comprise texture maps for six canonical views of an object being views of the object from the top <highlight><bold>50</bold></highlight>, bottom <highlight><bold>51</bold></highlight>, front <highlight><bold>52</bold></highlight>, back <highlight><bold>53</bold></highlight>, left <highlight><bold>54</bold></highlight> and right <highlight><bold>55</bold></highlight>. </paragraph>
<paragraph id="P-0044" lvl="0"><number>&lsqb;0044&rsqb;</number> The six canonical views <highlight><bold>50</bold></highlight>-<highlight><bold>55</bold></highlight> comprise three pairs <highlight><bold>50</bold></highlight>,<highlight><bold>51</bold></highlight>;<highlight><bold>52</bold></highlight>,<highlight><bold>53</bold></highlight>;<highlight><bold>54</bold></highlight>,<highlight><bold>55</bold></highlight> of parallel image planes, centred on the origin of the coordinate system of the model, with each of the three pairs of image planes aligned along one of the three coordinate axes of the coordinate system respectively. The relative positions of the viewpoints of the canonical views <highlight><bold>50</bold></highlight>-<highlight><bold>55</bold></highlight> are then selected so that relative to the size of the model <highlight><bold>56</bold></highlight> of the subject object created by the surface modeller <highlight><bold>34</bold></highlight>, the distance away from the object is selected so that the canonical views are equally spaced from the centre of the object and the extent of an object as viewed from each image plane is no more than an threshold number of pixels in extent. In this embodiment this threshold is set to be <highlight><bold>512</bold></highlight> pixels. Each of the texture maps for the model is then defined by a weak perspective projection of the subject object <highlight><bold>56</bold></highlight> onto the defined image planes. </paragraph>
<paragraph id="P-0045" lvl="0"><number>&lsqb;0045&rsqb;</number> After image data for each of the six canonical views <highlight><bold>50</bold></highlight>-<highlight><bold>55</bold></highlight> has been determined, image data of the model <highlight><bold>56</bold></highlight> of the subject object from any viewpoint can then be generated using conventional texture rendering techniques, where texture rendering data for each portion of the surface of the model <highlight><bold>56</bold></highlight> is generated utilizing selected portions of the canonical views <highlight><bold>50</bold></highlight>-<highlight><bold>56</bold></highlight>. </paragraph>
<paragraph id="P-0046" lvl="0"><number>&lsqb;0046&rsqb;</number> By generating texture data for a model of a subject object in this way the total number of required texture maps is limited to the six canonical views <highlight><bold>50</bold></highlight>-<highlight><bold>56</bold></highlight>. Furthermore, since each of the canonical views corresponds to a view of the projection of a real world object the canonical views <highlight><bold>50</bold></highlight>-<highlight><bold>55</bold></highlight> should be representative of realistic views of the subject object and hence be suited for compression by standard image compression algorithms such as JPEG which are optimised for compressing real world images. </paragraph>
<paragraph id="P-0047" lvl="0"><number>&lsqb;0047&rsqb;</number> As generated images of a model of a subject object may obtain texture rendering data from any of the six canonical texture maps, it is necessary to generate all of the texture maps in such a way that they are all consistent with one another. Thus in this way where different portions of an image of a subject object are rendered utilizing texture maps from different views no noticeable boundaries arise. As for any 3D object not all of the surface of a subject object from all six canonical views <highlight><bold>50</bold></highlight>-<highlight><bold>55</bold></highlight> will be visible in any single input image, it is necessary for the surface texture <highlight><bold>36</bold></highlight> to combine image data from the plurality of images available to generate texture maps for these six consistent views <highlight><bold>50</bold></highlight>-<highlight><bold>55</bold></highlight>. </paragraph>
<paragraph id="P-0048" lvl="0"><number>&lsqb;0048&rsqb;</number> Prior to describing in detail the processing by the surface texturer <highlight><bold>36</bold></highlight> which enables a set of six consistent texture maps to be generated from the available input images, the structure of the surface texturer <highlight><bold>36</bold></highlight> in terms of notional functional processing units will now be described. </paragraph>
<paragraph id="P-0049" lvl="7"><number>&lsqb;0049&rsqb;</number> Structure of Surface Texturer </paragraph>
<paragraph id="P-0050" lvl="0"><number>&lsqb;0050&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 3</cross-reference> is a schematic block diagram of the surface texturer <highlight><bold>36</bold></highlight> in accordance with this embodiment of the present invention. In this embodiment, the surface texturer <highlight><bold>36</bold></highlight> comprises a weight determination module <highlight><bold>58</bold></highlight> and a texture map determination module <highlight><bold>59</bold></highlight>. </paragraph>
<paragraph id="P-0051" lvl="0"><number>&lsqb;0051&rsqb;</number> In order to select portions of available image data to be utilized to generate the six canonical texture maps <highlight><bold>50</bold></highlight>-<highlight><bold>55</bold></highlight> the surface texturer <highlight><bold>36</bold></highlight> utilizes position data generated by the position determination module <highlight><bold>32</bold></highlight> identifying the viewpoints in which data has been obtained and 3D model data output by the surface modeler <highlight><bold>34</bold></highlight>. This position data and 3D model data is processed by the weight determination module <highlight><bold>58</bold></highlight> to generate weight function data identifying a relative preference for utilizing input images from an identified view point for generating texture data for each portion of the surface of the model of the subject object. </paragraph>
<paragraph id="P-0052" lvl="0"><number>&lsqb;0052&rsqb;</number> This weight function data is then passed to the texture map determination module <highlight><bold>59</bold></highlight> which processes the weight function data together with the position data generated by the position determination module <highlight><bold>32</bold></highlight>, the model data generated by the surface modeller <highlight><bold>34</bold></highlight> and the available image data stored within the input data store <highlight><bold>26</bold></highlight> to generate a set of six consistent texture maps for the canonical views <highlight><bold>50</bold></highlight>-<highlight><bold>55</bold></highlight>. </paragraph>
<paragraph id="P-0053" lvl="7"><number>&lsqb;0053&rsqb;</number> Generation of Weight Function Data </paragraph>
<paragraph id="P-0054" lvl="0"><number>&lsqb;0054&rsqb;</number> The processing of the weight determination module <highlight><bold>58</bold></highlight> to generate weight function data indicative of relative preferences for utilizing different input images for generating texture data for different portions of a model of a subject object will now be described in detail. </paragraph>
<paragraph id="P-0055" lvl="0"><number>&lsqb;0055&rsqb;</number> In this embodiment the weight function data is utilised to blend image data from different images of a subject object so that consistent texture maps for six canonical views <highlight><bold>50</bold></highlight>-<highlight><bold>55</bold></highlight> can be generated. To achieve this, weight function data is generated by the weight determination module <highlight><bold>58</bold></highlight> for the entire surface of a model of a subject object for each of the positions identified by the position data generated by the position determination module <highlight><bold>32</bold></highlight>. </paragraph>
<paragraph id="P-0056" lvl="0"><number>&lsqb;0056&rsqb;</number> In order for the images of a subject object stored in the input data store <highlight><bold>26</bold></highlight> to be smoothly blended to generate texture maps in which no discernible boundaries arise within the texture maps it is necessary for the weight function data for each viewpoint to vary smoothly across the surface of a model of a subject object. Furthermore, so that only image data representative of visible portions of a subject object is utilised to generate texture render data for a corresponding visible portion of the subject object as seen from a canonical view, it is necessary for the weight function data to define a function that falls to a level indicative of low reliability for parts of a surface of a subject object that are hidden or nearly hidden in a particular input image. </paragraph>
<paragraph id="P-0057" lvl="0"><number>&lsqb;0057&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 4</cross-reference> is a flow diagram of the processing of the weight determination module <highlight><bold>58</bold></highlight>. </paragraph>
<paragraph id="P-0058" lvl="0"><number>&lsqb;0058&rsqb;</number> Initially, the weight determination module <highlight><bold>58</bold></highlight> selects (S<highlight><bold>4</bold></highlight>-<highlight><bold>1</bold></highlight>) a first view point from the set of view points identified by position data output by the position determination module <highlight><bold>32</bold></highlight>. </paragraph>
<paragraph id="P-0059" lvl="0"><number>&lsqb;0059&rsqb;</number> After a particular viewpoint has been selected, the weight determination module <highlight><bold>58</bold></highlight> then (S<highlight><bold>4</bold></highlight>-<highlight><bold>2</bold></highlight>) calculates a weight map image for the selected viewpoint representative of the relative preference for utilising data from an input image from the currently selected viewpoint for texture rendering the corresponding portion of a subject object visible in that image. </paragraph>
<paragraph id="P-0060" lvl="0"><number>&lsqb;0060&rsqb;</number> In this embodiment, as will be described in detail later, this weight map image is generated so that values in the weight map image vary smoothly across the visible surface of a model of a subject object and falls to zero for all parts of a subject object adjacent to portions not visible in the selected view. In this way the weight map image is then suitable to be utilized to generate weight function data for a viewpoint as will be described in detail later. </paragraph>
<paragraph id="P-0061" lvl="0"><number>&lsqb;0061&rsqb;</number> After a weight map image has been generated by the weight determination module <highlight><bold>58</bold></highlight>, the weight determination module <highlight><bold>58</bold></highlight> then (S<highlight><bold>4</bold></highlight>-<highlight><bold>3</bold></highlight>) calculates weight function data for the selected view in the form of a texture map render function, generated utilizing the calculated weight map image. This texture map render function comprises data which varies smoothly across the surface of a model of a subject object and which is indicative of a low reliability for all portions of the surface of a subject object which are not visible in the selected view. This texture map render function is then (S<highlight><bold>4</bold></highlight>-<highlight><bold>4</bold></highlight>) stored as part of the weight function data to be output by the weight determination module and utilised by the texture map determination module <highlight><bold>59</bold></highlight>. </paragraph>
<paragraph id="P-0062" lvl="0"><number>&lsqb;0062&rsqb;</number> The weight determination module <highlight><bold>58</bold></highlight> then (S<highlight><bold>4</bold></highlight>-<highlight><bold>5</bold></highlight>) determines whether the viewpoint for which weight function data has been stored is the last viewpoint for which a texture map render function is to be generated and stored. If this is not the case the weight determination module <highlight><bold>58</bold></highlight> then (S<highlight><bold>4</bold></highlight>-<highlight><bold>6</bold></highlight>) proceeds to select the next viewpoint identified position data output by the position determination module <highlight><bold>32</bold></highlight> and generates a weight map image for the viewpoint (S<highlight><bold>4</bold></highlight>-<highlight><bold>2</bold></highlight>) before determining and storing a further texture map render function as weight function data for that viewpoint (S<highlight><bold>4</bold></highlight>-<highlight><bold>3</bold></highlight>-S<highlight><bold>4</bold></highlight>-<highlight><bold>4</bold></highlight>). The weight determination module <highlight><bold>58</bold></highlight> then checks (S<highlight><bold>4</bold></highlight>-<highlight><bold>5</bold></highlight>) once again whether the viewpoint for which weight function data has been stored corresponds to the final viewpoint identified by the position data output by the position determination module <highlight><bold>32</bold></highlight>. </paragraph>
<paragraph id="P-0063" lvl="0"><number>&lsqb;0063&rsqb;</number> Finally, when the weight determination module <highlight><bold>58</bold></highlight> determines that texture map render functions have been stored for each of the viewpoints identified by position data output by the position determination module <highlight><bold>32</bold></highlight>, these texture render functions are output (S<highlight><bold>4</bold></highlight>-<highlight><bold>7</bold></highlight>) as a weight function data by the weight determination module <highlight><bold>58</bold></highlight> for use by the texture map determination module <highlight><bold>59</bold></highlight>. </paragraph>
<paragraph id="P-0064" lvl="0"><number>&lsqb;0064&rsqb;</number> The processing of the weight determination module <highlight><bold>58</bold></highlight> in order to generate a weight map image for a selected viewpoint (S<highlight><bold>4</bold></highlight>-<highlight><bold>2</bold></highlight>) and for determining a texture map render function (S<highlight><bold>4</bold></highlight>-<highlight><bold>3</bold></highlight>) for a particular viewpoint will now be described in detail with reference to <cross-reference target="DRAWINGS">FIGS. 5, 6A</cross-reference> and <highlight><bold>6</bold></highlight>B, <highlight><bold>7</bold></highlight>A, <highlight><bold>7</bold></highlight>B, <highlight><bold>8</bold></highlight> and <highlight><bold>9</bold></highlight>. </paragraph>
<paragraph id="P-0065" lvl="0"><number>&lsqb;0065&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 5</cross-reference> is a f low diagram of the processing of the weight determination module <highlight><bold>58</bold></highlight> to generate a weight map image of a subject object from a selected viewpoint. </paragraph>
<paragraph id="P-0066" lvl="0"><number>&lsqb;0066&rsqb;</number> Initially in order to generate a weight map image the weight determination module <highlight><bold>58</bold></highlight> generates (S<highlight><bold>5</bold></highlight>-<highlight><bold>1</bold></highlight>) an initial reliability image of the subject object as viewed from the selected viewpoint. In the initial reliability image, the surface of each triangle in a subject object is flat rendered utilizing a visibility value with a value representative of the extent of visibility of the triangle from the selected viewpoint. </paragraph>
<paragraph id="P-0067" lvl="0"><number>&lsqb;0067&rsqb;</number> In this embodiment an initial value for texture rendering each triangle is calculated by determining the ratio of the number of pixels utilised to render a projection of the triangle in the selected viewpoint relative to the actual surface area of the triangle as calculated from the coordinates of the vertices of the triangle represented by the model data generated by the surface modeller <highlight><bold>34</bold></highlight>. The visibility value for each triangle is then calculated by calculating:  
<math-cwu id="MATH-US-00001">
<number>1</number>
<math>
<mrow>
  <mrow>
    <mi>visibility</mi>
    <mo>&it;</mo>
    <mstyle>
      <mtext>&emsp;</mtext>
    </mstyle>
    <mo>&it;</mo>
    <mi>value</mi>
  </mrow>
  <mo>=</mo>
  <mrow>
    <mi>int</mi>
    <mo>&af;</mo>
    <mrow>
      <mo>[</mo>
      <mrow>
        <mn>255</mn>
        <mo>*</mo>
        <mrow>
          <mo>(</mo>
          <mfrac>
            <mrow>
              <mi>initial</mi>
              <mo>&it;</mo>
              <mstyle>
                <mtext>&emsp;</mtext>
              </mstyle>
              <mo>&it;</mo>
              <mi>value</mi>
              <mo>&it;</mo>
              <mstyle>
                <mtext>&emsp;</mtext>
              </mstyle>
              <mo>&it;</mo>
              <mi>for</mi>
              <mo>&it;</mo>
              <mstyle>
                <mtext>&emsp;</mtext>
              </mstyle>
              <mo>&it;</mo>
              <mi>triangle</mi>
            </mrow>
            <mrow>
              <mi>greatest</mi>
              <mo>&it;</mo>
              <mstyle>
                <mtext>&emsp;</mtext>
              </mstyle>
              <mo>&it;</mo>
              <mi>initial</mi>
              <mo>&it;</mo>
              <mstyle>
                <mtext>&emsp;</mtext>
              </mstyle>
              <mo>&it;</mo>
              <mi>value</mi>
            </mrow>
          </mfrac>
          <mo>)</mo>
        </mrow>
      </mrow>
      <mo>]</mo>
    </mrow>
  </mrow>
</mrow>
</math>
<mathematica-file id="MATHEMATICA-00001" file="US20030001837A1-20030102-M00001.NB"/>
<image id="EMI-M00001" wi="216.027" he="18.96615" file="US20030001837A1-20030102-M00001.TIF" imf="TIFF" ti="MF"/>
</math-cwu>
</paragraph>
<paragraph id="P-0068" lvl="7"><number>&lsqb;0068&rsqb;</number> where greatest initial value is a calculated maximum value for initial values for triangles appearing in images as received from any of the viewpoints identified by position data output by the position determination module <highlight><bold>32</bold></highlight>. </paragraph>
<paragraph id="P-0069" lvl="0"><number>&lsqb;0069&rsqb;</number> Thus in this way each triangle is associated with a visibility value being an integer value between 0 and 255, where 255 is a value assigned to a square on view of a triangle from the closest of the viewpoints identified by the position determination module <highlight><bold>32</bold></highlight>. An image of a model the subject object seen from the selected viewpoint is then generated using standard Open GL function calls using depth buffering to remove hidden surfaces. In this calculated image each triangle is then rendered flat filled with an intensity value proportional to the calculated visibility value for the triangle, and the remainder of the image rendered with a zero visibility value. </paragraph>
<paragraph id="P-0070" lvl="0"><number>&lsqb;0070&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 6A</cross-reference> is an example of an image of a subject object rendered flat filled with an intensity values corresponding to visibility values for an exemplary object from a selected viewpoint. Specifically <cross-reference target="DRAWINGS">FIG. 6A</cross-reference> is an image generated for a model of a toy duck viewed from behind where the tail feathers of the toy duck protrude behind the head of the duck appearing at the top of the image. In the image visibility values corresponding to 0 are shown in black and values corresponding to 255 are shown in white with intermediate values shown as intermediate shades of grey. </paragraph>
<paragraph id="P-0071" lvl="0"><number>&lsqb;0071&rsqb;</number> As can be seen from <cross-reference target="DRAWINGS">FIG. 6A</cross-reference> the image generated by flat filling triangles with an intensity value corresponding to the visibility values results in an image in which boundaries between different triangles are apparent. Furthermore, as can be seen from <cross-reference target="DRAWINGS">FIG. 6A</cross-reference> in the case of portions of the image corresponding to the head and tail features of the toy duck adjacent pixels in the image do not necessarily correspond to neighbouring points on the object surface. Thus in order to generate a weight map image suitable for creating a weight function that varies smoothly across the surface of a model of the subject object and that falls to zero for portions of subject object not visible in a view, this initial reliability image is processed further as will now be described in detail. </paragraph>
<paragraph id="P-0072" lvl="0"><number>&lsqb;0072&rsqb;</number> Once an initial reliability image has been generated the initial reliability image is first (S<highlight><bold>5</bold></highlight>-<highlight><bold>2</bold></highlight>) smoothed using a Gaussian smoothing kernel with zero values clamped to zero. In this embodiment the size of the Gaussian blur is selected to represent the projection of a cube placed at the origin and viewed from the selected viewpoint for which weight function data is being generated. The size of the cube in this embodiment is selected to correspond to 1% of the diagonal of the bounding box of the canonical views <highlight><bold>50</bold></highlight>-<highlight><bold>55</bold></highlight> for which texture render data is to be created. The selection of a blur size based upon the size of a predefined cube as seen from the selected viewpoint ensures that the manner in which smoothing occurs is consistent across different images taken at different distances from the origin. </paragraph>
<paragraph id="P-0073" lvl="0"><number>&lsqb;0073&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 6B</cross-reference> is an example of a smoothed reliability image being a smoothed image of the initial reliability image of <cross-reference target="DRAWINGS">FIG. 6A</cross-reference>. Again, as in <cross-reference target="DRAWINGS">FIG. 6</cross-reference>A, values corresponding to 0 are shown in black and values corresponding to 255 are shown in white with intermediate values shown as intermediate shades of grey. By smoothing the image, a smoothed reliability image is generated in which values vary gradually across the surface of an object which correspond to adjacent visible portions of the image. </paragraph>
<paragraph id="P-0074" lvl="0"><number>&lsqb;0074&rsqb;</number> However, where adjacent portions of the surface of a subject object correspond to portions both visible and not visible from the selected viewpoint, associating the portions not visible from the selected viewpoint with a zero value can result in a discontinuity in the variation of values over the surface of the object. </paragraph>
<paragraph id="P-0075" lvl="0"><number>&lsqb;0075&rsqb;</number> To deal with this potential problem the weight determination module <highlight><bold>58</bold></highlight> then (S<highlight><bold>5</bold></highlight>-<highlight><bold>3</bold></highlight>) first generates an edge map image identifying all the pixels in an image of the subject object from the selected viewpoint corresponding to edges visible from the selected viewpoint where this problem may occur. </paragraph>
<paragraph id="P-0076" lvl="0"><number>&lsqb;0076&rsqb;</number> Specifically for each of the triangles in a model generated by the surface modeller <highlight><bold>34</bold></highlight>, the orientation of the surface each of the triangles is first identified. In a system following the open GL conventions for representing three dimensional models, the coordinates of triangles on a three dimensional model are stored so that successive coordinates correspond to coordinates clockwise from the previous set of coordinates. The orientation of an individual triangle can therefore be defined by calculating for a triangle having projected vertices W<highlight><subscript>0</subscript></highlight>, W<highlight><subscript>1</subscript></highlight>, W<highlight><subscript>2 </subscript></highlight>the cross product: </paragraph>
<paragraph lvl="0"><in-line-formula>(<highlight><italic>W</italic></highlight><highlight><subscript>1</subscript></highlight><highlight><italic>&minus;W</italic></highlight>)&times;(<highlight><italic>W</italic></highlight><highlight><subscript>2</subscript></highlight><highlight><italic>&minus;W</italic></highlight><highlight><subscript>1</subscript></highlight>) </in-line-formula></paragraph>
<paragraph id="P-0077" lvl="0"><number>&lsqb;0077&rsqb;</number> Whether a particular triangle is oriented towards or away from the camera viewpoint and then be determined by checking whether this calculated cross product is positive or negative. </paragraph>
<paragraph id="P-0078" lvl="0"><number>&lsqb;0078&rsqb;</number> Once the respective orientations of triangles have been determined, a list of edges can then be calculated by noting all the edges where neighbouring triangles comprise triangles having different orientations. An edge map image for the subject object from the selected viewpoint then can be calculated. </paragraph>
<paragraph id="P-0079" lvl="0"><number>&lsqb;0079&rsqb;</number> Specifically, utilizing conventional Open GL calls depth of each edge can be determined. The edges which are visible from the viewpoint can then be determined by comparing these depths of edges with the frozen Z buffer values for the initial reliability image. An edge map image of visible edges can then be generated by rendering the edges determined to be visible using the frozen Z buffer values for the initial reliability image. In this embodiment, this image is generated by associating each of the pixels corresponding to an edge with a visibility value of zero and setting the remaining portions of an image to have a visibility value of 255. </paragraph>
<paragraph id="P-0080" lvl="0"><number>&lsqb;0080&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 7A</cross-reference> is an example of an edge map of visibility values generated for the same viewpoint and subject object as the reliability images of <cross-reference target="DRAWINGS">FIGS. 6A and 6B</cross-reference>. In <cross-reference target="DRAWINGS">FIG. 7</cross-reference>A, pixels corresponding to edges are shown rendered in black with the remainder of the image shown in white. </paragraph>
<paragraph id="P-0081" lvl="0"><number>&lsqb;0081&rsqb;</number> After an edge map image has been generated for a subject object from the selected view the weight determination module <highlight><bold>58</bold></highlight> then proceeds to generate (S<highlight><bold>5</bold></highlight>-<highlight><bold>4</bold></highlight>) a feathered edge map image used for ramping the weight values of the corresponding potential problem portions of a smoothed reliability image. </paragraph>
<paragraph id="P-0082" lvl="0"><number>&lsqb;0082&rsqb;</number> Initially, a feathering window size is selected. In this embodiment, this is selected to correspond to the number of pixels representing the size of the projection of a cube 1% of the length of diagonal of the bounding box placed at the origin in a similar way in to which the blur size used to generate the smoothed reliability image is selected. </paragraph>
<paragraph id="P-0083" lvl="0"><number>&lsqb;0083&rsqb;</number> A feathered edge map image is then generated from the edge map image by considering for each pixel firstly whether the pixel under consideration is already rendered black, i.e. is associated with a visibility value of 0. If this is the case the next pixel is then selected. If the pixel is not already rendered black, the pixels identified to above and below the current selected pixel to the extent of the selected feathering window size are identified and it is determined whether any of the identified pixels are associated with a visibility value other than 255. If this is the case the selected pixel is then assigned a new feathered value determined by the following equation: </paragraph>
<paragraph lvl="0"><in-line-formula>feathered value&equals;Min &lsqb;<highlight><italic>p</italic></highlight><highlight><subscript>i</subscript></highlight><highlight><italic>&plus;&sgr;di</italic></highlight><highlight><subscript>i</subscript></highlight>&rsqb;</in-line-formula></paragraph>
<paragraph id="P-0084" lvl="7"><number>&lsqb;0084&rsqb;</number> where p<highlight><subscript>i </subscript></highlight>is the value of a pixel in the window and d<highlight><subscript>i </subscript></highlight>is the number of pixels between the current pixel, the pixel i in the window and &sgr; is a scaling factor proportional to 255 divided by the window size. </paragraph>
<paragraph id="P-0085" lvl="0"><number>&lsqb;0085&rsqb;</number> Thus for example where a white pixel in an edge image is directly beneath a pixel having a value equal to 0 the selected pixel would be assigned a value of 0&plus;&sgr;&equals;&sgr;. </paragraph>
<paragraph id="P-0086" lvl="0"><number>&lsqb;0086&rsqb;</number> After all the pixels in an image have been considered for feathering using a feathering window aligned along columns in an image, the process is then repeated using feathering windows to calculate values from the feathered image where the feathering windows are aligned along the y axis in the image. </paragraph>
<paragraph id="P-0087" lvl="0"><number>&lsqb;0087&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 7B</cross-reference> is an exemplary illustration of the edge map image of <cross-reference target="DRAWINGS">FIG. 7A</cross-reference> after the edge map has been feathered. In <cross-reference target="DRAWINGS">FIG. 7</cross-reference>B, pixels associated with a value of 255 are shown in white, pixels associated with a value of 0 are shown in black and intermediate values are shown in grey. </paragraph>
<paragraph id="P-0088" lvl="0"><number>&lsqb;0088&rsqb;</number> As can be seen from <cross-reference target="DRAWINGS">FIG. 7B</cross-reference> the feathered edge map image comprises a set of pixels where values of zero are associated with edges and where pixels close to the edges of an object as seen from the viewpoint have intermediate values which gradually increase up to a distance of the selected feathering window from the edge. The remainder of the image, shown in white corresponds with values of 255 for the pixels. </paragraph>
<paragraph id="P-0089" lvl="0"><number>&lsqb;0089&rsqb;</number> After a feathered edge map image has been generated a final weight map image for the identified viewpoint is then generated (S<highlight><bold>5</bold></highlight>-<highlight><bold>5</bold></highlight>). This weight map image is generated by selecting for each pixel, the value corresponding to the lesser value of the corresponding pixel in the feathered edge map image and the smoothed reliability image. </paragraph>
<paragraph id="P-0090" lvl="0"><number>&lsqb;0090&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 8</cross-reference> is an exemplary illustration of a weight map image generated by combining the smoothed reliability image of <cross-reference target="DRAWINGS">FIG. 6B</cross-reference> with the feathered edge map image of <cross-reference target="DRAWINGS">FIG. 7B</cross-reference>. Again as in <cross-reference target="DRAWINGS">FIGS. 6A, 6B</cross-reference>, <highlight><bold>7</bold></highlight>A and <highlight><bold>7</bold></highlight>B, values corresponding to a value of zero are shown in black, values corresponding to a value of 255 are shown in white with intermediate values shown as intermediate shades of grey. </paragraph>
<paragraph id="P-0091" lvl="0"><number>&lsqb;0091&rsqb;</number> As a result of the smoothing of an initial reliability image, a smoothed reliability image is generated which result in portions of adjacent surface of a subject object corresponding to adjacent portions in an image from a selected viewpoint being associated with values which vary gradually over the surface of the object. The effect of combining this smoothed image with a feathered edge map is to ensure that visible portions of the surface of an object adjacent to portions that are not visible from a selected viewpoint are associated with smoothly varying values that fall to zero adjacent to the parts of the surface of an object that are not visible. </paragraph>
<paragraph id="P-0092" lvl="0"><number>&lsqb;0092&rsqb;</number> The function implicitly defined by the weight map image for visible portions of a subject object together with identifying the remaining portions of a subject object to zero is then made explicit by utilizing the weight map image to generate a texture map render function for the entire of the surface of a subject object as will now be described with reference to <cross-reference target="DRAWINGS">FIG. 9</cross-reference> which is a flow diagram of the process of generating a texture map render function. </paragraph>
<paragraph id="P-0093" lvl="0"><number>&lsqb;0093&rsqb;</number> Initially (S<highlight><bold>9</bold></highlight>-<highlight><bold>1</bold></highlight>) the weight determination module <highlight><bold>58</bold></highlight> selects a first triangle from the three dimensional model data generated by the surface modeller <highlight><bold>34</bold></highlight> for a subject object. </paragraph>
<paragraph id="P-0094" lvl="0"><number>&lsqb;0094&rsqb;</number> The weight determination module <highlight><bold>58</bold></highlight> then (S<highlight><bold>9</bold></highlight>-<highlight><bold>2</bold></highlight>) determines whether the selected triangle is either completely visible, partially visible or not visible from the currently selected viewpoint. </paragraph>
<paragraph id="P-0095" lvl="0"><number>&lsqb;0095&rsqb;</number> In this embodiment this is achieved by utilizing conventional Open GL calls to render the 3D model generated by the surface modeller <highlight><bold>34</bold></highlight> as seen from the perspective defined by the position data of the image being processed. The generated image data is then utilized to calculate a visibility score. </paragraph>
<paragraph id="P-0096" lvl="0"><number>&lsqb;0096&rsqb;</number> Specifically, initially all of the triangles of the 3D model viewed from the viewpoint defined by the selected position data are rendered using a single colour with Z buffering enabled. This Z buffer data is then equivalent to a depth map. The triangle being processed is then re-rendered in a different colour with the Z buffering disabled. The selected triangle is then once again re-rendered utilizing a third colour with the Z buffering re-enabled so as to utilize the depth values already present in the Z buffer. When re-rendering the glPolygonOffset OpenGL function call is then utilized to shift the triangle slightly towards the defined camera viewpoint defined by the position data to avoid aliasing effects. </paragraph>
<paragraph id="P-0097" lvl="0"><number>&lsqb;0097&rsqb;</number> A visibility score for the triangle being processed as viewed from the viewpoint defined by the currently selected position data is then determined by calculating the number of pixels in the image rendered in the second and third colours. </paragraph>
<paragraph id="P-0098" lvl="0"><number>&lsqb;0098&rsqb;</number> Specifically, if there are any pixels corresponding to the second colour in the generated image data, these are parts of the currently selected triangle that are occluded by other triangles and hence the selected triangle is partially hidden from the viewpoint defined by the position data. A visibility score for the triangle as perceived from the defined viewpoint is then set for the triangle where:  
<math-cwu id="MATH-US-00002">
<number>2</number>
<math>
<mrow>
  <mrow>
    <mi>visibility</mi>
    <mo>&it;</mo>
    <mstyle>
      <mtext>&emsp;</mtext>
    </mstyle>
    <mo>&it;</mo>
    <mi>score</mi>
  </mrow>
  <mo>=</mo>
  <mfrac>
    <mrow>
      <mi>no</mi>
      <mo>&it;</mo>
      <mstyle>
        <mtext>&emsp;</mtext>
      </mstyle>
      <mo>&it;</mo>
      <mi>pixels</mi>
      <mo>&it;</mo>
      <mstyle>
        <mtext>&emsp;</mtext>
      </mstyle>
      <mo>&it;</mo>
      <mi>in</mi>
      <mo>&it;</mo>
      <mstyle>
        <mtext>&emsp;</mtext>
      </mstyle>
      <mo>&it;</mo>
      <mn>3</mn>
      <mo>&it;</mo>
      <mi>rd</mi>
      <mo>&it;</mo>
      <mstyle>
        <mtext>&emsp;</mtext>
      </mstyle>
      <mo>&it;</mo>
      <mi>colour</mi>
    </mrow>
    <mrow>
      <mi>no</mi>
      <mo>&it;</mo>
      <mstyle>
        <mtext>&emsp;</mtext>
      </mstyle>
      <mo>&it;</mo>
      <mi>pixels</mi>
      <mo>&it;</mo>
      <mstyle>
        <mtext>&emsp;</mtext>
      </mstyle>
      <mo>&it;</mo>
      <mi>in</mi>
      <mo>&it;</mo>
      <mstyle>
        <mtext>&emsp;</mtext>
      </mstyle>
      <mo>&it;</mo>
      <mn>2</mn>
      <mo>&it;</mo>
      <mi>nd</mi>
      <mo>&it;</mo>
      <mstyle>
        <mtext>&emsp;</mtext>
      </mstyle>
      <mo>&it;</mo>
      <mi>or</mi>
      <mo>&it;</mo>
      <mstyle>
        <mtext>&emsp;</mtext>
      </mstyle>
      <mo>&it;</mo>
      <mn>3</mn>
      <mo>&it;</mo>
      <mi>rd</mi>
      <mo>&it;</mo>
      <mstyle>
        <mtext>&emsp;</mtext>
      </mstyle>
      <mo>&it;</mo>
      <mi>colour</mi>
    </mrow>
  </mfrac>
</mrow>
</math>
<mathematica-file id="MATHEMATICA-00002" file="US20030001837A1-20030102-M00002.NB"/>
<image id="EMI-M00002" wi="216.027" he="18.96615" file="US20030001837A1-20030102-M00002.TIF" imf="TIFF" ti="MF"/>
</math-cwu>
</paragraph>
<paragraph id="P-0099" lvl="0"><number>&lsqb;0099&rsqb;</number> Thus in this way, where the entirety of a triangle is visible within an image a visibility value of one is associated with the triangle for the selected viewpoint. Where a triangle is occluded by other triangles when viewed from a selected viewpoint by the position data being processed, a visibility score of less than one is associated with the triangle at image position. If a triangle is completely occluded by other triangles a visibility score of zero is associated with the triangle. </paragraph>
<paragraph id="P-0100" lvl="0"><number>&lsqb;0100&rsqb;</number> The processing to colour a triangle by rendering and re-rendering a triangle utilizing the stored values in the Z buffer as previously described enables triangles to be processed even if two small adjacent triangles are mapped to the same pixel in the selected viewpoint. Specifically, if triangle depths are similar for triangles corresponding to the same portion of an image, the glPolygonOffset call has the effect of rendering the second colour in the front of other triangles the same distance from the selected viewpoint. Thus when the visibility of the selected triangle is determined, the triangle is only given a lower visibility value if it is obscured by other triangles closer to the defined viewpoint and not merely obscured by the rendering of triangles at the same distance to the same portion of an image. </paragraph>
<paragraph id="P-0101" lvl="0"><number>&lsqb;0101&rsqb;</number> Where the selected triangles determined to be completely visible, ie. the visibility value for the triangle&equals;1, a texture map render function is generated (S<highlight><bold>9</bold></highlight>-<highlight><bold>3</bold></highlight>) by identifying the source for a texture render function from the projection of the corresponding triangle into the weight map image generated for that viewpoint. </paragraph>
<paragraph id="P-0102" lvl="0"><number>&lsqb;0102&rsqb;</number> Where a triangle is determined not to be visible in a selected image, a texture render function for rendering the entirety of the triangle with a value of zero is stored (S<highlight><bold>9</bold></highlight>-<highlight><bold>4</bold></highlight>). </paragraph>
<paragraph id="P-0103" lvl="0"><number>&lsqb;0103&rsqb;</number> Where a triangle is determined to be partially visible a binary mask is utilized to generate (S<highlight><bold>9</bold></highlight>-<highlight><bold>5</bold></highlight>) a texture map for the triangle. </paragraph>
<paragraph id="P-0104" lvl="0"><number>&lsqb;0104&rsqb;</number> Specifically, in this embodiment, a bounding box for the pixels rendered in the second or third colour when calculating visibility score for the triangle is first determined. Initial texture render map data for the triangle is then generated by copying the region of the generated weight map image within the boundary box. All values in the generated copy of the portion of the image within the boundary box which corresponds to a colour other than the third colour utilised when calculating the visibility value for the triangle are then set to zero. The texture map render function for the triangle is then defined by the values of pixels in this amended copy of a region of the weight map image with texture coordinates being the projection of the triangle under consideration projected into the amended copy of the region of the weight map image in the same way in which the triangle is projected into the corresponding region of the weight map image. </paragraph>
<paragraph id="P-0105" lvl="0"><number>&lsqb;0105&rsqb;</number> After a texture render function has been assigned to the triangle currently under consideration, the weight determination module <highlight><bold>58</bold></highlight> then (S<highlight><bold>9</bold></highlight>-<highlight><bold>6</bold></highlight>) determines whether the triangle for which a texture render function has, just been calculated corresponds to the final triangle in a model generated by the texture modeller <highlight><bold>34</bold></highlight>. If this is not the case the weight determination module <highlight><bold>58</bold></highlight> then (S<highlight><bold>9</bold></highlight>-<highlight><bold>7</bold></highlight>) proceeds to select the next triangle in the model before determining a texture render function for the newly selected triangle (S<highlight><bold>9</bold></highlight>-<highlight><bold>2</bold></highlight>-S<highlight><bold>9</bold></highlight>-<highlight><bold>5</bold></highlight>) and then once again (S<highlight><bold>9</bold></highlight>-<highlight><bold>6</bold></highlight>) determining whether a texture render function for all the triangles in the model have been created. </paragraph>
<paragraph id="P-0106" lvl="0"><number>&lsqb;0106&rsqb;</number> When texture render functions have been determined and stored for all of the triangles for one viewpoint, the weight determination module <highlight><bold>58</bold></highlight> then selects a new viewpoint for generating texture render functions. </paragraph>
<paragraph id="P-0107" lvl="0"><number>&lsqb;0107&rsqb;</number> Thus in this way the weight determination module <highlight><bold>58</bold></highlight> generates and stores texture render functions for the entirety of the surface of a subject object for each of the viewpoints corresponding to viewpoints of input image data in the input data store <highlight><bold>26</bold></highlight>. These texture render functions are then output as weight function data by the weight determination module <highlight><bold>58</bold></highlight> to the texture map generation module <highlight><bold>59</bold></highlight>. </paragraph>
<paragraph id="P-0108" lvl="7"><number>&lsqb;0108&rsqb;</number> Generation of Texture Maps </paragraph>
<paragraph id="P-0109" lvl="0"><number>&lsqb;0109&rsqb;</number> Utilizing conventional texture rendering techniques, it is possible to generate image data corresponding to projected images as perceived from each of the canonical views <highlight><bold>50</bold></highlight>-<highlight><bold>55</bold></highlight> where the surface of a model of a subject object is texture rendered utilizing input image data identified as having been recorded from a camera viewpoint corresponding to position data output by the position determination module <highlight><bold>32</bold></highlight> for that image data. </paragraph>
<paragraph id="P-0110" lvl="0"><number>&lsqb;0110&rsqb;</number> It is also possible utilizing conventional techniques to generate projected images corresponding to the projection of the surface of a model texture rendered in accordance with calculated texture render functions for the surface of the model as viewed from each of the canonical views <highlight><bold>50</bold></highlight>-<highlight><bold>55</bold></highlight> from the output weight function data. </paragraph>
<paragraph id="P-0111" lvl="0"><number>&lsqb;0111&rsqb;</number> As described above, the weight function data generated and output by the weight determination module <highlight><bold>58</bold></highlight> is calculated so as to be representative of the relative visibility of portions of the surface of a model from defined viewpoints. The projected images of the weight functions are therefore indicative of relative preferences for using the corresponding portions of projections of image data from the corresponding viewpoints to generating the texture data for the texture map for each of the canonical views. These projected weight function images, hereinafter referred to as canonical confidence images can therefore be utilized to select portions of projected image data to blend to generate output texture maps for canonical views <highlight><bold>50</bold></highlight>-<highlight><bold>55</bold></highlight>. </paragraph>
<paragraph id="P-0112" lvl="0"><number>&lsqb;0112&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 10</cross-reference> is a schematic diagram of notional functional modules of the texture map determination module <highlight><bold>59</bold></highlight> for generating canonical texture maps for each of the canonical views <highlight><bold>50</bold></highlight>-<highlight><bold>55</bold></highlight> from image data stored within the input data store <highlight><bold>26</bold></highlight>, position data output by the position determination module <highlight><bold>32</bold></highlight>, 3D model data generated by the surface modeller <highlight><bold>34</bold></highlight> and the weight function data output by the weight determination module <highlight><bold>58</bold></highlight>. </paragraph>
<paragraph id="P-0113" lvl="0"><number>&lsqb;0113&rsqb;</number> The applicants have appreciated that generation of realistic texture for a model of a subject object can be achieved by blending images in different ways to average global lighting effects whilst maintaining within the images high frequency details such as highlights and shadows. </paragraph>
<paragraph id="P-0114" lvl="0"><number>&lsqb;0114&rsqb;</number> Thus in accordance with this embodiment of the present invention the texture generation module <highlight><bold>59</bold></highlight> comprises a low frequency image generation module <highlight><bold>60</bold></highlight> for extracting low frequency image information from image data stored within the input data store <highlight><bold>26</bold></highlight>, an image projection module <highlight><bold>62</bold></highlight> for generating high and low frequency canonical image projections; a confidence image generation module <highlight><bold>64</bold></highlight> for generating canonical confidence images; a weighted average filter <highlight><bold>66</bold></highlight> and a blending module <highlight><bold>68</bold></highlight> for processing high and low frequency canonical projections and the canonical confidence images to generate blended high and low frequency canonical images; and a re-combination of output module <highlight><bold>70</bold></highlight> for combining the high and low frequency images and outputting the combined images as texture maps for each of the canonical views <highlight><bold>50</bold></highlight>-<highlight><bold>55</bold></highlight>. </paragraph>
<paragraph id="P-0115" lvl="0"><number>&lsqb;0115&rsqb;</number> In order to generate high and low frequency canonical projections of each of the input images stored within the input data store <highlight><bold>26</bold></highlight> projected into each of the six canonical views, initially each item of image data in the input data store <highlight><bold>26</bold></highlight> is passed to the low frequency image generation module <highlight><bold>60</bold></highlight>. </paragraph>
<paragraph id="P-0116" lvl="0"><number>&lsqb;0116&rsqb;</number> This module <highlight><bold>60</bold></highlight> then generates a set of low frequency images by processing the image data for each of the views in a conventional way by blurring and sub-sampling each image. In this embodiment the blurring operation is achieved by performing a Gausian blur operation in which the blur radius is selected to be the size of the projection of a cube placed at the centre of the object bounding box defined by the six canonical views <highlight><bold>50</bold></highlight>-<highlight><bold>55</bold></highlight> and whose sides are 5% of the length of the diagonal of the bounding box. The selection of the blur radius in this way ensures that the radius is independent of image resolution and varies depending upon whether the image is a close up of a subject object (large radius) or the subject appears small in the image (small radius). </paragraph>
<paragraph id="P-0117" lvl="0"><number>&lsqb;0117&rsqb;</number> These low frequency images are then passed to the image projection module <highlight><bold>62</bold></highlight> together with copies of the original image data stored within the input data store <highlight><bold>26</bold></highlight>, position data for each of the images as determined by the position determination module <highlight><bold>32</bold></highlight> and 3D model data for the model generated by the surface modeller <highlight><bold>34</bold></highlight>. </paragraph>
<paragraph id="P-0118" lvl="0"><number>&lsqb;0118&rsqb;</number> For each of the input images from the input data store <highlight><bold>26</bold></highlight> the image projection module <highlight><bold>62</bold></highlight> then utilizes the position data associated with the image and the 3D model data output by the surface modeller <highlight><bold>34</bold></highlight> to determine calculated projections of the input images as perceived from the six canonical views <highlight><bold>50</bold></highlight>-<highlight><bold>55</bold></highlight> utilizing standard texture rendering techniques. </paragraph>
<paragraph id="P-0119" lvl="0"><number>&lsqb;0119&rsqb;</number> In the same way the image projection module <highlight><bold>62</bold></highlight> generates for each of low frequency images corresponding to image data processed by the low frequency image generation module <highlight><bold>60</bold></highlight>, six low frequency canonical projections for the six canonical views <highlight><bold>50</bold></highlight>-<highlight><bold>55</bold></highlight>. </paragraph>
<paragraph id="P-0120" lvl="0"><number>&lsqb;0120&rsqb;</number> Six high frequency canonical image projections for each image are then determined by the image projection module <highlight><bold>62</bold></highlight> by performing a difference operation, subtracting the low frequency canonical image projections for an image as viewed from a specified canonical view from the corresponding image projection of the raw image data for that view. </paragraph>
<paragraph id="P-0121" lvl="0"><number>&lsqb;0121&rsqb;</number> The low frequency canonical projections and high frequency canonical projections of each image for each of the six canonical views are then passed to the weighted average filter <highlight><bold>66</bold></highlight> and the blending module <highlight><bold>68</bold></highlight> for processing as will be detailed later. </paragraph>
<paragraph id="P-0122" lvl="0"><number>&lsqb;0122&rsqb;</number> By processing the image data from the input data store <highlight><bold>26</bold></highlight> in this way it is possible to ensure that the processing of images generates high and low frequency canonical projections of each image that are consistent with one another. In contrast, if a blurring operation is performed upon canonical image projections created from image data stored within the input data store <highlight><bold>26</bold></highlight>, as the pixel data in these generated projections is normally dependent upon different regions of the original image data, the blurring operation will not be consistent across all six canonical images and hence will introduce errors into the generated texture maps. </paragraph>
<paragraph id="P-0123" lvl="0"><number>&lsqb;0123&rsqb;</number> The confidence image generation module <highlight><bold>64</bold></highlight> in this embodiment is arranged to receive 3D model data from the surface modeller <highlight><bold>34</bold></highlight> and weight function data from the weight function determination module <highlight><bold>58</bold></highlight>. The confidence image generation module <highlight><bold>64</bold></highlight> then processes the 3D model data and weight function data to generate for each of the viewpoints corresponding to the viewpoints of each input images in the input data store <highlight><bold>26</bold></highlight>, a set of six canonical confidence images for the six canonical views <highlight><bold>50</bold></highlight>-<highlight><bold>55</bold></highlight>. This is achieved by the confidence image generation module <highlight><bold>64</bold></highlight> utilizing the texture render functions for each viewpoint to calculate a set of canonical confidence images for that viewpoint, using conventional texture rendering techniques. </paragraph>
<paragraph id="P-0124" lvl="0"><number>&lsqb;0124&rsqb;</number> Thus in this way for each of the six canonical views, a canonical confidence image for each viewpoint corresponding to a viewpoint from which image data of a subject object is available is generated. The pixel values of canonical confidence images generated in this way are each representative of the relative preference for utilizing corresponding portions of canonical projections of image data representative of the viewpoint for which the weight function was generated to generate texture map data. These canonical confidence images are then passed to the weighted average filter <highlight><bold>66</bold></highlight> and the blending module <highlight><bold>68</bold></highlight>, so that texture data for the six canonical views can be generated utilizing the data identifying preferred sources for generating portions of texture map data. </paragraph>
<paragraph id="P-0125" lvl="0"><number>&lsqb;0125&rsqb;</number> Thus after the processing of image data and weight function data by the low frequency image generation module <highlight><bold>60</bold></highlight>, the image projection module <highlight><bold>62</bold></highlight> and the confidence image generation module <highlight><bold>64</bold></highlight>, the weighted average filter <highlight><bold>66</bold></highlight> and blending module <highlight><bold>68</bold></highlight> receive, for each item of image data stored within the input data store <highlight><bold>26</bold></highlight> a set of six canonical confidence images identifying the extent to which portions of projected images derived from a specified image are to be preferred to generate texture data and a set of six projections of either the high or low frequency images corresponding to the item of image data. The weighted average filter <highlight><bold>66</bold></highlight> and the blending module <highlight><bold>68</bold></highlight> then proceed to process the confidence images and associated low and high frequency canonical projections in turn for each of the canonical views as will now be described in detail. </paragraph>
<paragraph id="P-0126" lvl="7"><number>&lsqb;0126&rsqb;</number> Processing of Low Frequency Canonical Projections </paragraph>
<paragraph id="P-0127" lvl="0"><number>&lsqb;0127&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 11</cross-reference> is a flow diagram of the processing of low frequency canonical projections and associated canonical confidence images for a specified canonical view for each of a set of input images stored within the input data store <highlight><bold>26</bold></highlight>. </paragraph>
<paragraph id="P-0128" lvl="0"><number>&lsqb;0128&rsqb;</number> Initially (S<highlight><bold>11</bold></highlight>-<highlight><bold>1</bold></highlight>) the weighted average filter <highlight><bold>66</bold></highlight> selects a first low frequency canonical projection and its associated confidence image. This is then made the basis of an initial low frequency canonical image to be generated by the weighted average filter <highlight><bold>66</bold></highlight>. The weighted average filter <highlight><bold>66</bold></highlight> then selects (S<highlight><bold>11</bold></highlight>-<highlight><bold>2</bold></highlight>) the next low frequency projection for the same canonical view together with the projection&apos;s associated confidence image. </paragraph>
<paragraph id="P-0129" lvl="0"><number>&lsqb;0129&rsqb;</number> The weighted average filter <highlight><bold>66</bold></highlight> then (S<highlight><bold>11</bold></highlight>-<highlight><bold>3</bold></highlight>) determines as a new low frequency canonical image a canonical image comprising for each pixel in the image a weighted average of the current low frequency canonical image and the selected low frequency canonical projection weighted by the confidence scores utilising the following formula:  
<math-cwu id="MATH-US-00003">
<number>3</number>
<math>
<mrow>
  <mrow>
    <mi>pixel</mi>
    <mo>&it;</mo>
    <mstyle>
      <mtext>&emsp;</mtext>
    </mstyle>
    <mo>&it;</mo>
    <mi>value</mi>
  </mrow>
  <mo>=</mo>
  <mrow>
    <mrow>
      <mfrac>
        <msub>
          <mi>C</mi>
          <mi>i</mi>
        </msub>
        <mrow>
          <mi>C</mi>
          <mo>+</mo>
          <msub>
            <mi>C</mi>
            <mi>i</mi>
          </msub>
        </mrow>
      </mfrac>
      <mo>*</mo>
      <mrow>
        <mo>[</mo>
        <mtable>
          <mtr>
            <mtd>
              <mrow>
                <mi>Pixel</mi>
                <mo>&it;</mo>
                <mstyle>
                  <mtext>&emsp;</mtext>
                </mstyle>
                <mo>&it;</mo>
                <mi>Value</mi>
                <mo>&it;</mo>
                <mstyle>
                  <mtext>&emsp;</mtext>
                </mstyle>
                <mo>&it;</mo>
                <mi>in</mi>
              </mrow>
            </mtd>
          </mtr>
          <mtr>
            <mtd>
              <mrow>
                <mi>selected</mi>
                <mo>&it;</mo>
                <mstyle>
                  <mtext>&emsp;</mtext>
                </mstyle>
                <mo>&it;</mo>
                <mi>projection</mi>
              </mrow>
            </mtd>
          </mtr>
        </mtable>
        <mo>]</mo>
      </mrow>
    </mrow>
    <mo>+</mo>
    <mrow>
      <mfrac>
        <mi>C</mi>
        <mrow>
          <mi>C</mi>
          <mo>+</mo>
          <msub>
            <mi>C</mi>
            <mi>i</mi>
          </msub>
        </mrow>
      </mfrac>
      <mo>*</mo>
      <mrow>
        <mo>[</mo>
        <mtable>
          <mtr>
            <mtd>
              <mrow>
                <mi>Pixel</mi>
                <mo>&it;</mo>
                <mstyle>
                  <mtext>&emsp;</mtext>
                </mstyle>
                <mo>&it;</mo>
                <mi>Value</mi>
                <mo>&it;</mo>
                <mstyle>
                  <mtext>&emsp;</mtext>
                </mstyle>
                <mo>&it;</mo>
                <mi>in</mi>
              </mrow>
            </mtd>
          </mtr>
          <mtr>
            <mtd>
              <mrow>
                <mi>canonical</mi>
                <mo>&it;</mo>
                <mstyle>
                  <mtext>&emsp;</mtext>
                </mstyle>
                <mo>&it;</mo>
                <mi>image</mi>
              </mrow>
            </mtd>
          </mtr>
        </mtable>
        <mo>]</mo>
      </mrow>
    </mrow>
  </mrow>
</mrow>
</math>
<mathematica-file id="MATHEMATICA-00003" file="US20030001837A1-20030102-M00003.NB"/>
<image id="EMI-M00003" wi="216.027" he="21.12075" file="US20030001837A1-20030102-M00003.TIF" imf="TIFF" ti="MF"/>
</math-cwu>
</paragraph>
<paragraph id="P-0130" lvl="7"><number>&lsqb;0130&rsqb;</number> where C is the current confidence score associated with the pixel being processed for the canonical image and C<highlight><subscript>i </subscript></highlight>is the confidence score associated with the pixel in the confidence image associated with the selected projection. </paragraph>
<paragraph id="P-0131" lvl="0"><number>&lsqb;0131&rsqb;</number> The confidence score for the pixel in the canonical image is then updated by adding the confidence score for the latest projection to be processed to the current confidence score. That is to say the new confidence score C<highlight><subscript>new </subscript></highlight>is calculated by the equation </paragraph>
<paragraph lvl="0"><in-line-formula><highlight><italic>C</italic></highlight><highlight><subscript>new</subscript></highlight><highlight><italic>&equals;C</italic></highlight><highlight><subscript>old</subscript></highlight><highlight><italic>&plus;C</italic></highlight><highlight><subscript>i </subscript></highlight></in-line-formula></paragraph>
<paragraph id="P-0132" lvl="7"><number>&lsqb;0132&rsqb;</number> where C<highlight><subscript>old </subscript></highlight>is the previous confidence score associated with the current pixel and C<highlight><subscript>i </subscript></highlight>is the confidence score of the current pixel in the projection being processed. </paragraph>
<paragraph id="P-0133" lvl="0"><number>&lsqb;0133&rsqb;</number> The weighted average filter <highlight><bold>66</bold></highlight> then (S<highlight><bold>11</bold></highlight>-<highlight><bold>4</bold></highlight>) determines whether the latest selected low frequency canonical projection is the last of the low frequency canonical projections for the canonical view currently being calculated. If this is not the case the weighted average filter <highlight><bold>66</bold></highlight> then proceeds to utilize the determined combined image to generate a new combined image utilizing the next confidence image and associated low frequency image projection (S<highlight><bold>11</bold></highlight>-<highlight><bold>2</bold></highlight>-S<highlight><bold>11</bold></highlight>-<highlight><bold>4</bold></highlight>). </paragraph>
<paragraph id="P-0134" lvl="0"><number>&lsqb;0134&rsqb;</number> When the weighted average filter <highlight><bold>66</bold></highlight> determines (S<highlight><bold>11</bold></highlight>-<highlight><bold>4</bold></highlight>) that the last of the low frequency canonical projections for a specified canonical view <highlight><bold>50</bold></highlight>-<highlight><bold>55</bold></highlight> has been processed the weighted average filter <highlight><bold>66</bold></highlight> outputs (S<highlight><bold>11</bold></highlight>-<highlight><bold>5</bold></highlight>) as a blended canonical low frequency image the image generated utilizing the weighted average of the last projected image processed by the average weighted filter <highlight><bold>66</bold></highlight>. </paragraph>
<paragraph id="P-0135" lvl="0"><number>&lsqb;0135&rsqb;</number> Thus in this way the weighted average filter <highlight><bold>66</bold></highlight> enables for each of the canonical views <highlight><bold>50</bold></highlight>-<highlight><bold>55</bold></highlight> a low frequency image to be created combining each of the low frequency canonical projections weighted by confidence scores indicative of the portions of the images identified as being most representative of the surface texture of a model in the canonical confidence images. As the low frequency images are representative of average local colour of surfaces as effected by global lighting effects, this processing by the weighted average filter <highlight><bold>66</bold></highlight> enables canonical low frequency images to be generated in which these global lighting effects are averaged across the best available images where greater weight is placed on images in which portions of a model are most easily viewed giving the resultant canonical low frequency images a realistic appearance and a neutral tone. </paragraph>
<paragraph id="P-0136" lvl="7"><number>&lsqb;0136&rsqb;</number> Processing of High Frequency Canonical Projections </paragraph>
<paragraph id="P-0137" lvl="0"><number>&lsqb;0137&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 12</cross-reference> is a flow diagram of the processing of canonical confidence images and associated high frequency canonical projections for generating a canonical high frequency image for one of the canonical views <highlight><bold>50</bold></highlight>-<highlight><bold>55</bold></highlight>. This processing is repeated for each of the canonical views <highlight><bold>50</bold></highlight>-<highlight><bold>55</bold></highlight>. </paragraph>
<paragraph id="P-0138" lvl="0"><number>&lsqb;0138&rsqb;</number> The processing of the blending module <highlight><bold>68</bold></highlight> for generating high frequency canonical images is identical to that of the processing of the weighted average filter <highlight><bold>66</bold></highlight> except instead of determining weighted averages of pixels in the canonical projections and canonical images, in this embodiment the blending module <highlight><bold>68</bold></highlight> selects as canonical image data pixel data, pixel data associated with the greatest confidence scores. </paragraph>
<paragraph id="P-0139" lvl="0"><number>&lsqb;0139&rsqb;</number> Specifically, after selecting an initial high frequency projection and associated confidence image to be the initial high frequency canonical image and associated confidence image (S<highlight><bold>12</bold></highlight>-<highlight><bold>1</bold></highlight>) and selecting the next projection and confidence image for processing (S<highlight><bold>12</bold></highlight>-<highlight><bold>2</bold></highlight>), the blending module <highlight><bold>68</bold></highlight> then updates the pixels in the current high frequency canonical image (S<highlight><bold>12</bold></highlight>-<highlight><bold>3</bold></highlight>). </paragraph>
<paragraph id="P-0140" lvl="0"><number>&lsqb;0140&rsqb;</number> This is achieved by the blending module <highlight><bold>68</bold></highlight> selecting for each pixel in the image either pixel data for the corresponding pixel in the high frequency canonical projection being processed or the pixel data for the current high frequency canonical image. The selection is made for each pixel by determining whether the corresponding pixel in the confidence image associated with the high frequency projection being processed has a greater confidence score than the confidence score associated with the pixel in the current canonical image. The pixel data associated with the highest score is then utilised as pixel data for the canonical image. The confidence score associated with the pixel in the updated canonical image is then set to the greater of the confidence scores for the pixel as identified by either the confidence image associated with the canonical image or the projection currently be processed. </paragraph>
<paragraph id="P-0141" lvl="0"><number>&lsqb;0141&rsqb;</number> The blending module <highlight><bold>68</bold></highlight> then determines (S<highlight><bold>12</bold></highlight>-<highlight><bold>4</bold></highlight>) whether all of the high frequency image projections for a canonical user have been processed. If this is not the case then the next projection and confidence image are utilised to update the high frequency canonical image being generated (S<highlight><bold>12</bold></highlight>-<highlight><bold>2</bold></highlight>-S<highlight><bold>12</bold></highlight>-<highlight><bold>4</bold></highlight>). When all of the canonical projections have been processed the final canonical image is then output (S<highlight><bold>12</bold></highlight>-<highlight><bold>5</bold></highlight>). </paragraph>
<paragraph id="P-0142" lvl="0"><number>&lsqb;0142&rsqb;</number> Thus in this way a high frequency canonical image is generated utilising all of the available high frequency image projections where each pixel of the high frequency canonical image corresponds to pixel data from the available high frequency image projections associated with the greatest of the confidence scores for that pixel As the confidence scores are indicative of the relative goodness for selecting pixels for generating image data, the high frequency canonical images therefore correspond to a patchwork of the best available pixel data for each part of the high frequency canonical images </paragraph>
<paragraph id="P-0143" lvl="0"><number>&lsqb;0143&rsqb;</number> Thus in this way six low frequency canonical images for the six canonical views <highlight><bold>50</bold></highlight>-<highlight><bold>55</bold></highlight> and six high frequency canonical images are generated by the weighted average filter <highlight><bold>66</bold></highlight> and the blending module <highlight><bold>68</bold></highlight> respectively. In the canonical low frequency images the global lighting effects are averaged across the available low frequency canonical projections in proportion to the relative preference for utilizing portions of an image as identified by the canonical confidence images associated with the projections. In the high frequency canonical images, high frequency canonical projections are selected in the above described manner to ensure that high frequency detail and contrast in the canonical images is maintained when the different high frequency canonical projections are combined. </paragraph>
<paragraph id="P-0144" lvl="0"><number>&lsqb;0144&rsqb;</number> When the six canonical low frequency images and six canonical high frequency images are then received by the recombination output module <highlight><bold>70</bold></highlight> for each canonical view <highlight><bold>50</bold></highlight>-<highlight><bold>55</bold></highlight> the recombination and output module <highlight><bold>70</bold></highlight> generates a single canonical texture map by adding the high and low frequency images for each view. The combined canonical texture maps are then output by the recombination and output module <highlight><bold>70</bold></highlight> for storage in the output data store <highlight><bold>44</bold></highlight> together with the 3D model data generated by the surface module <highlight><bold>34</bold></highlight>. </paragraph>
<paragraph id="P-0145" lvl="0"><number>&lsqb;0145&rsqb;</number> The output canonical texture maps can then be utilized to texture render image representations of the model generated by the surface modeller <highlight><bold>34</bold></highlight> by associating each of the triangles identified by the three-dimensional model data with one of the texture maps. In this embodiment the selection of texture data to texture render each triangle is determined by selecting as the map to be utilized the map in which the triangle has the greatest visibility. In this embodiment, the visibility of a triangle in an image is determined utilizing a visibility score calculated in the same way as has been described above for the calculation of visibility scores for generating texture map render functions. </paragraph>
<paragraph id="P-0146" lvl="0"><number>&lsqb;0146&rsqb;</number> By calculating the visibility in the manner described above, a value indicative of the extent to which a triangle in a model is or is not occluded from the selected viewpoint is determined. Other factors, however, also effect the extent to which use of particular portion image data for generating texture data may be preferred. For example, close up images are liable to contain more information about the texture of an object and therefore may be preferable sources for generating texture data. A further factor which may determine whether a particular image is preferable to use for generating texture data is the extent to which a portion of the image is viewed at oblique angle. Thus in this embodiment the visibility for a triangle in a canonical view is determined by utilizing the following equation:  
<math-cwu id="MATH-US-00004">
<number>4</number>
<math>
<mrow>
  <mi>visibility</mi>
  <mo>=</mo>
  <mfrac>
    <mrow>
      <mi>visibility</mi>
      <mo>&it;</mo>
      <mstyle>
        <mtext>&emsp;</mtext>
      </mstyle>
      <mo>&it;</mo>
      <mi>score</mi>
      <mo>*</mo>
      <mi>cos</mi>
      <mo>&it;</mo>
      <mstyle>
        <mtext>&emsp;</mtext>
      </mstyle>
      <mo>&it;</mo>
      <mi>&theta;</mi>
    </mrow>
    <mi>distance</mi>
  </mfrac>
</mrow>
</math>
<mathematica-file id="MATHEMATICA-00004" file="US20030001837A1-20030102-M00004.NB"/>
<image id="EMI-M00004" wi="216.027" he="17.03835" file="US20030001837A1-20030102-M00004.TIF" imf="TIFF" ti="MF"/>
</math-cwu>
</paragraph>
<paragraph id="P-0147" lvl="7"><number>&lsqb;0147&rsqb;</number> where &thgr; is the angle of incidence of a ray from the optical centre of the camera defining the image plane of a canonical being processed to the normal of the centroid of the selected triangle and the distance is the distance between the centroid of selected triangle and the optical centre of the camera. </paragraph>
<paragraph id="P-0148" lvl="0"><number>&lsqb;0148&rsqb;</number> Thus in this way in this embodiment the amount of occlusion, the obliqueness of view and the distance between image plane and a triangle being modelled all effect the visibility associated with a triangle. In alternative embodiments either only some of these factors could be utilized or alternatively greater weight could be placed on any particular factor. </paragraph>
<paragraph id="P-0149" lvl="0"><number>&lsqb;0149&rsqb;</number> When visibility has been calculated for a triangle for each of the canonical views, the view in which a triangle is most visible is selected and texture co-ordinates for texture rendering the model generated by the surface modeller <highlight><bold>34</bold></highlight> are then implicitly defined by the projection of the triangle onto the selected canonical view <highlight><bold>50</bold></highlight>-<highlight><bold>55</bold></highlight>. </paragraph>
<paragraph id="P-0150" lvl="0"><number>&lsqb;0150&rsqb;</number> Images of the model of the subject object can then be obtained for any viewpoint utilizing the output canonical texture maps and the 3D model data stored in the output data store <highlight><bold>44</bold></highlight>, for display on the display device <highlight><bold>4</bold></highlight> and a hard copies of the generated images can be made utilizing the printer. The model data and texture map stored in the output data store <highlight><bold>44</bold></highlight> can also be output as data onto a storage device <highlight><bold>46</bold></highlight>,<highlight><bold>48</bold></highlight>. </paragraph>
<paragraph id="P-0151" lvl="7"><number>&lsqb;0151&rsqb;</number> Second Embodiment </paragraph>
<paragraph id="P-0152" lvl="0"><number>&lsqb;0152&rsqb;</number> In the first embodiment of the present invention, an image processing apparatus was described in which input data defining a plurality of images of a subject object recorded from different viewpoints were processed to generate texture data comprising six canonical texture maps being views for a cuboid bounding a model of the subject object. Although generating texture maps in this way ensures that the total number of required view is limited to six, this does not guarantee that every triangle within the model is visible in at least one of the canonical views. As a result, some portions of a model may be texture rendered utilizing surface information corresponding to a different portion of the surface of the model. In this embodiment of the present invention an image processing apparatus is described in which texture data is generated to ensure that all triangles within a 3D model are rendered utilizing texture data for the corresponding portion of the surface of a subject object, if image data representative of the corresponding portion of the subject object is available. </paragraph>
<paragraph id="P-0153" lvl="0"><number>&lsqb;0153&rsqb;</number> The image processing apparatus in accordance with this embodiment of the present invention is identical to that of the previous embodiment except the surface texturer <highlight><bold>36</bold></highlight> is replaced with a modified surface texturer <highlight><bold>80</bold></highlight>. </paragraph>
<paragraph id="P-0154" lvl="0"><number>&lsqb;0154&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 13</cross-reference> is a schematic block diagram of the notional functional modules of the modified surface texturer module <highlight><bold>80</bold></highlight>. The modified surface texturer <highlight><bold>80</bold></highlight> comprises a canonical view determination module <highlight><bold>82</bold></highlight>; a weight determination module <highlight><bold>58</bold></highlight> identical to the weight determination module <highlight><bold>58</bold></highlight> of the surface texturer <highlight><bold>36</bold></highlight> of the previous embodiment and a modified texture map determination module <highlight><bold>86</bold></highlight>. </paragraph>
<paragraph id="P-0155" lvl="0"><number>&lsqb;0155&rsqb;</number> In this embodiment the canonical view determination module <highlight><bold>82</bold></highlight> is arranged to determine from the 3D model data output by the surface modeller <highlight><bold>34</bold></highlight> whether any portions of the surface of the 3D model are not substantially visible from the six initial canonical views. If this is determined to be the case the canonical view determination module <highlight><bold>82</bold></highlight> proceeds to define further sets of canonical views for storing texture data for these triangles. These view definitions are then passed to the texture map determination module <highlight><bold>86</bold></highlight>. The modified texture map determination module <highlight><bold>86</bold></highlight> then generates canonical texture map data for the initial canonical views and these additional canonical views which are then utilized to texture render the surface of the model generated by the surface modeller <highlight><bold>34</bold></highlight>. </paragraph>
<paragraph id="P-0156" lvl="0"><number>&lsqb;0156&rsqb;</number> The processing of the canonical view determination module <highlight><bold>82</bold></highlight> in accordance with this embodiment of the present invention will now be described with reference to <cross-reference target="DRAWINGS">FIG. 14</cross-reference> which is a flow diagram of the processing of the canonical view determination module <highlight><bold>82</bold></highlight>. </paragraph>
<paragraph id="P-0157" lvl="0"><number>&lsqb;0157&rsqb;</number> Initially (S<highlight><bold>14</bold></highlight>-<highlight><bold>1</bold></highlight>) the canonical view determination module <highlight><bold>82</bold></highlight> selects a first triangle of the model generated by the surface modeller <highlight><bold>34</bold></highlight>. The canonical view determination module <highlight><bold>82</bold></highlight> then (S<highlight><bold>14</bold></highlight>-<highlight><bold>2</bold></highlight>) generates a visibility score for the triangle as perceived from each of the six canonical views in a similar manner to which a visibility score is generated as has previously been described in relation to the previous embodiment. </paragraph>
<paragraph id="P-0158" lvl="0"><number>&lsqb;0158&rsqb;</number> The canonical view determination module <highlight><bold>82</bold></highlight> then (S<highlight><bold>14</bold></highlight>-<highlight><bold>3</bold></highlight>) determines whether the visibility score associated with the triangle being processed is greater than a threshold value indicating that the triangle is at least substantially visible in at least one of the views. In this embodiment this threshold value is set to 0.75 so that all triangles which are at least 75% visible in one of the views are identified as being substantially visible. </paragraph>
<paragraph id="P-0159" lvl="0"><number>&lsqb;0159&rsqb;</number> If the canonical view determination module <highlight><bold>82</bold></highlight> determines that the triangle being processed is substantially visible i.e. has a visibility score of greater than 0.75 in at least one view, the canonical view determination module selects from the available canonical views the view associated with the greatest visibility as determined in the manner previously described. Data identifying the canonical view in which the selected triangle is most visible is then stored. Thus in this way data identifying the canonical view in which texture data for rendering the triangle is subsequently to be utilized is generated. </paragraph>
<paragraph id="P-0160" lvl="0"><number>&lsqb;0160&rsqb;</number> The canonical view determination module <highlight><bold>82</bold></highlight> then (S<highlight><bold>14</bold></highlight>-<highlight><bold>5</bold></highlight>) determines whether the currently selected triangle corresponds to the last triangle of the 3D model generated by the surface modeller <highlight><bold>34</bold></highlight>. If this is not the case the canonical view determination module <highlight><bold>82</bold></highlight> then proceeds to select the next triangle and determine visibility scores for that next triangle (S<highlight><bold>14</bold></highlight>-<highlight><bold>6</bold></highlight>-S<highlight><bold>14</bold></highlight>-<highlight><bold>5</bold></highlight>). </paragraph>
<paragraph id="P-0161" lvl="0"><number>&lsqb;0161&rsqb;</number> When the canonical view determination module <highlight><bold>82</bold></highlight> determines that all of the triangles have been processed (S<highlight><bold>14</bold></highlight>-<highlight><bold>5</bold></highlight>) the canonical view determination module <highlight><bold>82</bold></highlight> then (S<highlight><bold>14</bold></highlight>-<highlight><bold>7</bold></highlight>) determines whether data identifying a best view has been stored for all of the triangles. </paragraph>
<paragraph id="P-0162" lvl="0"><number>&lsqb;0162&rsqb;</number> If this is not the case the canonical view determination module <highlight><bold>82</bold></highlight> generates and stores a list of triangles for which no best view data has been stored (S<highlight><bold>14</bold></highlight>-<highlight><bold>8</bold></highlight>) the canonical view determination module <highlight><bold>82</bold></highlight> then proceeds to establish visibility scores of these remaining triangles in the six canonical views in the absence of the triangles for which best view data has already been stored (S<highlight><bold>14</bold></highlight>-<highlight><bold>1</bold></highlight>-S<highlight><bold>14</bold></highlight>-<highlight><bold>7</bold></highlight>) and establish best views for substantially visible triangles. This process is then repeated until best view data is stored for all of the triangles. </paragraph>
<paragraph id="P-0163" lvl="0"><number>&lsqb;0163&rsqb;</number> When the canonical view determination module <highlight><bold>82</bold></highlight> determines (S<highlight><bold>14</bold></highlight>-<highlight><bold>7</bold></highlight>) that best view data has been stored for all the triangles the canonical view determination module <highlight><bold>82</bold></highlight> then outputs (S<highlight><bold>14</bold></highlight>-<highlight><bold>9</bold></highlight>) view definition data comprising for each triangle the canonical view in which it is most visible together with the generated lists of the triangles not visible canonical views. These lists and identified views in which the triangles are best represented are then output and passed to the modified texture map determination module <highlight><bold>86</bold></highlight>. </paragraph>
<paragraph id="P-0164" lvl="0"><number>&lsqb;0164&rsqb;</number> When the modified texture map determination module <highlight><bold>86</bold></highlight> processes the 3D model data, weight function data, image data and position data this generates the six original canonical views in the same manner as has been described in relation to the first embodiment. </paragraph>
<paragraph id="P-0165" lvl="0"><number>&lsqb;0165&rsqb;</number> However, additionally the texture map determination module <highlight><bold>86</bold></highlight> generates for the triangles not visible in the original six canonical views as identified by the lists output by the canonical view determination module <highlight><bold>82</bold></highlight> further texture maps corresponding to the projection of the triangles identified by the lists of triangles generated by the canonical view determination module <highlight><bold>82</bold></highlight> in the six canonical views in the absence of the other triangles. These additional canonical views are generated from the high and low frequency canonical projections of image data generated by the image projection module <highlight><bold>62</bold></highlight> of the modified texture map determination module <highlight><bold>86</bold></highlight> and additional canonical confidence images being canonical projections of only the triangles identified by the lists of triangles output by the canonical view determination module <highlight><bold>82</bold></highlight>. These additional confidence images being calculated for the triangles identified in the lists utilizing the weight function data generated by the weight determination module <highlight><bold>58</bold></highlight> in the same way as has previously been described in relation to the first embodiment. </paragraph>
<paragraph id="P-0166" lvl="0"><number>&lsqb;0166&rsqb;</number> Thus in this way additional texture maps are generated for those portions of the surface of the model which are not substantially visible in the original six canonical views. As these partial additional texture maps are generated in a similar way to the main canonical views they should also be representative of projections of portions of an image and hence suitable for compression utilizing the standard image compression techniques. </paragraph>
<paragraph id="P-0167" lvl="0"><number>&lsqb;0167&rsqb;</number> The original canonical views together with these additional canonical views can then be utilized to texture render the model where the texture maps utilized to render each triangle is selected utilizing the lists and the data identifying the best views as are output by the canonical view determination module <highlight><bold>82</bold></highlight>. </paragraph>
<paragraph id="P-0168" lvl="7"><number>&lsqb;0168&rsqb;</number> Third Embodiment </paragraph>
<paragraph id="P-0169" lvl="0"><number>&lsqb;0169&rsqb;</number> A third embodiment of the present invention will now be described. In the first embodiment a blending module <highlight><bold>68</bold></highlight> was described which selected pixel data corresponding to the pixels associated with the highest confidence scores for the corresponding pixel in associated confidence images. A problem with such a selection system is that boundaries can develop in the generated texture map data indicative of the selection of pixel data from different image sources. The obvious solution to this problem performing an averaging operation such as that performed by the weighted average filter <highlight><bold>66</bold></highlight> is undesirable as this results in loss of contrast within the texture maps data. In this embodiment the blending module <highlight><bold>68</bold></highlight> is arranged to process high frequency canonical projections in a way which avoids noticeable boundaries arising within texture map data, whilst maintaining image contrast. </paragraph>
<paragraph id="P-0170" lvl="0"><number>&lsqb;0170&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 15</cross-reference> is a flow diagram of the processing canonical confidence images and associated high frequency canonical image projections by the blending module <highlight><bold>68</bold></highlight> in accordance with this embodiment of the present invention. </paragraph>
<paragraph id="P-0171" lvl="0"><number>&lsqb;0171&rsqb;</number> Initially (S<highlight><bold>15</bold></highlight>-<highlight><bold>1</bold></highlight>) the blending module <highlight><bold>68</bold></highlight> selects a first one of the high frequency canonical projections for the canonical view for which a canonical high frequency image is to be generated and sets as an initial canonical high frequency image an image corresponding to the selected first high frequency projection. </paragraph>
<paragraph id="P-0172" lvl="0"><number>&lsqb;0172&rsqb;</number> The blending module <highlight><bold>68</bold></highlight> then (S<highlight><bold>15</bold></highlight>-<highlight><bold>2</bold></highlight>) selects the next high frequency canonical projection for that canonical view and its associated canonical confidence image for processing. </paragraph>
<paragraph id="P-0173" lvl="0"><number>&lsqb;0173&rsqb;</number> The first pixel within the canonical high frequency image is then selected (S<highlight><bold>15</bold></highlight>-<highlight><bold>3</bold></highlight>) and the blending module <highlight><bold>68</bold></highlight> then (S<highlight><bold>15</bold></highlight>-<highlight><bold>4</bold></highlight>) determines for the selected pixel in the high frequency canonical image a difference value between the pixel in the current high frequency canonical image and in the corresponding pixel in the high frequency canonical projection being processed. </paragraph>
<paragraph id="P-0174" lvl="0"><number>&lsqb;0174&rsqb;</number> In this embodiment, where the high frequency canonical projections and canonical high frequency images comprise colour data, this difference value may be calculated by determining the sum of the differences between corresponding values for each of the red, green and blue channels for the corresponding pixels in the selected high frequency canonical projection and the canonical high frequency image being generated. </paragraph>
<paragraph id="P-0175" lvl="0"><number>&lsqb;0175&rsqb;</number> Alternatively, where the image being processed comprises grey scale data, this difference value could be determined by calculating the difference between the pixel in the canonical high frequency image and the corresponding pixel in the high frequency canonical projection being processed. </paragraph>
<paragraph id="P-0176" lvl="0"><number>&lsqb;0176&rsqb;</number> It will be appreciated that more generally in further embodiments of the invention where colour data is being processed any suitable function of the red, green and blue channel values for corresponding pixels in the high frequency canonical projection being processed and the canonical high frequency image being generated could be utilized to generate a difference value for a pixel. </paragraph>
<paragraph id="P-0177" lvl="0"><number>&lsqb;0177&rsqb;</number> After the difference value for the pixel being processed has been determined, the blending module <highlight><bold>68</bold></highlight> then (S<highlight><bold>15</bold></highlight>-<highlight><bold>5</bold></highlight>) determines a blend function for the selected pixel from the confidence scores associated with the pixel and the determined difference value for the pixel. </paragraph>
<paragraph id="P-0178" lvl="0"><number>&lsqb;0178&rsqb;</number> In this embodiment this blend function is initially determined by selecting a gradient G in dependence upon the determined difference value for the pixel. Specifically, in this embodiment the value G is calculated by setting  
<math-cwu id="MATH-US-00005">
<number>5</number>
<math>
  <mrow>
    <mi>G</mi>
    <mo>=</mo>
    <mrow>
      <mrow>
        <mfrac>
          <mi>D</mi>
          <msub>
            <mi>D</mi>
            <mn>0</mn>
          </msub>
        </mfrac>
        <mo>&it;</mo>
        <mstyle>
          <mtext>&emsp;</mtext>
        </mstyle>
        <mo>&it;</mo>
        <mi>for</mi>
        <mo>&it;</mo>
        <mstyle>
          <mtext>&emsp;</mtext>
        </mstyle>
        <mo>&it;</mo>
        <mi>D</mi>
      </mrow>
      <mo>&GreaterEqual;</mo>
      <msub>
        <mi>D</mi>
        <mn>0</mn>
      </msub>
    </mrow>
  </mrow>
</math>
<math>
  <mrow>
    <mi>G</mi>
    <mo>=</mo>
    <mrow>
      <mrow>
        <mn>1</mn>
        <mo>&it;</mo>
        <mstyle>
          <mtext>&emsp;</mtext>
        </mstyle>
        <mo>&it;</mo>
        <mi>for</mi>
        <mo>&it;</mo>
        <mstyle>
          <mtext>&emsp;</mtext>
        </mstyle>
        <mo>&it;</mo>
        <mi>D</mi>
      </mrow>
      <mo>&lt;</mo>
      <msub>
        <mi>D</mi>
        <mn>0</mn>
      </msub>
    </mrow>
  </mrow>
</math>
<mathematica-file id="MATHEMATICA-00005" file="US20030001837A1-20030102-M00005.NB"/>
<image id="EMI-M00005" wi="216.027" he="29.9943" file="US20030001837A1-20030102-M00005.TIF" imf="TIFF" ti="MF"/>
</math-cwu>
</paragraph>
<paragraph id="P-0179" lvl="7"><number>&lsqb;0179&rsqb;</number> where D is the determined difference value for the pixel and D<highlight><subscript>0 </subscript></highlight>is a blend constant fixing the extent to which weighting factors are varied due to detected differences between images. In this embodiment where the difference values D are calculated from colour image data for three colour channels each varying from 0 to 255, the value of D<highlight><subscript>0 </subscript></highlight>is set to be 60. </paragraph>
<paragraph id="P-0180" lvl="0"><number>&lsqb;0180&rsqb;</number> An initial weighting fraction for calculating a weighted average between the pixel being processed in the current canonical high frequency image and the corresponding pixel in the high frequency canonical projection being processed is then set by calculating:  
<math-cwu id="MATH-US-00006">
<number>6</number>
<math>
<mrow>
  <msub>
    <mi>W</mi>
    <mn>0</mn>
  </msub>
  <mo>=</mo>
  <mrow>
    <mrow>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mfrac>
            <msub>
              <mi>C</mi>
              <mi>i</mi>
            </msub>
            <mrow>
              <mi>C</mi>
              <mo>+</mo>
              <msub>
                <mi>C</mi>
                <mi>i</mi>
              </msub>
            </mrow>
          </mfrac>
          <mo>-</mo>
          <mn>0.5</mn>
        </mrow>
        <mo>)</mo>
      </mrow>
      <mo>&it;</mo>
      <mi>G</mi>
    </mrow>
    <mo>+</mo>
    <mn>0.5</mn>
  </mrow>
</mrow>
</math>
<mathematica-file id="MATHEMATICA-00006" file="US20030001837A1-20030102-M00006.NB"/>
<image id="EMI-M00006" wi="216.027" he="18.00225" file="US20030001837A1-20030102-M00006.TIF" imf="TIFF" ti="MF"/>
</math-cwu>
</paragraph>
<paragraph id="P-0181" lvl="7"><number>&lsqb;0181&rsqb;</number> where C<highlight><subscript>i</subscript></highlight>(C&plus;C<highlight><subscript>i</subscript></highlight>) is the relative confidence score associated with the pixel being processed in the confidence image associated with the selected high frequency canonical projection being the ratio of the confidence score C<highlight><subscript>i </subscript></highlight>associated with the pixel in the current image being processed to the confidence score associated with the pixel by the canonical image and G is the gradient determined utilizing the determined difference value D for the pixel. </paragraph>
<paragraph id="P-0182" lvl="0"><number>&lsqb;0182&rsqb;</number> A final weighting fraction is then determined by setting the weighting value W with: </paragraph>
<paragraph lvl="0"><in-line-formula><highlight><italic>W&equals;</italic></highlight>0 if <highlight><italic>W</italic></highlight><highlight><subscript>o</subscript></highlight>&lt;0 </in-line-formula></paragraph>
<paragraph lvl="0"><in-line-formula><highlight><italic>W&equals;</italic></highlight>1 if <highlight><italic>W</italic></highlight><highlight><subscript>o</subscript></highlight>&gt;1; and </in-line-formula></paragraph>
<paragraph lvl="0"><in-line-formula><highlight><italic>W&equals;W</italic></highlight><highlight><subscript>o </subscript></highlight>if 0<highlight><italic>&lE;W</italic></highlight><highlight><subscript>o</subscript></highlight>&lE;1 </in-line-formula></paragraph>
<paragraph id="P-0183" lvl="0"><number>&lsqb;0183&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 16</cross-reference> is a graph of a generated blend function illustrating the manner in which the final weighting value W calculated in this way varies in dependence upon the relative confidence score C<highlight><subscript>i</subscript></highlight>/(C&plus;C<highlight><subscript>i</subscript></highlight>) for a pixel being processed and the determined difference value D for the pixel. </paragraph>
<paragraph id="P-0184" lvl="0"><number>&lsqb;0184&rsqb;</number> In the graph as illustrated, solid line <highlight><bold>72</bold></highlight> indicates a graph for calculating W for weighting values where D is equal to a high value, for example in this embodiment where D ranges between 0 and 765 a value of over 700. As such the graph indicates that where the relative confidence score for a particular source is low a weighting value of zero is selected and for high pixel confidence scores, a weighting factor of one is selected. For intermediate pixel confidence scores an intermediate weighting factor is determined, the weighting factor increasing as the confidence score increases. </paragraph>
<paragraph id="P-0185" lvl="0"><number>&lsqb;0185&rsqb;</number> The dotted line <highlight><bold>74</bold></highlight> in <cross-reference target="DRAWINGS">FIG. 16</cross-reference> is an illustration of the blend function where the calculated value D for the difference between the image data of the first and second images for a pixel is less than or equal to the blend constant D<highlight><subscript>0</subscript></highlight>. In this case the blend function comprises a function which sets a weighting value equal to the pixel relative confidence score for the pixel. </paragraph>
<paragraph id="P-0186" lvl="0"><number>&lsqb;0186&rsqb;</number> For intermediate values of D, the blend function varies between the solid line <highlight><bold>72</bold></highlight> and the dotted line <highlight><bold>74</bold></highlight> with the thresholds below which or above which a weighting value of zero or one is output reducing and increasing as D decreases. Thus as D decreases the proportion of relative confidence scores for which a weighting value other than zero or one is output increases. </paragraph>
<paragraph id="P-0187" lvl="0"><number>&lsqb;0187&rsqb;</number> Returning to <cross-reference target="DRAWINGS">FIG. 15</cross-reference>, after a final weighting value W has been determined the blending module <highlight><bold>68</bold></highlight> then proceeds to calculate and store blended pixel data (S<highlight><bold>15</bold></highlight>-<highlight><bold>6</bold></highlight>) for the pixel under consideration. In this embodiment, where the image data comprises colour image data, the calculated blended pixel data is determined for each of the three colour channels by:  
<math-cwu id="MATH-US-00007">
<number>7</number>
<math>
<mrow>
  <mrow>
    <mi>New</mi>
    <mo>&it;</mo>
    <mstyle>
      <mtext>&emsp;</mtext>
    </mstyle>
    <mo>&it;</mo>
    <mi>pixel</mi>
    <mo>&it;</mo>
    <mstyle>
      <mtext>&emsp;</mtext>
    </mstyle>
    <mo>&it;</mo>
    <mi>Value</mi>
  </mrow>
  <mo>=</mo>
  <mrow>
    <mrow>
      <mi>W</mi>
      <mo>&af;</mo>
      <mrow>
        <mo>[</mo>
        <mtable>
          <mtr>
            <mtd>
              <mrow>
                <mi>pixel</mi>
                <mo>&it;</mo>
                <mstyle>
                  <mtext>&emsp;</mtext>
                </mstyle>
                <mo>&it;</mo>
                <mi>value</mi>
                <mo>&it;</mo>
                <mstyle>
                  <mtext>&emsp;</mtext>
                </mstyle>
                <mo>&it;</mo>
                <mi>in</mi>
              </mrow>
            </mtd>
          </mtr>
          <mtr>
            <mtd>
              <mrow>
                <mi>selected</mi>
                <mo>&it;</mo>
                <mstyle>
                  <mtext>&emsp;</mtext>
                </mstyle>
                <mo>&it;</mo>
                <mi>image</mi>
              </mrow>
            </mtd>
          </mtr>
        </mtable>
        <mo>]</mo>
      </mrow>
    </mrow>
    <mo>+</mo>
    <mrow>
      <mrow>
        <mo>(</mo>
        <mrow>
          <mn>1</mn>
          <mo>-</mo>
          <mi>W</mi>
        </mrow>
        <mo>)</mo>
      </mrow>
      <mo>*</mo>
      <mrow>
        <mo>[</mo>
        <mtable>
          <mtr>
            <mtd>
              <mrow>
                <mi>pixel</mi>
                <mo>&it;</mo>
                <mstyle>
                  <mtext>&emsp;</mtext>
                </mstyle>
                <mo>&it;</mo>
                <mi>value</mi>
                <mo>&it;</mo>
                <mstyle>
                  <mtext>&emsp;</mtext>
                </mstyle>
                <mo>&it;</mo>
                <mi>in</mi>
              </mrow>
            </mtd>
          </mtr>
          <mtr>
            <mtd>
              <mrow>
                <mi>canonical</mi>
                <mo>&it;</mo>
                <mstyle>
                  <mtext>&emsp;</mtext>
                </mstyle>
                <mo>&it;</mo>
                <mi>image</mi>
              </mrow>
            </mtd>
          </mtr>
        </mtable>
        <mo>]</mo>
      </mrow>
    </mrow>
  </mrow>
</mrow>
</math>
<mathematica-file id="MATHEMATICA-00007" file="US20030001837A1-20030102-M00007.NB"/>
<image id="EMI-M00007" wi="216.027" he="21.12075" file="US20030001837A1-20030102-M00007.TIF" imf="TIFF" ti="MF"/>
</math-cwu>
</paragraph>
<paragraph id="P-0188" lvl="0"><number>&lsqb;0188&rsqb;</number> The confidence value associated with the blended pixel is then also updated by selecting and storing as a new confidence score for the pixel the greater of the current confidence score associated with the pixel in the canonical image and the confidence score of the pixel in the projection being processed. </paragraph>
<paragraph id="P-0189" lvl="0"><number>&lsqb;0189&rsqb;</number> The effect of calculating the pixel data for pixels in the high frequency canonical image in this way is to make the pixel data dependent upon both the difference data and the confidence data associated with the processed pixel in the selected high frequency canonical projection. Specifically, where the difference data for a pixel is low (i.e. less than or equal to the blend constant D<highlight><subscript>0</subscript></highlight>) calculated pixel data corresponding to a weighted average proportional to the relative confidence score for the pixel in the associated canonical image is utilized. Where difference data is higher (i.e. greater than the blend constant D<highlight><subscript>0</subscript></highlight>) a pair of threshold values are set based upon the actual value of the difference data. Pixel data is then generated in two different ways depending upon the relative confidence score for the pixel in the associated canonical image. If the relative confidence score is above or below these threshold values either only the original pixel data for the canonical image or the pixel data for the selected high frequency canonical projection is utilized as a composite image data. If the relative confidence score is between the threshold values a weighted average of the original pixel data and pixel data from the selected projection is utilized. </paragraph>
<paragraph id="P-0190" lvl="0"><number>&lsqb;0190&rsqb;</number> After the generated pixel data has been stored, the blending module <highlight><bold>68</bold></highlight> then determines (S<highlight><bold>15</bold></highlight>-<highlight><bold>7</bold></highlight>) whether the pixel currently under consideration is the last of the pixels in the canonical image. If this not the case the non-linear averaging filter then (S<highlight><bold>15</bold></highlight>-<highlight><bold>8</bold></highlight>) selects the next pixel and repeats the determination of a difference value (S<highlight><bold>15</bold></highlight>-<highlight><bold>4</bold></highlight>) a blend function (S<highlight><bold>15</bold></highlight>-<highlight><bold>5</bold></highlight>) and the calculation and storage of pixel data (S<highlight><bold>15</bold></highlight>-<highlight><bold>6</bold></highlight>) for the next pixel. </paragraph>
<paragraph id="P-0191" lvl="0"><number>&lsqb;0191&rsqb;</number> If after generated pixel data and a revised confidence score has been stored for a pixel the blending module <highlight><bold>68</bold></highlight> determines (S<highlight><bold>15</bold></highlight>-<highlight><bold>7</bold></highlight>) that blended pixel data has been stored for all of the pixels of the high frequency canonical image, the blending module <highlight><bold>68</bold></highlight> then determines (S<highlight><bold>15</bold></highlight>-<highlight><bold>9</bold></highlight>) whether the selected high frequency canonical projection and associated confidence image is the last high frequency canonical projection and confidence image to be processed. If this is not the case the next high frequency canonical projection is selected (S<highlight><bold>15</bold></highlight>-<highlight><bold>2</bold></highlight>) and the canonical high frequency image is updated utilizing this newly selected projection (S<highlight><bold>15</bold></highlight>-<highlight><bold>3</bold></highlight>-S<highlight><bold>15</bold></highlight>), and its associated confidence image. </paragraph>
<paragraph id="P-0192" lvl="0"><number>&lsqb;0192&rsqb;</number> When the blending module <highlight><bold>68</bold></highlight> determines that the latest high frequency canonical projection utilized to update a canonical high frequency image is the last of the high frequency canonical projections, the blending module <highlight><bold>68</bold></highlight> then (S<highlight><bold>15</bold></highlight>-<highlight><bold>10</bold></highlight>) outputs as a high frequency image for the canonical view <highlight><bold>50</bold></highlight>-<highlight><bold>55</bold></highlight> the high frequency canonical image updated by the final high frequency projection. </paragraph>
<paragraph id="P-0193" lvl="0"><number>&lsqb;0193&rsqb;</number> The applicants have realized that the undesirable effects of generating composite image data by determining a weighted average of pixels from high frequency image data including data representative of details arise in areas of images where different items of source data differ significantly. Thus for example, where highlights occur in one source image at one point but not at the same point in another source image, averaging across the images results in a loss of contrast for the highlight. If the highlights in the two images occur at different points within the two source images, averaging can also result in the generation of a &lsquo;ghost&rsquo; highlight in another portion of the composite image. Similarly, where different items of source data differ significantly other undesirable effects occur at points of shadow, where an averaging operation tends to cause an image to appear uniformly lit and therefore appear flat. </paragraph>
<paragraph id="P-0194" lvl="0"><number>&lsqb;0194&rsqb;</number> In contrast to areas of difference between source images, where corresponding points within different source images have a similar appearance, an averaging operation does not degrade apparent image quality. Performing an averaging across such areas of composite image, therefore provides a means by which the relative weight attached to different sources of image data can be varied so that the boundaries between areas of a composite image obtained from different source images can be made less distinct. </paragraph>
<paragraph id="P-0195" lvl="0"><number>&lsqb;0195&rsqb;</number> Thus in accordance with this embodiment of the present invention weighting factors are selected on the basis of both confidence data and difference data for a pixel so that, relative to weighting factors proportional to confidence data only as occurs in the weighted average filter <highlight><bold>66</bold></highlight>, in the blending module <highlight><bold>68</bold></highlight>, where determined differences between the source images are higher, the weighting given to preferred source data is increased and the corresponding weighting given to less preferred data is decreased. </paragraph>
<paragraph id="P-0196" lvl="0"><number>&lsqb;0196&rsqb;</number> Thus, where both confidence data and difference data are high, a greater weight is given to a particular source or alternatively only the preferred source data is utilized to generate composite image data. As these areas correspond to areas of detail and highlights or shadow in the high frequency images the detail and contrast in the resultant generated data is maintained. </paragraph>
<paragraph id="P-0197" lvl="0"><number>&lsqb;0197&rsqb;</number> In areas where there is less difference between different sources of image data, weighting values proportional to confidence data are not modified at all or only slightly so that the boundaries between portions of image data generated from different images are minimised across those areas. Thus in this way composite image data can be obtained which maintains the level of contrast and detail in original high frequency source data, whilst minimising apparent boundaries between portions of composite image generated from different items of source data. </paragraph>
<paragraph id="P-0198" lvl="0"><number>&lsqb;0198&rsqb;</number> In the above embodiments, a set of images of a subject object is described as being utilized to generate 3D model data and texture render data for generating representations of that subject object. However, texture render data obtained utilizing images of one object could for example be utilized to generate representations of a second object. The mixing of a model obtained for one object and texture derived from a second object could be utilized to establish how the application of texture from different objects varies the appearance of a model. </paragraph>
<paragraph id="P-0199" lvl="0"><number>&lsqb;0199&rsqb;</number> Specifically, confidence images of a first object from any number of views can be generated in a similar manner as has previously been described. Source images of a different object can then be processed using the confidence images as if the source images had been generated. The processing of the source images in this way would then generate texture data in which the texture in the source images was blended smoothly across the surface of the first object. </paragraph>
<paragraph id="P-0200" lvl="0"><number>&lsqb;0200&rsqb;</number> Thus for example a number of confidence images could be generated for views of a model. The confidence images of the model viewed from viewpoints to one side of the model might be associated with an image of a first texture for example a wood grain. The remaining confidence images of the model might be associated with a second texture e.g. marble. Processing these two source images with the generated confidence images would generate texture data in which one side of the model appeared having a wood grain texture and the other a marble texture with the two textures being smoothly blended across the surface of the model. </paragraph>
<paragraph id="P-0201" lvl="0"><number>&lsqb;0201&rsqb;</number> Alternatively, instead of using only a limited number of texture images, images of a different object as viewed from viewpoints for which confidence images were generated could be used. Thus for example, images of a model first object could be generated as has previously been described for a number of viewpoints. Confidence images for a second object could be generated for those corresponding viewpoints and texture data for the second object obtained by processing the images of the first object utilizing the confidence images of the second object. </paragraph>
<paragraph id="P-0202" lvl="0"><number>&lsqb;0202&rsqb;</number> Although in the above embodiments, confidence images are described that are generated utilizing visibility values calculated from the number of pixels used to represent triangles, other visibility values could be used. For example, a visibility value for a triangle could be set so that for each triangle: </paragraph>
<paragraph lvl="0"><in-line-formula>visibility value&equals;abs&lsqb;cos (&THgr;)&times;closeness&rsqb;</in-line-formula></paragraph>
<paragraph id="P-0203" lvl="7"><number>&lsqb;0203&rsqb;</number> where &thgr; is the angle of incidence of a ray from the camera centre defining the selected viewpoint to the triangle centroid of the triangle being texture rendered and closeness is a value inversely proportional to the distance between the camera centre defining the selected viewpoint and the centre of the triangle. </paragraph>
<paragraph id="P-0204" lvl="0"><number>&lsqb;0204&rsqb;</number> Although the embodiments of the invention described with reference to the drawings comprise computer apparatus and processes performed in computer apparatus, the invention also extends to computer programs, particularly computer programs on or in a carrier, adapted for putting the invention into practice. The program may be in the form of source or object code or in any other form suitable for use in the implementation of the processes according to the invention. The carrier be any entity or device capable of carrying the program. </paragraph>
<paragraph id="P-0205" lvl="0"><number>&lsqb;0205&rsqb;</number> For example, the carrier may comprise a storage medium, such as a ROM, for example a CD ROM or a semiconductor ROM, or a magnetic recording medium, for example a floppy disc or hard disk. Further, the carrier may be a transmissible carrier such as an electrical or optical signal which may be conveyed via electrical or optical cable or by radio or other means. </paragraph>
<paragraph id="P-0206" lvl="0"><number>&lsqb;0206&rsqb;</number> When a program is embodied in a signal which may be conveyed directly by a cable or other device or means, the carrier may be constituted by such cable or other device or means. </paragraph>
<paragraph id="P-0207" lvl="0"><number>&lsqb;0207&rsqb;</number> Alternatively, the carrier may be an integrated circuit in which the program is embedded, the integrated circuit being adapted for performing, or for use in the performance of, the relevant processes. </paragraph>
</detailed-description>
</subdoc-description>
<subdoc-claims>
<claim id="CLM-00001">
<claim-text><highlight><bold>1</bold></highlight>. A method of generating visibility data indicative of the relative visibility of points for at least part of the surface of an object from a viewpoint, said method comprising the steps of: 
<claim-text>receiving model data defining a plurality of polygons indicative of at least part of the surface of an object for which visibility data is to be determined; </claim-text>
<claim-text>generating a representation of said object as viewed from said viewpoint wherein the surface of said object viewed from said viewpoint in said representation is rendered utilizing a determined visibility for the corresponding portion of said object as seen from said viewpoint; and </claim-text>
<claim-text>generating visibility data utilizing said generated representation. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00002">
<claim-text><highlight><bold>2</bold></highlight>. A method in accordance with <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein said generation of visibility data comprises utilizing said representation to generate visibility data for portions of the surface of an object visible from said viewpoint and associating portions of the surface of an object not visible from said viewpoint with data indicative of low visibility. </claim-text>
</claim>
<claim id="CLM-00003">
<claim-text><highlight><bold>3</bold></highlight>. A method in accordance with <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein said generation of a representation comprises generation of a representation in which visible portions of an object viewed from said viewpoint adjacent to portions of said object the surface of which is not visible from said viewpoint are associated with data representative of a low level of visibility. </claim-text>
</claim>
<claim id="CLM-00004">
<claim-text><highlight><bold>4</bold></highlight>. A method in accordance with <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein said generation of a representation comprises for each polygon of said model of said object visible from said viewpoint, determining a visibility score representative of the relative visibility of said polygon viewed from said viewpoint, wherein said determined visibility scores are utilized to generate said representation of said object from said viewpoint. </claim-text>
</claim>
<claim id="CLM-00005">
<claim-text><highlight><bold>5</bold></highlight>. A method in accordance with <dependent-claim-reference depends_on="CLM-00004">claim 4</dependent-claim-reference>, wherein said determination of a visibility score comprises determining a visibility score proportional to the ratio of the number of pixels utilized to represent said polygon in an image of said object viewed from said viewpoint relative to the surface area of said polygon represents. </claim-text>
</claim>
<claim id="CLM-00006">
<claim-text><highlight><bold>6</bold></highlight>. A method in accordance with <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein said generation of said representation comprises the steps of: 
<claim-text>determining for polygons of an object as viewed from said viewpoint whether any edges of said polygons are adjacent to polygons oriented away from said viewpoint and associating said edges of said visible polygons adjacent to polygons orientated away from said viewpoint with data indicative of low visibility. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00007">
<claim-text><highlight><bold>7</bold></highlight>. A method in accordance with <dependent-claim-reference depends_on="CLM-00002">claim 2</dependent-claim-reference>, wherein said generation of a representation comprises generation of a representation in which data representative of the visibility of an object varies smoothly across the visible surface of an object as viewed from said viewpoint. </claim-text>
</claim>
<claim id="CLM-00008">
<claim-text><highlight><bold>8</bold></highlight>. A method in accordance with <dependent-claim-reference depends_on="CLM-00007">claim 7</dependent-claim-reference>, wherein said generation of a representation comprises the steps of generating an initial representation in which polygons visible from said viewpoint are uniformly represented by data indicative of the relative visibility of said polygons and generating said representation by performing a smoothing operation on said initial image. </claim-text>
</claim>
<claim id="CLM-00009">
<claim-text><highlight><bold>9</bold></highlight>. A method in accordance with <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein said generation of visibility data comprises: 
<claim-text>associating polygons not visible from said viewpoint with data indicative of low visibility; </claim-text>
<claim-text>for polygons completely visible from said viewpoint, utilizing said representation to determine visibility data for the surface of said polygons; </claim-text>
<claim-text>and for polygons partially visible from said viewpoint, utilizing said representation to determine visibility data for said visible portions; and associating the remaining portions of said partially visible polygons with data representative of low visibility. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00010">
<claim-text><highlight><bold>10</bold></highlight>. A method of generating data indicative of the reliability of data of an object from a viewpoint to generate corresponding data for another viewpoint comprising the steps of: 
<claim-text>generating visibility data for said viewpoint in accordance with <dependent-claim-reference depends_on="CLM-00011">claim 1;</dependent-claim-reference> and </claim-text>
<claim-text>determining a projection of said visibility data associated with points on the surface of said object as perceived from said another viewpoint. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00011">
<claim-text><highlight><bold>11</bold></highlight>. A method of utilising image data of an object from a plurality of viewpoints to generate data indicative of a view of said object of from another viewpoint comprising the steps of: 
<claim-text>receiving image data of an object from a plurality of viewpoints; </claim-text>
<claim-text>determining for each of said plurality of viewpoints data indicative of reliability of data from said viewpoint to generate corresponding data for said another viewpoint in accordance with claim <highlight><bold>10</bold></highlight>; </claim-text>
<claim-text>determining for each of said plurality of viewpoints, representations of said object as viewed from said another viewpoint utilising said image data for said viewpoints; and </claim-text>
<claim-text>generating image data for said another viewpoint by processing portions of said projected image data utilising said data indicative of reliability. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00012">
<claim-text><highlight><bold>12</bold></highlight>. A method in accordance with <dependent-claim-reference depends_on="CLM-00011">claim 11</dependent-claim-reference> wherein said processing comprises the step of selecting portions of representations associated with reliability data indicative of high reliability of data to generate said image data for said another viewpoint. </claim-text>
</claim>
<claim id="CLM-00013">
<claim-text><highlight><bold>13</bold></highlight>. A method in accordance with <dependent-claim-reference depends_on="CLM-00011">claim 12</dependent-claim-reference> wherein said processing comprises the step of determining image data for each portion of an image for said another viewpoint by calculating weighted averages of corresponding portions of said representations weighted by reliability data indicative of reliability for portion of said representations. </claim-text>
</claim>
<claim id="CLM-00014">
<claim-text><highlight><bold>14</bold></highlight>. Apparatus for generating visibility data indicative of the relative visibility of points for at least part of the surface of an object from a viewpoint, said apparatus comprising: 
<claim-text>a receiver operable to receive model data defining a plurality of polygons indicative of at least part of the surface of an object for which visibility data is to be determined; </claim-text>
<claim-text>a view generation unit operable to generate utilizing model data received by said receiver, a representation of an object as viewed from a viewpoint wherein the surface of said object viewed from said viewpoint in said representation is rendered utilizing a determined visibility for the corresponding portion of said object as seen from said viewpoint; and </claim-text>
<claim-text>a visibility data generation unit operable to generate visibility data utilizing representations generated by said view generation unit. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00015">
<claim-text><highlight><bold>15</bold></highlight>. An apparatus in accordance with <dependent-claim-reference depends_on="CLM-00011">claim 14</dependent-claim-reference>, wherein visibility generation unit is operable to utilize representation generated by said view generation unit to generate visibility data for portions of the surface of an object visible from said viewpoint and to associate portions of the surface of an object not visible from said viewpoint with data indicative of low visibility. </claim-text>
</claim>
<claim id="CLM-00016">
<claim-text><highlight><bold>16</bold></highlight>. An apparatus in accordance with <dependent-claim-reference depends_on="CLM-00011">claim 14</dependent-claim-reference>, wherein said view generation unit comprises a representation unit operable to generate a representation in which visible portions of an object viewed from a viewpoint adjacent to portions of a said object the surface of which is not visible from said viewpoint are associated with data representative of a low level of visibility. </claim-text>
</claim>
<claim id="CLM-00017">
<claim-text><highlight><bold>17</bold></highlight>. An apparatus in accordance with <dependent-claim-reference depends_on="CLM-00011">claim 14</dependent-claim-reference>, wherein said representation unit is operable to generate a representation in which each polygon of said object visible from said viewpoint is represented utilizing data determined on the basis of a visibility score representative of the relative visibility of said polygon viewed from said viewpoint. </claim-text>
</claim>
<claim id="CLM-00018">
<claim-text><highlight><bold>18</bold></highlight>. An apparatus in accordance with <dependent-claim-reference depends_on="CLM-00011">claim 17</dependent-claim-reference>, wherein said representation unit comprises: 
<claim-text>a visibility score determinator operable to determine visibility scores proportional to the ratio of the number of pixels utilized to represent a said polygon in an image of said object viewed from said viewpoint relative to the surface area of said polygon represents. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00019">
<claim-text><highlight><bold>19</bold></highlight>. An apparatus in accordance with <dependent-claim-reference depends_on="CLM-00011">claim 14</dependent-claim-reference>, wherein said view generation unit comprises: 
<claim-text>an edge determinator operable to determine for polygons of an object as viewed from a viewpoint whether any edges of said polygons are adjacent to polygons oriented away from said viewpoint and to associate said edges of said visible polygons adjacent to polygons pointed away from said viewpoint with data indicative of low visibility. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00020">
<claim-text><highlight><bold>20</bold></highlight>. An apparatus in accordance with <dependent-claim-reference depends_on="CLM-00011">claim 15</dependent-claim-reference>, wherein said view generation unit is operable to generate representations in which data representative of the visibility of an object varies smoothly across the visible surface of an object as viewed from a said viewpoint. </claim-text>
</claim>
<claim id="CLM-00021">
<claim-text><highlight><bold>21</bold></highlight>. An apparatus in accordance with <dependent-claim-reference depends_on="CLM-00022">claim 20</dependent-claim-reference>, wherein said view generation unit comprises: 
<claim-text>a first image generator operable to generate an initial representation in which polygons visible from said viewpoint are uniformly represented by data indicative of the relative visibility of said polygons; and </claim-text>
<claim-text>a second image generator operable to generate said representation by performing a smoothing operation on an initial image generated by said first image generator. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00022">
<claim-text><highlight><bold>22</bold></highlight>. An apparatus in accordance with <dependent-claim-reference depends_on="CLM-00011">claim 14</dependent-claim-reference>, wherein said visibility generation unit comprises: 
<claim-text>a first association unit operable to associate polygons not visible from said viewpoint with data indicative of low visibility; </claim-text>
<claim-text>a second association unit operable to associate polygons completely visible from said viewpoint, with visibility data utilizing a said representation generated by said view generation unit to determine visibility data for the surface of said polygons; and </claim-text>
<claim-text>a third association unit operable to associate polygons partially visible from said viewpoint with visibility data, utilizing a said representation generated by said view generation unit to determine visibility data for said visible portions; and associating the remaining portions of said partially visible polygons with data representative of low visibility. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00023">
<claim-text><highlight><bold>23</bold></highlight>. An apparatus for generating data indicative of the reliability of data of an object from a viewpoint to generate corresponding data for another viewpoint comprising: 
<claim-text>apparatus for generating visibility data for said viewpoint in accordance with claim <highlight><bold>14</bold></highlight>; and </claim-text>
<claim-text>a projector operable to determine a projection of said visibility data associated with points on the surface of said object as perceived from said another viewpoint. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00024">
<claim-text><highlight><bold>24</bold></highlight>. An apparatus for utilising image data of an object from a plurality of viewpoints to generate data indicative of a view of said object of from another viewpoint comprising: 
<claim-text>an image receiver operable to receive image data of an object from a plurality of viewpoints; </claim-text>
<claim-text>apparatus for generating data indicative of reliability of data from a viewpoint to generate corresponding data for said another viewpoint in accordance with claim <highlight><bold>23</bold></highlight>; </claim-text>
<claim-text>a view generator operable to determine for each of said plurality of viewpoints, projected image data for representations of said object as viewed from said another viewpoint utilising image data for said viewpoints received by said image receiver; and </claim-text>
<claim-text>an image generator operable to generate image data for said another viewpoint by processing portions of said projected image data generated by said view generator utilising data indicative of reliability generated by said apparatus for generating data indicative of reliability. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00025">
<claim-text><highlight><bold>25</bold></highlight>. An apparatus in accordance with <dependent-claim-reference depends_on="CLM-00022">claim 24</dependent-claim-reference> wherein said image generator is operable to process image data by selecting portions of representations associated with reliability data indicative of high reliability of data by said generated reliability data to generate said image data for said another viewpoint. </claim-text>
</claim>
<claim id="CLM-00026">
<claim-text><highlight><bold>26</bold></highlight>. An apparatus in accordance with <dependent-claim-reference depends_on="CLM-00022">claim 25</dependent-claim-reference> wherein said image generator is operable to process image data by determining image data for each portion of an image for said another viewpoint by calculating weighted averages of corresponding portions of said representations weighted by reliability data generated by said apparatus for generating data indicative of reliability. </claim-text>
</claim>
<claim id="CLM-00027">
<claim-text><highlight><bold>27</bold></highlight>. A recording medium for storing computer implementable process steps for generating within a programmable computer apparatus in accordance with any of <dependent-claim-reference depends_on="CLM-00014">claims 14</dependent-claim-reference> to <dependent-claim-reference depends_on="CLM-00026">26</dependent-claim-reference> or for causing a programmable computer to perform a method in accordance with any <dependent-claim-reference depends_on="CLM-00001">claims 1</dependent-claim-reference> to <dependent-claim-reference depends_on="CLM-00013">13</dependent-claim-reference>. </claim-text>
</claim>
<claim id="CLM-00028">
<claim-text><highlight><bold>28</bold></highlight>. A recording medium in accordance with <dependent-claim-reference depends_on="CLM-00022">claim 27</dependent-claim-reference> comprising a computer disc. </claim-text>
</claim>
<claim id="CLM-00029">
<claim-text><highlight><bold>29</bold></highlight>. A computer disc in accordance with <dependent-claim-reference depends_on="CLM-00022">claim 28</dependent-claim-reference> comprising an optical, magneto-optical or magnetic disc. </claim-text>
</claim>
<claim id="CLM-00030">
<claim-text><highlight><bold>30</bold></highlight>. A recording medium in accordance with <dependent-claim-reference depends_on="CLM-00022">claim 27</dependent-claim-reference>, comprising an electrical signal transferred via the Internet.</claim-text>
</claim>
</subdoc-claims>
<subdoc-drawings id="DRAWINGS">
<heading lvl="0" align="CENTER">Drawings</heading>
<representative-figure>1</representative-figure>
<figure id="figure-D00000">
<image id="EMI-D00000" file="US20030001837A1-20030102-D00000.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00001">
<image id="EMI-D00001" file="US20030001837A1-20030102-D00001.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00002">
<image id="EMI-D00002" file="US20030001837A1-20030102-D00002.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00003">
<image id="EMI-D00003" file="US20030001837A1-20030102-D00003.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00004">
<image id="EMI-D00004" file="US20030001837A1-20030102-D00004.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00005">
<image id="EMI-D00005" file="US20030001837A1-20030102-D00005.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00006">
<image id="EMI-D00006" file="US20030001837A1-20030102-D00006.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00007">
<image id="EMI-D00007" file="US20030001837A1-20030102-D00007.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00008">
<image id="EMI-D00008" file="US20030001837A1-20030102-D00008.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00009">
<image id="EMI-D00009" file="US20030001837A1-20030102-D00009.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00010">
<image id="EMI-D00010" file="US20030001837A1-20030102-D00010.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00011">
<image id="EMI-D00011" file="US20030001837A1-20030102-D00011.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00012">
<image id="EMI-D00012" file="US20030001837A1-20030102-D00012.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00013">
<image id="EMI-D00013" file="US20030001837A1-20030102-D00013.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00014">
<image id="EMI-D00014" file="US20030001837A1-20030102-D00014.TIF" imf="TIFF" ti="DR"/>
</figure>
</subdoc-drawings>
</patent-application-publication>
