<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE patent-application-publication SYSTEM "pap-v16-2002-01-01.dtd" [
<!ENTITY US20030004694A1-20030102-M00001.NB SYSTEM "US20030004694A1-20030102-M00001.NB" NDATA NB>
<!ENTITY US20030004694A1-20030102-M00001.TIF SYSTEM "US20030004694A1-20030102-M00001.TIF" NDATA TIF>
<!ENTITY US20030004694A1-20030102-M00002.NB SYSTEM "US20030004694A1-20030102-M00002.NB" NDATA NB>
<!ENTITY US20030004694A1-20030102-M00002.TIF SYSTEM "US20030004694A1-20030102-M00002.TIF" NDATA TIF>
<!ENTITY US20030004694A1-20030102-M00003.NB SYSTEM "US20030004694A1-20030102-M00003.NB" NDATA NB>
<!ENTITY US20030004694A1-20030102-M00003.TIF SYSTEM "US20030004694A1-20030102-M00003.TIF" NDATA TIF>
<!ENTITY US20030004694A1-20030102-M00004.NB SYSTEM "US20030004694A1-20030102-M00004.NB" NDATA NB>
<!ENTITY US20030004694A1-20030102-M00004.TIF SYSTEM "US20030004694A1-20030102-M00004.TIF" NDATA TIF>
<!ENTITY US20030004694A1-20030102-M00005.NB SYSTEM "US20030004694A1-20030102-M00005.NB" NDATA NB>
<!ENTITY US20030004694A1-20030102-M00005.TIF SYSTEM "US20030004694A1-20030102-M00005.TIF" NDATA TIF>
<!ENTITY US20030004694A1-20030102-M00006.NB SYSTEM "US20030004694A1-20030102-M00006.NB" NDATA NB>
<!ENTITY US20030004694A1-20030102-M00006.TIF SYSTEM "US20030004694A1-20030102-M00006.TIF" NDATA TIF>
<!ENTITY US20030004694A1-20030102-M00007.NB SYSTEM "US20030004694A1-20030102-M00007.NB" NDATA NB>
<!ENTITY US20030004694A1-20030102-M00007.TIF SYSTEM "US20030004694A1-20030102-M00007.TIF" NDATA TIF>
<!ENTITY US20030004694A1-20030102-M00008.NB SYSTEM "US20030004694A1-20030102-M00008.NB" NDATA NB>
<!ENTITY US20030004694A1-20030102-M00008.TIF SYSTEM "US20030004694A1-20030102-M00008.TIF" NDATA TIF>
<!ENTITY US20030004694A1-20030102-P00001.TIF SYSTEM "US20030004694A1-20030102-P00001.TIF" NDATA TIF>
<!ENTITY US20030004694A1-20030102-P00002.TIF SYSTEM "US20030004694A1-20030102-P00002.TIF" NDATA TIF>
<!ENTITY US20030004694A1-20030102-P00003.TIF SYSTEM "US20030004694A1-20030102-P00003.TIF" NDATA TIF>
<!ENTITY US20030004694A1-20030102-P00004.TIF SYSTEM "US20030004694A1-20030102-P00004.TIF" NDATA TIF>
<!ENTITY US20030004694A1-20030102-P00005.TIF SYSTEM "US20030004694A1-20030102-P00005.TIF" NDATA TIF>
<!ENTITY US20030004694A1-20030102-P00006.TIF SYSTEM "US20030004694A1-20030102-P00006.TIF" NDATA TIF>
<!ENTITY US20030004694A1-20030102-P00007.TIF SYSTEM "US20030004694A1-20030102-P00007.TIF" NDATA TIF>
<!ENTITY US20030004694A1-20030102-P00008.TIF SYSTEM "US20030004694A1-20030102-P00008.TIF" NDATA TIF>
<!ENTITY US20030004694A1-20030102-P00009.TIF SYSTEM "US20030004694A1-20030102-P00009.TIF" NDATA TIF>
<!ENTITY US20030004694A1-20030102-P00010.TIF SYSTEM "US20030004694A1-20030102-P00010.TIF" NDATA TIF>
<!ENTITY US20030004694A1-20030102-P00011.TIF SYSTEM "US20030004694A1-20030102-P00011.TIF" NDATA TIF>
<!ENTITY US20030004694A1-20030102-P00012.TIF SYSTEM "US20030004694A1-20030102-P00012.TIF" NDATA TIF>
<!ENTITY US20030004694A1-20030102-P00013.TIF SYSTEM "US20030004694A1-20030102-P00013.TIF" NDATA TIF>
<!ENTITY US20030004694A1-20030102-P00014.TIF SYSTEM "US20030004694A1-20030102-P00014.TIF" NDATA TIF>
<!ENTITY US20030004694A1-20030102-P00015.TIF SYSTEM "US20030004694A1-20030102-P00015.TIF" NDATA TIF>
<!ENTITY US20030004694A1-20030102-P00016.TIF SYSTEM "US20030004694A1-20030102-P00016.TIF" NDATA TIF>
<!ENTITY US20030004694A1-20030102-P00017.TIF SYSTEM "US20030004694A1-20030102-P00017.TIF" NDATA TIF>
<!ENTITY US20030004694A1-20030102-P00018.TIF SYSTEM "US20030004694A1-20030102-P00018.TIF" NDATA TIF>
<!ENTITY US20030004694A1-20030102-P00019.TIF SYSTEM "US20030004694A1-20030102-P00019.TIF" NDATA TIF>
<!ENTITY US20030004694A1-20030102-P00020.TIF SYSTEM "US20030004694A1-20030102-P00020.TIF" NDATA TIF>
<!ENTITY US20030004694A1-20030102-P00021.TIF SYSTEM "US20030004694A1-20030102-P00021.TIF" NDATA TIF>
<!ENTITY US20030004694A1-20030102-P00022.TIF SYSTEM "US20030004694A1-20030102-P00022.TIF" NDATA TIF>
<!ENTITY US20030004694A1-20030102-P00023.TIF SYSTEM "US20030004694A1-20030102-P00023.TIF" NDATA TIF>
<!ENTITY US20030004694A1-20030102-D00000.TIF SYSTEM "US20030004694A1-20030102-D00000.TIF" NDATA TIF>
<!ENTITY US20030004694A1-20030102-D00001.TIF SYSTEM "US20030004694A1-20030102-D00001.TIF" NDATA TIF>
<!ENTITY US20030004694A1-20030102-D00002.TIF SYSTEM "US20030004694A1-20030102-D00002.TIF" NDATA TIF>
<!ENTITY US20030004694A1-20030102-D00003.TIF SYSTEM "US20030004694A1-20030102-D00003.TIF" NDATA TIF>
<!ENTITY US20030004694A1-20030102-D00004.TIF SYSTEM "US20030004694A1-20030102-D00004.TIF" NDATA TIF>
<!ENTITY US20030004694A1-20030102-D00005.TIF SYSTEM "US20030004694A1-20030102-D00005.TIF" NDATA TIF>
]>
<patent-application-publication>
<subdoc-bibliographic-information>
<document-id>
<doc-number>20030004694</doc-number>
<kind-code>A1</kind-code>
<document-date>20030102</document-date>
</document-id>
<publication-filing-type>new</publication-filing-type>
<domestic-filing-data>
<application-number>
<doc-number>10156189</doc-number>
</application-number>
<application-number-series-code>10</application-number-series-code>
<filing-date>20020529</filing-date>
</domestic-filing-data>
<technical-information>
<classification-ipc>
<classification-ipc-primary>
<ipc>G06F017/10</ipc>
</classification-ipc-primary>
<classification-ipc-edition>07</classification-ipc-edition>
</classification-ipc>
<classification-us>
<classification-us-primary>
<uspc>
<class>703</class>
<subclass>002000</subclass>
</uspc>
</classification-us-primary>
</classification-us>
<title-of-invention>Camera model and calibration procedure for omnidirectional paraboloidal catadioptric cameras</title-of-invention>
</technical-information>
<continuity-data>
<non-provisional-of-provisional>
<document-id>
<doc-number>60294061</doc-number>
<document-date>20010529</document-date>
<country-code>US</country-code>
</document-id>
</non-provisional-of-provisional>
</continuity-data>
<inventors>
<first-named-inventor>
<name>
<given-name>Daniel G.</given-name>
<family-name>Aliaga</family-name>
</name>
<residence>
<residence-us>
<city>Millington</city>
<state>NJ</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</first-named-inventor>
<inventor>
<name>
<given-name>Ingrid B.</given-name>
<family-name>Carlbom</family-name>
</name>
<residence>
<residence-us>
<city>Summit</city>
<state>NJ</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
</inventors>
<correspondence-address>
<name-1>HARNESS, DICKEY &amp; PIERCE, P.L.C.</name-1>
<name-2></name-2>
<address>
<address-1>P.O. BOX 8910</address-1>
<city>RESTON</city>
<state>VA</state>
<postalcode>20195</postalcode>
<country>
<country-code>US</country-code>
</country>
</address>
</correspondence-address>
</subdoc-bibliographic-information>
<subdoc-abstract>
<paragraph id="A-0001" lvl="0">A paraboloidal catadioptric camera is calibrated by relaxing the assumption of an ideal system to account for perspective projection, radial distortion, and mirror misalignment occurring within the camera system. Calibration points, which are small and visually distinct objects, are distributed at fixed locations within an environment. Omnidirectional images are captured by the catadioptric camera at different locations of the environment. Data points are obtained by identifying the location of the calibration points in each captured image. An optimization algorithm best-fits the data points to a perspective camera model in order to derive parameters, which are used to calibrate the catadioptric camera. </paragraph>
</subdoc-abstract>
<subdoc-description>
<cross-reference-to-related-applications>
<heading lvl="1">CROSS-REFERENCE TO RELATED APPLICATIONS </heading>
<paragraph id="P-0001" lvl="0"><number>&lsqb;0001&rsqb;</number> This application claims the benefit of U.S. Provisional Application No. 60/294,061 filed May 29, 2001, the contents of which are hereby incorporated by reference in its entirety. This application is related to U.S. patent application Ser. No. 10/122,337 filed on Apr. 16, 2002 by the same named inventors of the present application, the contents of which are hereby incorporated by reference in its entirety.</paragraph>
</cross-reference-to-related-applications>
<summary-of-invention>
<section>
<heading lvl="1">BACKGROUND OF THE INVENTION </heading>
<paragraph id="P-0002" lvl="0"><number>&lsqb;0002&rsqb;</number> The present invention relates to the generation of omnidirectional images, and more particularly, to a calibration model for an omnidirectional camera. </paragraph>
</section>
<section>
<heading lvl="1">DESCRIPTION OF THE RELATED ART </heading>
<paragraph id="P-0003" lvl="0"><number>&lsqb;0003&rsqb;</number> Omnidirectional still and video cameras have become increasingly popular in applications such as telepresence, three-dimensional (3D) reconstruction, and autonomous navigation. Omnidirectional images can be created by several techniques, not all of which create images from a single center-of-projection (COP). One family of omnidirectional cameras uses a catadioptric system with a paraboloidal mirror, as proposed in the paper &ldquo;Catadioptric Omnidirectional Camera&rdquo;, by S. Nayar, IEEE Conference on Computer Vision and Pattern Recognition, 1997, pp. 482-488, and in U.S. Pat. No. 6,118,474, issued Sep. 12, 2000, both of which are hereby incorporated by reference in their entireties. </paragraph>
<paragraph id="P-0004" lvl="0"><number>&lsqb;0004&rsqb;</number> Nayar&apos;s camera design uses a convex paraboloidal mirror (i.e., a mirror whose profile is a parabola and whose reflective-side is convex, yielding a focal point of the parabola on the non-reflective side). The mirror is placed in the field-of-view of a camera equipped with a lens system that images a nearly orthographic projection of the mirror surface. The camera&apos;s image plane is perpendicular to the mirror&apos;s central axis. Therefore, a full hemisphere of the field-of-view (FOV) (360 degrees by 180 degrees) in front of the mirror&apos;s reflective surface is reflected onto the camera&apos;s image plane, with the exception of an area occluded by the mount near the center of the image. Each captured omnidirectional image has a substantially single COP, which yields simple transformations for obtaining several types of projections (e.g., planar, cylindrical). The camera installed in front of the mirror may comprise a still or video camera, or any other appropriate digitizing camera, as will be contemplated by those ordinarily skilled in the art. </paragraph>
<paragraph id="P-0005" lvl="0"><number>&lsqb;0005&rsqb;</number> In practice, it is difficult to achieve a perfect orthographic projection with current lens and camera technologies. This may result, for example, in a slight undesired perspective projection in addition to mirror misalignment and radial distortion of the lens. A camera implementation using a telecentric lens or a camera implementation using a zoom lens combined with magnification lenses may produce close to ideal orthographic projections but both severely limit the placement and size of the mirror. Thus, both implementations are prone to a slight undesired perspective projection. The presence of undesired perspective projection, mirror misalignment, and radial distortion make accurate calibration of a catadioptric paraboloidal camera a difficult task for applications such as determining the position and orientation of the camera in the surrounding environment. </paragraph>
<paragraph id="P-0006" lvl="0"><number>&lsqb;0006&rsqb;</number> Alternative mirror configurations have also been employed to ease the requirement for lens systems with ideal orthographic projection. For instance, a two parabolic mirror design has been disclosed by F. Bruckstein, T. Richardson in U.S. Pat. No. 5,920,376, which issued on Jul. 6, 1999, and in the paper &ldquo;Omniview Cameras with Curved Surface Mirrors,&rdquo; IEEE Workshop on Omnidirectional Vision, pp. 79-84, 2000. In this design, incoming rays bounce off a convex paraboloidal mirror and are reflected off a second concave paraboloidal mirror centered opposite the first mirror. The reflected rays converge at the second mirror&apos;s focal point, where a camera with a standard perspective lens is placed. This configuration relies on accurately computing the effective COP for the camera, and then precisely placing the camera so that the COP is located at the focal point of the second mirror. A deviation in the placement of the camera&apos;s COP will cause the camera to capture rays that correspond to slightly non-parallel rays reflected off the first mirror. </paragraph>
<paragraph id="P-0007" lvl="0"><number>&lsqb;0007&rsqb;</number> Other configurations have also been developed, as described in the paper &ldquo;A Theory of Catadioptric Image Formation&rdquo;, by S. Baker and S. Nayar, IEEE International Conference on Computer Vision, pp. 35-42, 1998. For instance, hyperboloid mirrors are designed to converge rays in front of the mirror. However, the performance of the hyperboloid mirror design similarly suffers when the camera is not precisely located at the converging point. </paragraph>
<paragraph id="P-0008" lvl="0"><number>&lsqb;0008&rsqb;</number> It would therefore be advantageous to be able to relax the assumption of an ideal projection system (i.e., perfect orthographic projection and perfect placement) for the camera and as well as incorporate into the camera model mirror misalignment and radial distortion of the lens system. </paragraph>
</section>
<section>
<heading lvl="1">SUMMARY OF THE INVENTION </heading>
<paragraph id="P-0009" lvl="0"><number>&lsqb;0009&rsqb;</number> The present invention provides a system and method for calibrating an omnidirectional paraboloidal catadioptric camera based on a model, which does not assume an ideal projection system for the camera. Instead, the present invention enhances the calibration model of the omnidirectional camera by taking into account and compensating for undesired perspective projection, mirror misalignment, and radial distortion. </paragraph>
<paragraph id="P-0010" lvl="0"><number>&lsqb;0010&rsqb;</number> Given the calibration model of the invention, pose estimation of the described omnidirectional camera may be performed with high accuracy. Pose estimation computes the position and orientation of the camera relative to the surrounding environment for either a stationary or moving camera. The calibration model may be used to measure the distance to known environment features by using their projection onto an omnidirectional image. Triangulation, or other methods known to those ordinarily skilled in the art, may be used to obtain the camera position and orientation. </paragraph>
<paragraph id="P-0011" lvl="0"><number>&lsqb;0011&rsqb;</number> Other advantages of the present invention will become more apparent from the detailed description given hereafter. However, it should be understood that the detailed description and specific examples, while indicating preferred embodiments of the invention, are given by way of illustration only, since various changes and at modifications within the spirit and scope of the invention will become apparent to those skilled in the art from this detailed description.</paragraph>
</section>
</summary-of-invention>
<brief-description-of-drawings>
<section>
<heading lvl="1">BRIEF DESCRIPTION OF THE DRAWINGS </heading>
<paragraph id="P-0012" lvl="0"><number>&lsqb;0012&rsqb;</number> The present invention will become more fully understood from the detailed description given below and the accompanying drawings, which are given for purposes of illustration only, and thus do not limit the present invention. </paragraph>
<paragraph id="P-0013" lvl="0"><number>&lsqb;0013&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 1</cross-reference> illustrates a camera model for an ideal paraboloidal catadioptric system. </paragraph>
<paragraph id="P-0014" lvl="0"><number>&lsqb;0014&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 2</cross-reference> illustrates a camera model for the paraboloidal catadioptric system of the present invention. </paragraph>
<paragraph id="P-0015" lvl="0"><number>&lsqb;0015&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 3</cross-reference> is a flowchart illustrating the steps for calibrating the camera system. </paragraph>
<paragraph id="P-0016" lvl="0"><number>&lsqb;0016&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 4</cross-reference> illustrates mapped calibration points used to optimize parameters of the camera model. </paragraph>
<paragraph id="P-0017" lvl="0"><number>&lsqb;0017&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 5</cross-reference> illustrates an exemplary configuration of the catadioptric camera system of the present invention. </paragraph>
<paragraph id="P-0018" lvl="0"><number>&lsqb;0018&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 6</cross-reference> illustrates the process of obtaining calibration points for the camera system. </paragraph>
<paragraph id="P-0019" lvl="0"><number>&lsqb;0019&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 7</cross-reference> illustrates a system for performing the calibration process of the present invention.</paragraph>
</section>
</brief-description-of-drawings>
<detailed-description>
<section>
<heading lvl="1">DETAILED DESCRIPTION OF THE INVENTION </heading>
<paragraph id="P-0020" lvl="7"><number>&lsqb;0020&rsqb;</number> Ideal Paraboloidal Catadioptric Camera Model </paragraph>
<paragraph id="P-0021" lvl="0"><number>&lsqb;0021&rsqb;</number> An ideal model for a paraboloidal catadioptric camera is illustrated in <cross-reference target="DRAWINGS">FIG. 1</cross-reference>. In a 3D environment, described by a coordinate system (x, y, z), a ray of light radiating from a given point P (p<highlight><subscript>x</subscript></highlight>, p<highlight><subscript>y</subscript></highlight>, p<highlight><subscript>z</subscript></highlight>) reflects off of point M (m<highlight><subscript>x, m</subscript></highlight><highlight><subscript>y</subscript></highlight>, m<highlight><subscript>z</subscript></highlight>) on the paraboloidal mirror <highlight><bold>20</bold></highlight>, and projects onto the image plane <highlight><bold>25</bold></highlight> at point I. Since the paraboloidal mirror <highlight><bold>20</bold></highlight> is symmetric about the z-axis, point M may be represented as (m<highlight><subscript>r</subscript></highlight>, &thgr;, m<highlight><subscript>z</subscript></highlight>) and P as (p<highlight><subscript>r</subscript></highlight>, &thgr;, p<highlight><subscript>z</subscript></highlight>), where m<highlight><subscript>r</subscript></highlight>&equals;{square root}{square root over (m<highlight><subscript>x</subscript></highlight><highlight><superscript>2</superscript></highlight>&plus;m<highlight><subscript>y</subscript></highlight><highlight><superscript>2</superscript></highlight>)}, p<highlight><subscript>r</subscript></highlight>&equals;{square root}{square root over (p<highlight><subscript>x</subscript></highlight><highlight><superscript>2</superscript></highlight>&plus;p<highlight><subscript>y</subscript></highlight><highlight><superscript>2</superscript></highlight>)}, and &thgr; is the angle between the projection of the light ray onto the xy-plane and the x-axis. The reference plane of <cross-reference target="DRAWINGS">FIG. 1</cross-reference>, which lies in the xy-plane, is perpendicular to the central axis of the paraboloidal mirror and goes through the focal point of the parabola. The camera image plane <highlight><bold>25</bold></highlight> is parallel to the aforementioned xy-plane. Since the profile of the mirror corresponds to the parabola  
<math-cwu id="MATH-US-00001">
<number>1</number>
<math>
<mrow>
  <msub>
    <mi>m</mi>
    <mi>z</mi>
  </msub>
  <mo>=</mo>
  <mrow>
    <mfrac>
      <msubsup>
        <mi>m</mi>
        <mi>r</mi>
        <mn>2</mn>
      </msubsup>
      <mrow>
        <mn>2</mn>
        <mo>&it;</mo>
        <mi>r</mi>
      </mrow>
    </mfrac>
    <mo>-</mo>
    <mfrac>
      <mi>r</mi>
      <mn>2</mn>
    </mfrac>
  </mrow>
</mrow>
</math>
<mathematica-file id="MATHEMATICA-00001" file="US20030004694A1-20030102-M00001.NB"/>
<image id="EMI-M00001" wi="216.027" he="18.96615" file="US20030004694A1-20030102-M00001.TIF" imf="TIFF" ti="MF"/>
</math-cwu>
</paragraph>
<paragraph id="P-0022" lvl="7"><number>&lsqb;0022&rsqb;</number> (where r is the measured radius of the mirror in the reference plane), the coordinates m<highlight><subscript>r </subscript></highlight>and m<highlight><subscript>z </subscript></highlight>are easily computed from the projected position I (i<highlight><subscript>x</subscript></highlight>, i<highlight><subscript>y</subscript></highlight>). </paragraph>
<paragraph id="P-0023" lvl="0"><number>&lsqb;0023&rsqb;</number> For applications that depend on pose estimation, the distance d between point P, as projected on the reference plane, and the focal point FP of the parabola of at least two fixed points P may be used to triangulate the camera&apos;s position. This projected distance d may be calculated as  
<math-cwu id="MATH-US-00002">
<number>2</number>
<math>
<mrow>
  <mi>d</mi>
  <mo>=</mo>
  <mrow>
    <mfrac>
      <mrow>
        <mo>(</mo>
        <mrow>
          <msub>
            <mi>p</mi>
            <mi>z</mi>
          </msub>
          <mo>&it;</mo>
          <msub>
            <mi>m</mi>
            <mi>r</mi>
          </msub>
        </mrow>
        <mo>)</mo>
      </mrow>
      <msub>
        <mi>m</mi>
        <mi>z</mi>
      </msub>
    </mfrac>
    <mo>.</mo>
  </mrow>
</mrow>
</math>
<mathematica-file id="MATHEMATICA-00002" file="US20030004694A1-20030102-M00002.NB"/>
<image id="EMI-M00002" wi="216.027" he="18.00225" file="US20030004694A1-20030102-M00002.TIF" imf="TIFF" ti="MF"/>
</math-cwu>
</paragraph>
<paragraph id="P-0024" lvl="0"><number>&lsqb;0024&rsqb;</number> However, due to the unknown perspective projection, mirror misalignment, and radial distortion inherent in most (if not all) paraboloidal catadioptric camera systems, the above method for distance calculation does not yield very accurate results. In fact, in a 5&times;5 meter room using distances, such as d, for triangulating the position of the camera may yield position errors close to 45 centimeters. </paragraph>
<paragraph id="P-0025" lvl="7"><number>&lsqb;0025&rsqb;</number> Paraboloidal Catadioptric Camera Model of the Present Invention </paragraph>
<paragraph id="P-0026" lvl="0"><number>&lsqb;0026&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 2</cross-reference> illustrates a camera model for a paraboloidal catadioptric system according to the present invention, which accounts for undesired perspective projection, mirror misalignment, and radial distortion. Points P, M, and I are equivalent to those illustrated in <cross-reference target="DRAWINGS">FIG. 1</cross-reference>. However, the model of <cross-reference target="DRAWINGS">FIG. 2</cross-reference> differs from the ideal model in that all of the light rays reflected by mirror <highlight><bold>20</bold></highlight> and captured by the camera lens are assumed to converge at a distance H from the mirror&apos;s focal point FP. </paragraph>
<paragraph id="P-0027" lvl="0"><number>&lsqb;0027&rsqb;</number> In the camera model of the present invention, the distance of convergence H is calculated. Also, the closest distance between the mirror <highlight><bold>20</bold></highlight> and the image plane <highlight><bold>25</bold></highlight> is computed, as well as radial distortion and mirror misalignment (e.g., unknown mirror translation and unknown mirror rotation with respect to the mirror reference plane). </paragraph>
<paragraph id="P-0028" lvl="0"><number>&lsqb;0028&rsqb;</number> In addition, because of the undesired perspective projection, mirror <highlight><bold>20</bold></highlight> reflects objects that are slightly behind the reference plane. Accordingly, the present invention determines the subset of the mirror image (i.e., a mirror radius) that exactly reflects a hemispherical FOV in the image plane. </paragraph>
<paragraph id="P-0029" lvl="0"><number>&lsqb;0029&rsqb;</number> According to basic principles of optics, the incident angle of a light ray is equivalent to the reflected angle. Using the notation of <cross-reference target="DRAWINGS">FIG. 1</cross-reference>, this may be expressed as:  
<math-cwu id="MATH-US-00003">
<number>3</number>
<math>
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mfrac>
            <mrow>
              <mi>i</mi>
              <mo>-</mo>
              <mi>m</mi>
            </mrow>
            <mrow>
              <mo>&LeftDoubleBracketingBar;</mo>
              <mrow>
                <mi>i</mi>
                <mo>-</mo>
                <mi>m</mi>
              </mrow>
              <mo>&RightDoubleBracketingBar;</mo>
            </mrow>
          </mfrac>
          <mo>&CenterDot;</mo>
          <mfrac>
            <mover>
              <mi>n</mi>
              <mo>^</mo>
            </mover>
            <mrow>
              <mo>&LeftDoubleBracketingBar;</mo>
              <mover>
                <mi>n</mi>
                <mo>^</mo>
              </mover>
              <mo>&RightDoubleBracketingBar;</mo>
            </mrow>
          </mfrac>
        </mrow>
        <mo>=</mo>
        <mrow>
          <mfrac>
            <mrow>
              <mi>p</mi>
              <mo>-</mo>
              <mi>m</mi>
            </mrow>
            <mrow>
              <mo>&LeftDoubleBracketingBar;</mo>
              <mrow>
                <mi>p</mi>
                <mo>-</mo>
                <mi>m</mi>
              </mrow>
              <mo>&RightDoubleBracketingBar;</mo>
            </mrow>
          </mfrac>
          <mo>&CenterDot;</mo>
          <mfrac>
            <mover>
              <mi>n</mi>
              <mo>^</mo>
            </mover>
            <mrow>
              <mo>&LeftDoubleBracketingBar;</mo>
              <mover>
                <mi>n</mi>
                <mo>^</mo>
              </mover>
              <mo>&RightDoubleBracketingBar;</mo>
            </mrow>
          </mfrac>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mstyle>
        <mtext>Eq.&nbsp;&nbsp;1</mtext>
      </mstyle>
    </mtd>
  </mtr>
</mtable>
</math>
<mathematica-file id="MATHEMATICA-00003" file="US20030004694A1-20030102-M00003.NB"/>
<image id="EMI-M00003" wi="216.027" he="21.12075" file="US20030004694A1-20030102-M00003.TIF" imf="TIFF" ti="MF"/>
</math-cwu>
</paragraph>
<paragraph id="P-0030" lvl="7"><number>&lsqb;0030&rsqb;</number> where {circumflex over (n)} is the surface normal at point M. </paragraph>
<paragraph id="P-0031" lvl="0"><number>&lsqb;0031&rsqb;</number> For a given 3D point P, a measured mirror radius r, and a current estimated convergence distance H, we may rearrange the terms of equation (1): </paragraph>
<paragraph lvl="0"><in-line-formula><highlight><italic>m</italic></highlight><highlight><subscript>r</subscript></highlight><highlight><superscript>5</superscript></highlight><highlight><italic>&minus;p</italic></highlight><highlight><subscript>r</subscript></highlight><highlight><italic>m</italic></highlight><highlight><subscript>r</subscript></highlight><highlight><superscript>4</superscript></highlight>&plus;2<highlight><italic>r</italic></highlight><highlight><superscript>2</superscript></highlight><highlight><italic>m</italic></highlight><highlight><subscript>r</subscript></highlight><highlight><superscript>3</superscript></highlight>&plus;(2<highlight><italic>p</italic></highlight><highlight><subscript>r</subscript></highlight><highlight><italic>rH&minus;</italic></highlight>2<highlight><italic>r</italic></highlight><highlight><superscript>2</superscript></highlight><highlight><italic>p</italic></highlight><highlight><subscript>r</subscript></highlight>)<highlight><italic>m</italic></highlight><highlight><subscript>r</subscript></highlight><highlight><superscript>2</superscript></highlight>&plus;(<highlight><italic>r</italic></highlight><highlight><superscript>4</superscript></highlight>&minus;4<highlight><italic>r</italic></highlight><highlight><superscript>2</superscript></highlight><highlight><italic>p</italic></highlight><highlight><subscript>z</subscript></highlight><highlight><italic>H</italic></highlight>)<highlight><italic>m</italic></highlight><highlight><subscript>r</subscript></highlight>&minus;(<highlight><italic>r</italic></highlight><highlight><superscript>4</superscript></highlight><highlight><italic>p</italic></highlight><highlight><subscript>r</subscript></highlight>&plus;2<highlight><italic>r</italic></highlight><highlight><superscript>3</superscript></highlight><highlight><italic>Hp</italic></highlight><highlight><subscript>r</subscript></highlight>)&equals;0&emsp;&emsp;Eq. 2 </in-line-formula></paragraph>
<paragraph id="P-0032" lvl="0"><number>&lsqb;0032&rsqb;</number> After solving this 5<highlight><superscript>th </superscript></highlight>degree polynomial for m<highlight><subscript>r</subscript></highlight>, and choosing the real solution in the range &lsqb;0, r&rsqb;, m<highlight><subscript>z </subscript></highlight>may be obtained from the parabola  
<math-cwu id="MATH-US-00004">
<number>4</number>
<math>
<mrow>
  <msub>
    <mi>m</mi>
    <mi>z</mi>
  </msub>
  <mo>=</mo>
  <mrow>
    <mfrac>
      <msubsup>
        <mi>m</mi>
        <mi>r</mi>
        <mn>2</mn>
      </msubsup>
      <mrow>
        <mn>2</mn>
        <mo>&it;</mo>
        <mi>r</mi>
      </mrow>
    </mfrac>
    <mo>-</mo>
    <mrow>
      <mfrac>
        <mi>r</mi>
        <mn>2</mn>
      </mfrac>
      <mo>.</mo>
    </mrow>
  </mrow>
</mrow>
</math>
<mathematica-file id="MATHEMATICA-00004" file="US20030004694A1-20030102-M00004.NB"/>
<image id="EMI-M00004" wi="216.027" he="18.96615" file="US20030004694A1-20030102-M00004.TIF" imf="TIFF" ti="MF"/>
</math-cwu>
</paragraph>
<paragraph id="P-0033" lvl="7"><number>&lsqb;0033&rsqb;</number> With this new information, the distance d between point P and the focal point FP of the parabola, as projected on the reference plane, may be more accurately determined and used for pose estimation. When extending the incident light ray (i.e., the ray from point P to the mirror) to intersect the reference plane of the mirror, the light ray overshoots the parabola&apos;s focal point FP by an overshoot distance  
<math-cwu id="MATH-US-00005">
<number>5</number>
<math>
<mrow>
  <mrow>
    <mi>os</mi>
    <mo>=</mo>
    <mrow>
      <mfrac>
        <msub>
          <mi>m</mi>
          <mi>z</mi>
        </msub>
        <mrow>
          <mi>tan</mi>
          <mo>&it;</mo>
          <mrow>
            <mo>(</mo>
            <mi>&alpha;</mi>
            <mo>)</mo>
          </mrow>
        </mrow>
      </mfrac>
      <mo>-</mo>
      <msub>
        <mi>m</mi>
        <mi>r</mi>
      </msub>
    </mrow>
  </mrow>
  <mo>,</mo>
</mrow>
</math>
<mathematica-file id="MATHEMATICA-00005" file="US20030004694A1-20030102-M00005.NB"/>
<image id="EMI-M00005" wi="216.027" he="17.03835" file="US20030004694A1-20030102-M00005.TIF" imf="TIFF" ti="MF"/>
</math-cwu>
</paragraph>
<paragraph id="P-0034" lvl="7"><number>&lsqb;0034&rsqb;</number> where &agr; is the angle of the incident ray with respect to the x-y plane. Using the camera model of the present invention, the distance d may be more accurately computed with equation (3):  
<math-cwu id="MATH-US-00006">
<number>6</number>
<math>
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mi>d</mi>
        <mo>=</mo>
        <mrow>
          <mfrac>
            <mrow>
              <msub>
                <mi>p</mi>
                <mi>z</mi>
              </msub>
              <mo>&it;</mo>
              <msub>
                <mi>m</mi>
                <mi>r</mi>
              </msub>
            </mrow>
            <msub>
              <mi>m</mi>
              <mi>z</mi>
            </msub>
          </mfrac>
          <mo>-</mo>
          <mfrac>
            <msub>
              <mi>m</mi>
              <mi>z</mi>
            </msub>
            <mrow>
              <mi>tan</mi>
              <mo>&af;</mo>
              <mrow>
                <mo>(</mo>
                <mi>&alpha;</mi>
                <mo>)</mo>
              </mrow>
            </mrow>
          </mfrac>
          <mo>+</mo>
          <msub>
            <mi>m</mi>
            <mi>r</mi>
          </msub>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mstyle>
        <mtext>Eq.&nbsp;&nbsp;3</mtext>
      </mstyle>
    </mtd>
  </mtr>
</mtable>
</math>
<mathematica-file id="MATHEMATICA-00006" file="US20030004694A1-20030102-M00006.NB"/>
<image id="EMI-M00006" wi="216.027" he="18.00225" file="US20030004694A1-20030102-M00006.TIF" imf="TIFF" ti="MF"/>
</math-cwu>
</paragraph>
<paragraph id="P-0035" lvl="0"><number>&lsqb;0035&rsqb;</number> The radius r<highlight><subscript>180 </subscript></highlight>of the mirror <highlight><bold>20</bold></highlight> that exactly reflects 180 degrees may be obtained by substituting the relation  
<math-cwu id="MATH-US-00007">
<number>7</number>
<math>
<mrow>
  <msub>
    <mi>p</mi>
    <mi>z</mi>
  </msub>
  <mo>=</mo>
  <mrow>
    <msub>
      <mi>m</mi>
      <mi>z</mi>
    </msub>
    <mo>=</mo>
    <mrow>
      <mfrac>
        <msubsup>
          <mi>m</mi>
          <mi>r</mi>
          <mn>2</mn>
        </msubsup>
        <mrow>
          <mn>2</mn>
          <mo>&it;</mo>
          <mi>r</mi>
        </mrow>
      </mfrac>
      <mo>-</mo>
      <mfrac>
        <mi>r</mi>
        <mn>2</mn>
      </mfrac>
    </mrow>
  </mrow>
</mrow>
</math>
<mathematica-file id="MATHEMATICA-00007" file="US20030004694A1-20030102-M00007.NB"/>
<image id="EMI-M00007" wi="216.027" he="18.96615" file="US20030004694A1-20030102-M00007.TIF" imf="TIFF" ti="MF"/>
</math-cwu>
</paragraph>
<paragraph id="P-0036" lvl="7"><number>&lsqb;0036&rsqb;</number> into equation (2). The polynomial expression for equation (2) may be simplified to equation (4): </paragraph>
<paragraph lvl="0"><in-line-formula><highlight><italic>r</italic></highlight><highlight><subscript>180</subscript></highlight><highlight><italic>&equals;{square root}{square root over (r(<?RTISPC +29?><?RTISPC +E,rad?>(<highlight><italic>H</italic></highlight><highlight><superscript>2</superscript></highlight>&plus;4<highlight><italic>Hr</italic></highlight>))}&minus;</italic></highlight><highlight><italic>H&minus;r</italic></highlight>)&emsp;&emsp;Eq. 4 </in-line-formula></paragraph>
<paragraph id="P-0037" lvl="7"><number>&lsqb;0037&rsqb;</number> Exemplary Configuration of the Camera System </paragraph>
<paragraph id="P-0038" lvl="0"><number>&lsqb;0038&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 5</cross-reference> illustrates an exemplary configuration of the catadioptric camera system <highlight><bold>200</bold></highlight> of the present invention. In an exemplary embodiment, the camera system <highlight><bold>200</bold></highlight> includes a paraboloidal catadioptric omnidirectional camera <highlight><bold>230</bold></highlight>, similar to the commercially available S<highlight><bold>1</bold></highlight> camera of Cyclo Vision/Remote Reality Incorporated, which is based on S. Nayar&apos;s camera design. </paragraph>
<paragraph id="P-0039" lvl="0"><number>&lsqb;0039&rsqb;</number> According to an exemplary embodiment, the omnidirectional camera <highlight><bold>230</bold></highlight> includes a convex paraboloidal mirror <highlight><bold>20</bold></highlight> attached to a transparent acrylic dome <highlight><bold>220</bold></highlight>, which is fastened to video camera <highlight><bold>210</bold></highlight>. The omnidirectional camera <highlight><bold>230</bold></highlight> is supported by support post <highlight><bold>240</bold></highlight>. </paragraph>
<paragraph id="P-0040" lvl="0"><number>&lsqb;0040&rsqb;</number> In a further exemplary embodiment, the camera <highlight><bold>210</bold></highlight> may comprise a 3-CCD color video camera (e.g., JVC KY-F70, 1360&times;1024 progressive pixels at 7.5 Hz), an NTSC-resolution 3-CCD color video camera (e.g., Hitachi HV-D25, 640&times;480 interlaced pixels at 30 Hz), or any other suitable type of still or video camera, as will be readily apparent to those ordinarily skilled in the art. </paragraph>
<paragraph id="P-0041" lvl="7"><number>&lsqb;0041&rsqb;</number> Exemplary Process for Calibrating the Camera System </paragraph>
<paragraph id="P-0042" lvl="0"><number>&lsqb;0042&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 3</cross-reference> is a flowchart illustrating the four main steps for calibrating the catadioptric camera system <highlight><bold>200</bold></highlight>. In step S<highlight><bold>1</bold></highlight>, calibration points are created. For each calibration point, the distance d of the point along with its projected position in image plane <highlight><bold>25</bold></highlight> is recorded (as shown in <cross-reference target="DRAWINGS">FIG. 2</cross-reference>). The mirror radius in the image plane, the projected mirror center in the image plane, and an approximate convergence distance H are measured manually in step S<highlight><bold>2</bold></highlight>. Using these parameters, the calibration points are mapped in step S<highlight><bold>3</bold></highlight> in order to make the catadioptric system <highlight><bold>200</bold></highlight> fit a pinhole camera model (as described below). In step S<highlight><bold>4</bold></highlight>, a best-fit optimization algorithm is performed to optimize the camera model parameters, based on the mapped calibration points. </paragraph>
<paragraph id="P-0043" lvl="7"><number>&lsqb;0043&rsqb;</number> System for Calibrating an Omnidirectional Camera </paragraph>
<paragraph id="P-0044" lvl="0"><number>&lsqb;0044&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 7</cross-reference> is a block diagram of an exemplary system <highlight><bold>300</bold></highlight> implementing the calibration process illustrated in <cross-reference target="DRAWINGS">FIG. 3</cross-reference>. Data storage device <highlight><bold>320</bold></highlight> stores the images recorded by the omnidirectional camera <highlight><bold>230</bold></highlight>. Calibration processing device <highlight><bold>310</bold></highlight> is connected to both the data storage device <highlight><bold>320</bold></highlight> and the omnidirectional camera <highlight><bold>230</bold></highlight>. The calibration processing device <highlight><bold>310</bold></highlight> also includes a user interface <highlight><bold>330</bold></highlight>. </paragraph>
<paragraph id="P-0045" lvl="0"><number>&lsqb;0045&rsqb;</number> It should be noted that the system of <cross-reference target="DRAWINGS">FIG. 7</cross-reference> is merely illustrative, and should in no way be construed as limiting the present invention. For example, in the description given below, the functions ascribed to the calibration processing device <highlight><bold>310</bold></highlight> may be distributed among several different processing devices or platforms. Further, the data storage device <highlight><bold>320</bold></highlight> may be configured as a standalone device, or integrated into either the omnidirectional camera system <highlight><bold>200</bold></highlight> or the calibration processing device <highlight><bold>310</bold></highlight>. The present invention includes any such modifications to the camera calibration system as will be contemplated by those skilled in the art. </paragraph>
<paragraph id="P-0046" lvl="7"><number>&lsqb;0046&rsqb;</number> Collecting Calibration Points (Step S<highlight><bold>1</bold></highlight>) </paragraph>
<paragraph id="P-0047" lvl="0"><number>&lsqb;0047&rsqb;</number> Calibration points are created in step Si by placing the camera system <highlight><bold>200</bold></highlight> in an environment at several positions and capturing images containing points from known 3D locations in the environment. According to an exemplary embodiment, these images are recorded in the data storage device <highlight><bold>320</bold></highlight>. </paragraph>
<paragraph id="P-0048" lvl="0"><number>&lsqb;0048&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 6</cross-reference> illustrates the process for obtaining calibration points according to an exemplary embodiment. Several small and visually distinct features are placed in the environment (e.g., small bright light bulbs), or are chosen from the environment (e.g. corners, points, small objects). The camera system <highlight><bold>200</bold></highlight> is positioned at a plurality of locations <highlight><bold>70</bold></highlight>, which are randomly (and approximately evenly) distributed within region <highlight><bold>100</bold></highlight>. In a further exemplary embodiment, twenty (20) locations are used and four (4) environment features (e.g., small light bulbs) are used, thereby allowing for eighty (80) calibration points to be obtained. </paragraph>
<paragraph id="P-0049" lvl="0"><number>&lsqb;0049&rsqb;</number> For each location <highlight><bold>70</bold></highlight>, the distance d in the reference plane from the focal point of the parabola <highlight><bold>20</bold></highlight> to each of the environment features is measured using, for example, plumb lines and tape measures. The calibration points are obtained by recording, at each location <highlight><bold>70</bold></highlight>, both the measured distance d and the projected location in the image of each environment feature. In an exemplary embodiment, the distances d corresponding to the calibration points captured at a particular location <highlight><bold>70</bold></highlight> may be input at the user interface <highlight><bold>330</bold></highlight>. The calibration processing device <highlight><bold>310</bold></highlight> then correlates and stores these distances d with the corresponding image stored in the data storage device <highlight><bold>320</bold></highlight>. </paragraph>
<paragraph id="P-0050" lvl="7"><number>&lsqb;0050&rsqb;</number> Determining Initial Parameters of the Mirror (Step S<highlight><bold>2</bold></highlight>) </paragraph>
<paragraph id="P-0051" lvl="0"><number>&lsqb;0051&rsqb;</number> According to an exemplary embodiment of the present invention, the following procedure is used to determine an approximate convergence distance H, the projected mirror center in the image plane, and the mirror radius in the image plane, as indicated in step S<highlight><bold>2</bold></highlight>. The reference plane of the mirror <highlight><bold>20</bold></highlight> is placed substantially parallel to the ground plane and at a pre-measured height off the ground. A marker is slowly raised until the reflection of the marker falls off the edge of the mirror. At this point, the marker&apos;s height off the ground and the marker&apos;s distance d, as projected onto the reference plane, from the focal point FP of the parabola is measured. Both the measurements of height and distance d may be used to make a rough approximation of the convergence distance H of the reflected rays. These manual measurements are entered into the calibration processing device <highlight><bold>310</bold></highlight> via the user interface <highlight><bold>330</bold></highlight> in an exemplary embodiment. </paragraph>
<paragraph id="P-0052" lvl="0"><number>&lsqb;0052&rsqb;</number> Next, using a captured image, a circle is fit through multiple approximately evenly sampled mirror-edge pixels of the image (e.g., <highlight><bold>20</bold></highlight> pixels) and may be used to obtain the position of the mirror&apos;s center and the radius of the mirror, both as projected onto the image plane. The determination of the mirror center and radius from the captured image may be performed using the calibration processing device <highlight><bold>310</bold></highlight>. </paragraph>
<paragraph id="P-0053" lvl="7"><number>&lsqb;0053&rsqb;</number> Mapping Calibration Points to fit a Pinhole Camera Model (Step S<highlight><bold>3</bold></highlight>) </paragraph>
<paragraph id="P-0054" lvl="0"><number>&lsqb;0054&rsqb;</number> As indicated above, the calibration points are mapped in step S<highlight><bold>3</bold></highlight> to fit a perspective-based camera model, such as the pinhole camera model. A detailed description of a pinhole camera model can be found in the paper by R. Tsai entitled, &ldquo;A Versatile Camera Calibration Technique for High-Accuracy 3D Machine Vision Metrology Using Off-the-Shelf TV Cameras and Lenses,&rdquo; IEEE Journal on Robotics and Automation, RA-3:4, 1987, pp. 323-344, which is hereby incorporated by reference in its entirety (and included in Appendix A). </paragraph>
<paragraph id="P-0055" lvl="0"><number>&lsqb;0055&rsqb;</number> Similar to Tsai&apos;s pinhole camera model, the camera model of the present invention contains eleven variable parameters, including five internal and six external parameters. The internal parameters include the empirically measured center of the mirror <highlight><bold>20</bold></highlight> on the image plane (x and y parameters). The other internal parameters to be determined during optimization step S<highlight><bold>4</bold></highlight> (<cross-reference target="DRAWINGS">FIG. 4</cross-reference>) include: </paragraph>
<paragraph id="P-0056" lvl="0"><number>&lsqb;0056&rsqb;</number> Focal length (f): the distance between the point of convergence of the reflected rays and the image plane. This parameter is dependent on the convergence angle of the reflected light rays. If these reflected rays were exactly parallel (as assumed for an orthographic projection in the ideal model), the value f would go to infinity. </paragraph>
<paragraph id="P-0057" lvl="0"><number>&lsqb;0057&rsqb;</number> Radial lens distortion coefficient (k): a parameter used to define a first-order approximation to the radial component of lens distortion. </paragraph>
<paragraph id="P-0058" lvl="0"><number>&lsqb;0058&rsqb;</number> Uncertainty scale factor (s<highlight><subscript>x</subscript></highlight>): a scale factor that compensates for a variety of factors (e.g., hardware timing mismatch between camera scanning hardware and image acquisition hardware, imprecision of scanning itself, etc.) that may change the effective size of the pixels in the image plane. </paragraph>
<paragraph id="P-0059" lvl="0"><number>&lsqb;0059&rsqb;</number> The six external parameters of the camera model, which are optimized, are: </paragraph>
<paragraph id="P-0060" lvl="0"><number>&lsqb;0060&rsqb;</number> Camera-to-world translation T&equals;(t<highlight><subscript>x</subscript></highlight>, t<highlight><subscript>y</subscript></highlight>, t<highlight><subscript>z</subscript></highlight>): these parameters represent the offset between the image plane <highlight><bold>25</bold></highlight> and the mirror reference plane; and </paragraph>
<paragraph id="P-0061" lvl="0"><number>&lsqb;0061&rsqb;</number> Camera-to-world rotation R&equals;(r<highlight><subscript>x</subscript></highlight>, r<highlight><subscript>y</subscript></highlight>, r<highlight><subscript>z</subscript></highlight>): these parameters represent the rotation of the mirror reference plane with respect to an assumed coordinate frame of the real world environment. They may also indirectly account for rotation of the mirror <highlight><bold>20</bold></highlight> with respect to the image plane <highlight><bold>25</bold></highlight>. </paragraph>
<paragraph id="P-0062" lvl="0"><number>&lsqb;0062&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 4</cross-reference> illustrates a situation where three calibration points P<highlight><subscript>0</subscript></highlight>, P<highlight><subscript>1</subscript></highlight>, P<highlight><subscript>2 </subscript></highlight>are mapped to points P<highlight><subscript>0</subscript></highlight>&prime;, P<highlight><subscript>1</subscript></highlight>&prime;, P<highlight><subscript>2</subscript></highlight>&prime; so as to fit a pinhole camera model. According to this model, the point at which the rays converge is an effective focal point EF of the catadioptric camera system <highlight><bold>200</bold></highlight>. The focal length f is the distance between the image plane <highlight><bold>25</bold></highlight> and the effective focal point EF. </paragraph>
<paragraph id="P-0063" lvl="0"><number>&lsqb;0063&rsqb;</number> The position of each mapped calibration point is obtained using the equations (2) and (3) above. For example, to obtain P<highlight><subscript>0</subscript></highlight>&prime;, the estimated convergence distance H, mirror radius, and parameters p<highlight><subscript>r </subscript></highlight>and p<highlight><subscript>z </subscript></highlight>of P<highlight><subscript>0 </subscript></highlight>are first substituted into equation (2) to determine the corresponding value of m<highlight><subscript>r</subscript></highlight>. Then, m<highlight><subscript>z </subscript></highlight>is calculated using the parabola  
<math-cwu id="MATH-US-00008">
<number>8</number>
<math>
<mrow>
  <mrow>
    <msub>
      <mi>m</mi>
      <mi>z</mi>
    </msub>
    <mo>=</mo>
    <mrow>
      <mfrac>
        <msubsup>
          <mi>m</mi>
          <mi>r</mi>
          <mn>2</mn>
        </msubsup>
        <mrow>
          <mn>2</mn>
          <mo>&it;</mo>
          <mi>r</mi>
        </mrow>
      </mfrac>
      <mo>-</mo>
      <mfrac>
        <mi>r</mi>
        <mn>2</mn>
      </mfrac>
    </mrow>
  </mrow>
  <mo>,</mo>
</mrow>
</math>
<mathematica-file id="MATHEMATICA-00008" file="US20030004694A1-20030102-M00008.NB"/>
<image id="EMI-M00008" wi="216.027" he="18.96615" file="US20030004694A1-20030102-M00008.TIF" imf="TIFF" ti="MF"/>
</math-cwu>
</paragraph>
<paragraph id="P-0064" lvl="7"><number>&lsqb;0064&rsqb;</number> and the distance d for P<highlight><subscript>0 </subscript></highlight>is obtained from equation (3). The position P<highlight><subscript>0</subscript></highlight>&prime; is then obtained by extending its reflected ray past its reflection point M by a distance equal to the distance from point M to the actual environment feature, namely (p<highlight><subscript>z</subscript></highlight>&minus;m<highlight><subscript>z</subscript></highlight>)/sin(&agr;) (where a is the angle of the ray emanating from P<highlight><subscript>o </subscript></highlight>with the reference plane). The position for the other mapped points P<highlight><subscript>1</subscript></highlight>&prime; and P<highlight><subscript>2</subscript></highlight>&prime; in <cross-reference target="DRAWINGS">FIG. 4</cross-reference> can be obtained using the same technique described for P<highlight><subscript>0</subscript></highlight>&prime;. In an exemplary embodiment, these calculations are performed by the calibration processing device <highlight><bold>310</bold></highlight>. </paragraph>
<paragraph id="P-0065" lvl="7"><number>&lsqb;0065&rsqb;</number> Optimizing the Pinhole Camera Model Parameters (Step S<highlight><bold>4</bold></highlight>) </paragraph>
<paragraph id="P-0066" lvl="0"><number>&lsqb;0066&rsqb;</number> In the pinhole camera model, the mapped position P<highlight><subscript>n</subscript></highlight>&prime; of a calibration point P<highlight><subscript>n </subscript></highlight>is transformed to its projected position I<highlight><subscript>n </subscript></highlight>on the image plane <highlight><bold>25</bold></highlight> based on the nine model parameters optimized in step S<highlight><bold>4</bold></highlight>. The equations describing these transformations, from which these model parameters can be recovered, are well known in the art. Tsai&apos;s aforementioned article, which is included in Appendix A, describes such equations. </paragraph>
<paragraph id="P-0067" lvl="0"><number>&lsqb;0067&rsqb;</number> During the optimization loop of step S<highlight><bold>4</bold></highlight>, which is also performed in the calibration processing device in an exemplary embodiment, the values of the nine internal and external parameters are determined so that the model best fits the measured calibration points. An initial prediction of the model parameter values is made, based in part on the manually determined values in step S<highlight><bold>2</bold></highlight>. These predicted values are plugged into the above-described transformation equations of the pinhole camera model to determine their fitness. If the fitness satisfies pre-determined criteria, then optimization of the parameter values is complete, and processing terminates. Otherwise, the algorithm adjusts the predicted values of the parameters according to the fit, and, accordingly, the convergence distance value H. Using the adjusted value of H, the mapped positions P<highlight><subscript>n</subscript></highlight>&prime; of all of the calibration A points are recalculated. Then, the steps of determining the fitness and readjusting the parameter values are repeated until the convergence criteria is satisfied (see below). </paragraph>
<paragraph id="P-0068" lvl="0"><number>&lsqb;0068&rsqb;</number> The fit may be measured by determining how well the predicted values, which use the model parameters, fit the measured calibration points. Specifically, the method of least squares may be used to calculate the sum of the squared differences between the projected positions of the calibration features, obtained using the model equations, and the projected positions measured manually during step S<highlight><bold>1</bold></highlight>. </paragraph>
<paragraph id="P-0069" lvl="0"><number>&lsqb;0069&rsqb;</number> It should be noted that the convergence distance H and the mapped calibration points P<highlight><subscript>n</subscript></highlight>&prime; need not be recomputed during every pass of the algorithm. It will be readily apparent to those skilled in the art that these values can be altered less frequently, e.g., during every other pass, every fourth pass, etc. </paragraph>
<paragraph id="P-0070" lvl="0"><number>&lsqb;0070&rsqb;</number> Optimization algorithms for best fitting the model parameters to the set of measured calibration points are well known in the art. Such algorithms are provided in Chapter 15, pages 656-706, of Numerical Recipes in C by W. Press, et al., Second Edition, Cambridge University Press, 1999, which is hereby incorporated by reference. Other optimization methods may be used, as will be readily apparent to those ordinarily skilled in the art. </paragraph>
<paragraph id="P-0071" lvl="7"><number>&lsqb;0071&rsqb;</number> Multiple-Pass Calibration Process </paragraph>
<paragraph id="P-0072" lvl="0"><number>&lsqb;0072&rsqb;</number> According to an exemplary embodiment of the present invention, multiple passes of the calibration process in <cross-reference target="DRAWINGS">FIG. 3</cross-reference> may be performed. In the first pass, steps S<highlight><bold>1</bold></highlight>-S<highlight><bold>4</bold></highlight> are performed, as described above. After convergence of the model parameters is obtained, the convergence distance H approximation is altered (using the optimized model parameters). The radius r<highlight><subscript>180 </subscript></highlight>of the mirror <highlight><bold>20</bold></highlight> that exactly reflects 180 degrees is then calculated based on the convergence distance H, using equation (4) above. </paragraph>
<paragraph id="P-0073" lvl="0"><number>&lsqb;0073&rsqb;</number> After the value r<highlight><subscript>180 </subscript></highlight>is computed, the mirror <highlight><bold>20</bold></highlight> and lens hardware settings are conservatively adjusted to maximize the image space used by the portion of the mirror reflecting 180 degrees of the FOV. Then each step of the calibration process in <cross-reference target="DRAWINGS">FIG. 3</cross-reference> is performed again using the reconfigured camera system <highlight><bold>200</bold></highlight>. This process may be repeated several times. </paragraph>
<paragraph id="P-0074" lvl="7"><number>&lsqb;0074&rsqb;</number> The user may control the calibration process using the user interface <highlight><bold>330</bold></highlight>. For example, the user may determine the number of passes associated with the calibration process, instruct the calibration processing device <highlight><bold>310</bold></highlight> to perform the next pass after the mirror <highlight><bold>20</bold></highlight> and lens settings have been adjusted, etc. </paragraph>
<paragraph id="P-0075" lvl="7"><number>&lsqb;0075&rsqb;</number> Applications for the Camera Model </paragraph>
<paragraph id="P-0076" lvl="0"><number>&lsqb;0076&rsqb;</number> An omnidirectional camera system <highlight><bold>200</bold></highlight> calibrated according to the present invention may be used for a variety of applications, including applications in computer vision and computer graphics. In particular, this invention permits accurate camera pose estimation algorithms to be applied to the camera <highlight><bold>200</bold></highlight>. Such algorithms may use the calibrated model to obtain distance estimates between the camera and objects in the environment. These distances may in turn be used to triangulate the camera&apos;s pose, or used in other pose estimation calculations as contemplated by those ordinarily skilled in the art. In an exemplary embodiment, the calibration processing device may be programmed to further perform the camera pose estimation algorithm. Alternatively, pose estimation may be executed using another processing device or system connected to the omnidirectional camera <highlight><bold>230</bold></highlight>. </paragraph>
<paragraph id="P-0077" lvl="0"><number>&lsqb;0077&rsqb;</number> The applications described above are merely illustrative, and neither the present invention, nor any camera utilizing the present invention, should be construed as being limited to any particular type of application. </paragraph>
<paragraph id="P-0078" lvl="0"><number>&lsqb;0078&rsqb;</number> The invention being thus described, it will be apparent that the same may be varied in many ways. Such variations are not to be regarded as a departure from the spirit and scope of the invention, and all such modifications as would be readily apparent to one skilled in the art are intended to be included within the scope of the following claims. 
<image file="US20030004694A1-20030102-P00001.TIF" id="EMI-00001"></image>
<image file="US20030004694A1-20030102-P00002.TIF" id="EMI-00002"></image>
<image file="US20030004694A1-20030102-P00003.TIF" id="EMI-00003"></image>
<image file="US20030004694A1-20030102-P00004.TIF" id="EMI-00004"></image>
<image file="US20030004694A1-20030102-P00005.TIF" id="EMI-00005"></image>
<image file="US20030004694A1-20030102-P00006.TIF" id="EMI-00006"></image>
<image file="US20030004694A1-20030102-P00007.TIF" id="EMI-00007"></image>
<image file="US20030004694A1-20030102-P00008.TIF" id="EMI-00008"></image>
<image file="US20030004694A1-20030102-P00009.TIF" id="EMI-00009"></image>
<image file="US20030004694A1-20030102-P00010.TIF" id="EMI-00010"></image>
<image file="US20030004694A1-20030102-P00011.TIF" id="EMI-00011"></image>
<image file="US20030004694A1-20030102-P00012.TIF" id="EMI-00012"></image>
<image file="US20030004694A1-20030102-P00013.TIF" id="EMI-00013"></image>
<image file="US20030004694A1-20030102-P00014.TIF" id="EMI-00014"></image>
<image file="US20030004694A1-20030102-P00015.TIF" id="EMI-00015"></image>
<image file="US20030004694A1-20030102-P00016.TIF" id="EMI-00016"></image>
<image file="US20030004694A1-20030102-P00017.TIF" id="EMI-00017"></image>
<image file="US20030004694A1-20030102-P00018.TIF" id="EMI-00018"></image>
<image file="US20030004694A1-20030102-P00019.TIF" id="EMI-00019"></image>
<image file="US20030004694A1-20030102-P00020.TIF" id="EMI-00020"></image>
<image file="US20030004694A1-20030102-P00021.TIF" id="EMI-00021"></image>
<image file="US20030004694A1-20030102-P00022.TIF" id="EMI-00022"></image>
<image file="US20030004694A1-20030102-P00023.TIF" id="EMI-00023"></image>
</paragraph>
</section>
</detailed-description>
</subdoc-description>
<subdoc-claims>
<heading lvl="1">We claim: </heading>
<claim id="CLM-00001">
<claim-text><highlight><bold>1</bold></highlight>. A method of calibrating an omnidirectional camera comprising: 
<claim-text>collecting calibration points using the omnidirectional camera; and </claim-text>
<claim-text>determining at least one correction factor based on the plurality of calibration points, each correction factor being used to correct a deviation of the omnidirectional camera from an ideal camera model. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00002">
<claim-text><highlight><bold>2</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein 
<claim-text>the omnidirectional camera records images reflected by an attached convex paraboloidal mirror onto an image plane, and </claim-text>
<claim-text>the determining step determines at least one correction factor that includes at least one of an effective convergence distance of reflected lights rays from the mirror, a camera-to-world translation, a camera-to-world rotation, a radial distortion, and an uncertainty scale factor from a series of calibration points. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00003">
<claim-text><highlight><bold>3</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00002">claim 2</dependent-claim-reference>, wherein the at least one correction factor is used to obtain an effective subset of the mirror reflecting a 360 degree by 180 degree field of view of the environment. </claim-text>
</claim>
<claim id="CLM-00004">
<claim-text><highlight><bold>4</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00002">claim 2</dependent-claim-reference>, wherein the camera-to-world translation represents a lateral offset between the image plane and a reference plane of the mirror. </claim-text>
</claim>
<claim id="CLM-00005">
<claim-text><highlight><bold>5</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00002">claim 2</dependent-claim-reference>, wherein the camera-to-world rotation represents a rotation of a reference plane of the mirror with respect to an assumed coordinate frame of the environment. </claim-text>
</claim>
<claim id="CLM-00006">
<claim-text><highlight><bold>6</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00002">claim 2</dependent-claim-reference>, wherein the effective convergence distance of light rays reflected off the mirror represents a distance in front of the reflective side of the mirror at which a convergence point of the reflected light rays is located. </claim-text>
</claim>
<claim id="CLM-00007">
<claim-text><highlight><bold>7</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00002">claim 2</dependent-claim-reference>, wherein the radial distortion includes at least one coefficient used to define a first-order or higher approximation of the radial component of lens distortion. </claim-text>
</claim>
<claim id="CLM-00008">
<claim-text><highlight><bold>8</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00002">claim 2</dependent-claim-reference>, wherein the uncertainty scale factor includes at least one coefficient used to scale an effective size of pixels in the image plane. </claim-text>
</claim>
<claim id="CLM-00009">
<claim-text><highlight><bold>9</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein the step of collecting calibration points comprises: 
<claim-text>moving the omnidirectional camera to various positions within an environment; </claim-text>
<claim-text>recording an image of the environment with the omnidirectional camera at each of the various positions; and </claim-text>
<claim-text>for each recorded image, 
<claim-text>identifying calibration points in the recorded image, each calibration point having a fixed position within the environment, and </claim-text>
<claim-text>determining an image position of each identified calibration point. </claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010">
<claim-text><highlight><bold>10</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00009">claim 9</dependent-claim-reference>, wherein the calibration points comprise visually distinct features within the environment. </claim-text>
</claim>
<claim id="CLM-00011">
<claim-text><highlight><bold>11</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00002">claim 2</dependent-claim-reference>, wherein the step of determining at least one correction factor comprises: 
<claim-text>modeling the omnidirectional camera using at least one model parameter; </claim-text>
<claim-text>optimizing the at least one model parameter to best-fit the calibration points; and </claim-text>
<claim-text>calculating the at least one correction factor according to the at least one model parameter. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00012">
<claim-text><highlight><bold>12</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00011">claim 11</dependent-claim-reference>, wherein the modeling steps models the omnidirectional camera according to a perspective camera model. </claim-text>
</claim>
<claim id="CLM-00013">
<claim-text><highlight><bold>13</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00011">claim 12</dependent-claim-reference>, wherein the perspective camera model constitutes a pinhole camera model. </claim-text>
</claim>
<claim id="CLM-00014">
<claim-text><highlight><bold>14</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00011">claim 11</dependent-claim-reference>, wherein the optimizing step includes, 
<claim-text>making an initial estimate of the value of each of the at least one model parameter, the at least one model parameter including a distance from the focal point of the mirror from which light rays reflected from the mirror converge, a camera-to-world translation, a camera-to-world rotation, a radial distortion, and an uncertainty scale factor; and </claim-text>
<claim-text>performing an iterative procedure including, 
<claim-text>a) determining a fitness of the estimated values for the at least one model parameter according to predetermined criteria; </claim-text>
<claim-text>b) re-estimating the value of each the at least one model parameter according to the determined fitness when the predetermined criteria is not satisfied; and </claim-text>
<claim-text>c) repeating the above steps until the predetermined criteria is satisfied. </claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00015">
<claim-text><highlight><bold>15</bold></highlight>. A system for calibrating an omnidirectional camera comprising: 
<claim-text>data storage device for storing images recorded using the omnidirectional camera; </claim-text>
<claim-text>extracting means for extracting calibration points from the stored images; and </claim-text>
<claim-text>processing means for determining at least one correction factor based on the plurality of calibration points, each correction factor being used to correct a deviation of the omnidirectional camera from an ideal camera model. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00016">
<claim-text><highlight><bold>16</bold></highlight>. The system of <dependent-claim-reference depends_on="CLM-00011">claim 15</dependent-claim-reference>, wherein 
<claim-text>the omnidirectional camera records images reflected by an attached convex paraboloidal mirror onto an image plane, and </claim-text>
<claim-text>the processing means determines at least one calibration factor that includes at least one of an effective convergence distance of recorded light rays from the mirror, an effective subset of the mirror, a camera-to-world translation, a radial distortion, and an uncertainty factor in the horizontal direction from a series of calibration points. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00017">
<claim-text><highlight><bold>17</bold></highlight>. The system of <dependent-claim-reference depends_on="CLM-00011">claim 16</dependent-claim-reference>, wherein the processing means further includes, 
<claim-text>modeling means for modeling the omnidirectional camera using at least one model parameter; </claim-text>
<claim-text>optimizing means for optimizing the at least one model parameter to best-fit the extracted calibration points; and </claim-text>
<claim-text>calculating means for calculating the at least one correction factor according to the at least one model parameter. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00018">
<claim-text><highlight><bold>18</bold></highlight>. A method for modeling an omnidirectional camera, which is configured to record images reflected by an attached convex paraboloidal mirror onto an image plane, the method comprising: 
<claim-text>defining at least one model parameter including at least one of an effective convergence distance of reflected light rays from the mirror, a camera-to-world translation, a camera-to-world rotation, a radial distortion, and an uncertainty scale factor; </claim-text>
<claim-text>optimizing the at least one model parameter to best-fit a set of calibration points obtained using the omnidirectional camera, the calibration points comprising a set of visually distinct features of fixed location within an environment of the omnidirectional camera. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00019">
<claim-text><highlight><bold>19</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00011">claim 18</dependent-claim-reference>, further comprising, 
<claim-text>using the optimized at least one model parameter to estimate a pose of the omnidirectional camera based on images recorded by the omnidirectional camera.</claim-text>
</claim-text>
</claim>
</subdoc-claims>
<subdoc-drawings id="DRAWINGS">
<heading lvl="0" align="CENTER">Drawings</heading>
<representative-figure>1</representative-figure>
<figure id="figure-D00000">
<image id="EMI-D00000" file="US20030004694A1-20030102-D00000.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00001">
<image id="EMI-D00001" file="US20030004694A1-20030102-D00001.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00002">
<image id="EMI-D00002" file="US20030004694A1-20030102-D00002.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00003">
<image id="EMI-D00003" file="US20030004694A1-20030102-D00003.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00004">
<image id="EMI-D00004" file="US20030004694A1-20030102-D00004.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00005">
<image id="EMI-D00005" file="US20030004694A1-20030102-D00005.TIF" imf="TIFF" ti="DR"/>
</figure>
</subdoc-drawings>
</patent-application-publication>
