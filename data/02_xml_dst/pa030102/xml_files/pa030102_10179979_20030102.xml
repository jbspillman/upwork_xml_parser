<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE patent-application-publication SYSTEM "pap-v16-2002-01-01.dtd" [
<!ENTITY US20030001843A1-20030102-M00001.NB SYSTEM "US20030001843A1-20030102-M00001.NB" NDATA NB>
<!ENTITY US20030001843A1-20030102-M00001.TIF SYSTEM "US20030001843A1-20030102-M00001.TIF" NDATA TIF>
<!ENTITY US20030001843A1-20030102-M00002.NB SYSTEM "US20030001843A1-20030102-M00002.NB" NDATA NB>
<!ENTITY US20030001843A1-20030102-M00002.TIF SYSTEM "US20030001843A1-20030102-M00002.TIF" NDATA TIF>
<!ENTITY US20030001843A1-20030102-M00003.NB SYSTEM "US20030001843A1-20030102-M00003.NB" NDATA NB>
<!ENTITY US20030001843A1-20030102-M00003.TIF SYSTEM "US20030001843A1-20030102-M00003.TIF" NDATA TIF>
<!ENTITY US20030001843A1-20030102-M00004.NB SYSTEM "US20030001843A1-20030102-M00004.NB" NDATA NB>
<!ENTITY US20030001843A1-20030102-M00004.TIF SYSTEM "US20030001843A1-20030102-M00004.TIF" NDATA TIF>
<!ENTITY US20030001843A1-20030102-D00000.TIF SYSTEM "US20030001843A1-20030102-D00000.TIF" NDATA TIF>
<!ENTITY US20030001843A1-20030102-D00001.TIF SYSTEM "US20030001843A1-20030102-D00001.TIF" NDATA TIF>
<!ENTITY US20030001843A1-20030102-D00002.TIF SYSTEM "US20030001843A1-20030102-D00002.TIF" NDATA TIF>
<!ENTITY US20030001843A1-20030102-D00003.TIF SYSTEM "US20030001843A1-20030102-D00003.TIF" NDATA TIF>
<!ENTITY US20030001843A1-20030102-D00004.TIF SYSTEM "US20030001843A1-20030102-D00004.TIF" NDATA TIF>
<!ENTITY US20030001843A1-20030102-D00005.TIF SYSTEM "US20030001843A1-20030102-D00005.TIF" NDATA TIF>
<!ENTITY US20030001843A1-20030102-D00006.TIF SYSTEM "US20030001843A1-20030102-D00006.TIF" NDATA TIF>
<!ENTITY US20030001843A1-20030102-D00007.TIF SYSTEM "US20030001843A1-20030102-D00007.TIF" NDATA TIF>
<!ENTITY US20030001843A1-20030102-D00008.TIF SYSTEM "US20030001843A1-20030102-D00008.TIF" NDATA TIF>
<!ENTITY US20030001843A1-20030102-D00009.TIF SYSTEM "US20030001843A1-20030102-D00009.TIF" NDATA TIF>
<!ENTITY US20030001843A1-20030102-D00010.TIF SYSTEM "US20030001843A1-20030102-D00010.TIF" NDATA TIF>
<!ENTITY US20030001843A1-20030102-D00011.TIF SYSTEM "US20030001843A1-20030102-D00011.TIF" NDATA TIF>
<!ENTITY US20030001843A1-20030102-D00012.TIF SYSTEM "US20030001843A1-20030102-D00012.TIF" NDATA TIF>
<!ENTITY US20030001843A1-20030102-D00013.TIF SYSTEM "US20030001843A1-20030102-D00013.TIF" NDATA TIF>
<!ENTITY US20030001843A1-20030102-D00014.TIF SYSTEM "US20030001843A1-20030102-D00014.TIF" NDATA TIF>
<!ENTITY US20030001843A1-20030102-D00015.TIF SYSTEM "US20030001843A1-20030102-D00015.TIF" NDATA TIF>
<!ENTITY US20030001843A1-20030102-D00016.TIF SYSTEM "US20030001843A1-20030102-D00016.TIF" NDATA TIF>
<!ENTITY US20030001843A1-20030102-D00017.TIF SYSTEM "US20030001843A1-20030102-D00017.TIF" NDATA TIF>
<!ENTITY US20030001843A1-20030102-D00018.TIF SYSTEM "US20030001843A1-20030102-D00018.TIF" NDATA TIF>
<!ENTITY US20030001843A1-20030102-D00019.TIF SYSTEM "US20030001843A1-20030102-D00019.TIF" NDATA TIF>
<!ENTITY US20030001843A1-20030102-D00020.TIF SYSTEM "US20030001843A1-20030102-D00020.TIF" NDATA TIF>
<!ENTITY US20030001843A1-20030102-D00021.TIF SYSTEM "US20030001843A1-20030102-D00021.TIF" NDATA TIF>
<!ENTITY US20030001843A1-20030102-D00022.TIF SYSTEM "US20030001843A1-20030102-D00022.TIF" NDATA TIF>
<!ENTITY US20030001843A1-20030102-D00023.TIF SYSTEM "US20030001843A1-20030102-D00023.TIF" NDATA TIF>
<!ENTITY US20030001843A1-20030102-D00024.TIF SYSTEM "US20030001843A1-20030102-D00024.TIF" NDATA TIF>
<!ENTITY US20030001843A1-20030102-D00025.TIF SYSTEM "US20030001843A1-20030102-D00025.TIF" NDATA TIF>
<!ENTITY US20030001843A1-20030102-D00026.TIF SYSTEM "US20030001843A1-20030102-D00026.TIF" NDATA TIF>
<!ENTITY US20030001843A1-20030102-D00027.TIF SYSTEM "US20030001843A1-20030102-D00027.TIF" NDATA TIF>
]>
<patent-application-publication>
<subdoc-bibliographic-information>
<document-id>
<doc-number>20030001843</doc-number>
<kind-code>A1</kind-code>
<document-date>20030102</document-date>
</document-id>
<publication-filing-type>new</publication-filing-type>
<domestic-filing-data>
<application-number>
<doc-number>10179979</doc-number>
</application-number>
<application-number-series-code>10</application-number-series-code>
<filing-date>20020626</filing-date>
</domestic-filing-data>
<foreign-priority-data>
<priority-application-number>
<doc-number>7-016767</doc-number>
</priority-application-number>
<filing-date>19950203</filing-date>
<country-code>JP</country-code>
</foreign-priority-data>
<foreign-priority-data>
<priority-application-number>
<doc-number>7-016766</doc-number>
</priority-application-number>
<filing-date>19950203</filing-date>
<country-code>JP</country-code>
</foreign-priority-data>
<technical-information>
<classification-ipc>
<classification-ipc-primary>
<ipc>G06T015/50</ipc>
</classification-ipc-primary>
<classification-ipc-edition>07</classification-ipc-edition>
</classification-ipc>
<classification-us>
<classification-us-primary>
<uspc>
<class>345</class>
<subclass>426000</subclass>
</uspc>
</classification-us-primary>
</classification-us>
<title-of-invention>Computer graphics data generating apparatus, computer graphics animation editing apparatus, and animation path generating apparatus</title-of-invention>
</technical-information>
<continuity-data>
<division-of>
<parent-child>
<child>
<document-id>
<doc-number>10179979</doc-number>
<kind-code>A1</kind-code>
<document-date>20020626</document-date>
</document-id>
</child>
<parent>
<document-id>
<doc-number>08597075</doc-number>
<document-date>19960205</document-date>
<country-code>US</country-code>
</document-id>
</parent>
<parent-status>GRANTED</parent-status>
<parent-patent>
<document-id>
<doc-number>6473083</doc-number>
<country-code>US</country-code>
</document-id>
</parent-patent>
</parent-child>
</division-of>
</continuity-data>
<inventors>
<first-named-inventor>
<name>
<given-name>Kaori</given-name>
<family-name>Suzuki</family-name>
</name>
<residence>
<residence-non-us>
<city>Kawasaki-shi</city>
<country-code>JP</country-code>
</residence-non-us>
</residence>
<authority-applicant>INV</authority-applicant>
</first-named-inventor>
<inventor>
<name>
<given-name>Atsuko</given-name>
<family-name>Tada</family-name>
</name>
<residence>
<residence-non-us>
<city>Kawasaka-shi</city>
<country-code>JP</country-code>
</residence-non-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
<inventor>
<name>
<given-name>Hiroshi</given-name>
<family-name>Kamada</family-name>
</name>
<residence>
<residence-non-us>
<city>Kawasaki-shi</city>
<country-code>JP</country-code>
</residence-non-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
<inventor>
<name>
<given-name>Katsuhiko</given-name>
<family-name>Hirota</family-name>
</name>
<residence>
<residence-non-us>
<city>Nagano-shi</city>
<country-code>JP</country-code>
</residence-non-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
<inventor>
<name>
<given-name>Asako</given-name>
<family-name>Yumoto</family-name>
</name>
<residence>
<residence-non-us>
<city>Kawasaki-shi</city>
<country-code>JP</country-code>
</residence-non-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
<inventor>
<name>
<given-name>Satoshi</given-name>
<family-name>Kasai</family-name>
</name>
<residence>
<residence-non-us>
<city>Shizuoka-shi</city>
<country-code>JP</country-code>
</residence-non-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
<inventor>
<name>
<given-name>Kazumi</given-name>
<family-name>Shibata</family-name>
</name>
<residence>
<residence-non-us>
<city>Shizuoka-shi</city>
<country-code>JP</country-code>
</residence-non-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
</inventors>
<assignee>
<organization-name>Fujitsu Limited</organization-name>
<address>
<city>Kawasaki-shi</city>
<country>
<country-code>JP</country-code>
</country>
</address>
<assignee-type>03</assignee-type>
</assignee>
<correspondence-address>
<name-1>STAAS &amp; HALSEY LLP</name-1>
<name-2></name-2>
<address>
<address-1>700 11TH STREET, NW</address-1>
<address-2>SUITE 500</address-2>
<city>WASHINGTON</city>
<state>DC</state>
<postalcode>20001</postalcode>
<country>
<country-code>US</country-code>
</country>
</address>
</correspondence-address>
</subdoc-bibliographic-information>
<subdoc-abstract>
<paragraph id="A-0001" lvl="0">In one aspect of the present invention, a test is made as to whether or not polygons which form objects defined by graphic data generated by CAD intersect with virtually generated light rays, with polygons which are not intersected being removed from the graphic data as unnecessary for the generation of CG data. In another aspect of the present invention, path passage points which pass through an animation path within a virtual world are interactively set, and if an object exists that interferes with a path that joins adjacent path passage points, an alternate route path which detours around the interfering object is generated. In yet another aspect of the present invention, a path of a moving object is interactively set on a perspective view of the virtual world as viewed from one direction, the position of the set path in the virtual world being calculated, and the position of the path in the direction of viewing being corrected to a position that is removed from an object by a prescribed distance. </paragraph>
</subdoc-abstract>
<subdoc-description>
<summary-of-invention>
<section>
<heading lvl="1">BACKGROUND OF THE INVENTION </heading>
<paragraph id="P-0001" lvl="0"><number>&lsqb;0001&rsqb;</number> 1. Field of the Invention </paragraph>
<paragraph id="P-0002" lvl="0"><number>&lsqb;0002&rsqb;</number> The present invention relates to a CG (computer graphics) data generating apparatus which automatically generates from CAD (computer aided design) data of objects CG data (data for generating drawings of computer graphics images) for the objects, a CG animation editing apparatus which uses a CG data generating apparatus to perform high-speed generation of CG animation, and an animation path generating apparatus which generates paths which are used in path animation of computer graphics. </paragraph>
<paragraph id="P-0003" lvl="0"><number>&lsqb;0003&rsqb;</number> 2. Description of Related Art </paragraph>
<paragraph id="P-0004" lvl="0"><number>&lsqb;0004&rsqb;</number> With recent improvements in animation technology for computer graphics, there has been an increase in the use of what is known as walk-through computer graphics, in which the user within a virtual reality world moves about and can easily experience the virtual reality world. Of this application, use is broadening of what is known as scene simulation, in which the manner in which objects in the virtual world are seen is somewhat limited and the viewing point of the user within this world can be freely moved about. To make this type of CG simulation practical, it is necessary to make it easy to generate CG data of real objects. </paragraph>
<paragraph id="P-0005" lvl="0"><number>&lsqb;0005&rsqb;</number> In a CG animation editing apparatus which executes editing processing of CG animation, CG data of an object is generated, and then CG animation of the object is generated based on this data. Because the job of generating the CG data from the beginning is very difficult, and because it is difficult to generate realistic-looking objects, plotting data generated using CAD (computer aided design) has recently been used. Because this CAD plotting data is generated for the purpose of manufacturing, it has many parts which are not required for display, such as plotting data elements (polygons) for parts of the object that cannot be seen, and detailed shapes of objects at a distance. </paragraph>
<paragraph id="P-0006" lvl="0"><number>&lsqb;0006&rsqb;</number> However, the generation of CG animation requires such things as judgments with regard to the intersection of the plotting data and the line of view, calculation of illumination at vertices of the plotting data, and judgments of intersections of light rays with plotting data, for the purpose of applying shadows, the amount of calculations being greater the more numerous are the plotting data elements. Therefore, the use of plotting data generated using CAD as is results in a significant drop in computer graphics drawing speed. </paragraph>
<paragraph id="P-0007" lvl="0"><number>&lsqb;0007&rsqb;</number> When using CAD plotting data in the generation of CG data, it is necessary to remove the unwanted parts from the CAD plotting data. For example, because a box made using 6 sheets is generated in CAD using 36 (6&times;6) square polygons, whereas in computer graphics this can be represented by 6 square polygons that can be seen from the outside, it is necessary to remove the unwanted parts from the CAD plotting data when generating CG data from this CAD plotting data. </paragraph>
<paragraph id="P-0008" lvl="0"><number>&lsqb;0008&rsqb;</number> With regard to this requirement, in the past the practice has been to first use the CAD plotting data as is to make a conversion to CG form, the user then viewing the resulting CG image, predicting which polygons are likely to be unnecessary, and extracting the unnecessary parts from the CAD plotting data. That is, in the past, the method used was that of manual removal by the user of unnecessary parts from the CAD plotting data. </paragraph>
<paragraph id="P-0009" lvl="0"><number>&lsqb;0009&rsqb;</number> However, in the past when polygons unnecessary for CG animation were manually selected and removed using visual observation, the first problems existed with regard to presenting user with an unavoidably large work load. In addition, as new CAD plotting data was added, it was necessary to manually select and remove the unnecessary parts from the newly added plotting data. </paragraph>
<paragraph id="P-0010" lvl="0"><number>&lsqb;0010&rsqb;</number> One important application of computer graphics, as noted above, is the creation of graphics which have an effective impact in presenting a user the situation of moving about within a virtual world created by means of computer graphics. For example, in the field of architecture, computer graphics are used to allow the user to walk through a CG-generated building before it is constructed and to verify harmony with the surrounding environment, and in driver education it is possible for a student to drive through a CG-generated course. These applications have made path animation, in which the viewing point or object is moved through a pre-established path, an important technology. </paragraph>
<paragraph id="P-0011" lvl="0"><number>&lsqb;0011&rsqb;</number> In path animation in the past, the user was made to set the points through which the path passed for each frame (for each time), linear interpolation between the path points being used to set the path of the viewing point or an object. </paragraph>
<paragraph id="P-0012" lvl="0"><number>&lsqb;0012&rsqb;</number> This path point setting must in actuality be done by specifying positions as 3-axis coordinates. In the past, the path point setting method used was that of using three drawing views, the coordinate positions in the forward path of movement being set using a setting screen viewed from the top, and the height-direction coordinate positions being set from a setting screen viewed from the side. For example, in the case of generating a path animation in which a bird is caused to move, the path points are set in consideration of the forward-direction movement and height movement of the bird, so that the bird does not collide with either the ground or buildings. </paragraph>
<paragraph id="P-0013" lvl="0"><number>&lsqb;0013&rsqb;</number> In this manner, in the past the method used was that of having the user manually specify 3-dimensional coordinates to set the points through which the path passes, thus generating the paths of the viewing point or objects. </paragraph>
<paragraph id="P-0014" lvl="0"><number>&lsqb;0014&rsqb;</number> However, in following this type of technology of the past, if an obstacle exists, to avoid this object, because the user must specify a large number of path points in the form of 3-dimensional coordinate positions, not only does the task of performing the setting of path passage points become an extremely troublesome one, but also a large memory capacity is required. </paragraph>
<paragraph id="P-0015" lvl="0"><number>&lsqb;0015&rsqb;</number> With the technology of the past, it is necessary to generate the paths for objects and viewing points beforehand, and because these cannot be dynamically created, if the condition of an obstacle changes, it can be impossible to avoid the obstacle. Additionally, in the case in which a number of objects are moving simultaneously, it is necessary to consider the relationship between the objects. </paragraph>
<paragraph id="P-0016" lvl="0"><number>&lsqb;0016&rsqb;</number> Because of the above problems, in practice in the past, it has been necessary to perform several tests, setting the paths while verifying the relationship between the objects, this making the path generating task extremely time-consuming, which is the second problem involved with the technology of the past. </paragraph>
</section>
<section>
<heading lvl="1">SUMMARY OF THE INVENTION </heading>
<paragraph id="P-0017" lvl="0"><number>&lsqb;0017&rsqb;</number> The present invention was made in view of the above-noted situation, and has as an object the provision of a CG data generating apparatus which is capable of automatically generating CG data for an object from CAD data of the object, and the provision of a CG animation editing apparatus which is capable of using the above-noted CG data generating apparatus to generate CG animation at a high speed. </paragraph>
<paragraph id="P-0018" lvl="0"><number>&lsqb;0018&rsqb;</number> A second object of the present invention is the provision of an animation path generating apparatus which can generate both automatically and dynamically a path which avoids an obstacle. </paragraph>
<paragraph id="P-0019" lvl="0"><number>&lsqb;0019&rsqb;</number> A third object of the present invention is the provision of an animation path generating apparatus which can both automatically and dynamically a final three-dimensional path by just the specification of a planar path. </paragraph>
<paragraph id="P-0020" lvl="0"><number>&lsqb;0020&rsqb;</number> According to the present invention, a CG data generating apparatus is provided which generates CG data for a CG screen from CAD data that is generated by CAD to define a virtual object made up of polygons, this CG data generating apparatus having a light ray generating/intersecting judgment section which virtually generates and aims at an object defined by CD data a plurality of light rays and which makes a judgment as to whether or not each of the light rays intersects with the polygons which make up the object, and a data deleting section which, eliminates, from the CAD data, data with respect to a polygon for which the judgment of the light ray generating/intersecting judgment section is that none of the light rays intersect therewith, thereby generating CG data. </paragraph>
<paragraph id="P-0021" lvl="0"><number>&lsqb;0021&rsqb;</number> According to the present invention, a CG animation editing apparatus is provided which generates CG animation of an object from CAD data that is generated by CAD to define a corresponding virtual object made up of polygons, this CG animation editing apparatus having a light ray generating/intersecting judgment section which virtually generates and aims at an object defined by CD data a plurality of light rays and which makes a judgment as to whether or not each of the light rays intersects with the polygons which make up the object, a data deleting section which eliminates from the CAD data, data with respect to a polygon for which the judgment of the light ray generating/intersecting judgment section is that none of the light rays intersect therewith, thereby generating CG data which is suitable for the generation of a computer graphics image, and a CG animation generating section which generates CG animation by using the CG data generated by the data deleting section. </paragraph>
<paragraph id="P-0022" lvl="0"><number>&lsqb;0022&rsqb;</number> According to the present invention, an animation path generating apparatus is provided, this animation path generating apparatus having means for interactively setting a plurality of points through which a path passes within a virtual space inside which an object exists, means for judging, when a path which joins adjacent path points is set, whether or not there exists an object which interferes, either directly or indirectly, with the path to be set, means for setting a path which joins the above-noted adjacent points when the means for judging the existence of an interfering object judges that such a interfering objects does not exist, and means for automatically setting an alternate route path between the above-noted adjacent points when the means for judging the existence of an interfering object judges that such a interfering objects does exist. </paragraph>
<paragraph id="P-0023" lvl="0"><number>&lsqb;0023&rsqb;</number> According to the present invention, a path animation generating apparatus is provided, this path animation generating apparatus having means for interactively inputting a path of a moving object within a perspective view as seen from a selected direction in a virtual space in which an object exists, means for calculating the position in the virtual space of a path set by the path input means, means for correcting the positions of a path at each time, which are calculated by the position calculation means, to positions which are removed from the object by a prescribed distance along parallel lines which pass through those positions in the above-noted selected direction.</paragraph>
</section>
</summary-of-invention>
<brief-description-of-drawings>
<section>
<heading lvl="1">BRIEF DESCRIPTION OF THE DRAWINGS </heading>
<paragraph id="P-0024" lvl="0"><number>&lsqb;0024&rsqb;</number> The present invention will be more clearly understood from the description as set forth below, with reference being made to the accompanying drawings, wherein </paragraph>
<paragraph id="P-0025" lvl="0"><number>&lsqb;0025&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 1</cross-reference> is a drawing which shows the system configuration of a CG animation editing system according to the present invention; </paragraph>
<paragraph id="P-0026" lvl="0"><number>&lsqb;0026&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 2</cross-reference> is a flowchart which shows an example of the flow of processing in a CG data generating mechanism; </paragraph>
<paragraph id="P-0027" lvl="0"><number>&lsqb;0027&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 3</cross-reference> is a drawing which illustrates the direction of a generated light ray; </paragraph>
<paragraph id="P-0028" lvl="0"><number>&lsqb;0028&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 4</cross-reference> is a drawing which shows the processing to eliminate an unnecessary polygon; </paragraph>
<paragraph id="P-0029" lvl="0"><number>&lsqb;0029&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 5</cross-reference> is a flowchart which shows another example of the flow of processing in a CG data generating mechanism; </paragraph>
<paragraph id="P-0030" lvl="0"><number>&lsqb;0030&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 6</cross-reference> is a drawing which illustrates the direction of a generated light ray; </paragraph>
<paragraph id="P-0031" lvl="0"><number>&lsqb;0031&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 7</cross-reference> is a block diagram which shows the configuration of a CG data generating mechanism; </paragraph>
<paragraph id="P-0032" lvl="0"><number>&lsqb;0032&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 8</cross-reference> is a block diagram which shows the configuration of a CG animation editing apparatus; </paragraph>
<paragraph id="P-0033" lvl="0"><number>&lsqb;0033&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 9</cross-reference> is a apparatus block diagram which shows the configuration of an animation display apparatus according to the present invention; </paragraph>
<paragraph id="P-0034" lvl="0"><number>&lsqb;0034&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 10</cross-reference> is a flowchart which shows the flow of processing in an animation path generating mechanism; </paragraph>
<paragraph id="P-0035" lvl="0"><number>&lsqb;0035&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 11</cross-reference> is a flowchart which shows an example of the flow of processing for avoidance path generation; </paragraph>
<paragraph id="P-0036" lvl="0"><number>&lsqb;0036&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 12</cross-reference> is a drawing which illustrates the processing for avoidance path generation; </paragraph>
<paragraph id="P-0037" lvl="0"><number>&lsqb;0037&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 13</cross-reference> is a flowchart of another example of avoidance path generation processing; </paragraph>
<paragraph id="P-0038" lvl="0"><number>&lsqb;0038&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 14</cross-reference> is a drawing which illustrates the processing for avoidance path generation; </paragraph>
<paragraph id="P-0039" lvl="0"><number>&lsqb;0039&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 15</cross-reference> is a flowchart of another example of avoidance path generation processing; </paragraph>
<paragraph id="P-0040" lvl="0"><number>&lsqb;0040&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 16</cross-reference> is a drawing which illustrates the processing for avoidance path generation; </paragraph>
<paragraph id="P-0041" lvl="0"><number>&lsqb;0041&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 17</cross-reference> is a flowchart of another example of avoidance path generation processing; </paragraph>
<paragraph id="P-0042" lvl="0"><number>&lsqb;0042&rsqb;</number> <cross-reference target="DRAWINGS">FIGS. 18A and 18B</cross-reference> are drawings which illustrate emergence and intake; </paragraph>
<paragraph id="P-0043" lvl="0"><number>&lsqb;0043&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 19</cross-reference> is a drawing which illustrates avoidance path generation processing; </paragraph>
<paragraph id="P-0044" lvl="0"><number>&lsqb;0044&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 20</cross-reference> is a drawing which illustrates the derivation of flow-velocity vectors; </paragraph>
<paragraph id="P-0045" lvl="0"><number>&lsqb;0045&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 21</cross-reference> is a drawing which illustrates an animation path input mechanism; </paragraph>
<paragraph id="P-0046" lvl="0"><number>&lsqb;0046&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 22</cross-reference> is a drawing which illustrates an animation path input mechanism; </paragraph>
<paragraph id="P-0047" lvl="0"><number>&lsqb;0047&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 23</cross-reference> is a drawing which illustrates path information which is stored in an animation data management mechanism; </paragraph>
<paragraph id="P-0048" lvl="0"><number>&lsqb;0048&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 24</cross-reference> is a flowchart which shows the flow of processing in an animation generation mechanism; </paragraph>
<paragraph id="P-0049" lvl="0"><number>&lsqb;0049&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 25</cross-reference> is a flowchart which shows an example of the flow of processing in an animation path generating mechanism; </paragraph>
<paragraph id="P-0050" lvl="0"><number>&lsqb;0050&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 26</cross-reference> is a drawing which illustrates the processing in an animation path generating mechanism; </paragraph>
<paragraph id="P-0051" lvl="0"><number>&lsqb;0051&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 27</cross-reference>A, <cross-reference target="DRAWINGS">FIG. 27</cross-reference>B, and <cross-reference target="DRAWINGS">FIG. 27C</cross-reference> are drawings which illustrate the processing in an animation path generating mechanism; </paragraph>
<paragraph id="P-0052" lvl="0"><number>&lsqb;0052&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 28</cross-reference> is a flowchart which shows another example of the flow of processing in an animation path processing mechanism; </paragraph>
<paragraph id="P-0053" lvl="0"><number>&lsqb;0053&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 29</cross-reference> is a drawing which illustrates another embodiment of the present invention; </paragraph>
<paragraph id="P-0054" lvl="0"><number>&lsqb;0054&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 30</cross-reference> is a drawing which illustrates another embodiment of the present invention; and </paragraph>
<paragraph id="P-0055" lvl="0"><number>&lsqb;0055&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 31</cross-reference>A and <cross-reference target="DRAWINGS">FIG. 31B</cross-reference> are drawings which illustrate another embodiment of the present invention.</paragraph>
</section>
</brief-description-of-drawings>
<detailed-description>
<section>
<heading lvl="1">DESCRIPTION OF PREFERRED EMBODIMENTS </heading>
<paragraph id="P-0056" lvl="0"><number>&lsqb;0056&rsqb;</number> The preferred embodiments of the present invention will be described below in detail, with reference being made to the relevant accompanying drawings. </paragraph>
<paragraph id="P-0057" lvl="0"><number>&lsqb;0057&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 1</cross-reference> is a drawing which shows the system configuration of a CG animation editing system according to an embodiment of the present invention. In this drawing, reference numeral <highlight><bold>30</bold></highlight> denotes a CG animation editing apparatus, <highlight><bold>31</bold></highlight> is a CAD data file into which is stored CAD data for an object that is generated on a CAD system (not shown in the drawing), <highlight><bold>32</bold></highlight> is a CG data file into which is stored CG data for the generation of computer graphics, and <highlight><bold>33</bold></highlight> is a display which displays CG animation that is generated by the CG animation editing apparatus <highlight><bold>30</bold></highlight>. </paragraph>
<paragraph id="P-0058" lvl="0"><number>&lsqb;0058&rsqb;</number> The CG animation editing apparatus <highlight><bold>30</bold></highlight> is implemented by, for example, a personal computer or workstation, and has a CG data generating mechanism <highlight><bold>34</bold></highlight> which generates CG data of an object from CAD data of the object stored in a CAD data file <highlight><bold>31</bold></highlight> and which stores this CG data into a CG data file <highlight><bold>32</bold></highlight>, and a CG animation generating mechanism <highlight><bold>35</bold></highlight> which uses the CG data of an object, which is stored in the CG data file <highlight><bold>32</bold></highlight>, to generate a CG animation and display it on the display <highlight><bold>33</bold></highlight>. </paragraph>
<paragraph id="P-0059" lvl="0"><number>&lsqb;0059&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 2</cross-reference> is a drawing which shows an embodiment of the processing flow which is executed by the CG generating mechanism <highlight><bold>34</bold></highlight> of a CG animation editing apparatus <highlight><bold>30</bold></highlight>. This processing flow will be described below, with regard to the processing for generating CG data according to the present invention. </paragraph>
<paragraph id="P-0060" lvl="0"><number>&lsqb;0060&rsqb;</number> When CG data is generated in accordance with the processing flow shown in <cross-reference target="DRAWINGS">FIG. 2</cross-reference>, the CG data generating mechanism <highlight><bold>34</bold></highlight> first, at step <highlight><bold>1</bold></highlight>, tests whether or not the processing with respect to all objects stored in the CAD data file <highlight><bold>31</bold></highlight> has been completed. If the results of this test are that the processing for all objects has been completed, all processing is ended. If, however, the judgment is made that unprocessed objects remain, control proceeds to step <highlight><bold>2</bold></highlight>, at which one of the remaining unprocessed objects is selected as an object to be processed, all the polygons that this object has being specified as polygons to be tested. </paragraph>
<paragraph id="P-0061" lvl="0"><number>&lsqb;0061&rsqb;</number> Next, at step <highlight><bold>3</bold></highlight>, a test is made to determine whether or not the number of light rays that have already been generated has reached a specified number. If the judgment is that the specified number has not yet been reached, one more virtual light ray is generated from outside the object pointing toward the inside of the object. The form of the generation of this light ray is implemented, as shown in <cross-reference target="DRAWINGS">FIG. 3</cross-reference>, by successively generating light rays which are parallel to the &plusmn;XYZ axes in the object&apos;s modeling coordinate system (a separate system for each object), the number of light rays being generated in this manner being established, for example, in accordance with the cross-sectional area of the object in that direction. Because information with regard to the bounding box of the object is stored in the CAD data, it is possible to determine the cross-sectional area by using this information. </paragraph>
<paragraph id="P-0062" lvl="0"><number>&lsqb;0062&rsqb;</number> When a light ray aimed at the inside of the object from outside the object is generated at step <highlight><bold>3</bold></highlight>, following this at step <highlight><bold>4</bold></highlight> a test is made as to whether or not there is a remaining polygon to be examined. If the judgment is that there is no remaining polygon, control returns to step <highlight><bold>3</bold></highlight>. If, however, the judgment is that there is a remaining polygon, control proceeds to step <highlight><bold>5</bold></highlight>, at which point a test is made as to whether or not the remaining polygon to be tested and the light ray generated at step <highlight><bold>3</bold></highlight> intersect. </paragraph>
<paragraph id="P-0063" lvl="0"><number>&lsqb;0063&rsqb;</number> At step <highlight><bold>5</bold></highlight>, if the condition in which there is no remaining intersecting polygon is detected, control returns to step <highlight><bold>3</bold></highlight>, but if the existence of an intersecting polygon is detected, control proceeds to step <highlight><bold>6</bold></highlight>, after which the intersecting polygon is removed from the polygons to be examined and control returns to step <highlight><bold>3</bold></highlight>. </paragraph>
<paragraph id="P-0064" lvl="0"><number>&lsqb;0064&rsqb;</number> At step <highlight><bold>3</bold></highlight>, if the judgment is made that the number of already generated light rays has reached a preset number (including the case in which the judgment is made at step <highlight><bold>4</bold></highlight> that there is no remaining polygon to be examined), control proceeds to step <highlight><bold>7</bold></highlight>, at which the polygons which remain as polygone which do not intersect with light rays are removed from the CAD data, the data of the remaining polygons being stored in the CG data file <highlight><bold>32</bold></highlight>, after which control returns to step <highlight><bold>1</bold></highlight> in order to perform the processing of the next object. </paragraph>
<paragraph id="P-0065" lvl="0"><number>&lsqb;0065&rsqb;</number> The test at step <highlight><bold>3</bold></highlight> as to whether or not the light ray intersects with a polygon can be performed, for example, in the following manner. The position vector r represents the axis of the light ray, which is represented as a function of the position vector k<highlight><subscript>O </subscript></highlight>of the initial point, the direction vector k, and the parameter t, as follows.</paragraph>
<paragraph lvl="0"><in-line-formula><highlight><italic>r&equals;kt&plus;k</italic></highlight><highlight><subscript>O</subscript></highlight>&emsp;&emsp;(1)</in-line-formula></paragraph>
<paragraph id="P-0066" lvl="0"><number>&lsqb;0066&rsqb;</number> The parameter values of t at the intersection points of the ray and the planes of each polygon are calculated. Next, a test is made to determine whether or not that intersection point is inside each of the polygons is performed, with the points lying outside the polygon being discarded. If one remaining intersection point exists, a polygon which includes that intersection point is the polygon which intersects with the light ray and, in the case in which a plurality of intersection points remain, the polygon which includes the intersection which has the minimum value of t of these is the polygon which intersects with the light ray. At step <highlight><bold>6</bold></highlight>, even if an intersected polygon is removed from the polygons to be examined, the graphic data for the purpose of performing the examination of intersection is, of course, kept. </paragraph>
<paragraph id="P-0067" lvl="0"><number>&lsqb;0067&rsqb;</number> In this manner, by executing the processing flow shown in <cross-reference target="DRAWINGS">FIG. 2</cross-reference>, the CG data generating mechanism <highlight><bold>34</bold></highlight> generates, with respect to CAD data of an object, a plurality of virtual light rays aimed at the inside of the object from outside the object, thereby identifying and eliminating polygons inside the object which are not in the field of view, in the process of automatically generating CG data from CAD data. </paragraph>
<paragraph id="P-0068" lvl="0"><number>&lsqb;0068&rsqb;</number> For example, as shown in <cross-reference target="DRAWINGS">FIG. 4</cross-reference>, assume the case in which an object exists which is formed by joining together two sheets. Because this object must be represented in CAD data as being formed by joining together two sheets, it is made up of 12 square polygons, whereas as CG data, the two square polygons making contact at the central part are not required. In a case such as this, the CG data generating mechanism <highlight><bold>34</bold></highlight> executes the processing flow which is shown in <cross-reference target="DRAWINGS">FIG. 2</cross-reference>, thereby automatically deleting the two square polygons which do not enter the field of view as CG data is generated. </paragraph>
<paragraph id="P-0069" lvl="0"><number>&lsqb;0069&rsqb;</number> While in the processing flow which is shown in <cross-reference target="DRAWINGS">FIG. 2</cross-reference>, this assumption is that an object is viewed only from outside, there is a possibility, depending upon the object, that the object will be viewed from within, or be viewed from both outside and within the object. </paragraph>
<paragraph id="P-0070" lvl="0"><number>&lsqb;0070&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 5</cross-reference> shows an embodiment of the processing flow executed by the CG data generating mechanism <highlight><bold>34</bold></highlight> in such cases. The CG data generation processing according to the present invention will be described below in detail, according to this processing flow. </paragraph>
<paragraph id="P-0071" lvl="0"><number>&lsqb;0071&rsqb;</number> At step <highlight><bold>1</bold></highlight>, a test is made to determine whether the processing for all objects stored in the CAD data file <highlight><bold>31</bold></highlight> has been completed. If the judgment is that processing for all the objects has been completed, all processing is ended. If, however, the judgment is that unprocessed objects remain, control proceeds to step <highlight><bold>2</bold></highlight>, at which one of the remaining unprocessed objects is selected as an object to be processed, all the polygons that this object has being specified as polygons to be examined. </paragraph>
<paragraph id="P-0072" lvl="0"><number>&lsqb;0072&rsqb;</number> Next, at step <highlight><bold>3</bold></highlight>, a test is made to determine whether or not the object which was selected at step <highlight><bold>2</bold></highlight> is an object which is accessed from outside. If the judgment is that the object is an object that is accessed from the outside, control proceeds to step <highlight><bold>4</bold></highlight>, at which a test is made to determine whether or not the number of light rays aimed at the inside of the object which have already been generated has reached a specified number. If the judgment is that the specified number has not yet been reached, one more virtual light ray is generated from outside the object pointing toward the inside of the object. For example, as shown in <cross-reference target="DRAWINGS">FIG. 3, a</cross-reference> light ray is generated which is parallel to the &plusmn;XYZ axes of the modeling coordinate system of the object. </paragraph>
<paragraph id="P-0073" lvl="0"><number>&lsqb;0073&rsqb;</number> After a light ray is generated which is aimed at the inside of the object from outside at step <highlight><bold>4</bold></highlight>, at step <highlight><bold>5</bold></highlight> a test is made as to whether or not a polygon to be examined remains. If the judgment is that no such polygon remains, control returns to step <highlight><bold>4</bold></highlight>, but if the judgment is that there is such a polygon remaining, control proceeds to step <highlight><bold>6</bold></highlight>, at which a check is made to determine whether or not the remaining polygons to be examined and the light ray which was generated at step <highlight><bold>4</bold></highlight> intersect. </paragraph>
<paragraph id="P-0074" lvl="0"><number>&lsqb;0074&rsqb;</number> If the condition in which no intersecting polygon exists is detected at step <highlight><bold>6</bold></highlight>, control returns to step <highlight><bold>4</bold></highlight>, but if the condition in which an intersecting polygon exists is detected, control proceeds to step <highlight><bold>7</bold></highlight>, at which the intersecting polygon is removed from the polygons to be examined, after which control returns to step <highlight><bold>4</bold></highlight>. </paragraph>
<paragraph id="P-0075" lvl="0"><number>&lsqb;0075&rsqb;</number> If at step <highlight><bold>4</bold></highlight> the judgment is made that the number of light rays already generated which are aimed at the inside of the object has reached the specified number (including the condition in which at step <highlight><bold>5</bold></highlight> a judgment is made that there is no remaining polygon to be examined), control proceeds to step <highlight><bold>8</bold></highlight>, at which point a test is made to determine whether or not the object which was selected at step <highlight><bold>2</bold></highlight> is one that is accessed from within. </paragraph>
<paragraph id="P-0076" lvl="0"><number>&lsqb;0076&rsqb;</number> At this step <highlight><bold>8</bold></highlight>, if the judgment is made that the object is not an object which is accessed from within, control proceeds to step <highlight><bold>14</bold></highlight>, at which the polygons which remain as polygons which do not intersect with the generated light ray are removed from the CAD data of the object, after which control returns to step <highlight><bold>1</bold></highlight> for the purpose of generating CG data for the next object. That is, in the case of passing through this route, because the object selected at step <highlight><bold>2</bold></highlight> is one which is accessed from the outside only, the CG data for the case of viewing the object from the outside of the object is generated and stored in the CG data file <highlight><bold>32</bold></highlight>. </paragraph>
<paragraph id="P-0077" lvl="0"><number>&lsqb;0077&rsqb;</number> If, however, the judgment is made at step <highlight><bold>8</bold></highlight> that the object is one which is accessed from within, control proceeds to step <highlight><bold>9</bold></highlight>, at which the setting is made as to the position within the object from which it is viewed. The processing for this setting is made, for example, by generating a wire-frame CG animation (colorless CG animation) from the CAD data for the object, with the user making the setting by entering the object in accordance with that CG animation, it being possible to set not just one, but a plurality of positions. </paragraph>
<paragraph id="P-0078" lvl="0"><number>&lsqb;0078&rsqb;</number> Next, at step <highlight><bold>10</bold></highlight>, a test is made to determine whether or not the light rays already generated which aim at the outside of the object have reached a specified number. If the judgment is the specified number has not been reached, one new light ray is generated, with respect to the CAD data of the object, from the internal point set at step <highlight><bold>9</bold></highlight> as the point of generation. One form of generating this light ray is to generate the light ray so that the light rays generated are as uniform as possible in the space from the point of generation set at step <highlight><bold>9</bold></highlight>, for example as shown in <cross-reference target="DRAWINGS">FIG. 6</cross-reference>, by generating them radially from the point of generation. </paragraph>
<paragraph id="P-0079" lvl="0"><number>&lsqb;0079&rsqb;</number> After the light ray aimed toward the outside of the object from within is generated at step <highlight><bold>10</bold></highlight>, at step <highlight><bold>11</bold></highlight> a test is made to determine whether or not a polygon to be examined remains. If the judgment is that no polygon to be examined remained, control returns to step <highlight><bold>10</bold></highlight>, but if the judgment is that a polygon to be examined remains, control proceeds to step <highlight><bold>12</bold></highlight>, at which a check is made as to whether or not the remaining polygon to be examined and the light ray generated at step <highlight><bold>15</bold></highlight> intersect. </paragraph>
<paragraph id="P-0080" lvl="0"><number>&lsqb;0080&rsqb;</number> If the condition in which no intersecting polygon exists is detected at step <highlight><bold>12</bold></highlight>, control returns to step <highlight><bold>10</bold></highlight>, but if the condition in which an intersecting polygon exists is detected, control proceeds to step <highlight><bold>13</bold></highlight>, at which the intersecting polygon is removed from the polygons to be examined, after which control returns to step <highlight><bold>15</bold></highlight>. </paragraph>
<paragraph id="P-0081" lvl="0"><number>&lsqb;0081&rsqb;</number> If at step <highlight><bold>10</bold></highlight> the judgment is that the number of light rays already generated which are aimed at the inside of the object has reached the specified number (including the condition in which at step <highlight><bold>11</bold></highlight> a judgment is made that there is no remaining polygon to be examined), control proceeds to step <highlight><bold>14</bold></highlight>, at which the polygons which remain as polygons which do not intersect with the generated light ray are removed from the CAD data of the object, thereby generating the CG data for the object selected at step <highlight><bold>2</bold></highlight> and storing this CG data in the CG data file <highlight><bold>32</bold></highlight>, after which control returns to step <highlight><bold>1</bold></highlight> for the purpose of generating the CG data for the next object. That is, in the case of passing through this route, because the object selected at step <highlight><bold>2</bold></highlight> is one which is accessed from both the outside and within, the CG data which is usable for the case of viewing the object from both the outside and within is generated and stored in the CG data file <highlight><bold>32</bold></highlight>. </paragraph>
<paragraph id="P-0082" lvl="0"><number>&lsqb;0082&rsqb;</number> If, however, the judgment is made that the object selected at step <highlight><bold>2</bold></highlight> is not an object that is accessed from outside, rather than executing the processing of steps <highlight><bold>4</bold></highlight> through <highlight><bold>7</bold></highlight>, control proceeds to step <highlight><bold>8</bold></highlight> directly, the processing of steps <highlight><bold>9</bold></highlight> through <highlight><bold>14</bold></highlight> being executed, thereby generating the CG data for the case of viewing this object from within and storing this CG data in the CG data file <highlight><bold>32</bold></highlight>, after which control returns to step <highlight><bold>1</bold></highlight>, for the purpose of generating the CG data for the next object. That is, in the case of passing through this route, because the object selected at step <highlight><bold>2</bold></highlight> is one which is accessed only from within, the CG data which is usable for the case of viewing the object from within is generated and stored in the CG data file <highlight><bold>32</bold></highlight>. </paragraph>
<paragraph id="P-0083" lvl="0"><number>&lsqb;0083&rsqb;</number> In this manner, by executing the processing flow shown in <cross-reference target="DRAWINGS">FIG. 5</cross-reference>, the CG data generating mechanism <highlight><bold>34</bold></highlight> generates, with respect to CAD data of an object which is viewed only from outside, a plurality of virtual light rays aimed at the inside of the object from outside the object, thereby identifying and eliminating polygons inside the object which are not in the field of view in the process of automatically generating CG data, and with respect to CAD data of an object that is viewed only from within, the CG data generating mechanism <highlight><bold>34</bold></highlight> generates a plurality of virtual light rays which are aimed at the outside of the object from inside the object, thereby identifying and eliminating polygons inside the object which are not in the field of view in the process of automatically generating CG data. Further, in the case of an object that is viewed from both outside and within, the CG data generating mechanism <highlight><bold>34</bold></highlight> executes the above two processings in series, thereby identifying and eliminating polygons which are not only not in the field of view as viewed from outside, but also not in the field of view when viewed from inside, in the process of automatically generating CG data. </paragraph>
<paragraph id="P-0084" lvl="0"><number>&lsqb;0084&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 7</cross-reference> shows the block diagram configuration of a CG data generating mechanism <highlight><bold>34</bold></highlight> for the case in which processing follows the processing flow which is shown in <cross-reference target="DRAWINGS">FIG. 5</cross-reference>. </paragraph>
<paragraph id="P-0085" lvl="0"><number>&lsqb;0085&rsqb;</number> As shown in this drawing, in the case in which processing follows the processing flow of <cross-reference target="DRAWINGS">FIG. 5</cross-reference>, the CG data generating mechanism is formed by an object viewing setting section <highlight><bold>40</bold></highlight>, a generated ray testing section <highlight><bold>41</bold></highlight>, an inwardly directed ray generation/intersection testing section <highlight><bold>42</bold></highlight>, an outwardly directed ray generation/intersection testing section <highlight><bold>43</bold></highlight>, and an unnecessary polygon deleting section <highlight><bold>44</bold></highlight>. </paragraph>
<paragraph id="P-0086" lvl="0"><number>&lsqb;0086&rsqb;</number> By way of a simplified explanation of the functions of the above elements, the object viewing setting section <highlight><bold>40</bold></highlight> performs setting of the class of viewing of an object which is stored in the CAD data file <highlight><bold>31</bold></highlight> as an object that the user views from the outside only, views from outside and from within, or views from the inside only, in accordance with the CG walk-through scenario. For an object which is viewed from the within, processing is also performed for positions from which the object is viewed. </paragraph>
<paragraph id="P-0087" lvl="0"><number>&lsqb;0087&rsqb;</number> The generated ray testing section <highlight><bold>41</bold></highlight> performs processing which assigns the processing of inwardly directed ray generation/intersection testing section <highlight><bold>42</bold></highlight> for objects which are viewed from outside only, assigns the processing of outwardly direct ray generation/intersection testing section <highlight><bold>43</bold></highlight> for objects which are viewed from inside only, and assigns the processing of both the inwardly directed ray generation/intersection testing section <highlight><bold>42</bold></highlight> and the outwardly direct ray generation/intersection testing section <highlight><bold>43</bold></highlight> for objects which are viewed from both outside and inside. </paragraph>
<paragraph id="P-0088" lvl="0"><number>&lsqb;0088&rsqb;</number> The inwardly directed ray generation/intersection testing section <highlight><bold>42</bold></highlight> causes an appropriate number of rays to be generated in the six directions of front, back, up, down, left, and right, aimed from the outside of an object to the inside, and performs testing of intersection with all polygons. The outwardly directed ray generation/intersection testing section <highlight><bold>43</bold></highlight> causes an appropriate number of rays to be generated, aimed at the outside from the inside of the object, and performs testing of intersection with all polygons. </paragraph>
<paragraph id="P-0089" lvl="0"><number>&lsqb;0089&rsqb;</number> The unnecessary polygon deleting section <highlight><bold>44</bold></highlight> generates CG data by removing from the CAD data polygons which have been judged to be non-intersected by the inwardly directed ray generation/intersection testing section <highlight><bold>42</bold></highlight> and the outwardly direct ray generation/intersection testing section <highlight><bold>43</bold></highlight>. </paragraph>
<paragraph id="P-0090" lvl="0"><number>&lsqb;0090&rsqb;</number> When the CG data generating mechanism <highlight><bold>34</bold></highlight> generates CG data in this manner from the CAD data of an object and stores this CG data in the CG data file <highlight><bold>32</bold></highlight>, the CG animation generating mechanism <highlight><bold>35</bold></highlight> of the CG animation editing apparatus <highlight><bold>30</bold></highlight> uses this CG data in performing processing to generate a CG animation and display it on the display <highlight><bold>33</bold></highlight>. That is, the CG animation generating mechanism <highlight><bold>35</bold></highlight>, as will be described later, uses the method of generating a perspective view of a virtual world as viewed when moving along an animation path, and of presenting this on the display <highlight><bold>33</bold></highlight>. </paragraph>
<paragraph id="P-0091" lvl="0"><number>&lsqb;0091&rsqb;</number> In this CG animation generation processing, the processing of objects which are viewed from both outside and inside presents a problem. </paragraph>
<paragraph id="P-0092" lvl="0"><number>&lsqb;0092&rsqb;</number> In such as case, in the above example, the configuration is such that only polygons which can be seen from neither outside nor inside are removed. However, in doing this because when viewing from outside polygons only viewable from inside are unnecessary and when viewing from inside polygons viewable only from outside are unnecessary, the speed of plotting is reduced. </paragraph>
<paragraph id="P-0093" lvl="0"><number>&lsqb;0093&rsqb;</number> Because of this problem, in an embodiment of the present invention, the method adopted is that of using a configuration in which, for an object which can be seen from both outside and inside, CG data for the case of the object viewed from outside which is determined by the processing performed by the inwardly directed ray generation/intersection testing section <highlight><bold>42</bold></highlight> and the unnecessary polygon deleting section <highlight><bold>44</bold></highlight>, and the CG data for the case of the object viewed from the inside which is determined by the processing performed by the outwardly directed ray generation/intersection testing section <highlight><bold>43</bold></highlight> and the unnecessary polygon deleting section <highlight><bold>44</bold></highlight> are prepared separately, and when changing the viewing point from outside to inside the object or from inside to outside the object, the CG data used is dynamically switched as the CG animation is being generated. </paragraph>
<paragraph id="P-0094" lvl="0"><number>&lsqb;0094&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 8</cross-reference> shows the block diagram configuration of CG animation editing apparatus <highlight><bold>30</bold></highlight> in the case of following the above-noted configuration. In this drawing, previously described elements have been assigned the same reference numerals as used previously. </paragraph>
<paragraph id="P-0095" lvl="0"><number>&lsqb;0095&rsqb;</number> The outside-viewing data setting section <highlight><bold>45</bold></highlight> shown in this drawing sets CG data of an object which is determined by the processing in the inwardly directed ray generation/testing section <highlight><bold>42</bold></highlight> and the unnecessary polygon deleting section <highlight><bold>44</bold></highlight> as the CG data to be used when the viewing point is located outside the object. The inside-viewing data setting section <highlight><bold>46</bold></highlight> shown in this drawing sets CG data of an object which is determined by the processing in the outwardly directed ray generation/testing section <highlight><bold>43</bold></highlight> and the unnecessary polygon deleting section <highlight><bold>44</bold></highlight> as the CG data to be used when the viewing point is located inside the object. </paragraph>
<paragraph id="P-0096" lvl="0"><number>&lsqb;0096&rsqb;</number> The viewing point testing section <highlight><bold>47</bold></highlight> performs a test of whether or not, during a walk-through, there is an intersection between the viewing point and an object that is viewed from both outside and inside, thereby determining whether the walk-through is outside or inside the object, so that, for example, if the walk-through is outside and the viewing point and object intersect, the judgment is that the viewing point has entered the inside of the object. </paragraph>
<paragraph id="P-0097" lvl="0"><number>&lsqb;0097&rsqb;</number> When the viewing point testing section <highlight><bold>47</bold></highlight> determines that the viewing point has moved from inside an object to outside the object, the shape data interchanging section <highlight><bold>48</bold></highlight> changes the CG data to be used in generating the CG animation to the CG data which is set by the outside-viewing data setting section <highlight><bold>45</bold></highlight>, and when the viewing point testing section <highlight><bold>47</bold></highlight> determines that the viewing point has moved from outside an object to inside the object, the shape data interchanging section <highlight><bold>48</bold></highlight> changes the CG data to be used in generating the CG animation to the CG data which is set by the inside-viewing data setting section <highlight><bold>46</bold></highlight>. </paragraph>
<paragraph id="P-0098" lvl="0"><number>&lsqb;0098&rsqb;</number> By using the above-noted configuration, it is possible to generate CG animation using CG data having fewer polygons than in the past and in the previously described embodiment. </paragraph>
<paragraph id="P-0099" lvl="0"><number>&lsqb;0099&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 9</cross-reference> shows the configuration of a second embodiment of an animation display apparatus according to the present invention. In this drawing, the reference numeral <highlight><bold>50</bold></highlight> denotes an animation display apparatus according to the present invention, which presents a virtual world by means of computer graphics, <highlight><bold>52</bold></highlight> is a terminal which the animation display apparatus <highlight><bold>50</bold></highlight> has, which displays a virtual world and serves as a means for interaction with a user. </paragraph>
<paragraph id="P-0100" lvl="0"><number>&lsqb;0100&rsqb;</number> This animation display apparatus <highlight><bold>50</bold></highlight> has an animation data management mechanism <highlight><bold>54</bold></highlight>, an animation path input mechanism <highlight><bold>56</bold></highlight>, an animation path generating mechanism <highlight><bold>58</bold></highlight>, an animation generating mechanism <highlight><bold>60</bold></highlight>, a display data storage mechanism <highlight><bold>62</bold></highlight>, and an animation plotting mechanism <highlight><bold>64</bold></highlight>. </paragraph>
<paragraph id="P-0101" lvl="0"><number>&lsqb;0101&rsqb;</number> The animation data management mechanism <highlight><bold>54</bold></highlight> manages object data such as the shapes and textures of each object which makes up-the virtual world, animation data which represents temporal changes in each of the objects, and information which is required for generation of the animation path. The animation path input mechanism <highlight><bold>56</bold></highlight> generates, interactively with a user, information which is required in the generation of an animation path, and stores this information in the animation data management mechanism <highlight><bold>54</bold></highlight>. </paragraph>
<paragraph id="P-0102" lvl="0"><number>&lsqb;0102&rsqb;</number> The animation path generating mechanism <highlight><bold>58</bold></highlight> is included within the animation generating mechanism <highlight><bold>60</bold></highlight>, and uses the information which is required for generation of an animation path which is managed by the animation data management mechanism <highlight><bold>54</bold></highlight> to generate an animation path which is used in an animation. The animation generating mechanism <highlight><bold>60</bold></highlight> uses the animation path which is generated by the animation path generating mechanism <highlight><bold>58</bold></highlight> and the data managed by the animation data management mechanism to generate an animation by calculating the positions and condition of an object at each time. </paragraph>
<paragraph id="P-0103" lvl="0"><number>&lsqb;0103&rsqb;</number> The display data storage mechanism <highlight><bold>62</bold></highlight> has stored in it the animation display data which is generated by the animation generating mechanism <highlight><bold>60</bold></highlight>. The animation plotting mechanism <highlight><bold>64</bold></highlight> receives plotting instructions from the animation generating mechanism <highlight><bold>60</bold></highlight>, reads out display data from the display data storage mechanism <highlight><bold>62</bold></highlight>, and displays it on a display screen, thereby plotting the animation. </paragraph>
<paragraph id="P-0104" lvl="0"><number>&lsqb;0104&rsqb;</number> Next, the operation of a first example of an animation display mechanism <highlight><bold>50</bold></highlight> configured as described above will be described. </paragraph>
<paragraph id="P-0105" lvl="0"><number>&lsqb;0105&rsqb;</number> In the first example, the animation path input mechanism <highlight><bold>56</bold></highlight> performs interaction with a user to set the points though which the path passes as 3-dimensional coordinate positions, these being stored as is in the animation data management mechanism <highlight><bold>54</bold></highlight>. The processing to input these path points is performed no differently than in the previous technology which is described above. However, when an interfering object exists along the animation path, the configuration is such that the interfering object is automatically avoided, enabling the user to input the points through which the path passes without the difficulty encountered in the prior art. </paragraph>
<paragraph id="P-0106" lvl="0"><number>&lsqb;0106&rsqb;</number> The animation path generating mechanism <highlight><bold>58</bold></highlight> uses the path passage points which are stored in the animation data management mechanism <highlight><bold>54</bold></highlight> to execute the generation of an animation path. <cross-reference target="DRAWINGS">FIG. 10</cross-reference> shows the basic configuration of the processing flow which is executed by this animation path generating mechanism <highlight><bold>58</bold></highlight> </paragraph>
<paragraph id="P-0107" lvl="0"><number>&lsqb;0107&rsqb;</number> As shown in this processing flow, when the animation path generation is started, for the case of causing an object to be moved from a path passage point P(n) to a path passage point P(n&plus;1), for example directly, the animation path generating mechanism <highlight><bold>58</bold></highlight> first at step <highlight><bold>1</bold></highlight> (s<highlight><bold>1</bold></highlight>) makes a test as to whether or not there is interference with another object. If the judgment is that there is no interference, control proceeds to step <highlight><bold>2</bold></highlight> (s<highlight><bold>2</bold></highlight>), at which the object is moved from path passage point P(n) to path passage point P(n&plus;1), for example directly. </paragraph>
<paragraph id="P-0108" lvl="0"><number>&lsqb;0108&rsqb;</number> If, however, the judgment at step <highlight><bold>1</bold></highlight> (s<highlight><bold>1</bold></highlight>) is that there is interference with another object, at step <highlight><bold>5</bold></highlight> (s<highlight><bold>5</bold></highlight>), a path which avoids the obstructing object is successively generated, as the object being moved is moved from path passage point P(n) to path passage point P(n&plus;1) at step <highlight><bold>2</bold></highlight> (s<highlight><bold>2</bold></highlight>). </paragraph>
<paragraph id="P-0109" lvl="0"><number>&lsqb;0109&rsqb;</number> When the processing at step <highlight><bold>2</bold></highlight> (s<highlight><bold>2</bold></highlight>) is completed, at step <highlight><bold>3</bold></highlight> the value of the variable n is incremented by 1, after which at step <highlight><bold>4</bold></highlight> (s<highlight><bold>4</bold></highlight>) a test is made to determine whether or not the value of the variable n has reached the final path passage point N. If the judgment is that it has reached that value, the animation path generation processing is ended, but if the judgment is that value has not been reached, return in made to step <highlight><bold>1</bold></highlight> (s<highlight><bold>1</bold></highlight>). </paragraph>
<paragraph id="P-0110" lvl="0"><number>&lsqb;0110&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 11</cross-reference> shows the flow of the avoidance path generation processing at step <highlight><bold>5</bold></highlight> (s<highlight><bold>5</bold></highlight>) in <cross-reference target="DRAWINGS">FIG. 10</cross-reference> and the flow of the movement processing at step <highlight><bold>2</bold></highlight> (s<highlight><bold>2</bold></highlight>) along the generated avoidance path. </paragraph>
<paragraph id="P-0111" lvl="0"><number>&lsqb;0111&rsqb;</number> Following this processing flow, first at step <highlight><bold>1</bold></highlight> (t<highlight><bold>1</bold></highlight>) the animation path generating mechanism <highlight><bold>58</bold></highlight> establishes a plane PL, for example one which includes the points P(n&minus;1), P(n) and P(n&plus;1), for generation of an avoidance path, onto which the path passage points P(n) and P(n&plus;1) are placed. Next, at step <highlight><bold>2</bold></highlight> (t<highlight><bold>2</bold></highlight>), the bounding spheres which include the other objects are sliced by the plane PL to determine bounding circles, within which bounding circles which interfere with a straight line joining the path passage point P(n) the path passage point P(n&plus;1), that is, bounding circles which interfere directly, are determined, and further bounding circles are determined which interfere with these, that is, which interfere indirectly, are determined, these being stored in a bounding circle list CL. </paragraph>
<paragraph id="P-0112" lvl="0"><number>&lsqb;0112&rsqb;</number> With regard to the example shown in <cross-reference target="DRAWINGS">FIG. 12</cross-reference>, in addition to determining bounding circles C<highlight><bold>1</bold></highlight> and C<highlight><bold>2</bold></highlight> which interfere with a straight line that joins the path passage point P(n) and the path passage point P(n&plus;1), the bounding circles C<highlight><bold>3</bold></highlight> and C<highlight><bold>4</bold></highlight> which interfere with these are determined, and these bounding circles C<highlight><bold>1</bold></highlight>, C<highlight><bold>2</bold></highlight>, C<highlight><bold>3</bold></highlight>, and C<highlight><bold>4</bold></highlight> are stored in the bounding circle list CL. </paragraph>
<paragraph id="P-0113" lvl="0"><number>&lsqb;0113&rsqb;</number> Next, at step <highlight><bold>3</bold></highlight> (t<highlight><bold>3</bold></highlight>), intersection points of the straight line which joins the path passage point P(n) and the path passage point P(n&plus;1) with the bounding circles which are stored in the bounding circle list are determined, and of these points, the point A which is closest to the path passage point P(n) and the point B which is closest to the path passage point P(n&plus;1) are also determined. </paragraph>
<paragraph id="P-0114" lvl="0"><number>&lsqb;0114&rsqb;</number> Then, at step <highlight><bold>4</bold></highlight> (t<highlight><bold>4</bold></highlight>), the object to be moved is moved from the path passage point P(n) to the point A. Next, at step <highlight><bold>5</bold></highlight> (t<highlight><bold>5</bold></highlight>), of the two surfaces which are formed by dividing the surface PL by the straight line which joins the path passage point P(n) and the path passage point P(n&plus;1), the surface PL<highlight><bold>2</bold></highlight> for the generation of the avoidance path is determined. The processing for this determination can be implemented by the user making a specification such as the right side, and can also be implemented by a configuration in which the system automatically selects the surface in accordance with the definition as the surface with few bounding circles. </paragraph>
<paragraph id="P-0115" lvl="0"><number>&lsqb;0115&rsqb;</number> Next, at step <highlight><bold>6</bold></highlight> (t<highlight><bold>6</bold></highlight>), the point C is set in the region point A on the line segment AB. Then at step <highlight><bold>7</bold></highlight> (t<highlight><bold>7</bold></highlight>), the intersection points of a straight line V which not only passes through point C but also is perpendicular with a line that joins points A and B and positioned on the PL plane with the bounding circles which are stored in the bounding circle list PL are determined, ones of these which are located on the surface PL<highlight><bold>2</bold></highlight> being identified. </paragraph>
<paragraph id="P-0116" lvl="0"><number>&lsqb;0116&rsqb;</number> Then, at step <highlight><bold>8</bold></highlight> (t<highlight><bold>8</bold></highlight>), if the judgment made is that the number of intersecting points identified at step <highlight><bold>7</bold></highlight> is zero, a &ldquo;next point&rdquo; is set at point C position, if the number of identified points is <highlight><bold>1</bold></highlight> the &ldquo;next point&rdquo; is set at the identified point position, and if the number of identified points is greater than 1, &ldquo;the next point&rdquo; is set at the position of the point of these identified intersection points which is the greatest distance from the line which joins points A and B. </paragraph>
<paragraph id="P-0117" lvl="0"><number>&lsqb;0117&rsqb;</number> Next, at step <highlight><bold>9</bold></highlight> (t<highlight><bold>9</bold></highlight>), the object to be moved is moved to the &ldquo;next point&rdquo; which was set at step <highlight><bold>8</bold></highlight>. Then, at step <highlight><bold>10</bold></highlight> (t<highlight><bold>10</bold></highlight>), the point C is moved towards point B along the line segment AB, and at step <highlight><bold>11</bold></highlight> (t<highlight><bold>11</bold></highlight>) a test is made as to whether or not the point C has exceeded the path passage point P(n&plus;1). If the judgment is made that it has done so, the processing is ended. If, however, the judgment is made that the path passage point P(n&plus;1) has not yet been exceeded, control returns to step <highlight><bold>7</bold></highlight>. </paragraph>
<paragraph id="P-0118" lvl="0"><number>&lsqb;0118&rsqb;</number> In this manner, described with regard to <cross-reference target="DRAWINGS">FIG. 12</cross-reference>, the avoidance path from the path passage point P(n) and then through the sequence of points A, K<highlight><bold>1</bold></highlight>, K<highlight><bold>2</bold></highlight>, K<highlight><bold>3</bold></highlight>, K<highlight><bold>4</bold></highlight>, B. and finally the path passage point P(n&plus;1) is automatically generated, passing to the right side of the bounding circles C<highlight><bold>1</bold></highlight>, C<highlight><bold>2</bold></highlight>, and C<highlight><bold>4</bold></highlight>. </paragraph>
<paragraph id="P-0119" lvl="0"><number>&lsqb;0119&rsqb;</number> Following the processing flow shown in <cross-reference target="DRAWINGS">FIG. 11</cross-reference>, the animation path generating mechanism <highlight><bold>58</bold></highlight> uses relatively simple bounding circles to generate an animation path which avoids interfering objects, thus providing the advantage of high-speed generation of an avoidance path. </paragraph>
<paragraph id="P-0120" lvl="0"><number>&lsqb;0120&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 13</cross-reference> shows another processing flow for generation of an avoidance path and the processing of movement therealong which is executed by the animation path generation mechanism <highlight><bold>58</bold></highlight>. </paragraph>
<paragraph id="P-0121" lvl="0"><number>&lsqb;0121&rsqb;</number> Following this processing flow, first at step <highlight><bold>1</bold></highlight> (t<highlight><bold>1</bold></highlight>) the animation path generating mechanism <highlight><bold>58</bold></highlight> establishes a plane PL, for example one which includes the points P(n&minus;1), P(n) and P(n&plus;1), for generation of an avoidance path, onto which the path passage points P(n) and P(n&plus;1) are placed. Next, at step <highlight><bold>2</bold></highlight> (t<highlight><bold>2</bold></highlight>), cross sections are determined by slicing the other objects, and of these cross sections which interfere with a straight line joining the path passage point P(n) and the path passage point P(n&plus;1) are determined, and further cross sections which interfere with these are determined these being stored in an obstacle list OL. </paragraph>
<paragraph id="P-0122" lvl="0"><number>&lsqb;0122&rsqb;</number> Explained in terms of the example shown in <cross-reference target="DRAWINGS">FIG. 14</cross-reference>, in addition to determining the cross sections D<highlight><bold>1</bold></highlight> and D<highlight><bold>2</bold></highlight> of the obstacle which interfere with a straight line which joins the path passage point P(n) and the path passage point P(n&plus;1), the cross section D<highlight><bold>3</bold></highlight> of the obstacle which interferes with these is determined, these cross sections D<highlight><bold>1</bold></highlight>, D<highlight><bold>2</bold></highlight>, and D<highlight><bold>3</bold></highlight> being stored in the obstacle list OL. </paragraph>
<paragraph id="P-0123" lvl="0"><number>&lsqb;0123&rsqb;</number> Next, at step <highlight><bold>3</bold></highlight> (t<highlight><bold>3</bold></highlight>), the points of intersection between the straight line which joins the path passage point P(n) and the path passage point P(n&plus;1) and the cross sections which are stored in the obstacle list OL are determined, and of these the point A which is closest to the path passage point P(n) and the point B which is the closest to the path passage point P(n&plus;1) are determined. </paragraph>
<paragraph id="P-0124" lvl="0"><number>&lsqb;0124&rsqb;</number> Then, at step <highlight><bold>4</bold></highlight> (t<highlight><bold>4</bold></highlight>), the object to be moved is moved from path passage point P(n) to point A. Next, at step <highlight><bold>5</bold></highlight> (t<highlight><bold>5</bold></highlight>) of two surfaces which are formed by dividing the surface PL by the straight line which joins the path passage point P(n) and the path passage point P(n&plus;1), the surface PL<highlight><bold>2</bold></highlight> for the generation of the avoidance path is determined. The processing for this determination can be implemented by the user making a specification such as the right side, and can also be implemented by a configuration in which the system automatically selects the surface in accordance with the definition as the surface with few cross sections. </paragraph>
<paragraph id="P-0125" lvl="0"><number>&lsqb;0125&rsqb;</number> Next, at step <highlight><bold>6</bold></highlight> (t<highlight><bold>6</bold></highlight>), the point C is set in the region of point A on the line segment AB. Then at step <highlight><bold>7</bold></highlight> (t<highlight><bold>7</bold></highlight>), the intersection points of a straight line V which not only passes through point C but also is perpendicular to a line that joins points A and B and located on the plane PL with the cross sections which are stored in the obstacle list OL are determined, ones of these which are located on the surface PL<highlight><bold>2</bold></highlight> being identified. </paragraph>
<paragraph id="P-0126" lvl="0"><number>&lsqb;0126&rsqb;</number> Then, at step <highlight><bold>8</bold></highlight> (t<highlight><bold>8</bold></highlight>), if the judgment made is that the number of intersecting points identified at step <highlight><bold>7</bold></highlight> is zero, a &ldquo;next point&rdquo; is set at the current point C position, if the number of identified points is 1 the &ldquo;next point&rdquo; is set at the identified point position, and if the number of identified points is greater than 1, the &ldquo;next point&rdquo; is set at the position of the point of these identified intersection points which is the greatest distance from the line which joins points A and B. </paragraph>
<paragraph id="P-0127" lvl="0"><number>&lsqb;0127&rsqb;</number> Next, at step <highlight><bold>9</bold></highlight> (t<highlight><bold>9</bold></highlight>), the object to be moved is moved to the &ldquo;next point&rdquo; which was set at step <highlight><bold>8</bold></highlight>. Then, at step <highlight><bold>10</bold></highlight> (t<highlight><bold>10</bold></highlight>), the point C is moved towards point B along the line segment AB, and at step <highlight><bold>11</bold></highlight> (t<highlight><bold>11</bold></highlight>) a test is made as to whether or not the point C has exceeded the path passage point P(n&plus;1). If the judgment is made that it has done so, the processing is ended. If, however, the judgment is made that the path passage point P(n&plus;1) has not yet been exceeded, control returns to step <highlight><bold>7</bold></highlight>. </paragraph>
<paragraph id="P-0128" lvl="0"><number>&lsqb;0128&rsqb;</number> In this manner, as described with regard to <cross-reference target="DRAWINGS">FIG. 14</cross-reference>, the avoidance path from the path passage point P(n) and then through the sequence of points A, K<highlight><bold>1</bold></highlight>, K<highlight><bold>2</bold></highlight>, K<highlight><bold>3</bold></highlight>, K<highlight><bold>4</bold></highlight>, B, and finally the path passage point P(n&plus;1) is automatically generated, passing to the right side of the cross-sections D<highlight><bold>1</bold></highlight>, D<highlight><bold>2</bold></highlight>, and D<highlight><bold>4</bold></highlight>. </paragraph>
<paragraph id="P-0129" lvl="0"><number>&lsqb;0129&rsqb;</number> Following the processing flow shown in <cross-reference target="DRAWINGS">FIG. 13</cross-reference>, the animation path generating mechanism <highlight><bold>58</bold></highlight> uses the shape of obstacles to generate an animation path which avoids interfering objects, thus providing the advantage of being able to generate a realistic avoidance path which follows the variations in height representing the shapes of obstacles. </paragraph>
<paragraph id="P-0130" lvl="0"><number>&lsqb;0130&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 15</cross-reference> shows another processing flow for generation of an avoidance path and the processing of movement therealong which is executed by the animation path generation mechanism <highlight><bold>58</bold></highlight>. </paragraph>
<paragraph id="P-0131" lvl="0"><number>&lsqb;0131&rsqb;</number> Following this processing flow, first at step <highlight><bold>1</bold></highlight> (t<highlight><bold>1</bold></highlight>) the animation path generating mechanism <highlight><bold>58</bold></highlight> establishes a plane PL, for example one which includes the points P(n&minus;1), P(n) and P(n&plus;1), for generation of an avoidance path, onto which the path passage points P(n) and P(n&plus;1) are placed. Next, at step <highlight><bold>2</bold></highlight> (t<highlight><bold>2</bold></highlight>), cross sections are determined by slicing the other objects, these then being scaled by a multiple of, for example, 1.2, and of these scaled cross sections, scaled cross sections which interfere with a straight line joining the path passage point P(n) and the path passage point P(n&plus;1) are determined, and further scaled cross sections which interfere with these are determined, these being stored in an obstacle list SL. </paragraph>
<paragraph id="P-0132" lvl="0"><number>&lsqb;0132&rsqb;</number> Explained in terms of the example shown in <cross-reference target="DRAWINGS">FIG. 16</cross-reference>, in addition to determining the scaled cross sections E<highlight><bold>1</bold></highlight> and E<highlight><bold>2</bold></highlight> which interfere with a straight line which joins the path passage point P(n) and the path passage point P(n&plus;1), the scaled-cross section E<highlight><bold>3</bold></highlight> of the obstacle which interferes with these is determined, these scaled cross sections E<highlight><bold>1</bold></highlight>, E<highlight><bold>2</bold></highlight>, and E<highlight><bold>3</bold></highlight> being stored in the obstacle list SL. The broken lines in <cross-reference target="DRAWINGS">FIG. 16</cross-reference> are the obstacle cross sections before scaling. </paragraph>
<paragraph id="P-0133" lvl="0"><number>&lsqb;0133&rsqb;</number> Next, at step <highlight><bold>3</bold></highlight> (t<highlight><bold>3</bold></highlight>), the points of intersection between the straight line which joins the path passage point P(n) and the path passage point P(n&plus;1) and the scaled cross sections which are stored in the obstacle list SL are determined, and of these the point A which is closest to the path passage point P(n) and the point B which is the closest to the path passage point P(n&plus;1) are determined. </paragraph>
<paragraph id="P-0134" lvl="0"><number>&lsqb;0134&rsqb;</number> Then, at step <highlight><bold>4</bold></highlight> (t<highlight><bold>4</bold></highlight>), the object to be moved is moved from path passage point P(n) to point A. Next, at step <highlight><bold>5</bold></highlight> (t<highlight><bold>5</bold></highlight>) of two surfaces which are formed by dividing the surface PL by the straight line which joins the path passage point P(n) and the path passage point P(n&plus;1), the surface PL<highlight><bold>2</bold></highlight> for the generation of the avoidance path is determined. The processing for this determination can be implemented by the user making a specification such as the right side, and can also be implemented by a configuration in which the system automatically selects the surface in accordance with the definition as the surface with few scaled cross sections. </paragraph>
<paragraph id="P-0135" lvl="0"><number>&lsqb;0135&rsqb;</number> Next, at step <highlight><bold>6</bold></highlight> (t<highlight><bold>6</bold></highlight>), the point C is set in the region of point A on the line segment AB. Then at step <highlight><bold>7</bold></highlight> (t<highlight><bold>7</bold></highlight>), the intersection points of a straight line V which not only passes through point C but also is perpendicular with a line that joins points A and B and located on the plane PL with the scaled cross sections which are stored in the obstacle list SL are determined, ones of these which are located on the surface PL<highlight><bold>2</bold></highlight> being identified. </paragraph>
<paragraph id="P-0136" lvl="0"><number>&lsqb;0136&rsqb;</number> Then, at step <highlight><bold>8</bold></highlight> (t<highlight><bold>8</bold></highlight>), if the judgment made is that the number of intersecting points identified at step <highlight><bold>7</bold></highlight> is zero, a &ldquo;next point&rdquo; is set at the current point C position, if the number of identified points is 1 the &ldquo;next point&rdquo; is set at the identified point position, and if the number of identified points is greater than 1, the &ldquo;next point&rdquo; is set at the position of the point of these identified intersection points which is the greatest distance from the line which joins points A and B. </paragraph>
<paragraph id="P-0137" lvl="0"><number>&lsqb;0137&rsqb;</number> Next, at step <highlight><bold>9</bold></highlight> (t<highlight><bold>9</bold></highlight>), the object to be moved is moved to the &ldquo;next point&rdquo; which was set at step <highlight><bold>8</bold></highlight>. Then, at step <highlight><bold>10</bold></highlight> (t<highlight><bold>10</bold></highlight>), the point C is moved towards point B along the line segment AB, and at step <highlight><bold>11</bold></highlight> (t<highlight><bold>11</bold></highlight>) a test is made as to whether or not the point C has exceeded the path passage point P(n&plus;1). If the judgment is made that it has done so, the processing is ended. If, however, the judgment is made that the path passage point P(n&plus;1) has not yet been exceeded, control returns to step <highlight><bold>7</bold></highlight>. </paragraph>
<paragraph id="P-0138" lvl="0"><number>&lsqb;0138&rsqb;</number> In this manner, described with regard to <cross-reference target="DRAWINGS">FIG. 16</cross-reference>, the avoidance path from the path passage point P(n) and then through the sequence of points A, K<highlight><bold>1</bold></highlight>, K<highlight><bold>2</bold></highlight>, K<highlight><bold>3</bold></highlight>, K<highlight><bold>4</bold></highlight>, B, and finally the path passage point P(n&plus;<highlight><bold>1</bold></highlight>) is automatically generated, passing by the scaled cross sections E<highlight><bold>1</bold></highlight>, E<highlight><bold>2</bold></highlight>, and E<highlight><bold>3</bold></highlight>. </paragraph>
<paragraph id="P-0139" lvl="0"><number>&lsqb;0139&rsqb;</number> Following the processing flow shown in <cross-reference target="DRAWINGS">FIG. 15</cross-reference>, the animation path generating mechanism <highlight><bold>58</bold></highlight> uses the expanded shape of obstacles to generate an animation path which avoids interfering objects, thus providing the advantage of being able to generate an avoidance path which provides a feeling of avoiding obstacles while walking about which is close to that experienced in the real world. </paragraph>
<paragraph id="P-0140" lvl="0"><number>&lsqb;0140&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 17</cross-reference> shows another processing flow for generation of an avoidance path which is executed by the animation path generation mechanism <highlight><bold>58</bold></highlight>. </paragraph>
<paragraph id="P-0141" lvl="0"><number>&lsqb;0141&rsqb;</number> This example uses a flow-velocity field, and generates an avoidance path which joins the path passage point P(n) with the path passage point P(n&plus;1). Following this processing flow, first at step <highlight><bold>1</bold></highlight> (t<highlight><bold>1</bold></highlight>) the animation path generating mechanism <highlight><bold>58</bold></highlight> establishes a plane PL, for example one which includes the points P(n&minus;1), P(n) and P(n&plus;1), for generation of an avoidance path, onto which the path passage points P(n) and P(n&plus;1) are placed. Next, at step <highlight><bold>2</bold></highlight> (t<highlight><bold>2</bold></highlight>),.an emergence point is positioned at the path passage point P(n) and an intake point is positioned at the path passage point P(n&plus;1). </paragraph>
<paragraph id="P-0142" lvl="0"><number>&lsqb;0142&rsqb;</number> Next, at step <highlight><bold>3</bold></highlight> (t<highlight><bold>3</bold></highlight>), cross sections are determined by slicing the other objects with the plane PL, those cross sections which interfere with a straight line which joins path passage point P(n) and path passage point P(n&plus;1) being determined, along with cross sections which interfere with these, and among the positions of these obstacles (interfering objects), an emergence point is positioned at the position that is the closest to the path passage point P(n), and the intake point is positioned at the position is closest to the path passage point P(n&plus;1), these two points forming an emergence-intake pair. </paragraph>
<paragraph id="P-0143" lvl="0"><number>&lsqb;0143&rsqb;</number> These emergence and intake points which form a pair are used to represent an obstacle as a fluid dynamics equivalent and, as shown in <cross-reference target="DRAWINGS">FIG. 18</cross-reference>A, an obstacle is equivalently represented by a number of pairs having an equal intensity, the number of pairs being responsive to the size of the cross section of the obstacle. As shown in <cross-reference target="DRAWINGS">FIG. 18</cross-reference>B, it is also possible to position a pair having intensities which are responsive to the size of the cross section of the obstacle. </paragraph>
<paragraph id="P-0144" lvl="0"><number>&lsqb;0144&rsqb;</number> By doing this, it is possible, as shown in <cross-reference target="DRAWINGS">FIG. 19</cross-reference>, to form a flow-velocity field between the path passage point P(n) which is a starting point and the path passage point P(n&plus;1) which is the ending point, in accordance with known principles of fluid dynamics, which avoids the obstacle, this field being equivalently represented by an emergence-intake point pair. In the drawing, the plain circle represents an emergence point, while the black-filled circle represents an intake point. </paragraph>
<paragraph id="P-0145" lvl="0"><number>&lsqb;0145&rsqb;</number> The flow velocity Ux in the X direction and the flow velocity Uy in the Y direction observed at any arbitrary point within the field is known from fluid dynamics to be derivable as follows.  
<math-cwu id="MATH-US-00001">
<number>1</number>
<math>
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mi>Ux</mi>
        <mo>=</mo>
        <mrow>
          <munder>
            <mo>&Sum;</mo>
            <mi>i</mi>
          </munder>
          <mo>&it;</mo>
          <mrow>
            <mfrac>
              <msub>
                <mi>K</mi>
                <mi>i</mi>
              </msub>
              <msub>
                <mi>r</mi>
                <mi>i</mi>
              </msub>
            </mfrac>
            <mo>&it;</mo>
            <mi>cos</mi>
            <mo>&it;</mo>
            <mstyle>
              <mtext>&emsp;</mtext>
            </mstyle>
            <mo>&it;</mo>
            <msub>
              <mi>&theta;</mi>
              <mi>i</mi>
            </msub>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>2</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <mi>Uy</mi>
        <mo>=</mo>
        <mrow>
          <munder>
            <mo>&Sum;</mo>
            <mi>i</mi>
          </munder>
          <mo>&it;</mo>
          <mrow>
            <mfrac>
              <msub>
                <mi>K</mi>
                <mi>i</mi>
              </msub>
              <msub>
                <mi>r</mi>
                <mi>i</mi>
              </msub>
            </mfrac>
            <mo>&it;</mo>
            <mi>sin</mi>
            <mo>&it;</mo>
            <mstyle>
              <mtext>&emsp;</mtext>
            </mstyle>
            <mo>&it;</mo>
            <msub>
              <mi>&theta;</mi>
              <mi>i</mi>
            </msub>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>3</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
<mathematica-file id="MATHEMATICA-00001" file="US20030001843A1-20030102-M00001.NB"/>
<image id="EMI-M00001" wi="216.027" he="47.99655" file="US20030001843A1-20030102-M00001.TIF" imf="TIFF" ti="MF"/>
</math-cwu>
</paragraph>
<paragraph id="P-0146" lvl="0"><number>&lsqb;0146&rsqb;</number> In the above relationships, r<highlight><subscript>i </subscript></highlight>is the distance of each emergence/intake point from the observation point, K<highlight><subscript>i </subscript></highlight>is the intensity of each emergence/intake point, and &thgr;<highlight><subscript>i </subscript></highlight>is the angle formed between the observation point and each emergence/intake point, these being illustrated in <cross-reference target="DRAWINGS">FIG. 20</cross-reference>. </paragraph>
<paragraph id="P-0147" lvl="0"><number>&lsqb;0147&rsqb;</number> The above are used to form a flow-velocity field, after which at step <highlight><bold>4</bold></highlight> (t<highlight><bold>4</bold></highlight>) the velocity vectors are determined at the path passage point P(n), according to the.above equations. Then, at step <highlight><bold>5</bold></highlight> (t<highlight><bold>5</bold></highlight>), the position C on plane PL which is taken as the next observation point is determined from these velocity vectors at path passage point P(n) and the time step. Next, at step <highlight><bold>6</bold></highlight> (t<highlight><bold>6</bold></highlight>), a test is made as to whether or not the position C has reached the path passage point P(n&plus;1). If the judgment is made that it has reached this point, processing is ended. If, however, the judgment is that it has not reached the path passage point P(n&plus;1), control proceeds to step <highlight><bold>7</bold></highlight> (t<highlight><bold>7</bold></highlight>), at which the velocity vectors at that position C are determined in accordance with equations (1) and (2), after which control returns to step <highlight><bold>5</bold></highlight>. The time step used at step <highlight><bold>5</bold></highlight> can be given beforehand by a user, or established by using the amount of time required for the previous frame processing. </paragraph>
<paragraph id="P-0148" lvl="0"><number>&lsqb;0148&rsqb;</number> In this manner, described in terms of the example shown in <cross-reference target="DRAWINGS">FIG. 19</cross-reference>, an avoidance path is automatically generated from the path passage point P(n), which is the starting point in the drawing, to the path passage point P(n&plus;1), which is the ending point in the drawing. </paragraph>
<paragraph id="P-0149" lvl="0"><number>&lsqb;0149&rsqb;</number> The advantage of following the processing flow shown in <cross-reference target="DRAWINGS">FIG. 19</cross-reference> is that, if the path passage point P(n), which is the starting point and the path passage point P(n&plus;1), which is the ending point, are given, the animation path generating mechanism <highlight><bold>58</bold></highlight> can automatically generate an avoidance path, even if intermediate path passage points in the path are unknown, thereby not only greatly reducing the work of generating an avoidance path, but also greatly reducing the amount of memory required for avoidance path generation. </paragraph>
<paragraph id="P-0150" lvl="0"><number>&lsqb;0150&rsqb;</number> Furthermore, in the processing flow shown in <cross-reference target="DRAWINGS">FIG. 19</cross-reference>, while a plane PL for generation of an avoidance path, and which passes through the path passage point P(n) and the path passage point P(n&plus;1) is established, a flow-velocity field being formed on this plane PL to generate an avoidance path which joins the path passage point P(n) and the path passage point P(n&plus;1), it is also possible, without using the plane PL, to use a flow-velocity field, and to generate an avoidance path in accordance with 3-dimensional processing. </paragraph>
<paragraph id="P-0151" lvl="0"><number>&lsqb;0151&rsqb;</number> Next, another embodiment of the present invention will be described. In this embodiment of the present invention, the animation path input mechanism <highlight><bold>56</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 9</cross-reference> first interacts with a user to establish the viewing direction in the virtual world, and to set the path passage points in the perspective view of the virtual world when viewing is done in that set direction, thereby setting the animation path of a moving object. For example, as shown in <cross-reference target="DRAWINGS">FIG. 21</cross-reference>, with a plane that intersects the direction of the pull of gravity perpendicularly selected as the projection plane, by setting path passage points indicated by &lt;1&gt; through &lt;6&gt; on a screen onto which the virtual world is projected, an animation path of a moving object or viewing point (hereinafter referred to as a moving body) is set. </paragraph>
<paragraph id="P-0152" lvl="0"><number>&lsqb;0152&rsqb;</number> The orientation of the projection plane used in the setting of this animation path can be selected, for example, as a plane which is parallel to the direction of the force of gravity. </paragraph>
<paragraph id="P-0153" lvl="0"><number>&lsqb;0153&rsqb;</number> After the animation path input mechanism <highlight><bold>56</bold></highlight> sets a planar animation path as described above, the position in the virtual world of the thus-set planar animation path is calculated, this position being stored in the animation data management mechanism <highlight><bold>30</bold></highlight>. For example, because a point (x<highlight><subscript>w</subscript></highlight>, y<highlight><subscript>w</subscript></highlight>) set on a 2-dimensional projection has in a 3-dimensional virtual world the 3-dimensional coordinates (x<highlight><subscript>c</subscript></highlight>, y<highlight><subscript>c</subscript></highlight>, z<highlight><subscript>c</subscript></highlight>), the position in this virtual world is calculated. </paragraph>
<paragraph id="P-0154" lvl="0"><number>&lsqb;0154&rsqb;</number> Specifically, this calculation processing is performed by the following procedure. </paragraph>
<paragraph id="P-0155" lvl="0"><number>&lsqb;0155&rsqb;</number> As shown in <cross-reference target="DRAWINGS">FIG. 22</cross-reference>, the coordinates x<highlight><subscript>c</subscript></highlight>, y<highlight><subscript>c</subscript></highlight>, and z<highlight><subscript>c </subscript></highlight>in the world coordinate system are converted to the normalized projection coordinate system coordinate values x<highlight><subscript>n</subscript></highlight>, y<highlight><subscript>n</subscript></highlight>, and z<highlight><subscript>n </subscript></highlight>by means of the following equation (4).  
<math-cwu id="MATH-US-00002">
<number>2</number>
<math>
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mo>(</mo>
          <mtable>
            <mtr>
              <mtd>
                <msub>
                  <mi>x</mi>
                  <mi>n</mi>
                </msub>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msub>
                  <mi>y</mi>
                  <mi>n</mi>
                </msub>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msub>
                  <mi>z</mi>
                  <mi>n</mi>
                </msub>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <mn>1</mn>
              </mtd>
            </mtr>
          </mtable>
          <mo>)</mo>
        </mrow>
        <mo>=</mo>
        <mrow>
          <mi>AB</mi>
          <mo>&af;</mo>
          <mrow>
            <mo>(</mo>
            <mtable>
              <mtr>
                <mtd>
                  <msub>
                    <mi>x</mi>
                    <mi>c</mi>
                  </msub>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <msub>
                    <mi>y</mi>
                    <mi>c</mi>
                  </msub>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <msub>
                    <mi>z</mi>
                    <mi>c</mi>
                  </msub>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <mn>1</mn>
                </mtd>
              </mtr>
            </mtable>
            <mo>)</mo>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>4</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
<mathematica-file id="MATHEMATICA-00002" file="US20030001843A1-20030102-M00002.NB"/>
<image id="EMI-M00002" wi="216.027" he="42.9786" file="US20030001843A1-20030102-M00002.TIF" imf="TIFF" ti="MF"/>
</math-cwu>
</paragraph>
<paragraph id="P-0156" lvl="0"><number>&lsqb;0156&rsqb;</number> In the above, the matrix B is a 4-row 4-column view orientation conversion matrix which performs conversion from the world coordinate system to the viewing reference coordinate system, and matrix A is a 4-row 4-column view mapping matrix which performs conversion from the viewing reference coordinate system to the normalized projection coordinate system. The view orientation conversion matrix B is the product of the matrix for shifting of the origin from the origin of the world coordinate system to the origin of the viewing reference coordinate system, a matrix which rotates the coordinates about the Y axis of the world coordinate system by the viewing axis direction angle &agr;, and a matrix which rotates the coordinates about the X axis of the world coordinate system by the viewing axis elevation angle &bgr;. </paragraph>
<paragraph id="P-0157" lvl="0"><number>&lsqb;0157&rsqb;</number> The view map matrix A is a matrix which expands or reduces the coordinates in X-axis and Y-axis directions, in proportion to the distance from the viewing point. </paragraph>
<paragraph id="P-0158" lvl="0"><number>&lsqb;0158&rsqb;</number> From equation (4), we have the following relationship.  
<math-cwu id="MATH-US-00003">
<number>3</number>
<math>
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mo>(</mo>
          <mtable>
            <mtr>
              <mtd>
                <msub>
                  <mi>x</mi>
                  <mi>c</mi>
                </msub>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msub>
                  <mi>y</mi>
                  <mi>c</mi>
                </msub>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msub>
                  <mi>z</mi>
                  <mi>c</mi>
                </msub>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <mn>1</mn>
              </mtd>
            </mtr>
          </mtable>
          <mo>)</mo>
        </mrow>
        <mo>=</mo>
        <mrow>
          <msup>
            <mrow>
              <mo>(</mo>
              <mi>AB</mi>
              <mo>)</mo>
            </mrow>
            <mrow>
              <mo>-</mo>
              <mn>1</mn>
            </mrow>
          </msup>
          <mo>&it;</mo>
          <mrow>
            <mo>(</mo>
            <mtable>
              <mtr>
                <mtd>
                  <msub>
                    <mi>x</mi>
                    <mi>n</mi>
                  </msub>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <msub>
                    <mi>y</mi>
                    <mi>n</mi>
                  </msub>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <msub>
                    <mi>z</mi>
                    <mi>n</mi>
                  </msub>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <mn>1</mn>
                </mtd>
              </mtr>
            </mtable>
            <mo>)</mo>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
    <mtd>
      <mrow>
        <mo>(</mo>
        <mn>5</mn>
        <mo>)</mo>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
<mathematica-file id="MATHEMATICA-00003" file="US20030001843A1-20030102-M00003.NB"/>
<image id="EMI-M00003" wi="216.027" he="42.9786" file="US20030001843A1-20030102-M00003.TIF" imf="TIFF" ti="MF"/>
</math-cwu>
</paragraph>
<paragraph id="P-0159" lvl="0"><number>&lsqb;0159&rsqb;</number> If into the x<highlight><subscript>n </subscript></highlight>and y<highlight><subscript>n </subscript></highlight>of equation (5) are substituted, respectively, the coordinates x<highlight><subscript>w </subscript></highlight>and y<highlight><subscript>w </subscript></highlight>of the path passage points set on the projection plane, and into Z<highlight><subscript>n </subscript></highlight>of this equation is substituted provisionally the maximum value Zmax, the coordinate values x<highlight><subscript>c</subscript></highlight>, y<highlight><subscript>c</subscript></highlight>, and z<highlight><subscript>c </subscript></highlight>in the world coordinate system of the path passage point which are set in the perspective view in the virtual world are calculated. In this relationship, z<highlight><subscript>c </subscript></highlight>is a provisional value, while the values of x<highlight><subscript>c </subscript></highlight>and y<highlight><subscript>c </subscript></highlight>are values based on settings made by a user. </paragraph>
<paragraph id="P-0160" lvl="0"><number>&lsqb;0160&rsqb;</number> In this manner the animation path input mechanism <highlight><bold>56</bold></highlight> performs interaction with the user so that selection is made of the direction of projection for the perspective view for the purpose of setting the animation path, and by setting path passage points in this perspective view, a planar animation path of an object which moves is set, the position of this animation path in the virtual world being calculated, and this being stored in the animation data management mechanism <highlight><bold>54</bold></highlight>. </paragraph>
<paragraph id="P-0161" lvl="0"><number>&lsqb;0161&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 23</cross-reference> shows an example of path information stored in the animation data management mechanism <highlight><bold>54</bold></highlight> in accordance with processing by the animation path input mechanism <highlight><bold>56</bold></highlight>. </paragraph>
<paragraph id="P-0162" lvl="0"><number>&lsqb;0162&rsqb;</number> As shown in this drawing, in addition to storing, as shown at (a), the animation path position information x<highlight><subscript>c</subscript></highlight>, y<highlight><subscript>c </subscript></highlight>for example, for the moving object position (path passage point) in the first frame and the moving object position in the third frame, as shown at (b) in this drawing, in setting this animation path, there is storage of the normal direction vector data (viewing axis) of the plane selected by the animation path input mechanism <highlight><bold>56</bold></highlight>. In addition to this information, there is storage of displacement data H (to be described later), which is used as the basis for establishing the position of the moving body in the normal direction, which was not established at the animation path input mechanism <highlight><bold>56</bold></highlight>. This displacement data H can be defined in the system or set by the user. </paragraph>
<paragraph id="P-0163" lvl="0"><number>&lsqb;0163&rsqb;</number> The animation path generating mechanism <highlight><bold>58</bold></highlight> uses this path information which is stored in the animation data management mechanism <highlight><bold>54</bold></highlight> in executing the generation of the final animation path, and the animation generating mechanism <highlight><bold>60</bold></highlight> uses the thus-created animation path in generating an animation and storing it in the display data storage mechanism <highlight><bold>62</bold></highlight>, so that processing is done to present a display on the display screen. </paragraph>
<paragraph id="P-0164" lvl="0"><number>&lsqb;0164&rsqb;</number> Specifically, the animation generating mechanism <highlight><bold>60</bold></highlight>, as shown in the processing flow shown in <cross-reference target="DRAWINGS">FIG. 24</cross-reference>, initializes the starting time of the animation at step <highlight><bold>1</bold></highlight> (u<highlight><bold>1</bold></highlight>), after which at step <highlight><bold>2</bold></highlight> (u<highlight><bold>2</bold></highlight>) it makes a test as to whether or not the time is the ending time of the animation. If the judgment is that the ending time has been reached, processing is ended. If, however, the judgment is that the ending time has not been reached, control proceeds to step <highlight><bold>3</bold></highlight> (u<highlight><bold>3</bold></highlight>), at which the condition of each object at time T is calculated and stored in the display data storage mechanism <highlight><bold>62</bold></highlight> while referencing of the animation path generated by the animation path generating mechanism <highlight><bold>58</bold></highlight>, thereby causing a display on the display screen, after which at step <highlight><bold>4</bold></highlight> (u<highlight><bold>4</bold></highlight>) the time T is incremented and control returns to step <highlight><bold>2</bold></highlight>, from which the above processing is repeated. It should be noted that the viewing axis for the purpose of display in this processing is not necessarily parallel with the viewing axis for the purpose of display of the screen for setting the path passage points. </paragraph>
<paragraph id="P-0165" lvl="0"><number>&lsqb;0165&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 25</cross-reference> shows an example of the animation path generation processing flow which is executed by the animation path generating mechanism <highlight><bold>58</bold></highlight>. In this processing flow, the user selects a plane which perpendicularly intersects with the direction of the force of gravity as the projection plane, the assumption being made that the animation path set is of a moving object in a perspective view projected onto that plane. That is, the direction data indicated in <cross-reference target="DRAWINGS">FIG. 23</cross-reference> represents the direction of the force of gravity, and the displacement data H indicated in <cross-reference target="DRAWINGS">FIG. 23</cross-reference> is used for the purpose of setting the path position of the direction of the force of gravity, which is not established at the animation path input mechanism <highlight><bold>56</bold></highlight>. </paragraph>
<paragraph id="P-0166" lvl="0"><number>&lsqb;0166&rsqb;</number> As shown by this processing flow, when the animation path generating mechanism <highlight><bold>58</bold></highlight> generates the animation path, first at step <highlight><bold>1</bold></highlight> (v<highlight><bold>1</bold></highlight>), it calculates the position of the moving object at the time T set by the animation generating mechanism <highlight><bold>60</bold></highlight>, in accordance with management data of the animation data management mechanism <highlight><bold>54</bold></highlight>. As shown in <cross-reference target="DRAWINGS">FIG. 23</cross-reference>, because the animation data management mechanism <highlight><bold>54</bold></highlight> has stored in it positions of a moving object at discrete times (frames) set by the user, when the position of a moving body at time T is not stored in the animation data management mechanism <highlight><bold>54</bold></highlight>, it is determined by interpolation of the object positions for times which are stored in this animation data management mechanism <highlight><bold>54</bold></highlight>. </paragraph>
<paragraph id="P-0167" lvl="0"><number>&lsqb;0167&rsqb;</number> The position of a moving object at time T is determined, after which at step <highlight><bold>2</bold></highlight> (v<highlight><bold>2</bold></highlight>) a straight line which passes through this position and which is parallel to the direction indicated by the direction data is established, the position of the intersections of that straight line with an object being determined. In this example, because the direction of the force of gravity is used as the direction data, in accordance with the processing of step <highlight><bold>2</bold></highlight>, the determination will be made of the position of intersections with an object in the vertical direction. </paragraph>
<paragraph id="P-0168" lvl="0"><number>&lsqb;0168&rsqb;</number> Next, at step <highlight><bold>3</bold></highlight> (v<highlight><bold>3</bold></highlight>), of the positions of intersections which were determined at step <highlight><bold>2</bold></highlight>, the intersecting position which is most distant in a direction which is Opposite the direction indicated by the direction data is determined. In this example, because the direction of the force of gravity is used as the direction data, in accordance with the processing of step <highlight><bold>3</bold></highlight>, the determination will be made of the intersecting point among the object intersecting points that is the highest. </paragraph>
<paragraph id="P-0169" lvl="0"><number>&lsqb;0169&rsqb;</number> Then, at step <highlight><bold>4</bold></highlight> (v<highlight><bold>4</bold></highlight>), from the position of the intersection which was determined at step <highlight><bold>3</bold></highlight>, position data is derived by displacing this position in a direction opposite that indicated by the direction data by an amount equal to the displacement data which is stored in the animation data management mechanism <highlight><bold>54</bold></highlight>, correction being made by replacing the Position data of the moving object determined at step <highlight><bold>1</bold></highlight> with the derived position data. In this example, because the direction of the force of gravity is used as the direction data, in accordance with the processing of step <highlight><bold>4</bold></highlight>, the position data of the moving object determined at step <highlight><bold>1</bold></highlight> is corrected to the position data an amount H above the position determined at step <highlight><bold>3</bold></highlight>. </paragraph>
<paragraph id="P-0170" lvl="0"><number>&lsqb;0170&rsqb;</number> Next, at step <highlight><bold>5</bold></highlight> (v<highlight><bold>5</bold></highlight>), the animation path which is specified from the corrected position determined at step <highlight><bold>4</bold></highlight> is stored in the display data storage mechanism <highlight><bold>62</bold></highlight>, and processing is ended. </paragraph>
<paragraph id="P-0171" lvl="0"><number>&lsqb;0171&rsqb;</number> In this manner, by executing the processing flow shown in <cross-reference target="DRAWINGS">FIG. 25</cross-reference>, the animation path generating mechanism <highlight><bold>58</bold></highlight>, as shown in <cross-reference target="DRAWINGS">FIG. 26</cross-reference>, determines the position (x<highlight><subscript>f</subscript></highlight>, y<highlight><subscript>f</subscript></highlight>, z<highlight><subscript>f</subscript></highlight>) of the moving object at time T at step <highlight><bold>1</bold></highlight>, after which, in accordance with the direction data (x<highlight><subscript>d</subscript></highlight>, y<highlight><subscript>d</subscript></highlight>, z<highlight><subscript>d</subscript></highlight>) which indicates the direction of the force of gravity, the position of intersection (x<highlight><subscript>g</subscript></highlight>, y<highlight><subscript>g</subscript></highlight>, z<highlight><subscript>g</subscript></highlight>) that is highest in the vertical direction is determined at step <highlight><bold>3</bold></highlight>, after which at step <highlight><bold>4</bold></highlight> the position (x<highlight><subscript>f</subscript></highlight>, y<highlight><subscript>f</subscript></highlight>, z<highlight><subscript>f</subscript></highlight>) which was determined at step <highlight><bold>1</bold></highlight> is corrected so that it is the position (x<highlight><subscript>h</subscript></highlight>, y<highlight><subscript>h</subscript></highlight>, z<highlight><subscript>h</subscript></highlight>) that is higher than this position (x<highlight><subscript>g</subscript></highlight>, y<highlight><subscript>g</subscript></highlight>, z<highlight><subscript>g</subscript></highlight>) by the amount of the displacement data H. That is, the position (x<highlight><subscript>f</subscript></highlight>, y<highlight><subscript>f</subscript></highlight>, z<highlight><subscript>f</subscript></highlight>) of the moving object at time T is corrected to the position (x<highlight><subscript>h</subscript></highlight>, y<highlight><subscript>h</subscript></highlight>, z<highlight><subscript>h</subscript></highlight>), defined as (x<highlight><subscript>h</subscript></highlight>, y<highlight><subscript>h</subscript></highlight>, z<highlight><subscript>h</subscript></highlight>)&equals;(x<highlight><subscript>f</subscript></highlight>, y<highlight><subscript>f</subscript></highlight>, z<highlight><subscript>f</subscript></highlight>)&plus;&lcub;((Distance between (x<highlight><subscript>f</subscript></highlight>, y<highlight><subscript>f</subscript></highlight>, z<highlight><subscript>f</subscript></highlight>) and (x<highlight><subscript>g</subscript></highlight>, y<highlight><subscript>g</subscript></highlight>, z<highlight><subscript>g</subscript></highlight>))&minus;H&rcub;&times;(x<highlight><subscript>d</subscript></highlight>, y<highlight><subscript>d</subscript></highlight>, z<highlight><subscript>d</subscript></highlight>). </paragraph>
<paragraph id="P-0172" lvl="0"><number>&lsqb;0172&rsqb;</number> As described above, the animation generating mechanism <highlight><bold>60</bold></highlight> uses the animation path which is generated by the animation path generating mechanism <highlight><bold>58</bold></highlight> to generate an animation and store this in the display data storage mechanism <highlight><bold>62</bold></highlight>, thereby performing the processing for presenting the animation on a display screen. </paragraph>
<paragraph id="P-0173" lvl="0"><number>&lsqb;0173&rsqb;</number> In this manner, when the animation path input mechanism <highlight><bold>56</bold></highlight>, as shown in <cross-reference target="DRAWINGS">FIG. 27</cross-reference>A, sets the animation path of a moving object by using a perspective which is projected onto a plane which is perpendicular to the force of gravity, this being done by means of interaction with the user, the animation path generating mechanism <highlight><bold>58</bold></highlight>, as shown in <cross-reference target="DRAWINGS">FIG. 27</cross-reference>B, while maintaining the path position on the plane which intersects perpendicularly with the direction of the force of gravity as is, as shown in <cross-reference target="DRAWINGS">FIG. 27</cross-reference>C, corrects the position in the direction of gravity to a position that is higher by the displacement data H than the highest object, thereby generating the animation path which is used by the animation generating mechanism <highlight><bold>60</bold></highlight>. </paragraph>
<paragraph id="P-0174" lvl="0"><number>&lsqb;0174&rsqb;</number> In this manner, it is possible to generate an animation which moves a moving object along the ground or buildings, even without setting the path of the moving object 3-dimensionally. </paragraph>
<paragraph id="P-0175" lvl="0"><number>&lsqb;0175&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 28</cross-reference> shows another example of the processing flow for animation generation, which is executed by the animation path generating mechanism <highlight><bold>58</bold></highlight>. In this processing flow, as is the case with the processing flow shown in <cross-reference target="DRAWINGS">FIG. 25</cross-reference>, the user selects a projection plane which perpendicularly intersects with the direction of the force of gravity, the assumption being made that the animation path set is of a moving object in a perspective view projected onto that plane. That is, the direction data indicated in <cross-reference target="DRAWINGS">FIG. 23</cross-reference> represents the direction of the force of gravity, and the displacement data indicated in <cross-reference target="DRAWINGS">FIG. 23</cross-reference> is used for the purpose of setting the path position of the direction of the force of gravity, which is not established at the animation path input mechanism <highlight><bold>56</bold></highlight>. </paragraph>
<paragraph id="P-0176" lvl="0"><number>&lsqb;0176&rsqb;</number> According to this processing flow, when the animation path generating mechanism <highlight><bold>58</bold></highlight> generates the animation path, first at step <highlight><bold>1</bold></highlight> (v<highlight><bold>1</bold></highlight>), in the same manner as at step <highlight><bold>1</bold></highlight> shown in the processing flow shown in <cross-reference target="DRAWINGS">FIG. 25</cross-reference>, it calculates the position of the moving object at the time T set by the animation generating mechanism <highlight><bold>60</bold></highlight>, in accordance with management data of the animation data management mechanism <highlight><bold>54</bold></highlight>. </paragraph>
<paragraph id="P-0177" lvl="0"><number>&lsqb;0177&rsqb;</number> The position of a moving object at time T is determined, after which at step <highlight><bold>2</bold></highlight> (v<highlight><bold>2</bold></highlight>), in the same manner as at step <highlight><bold>1</bold></highlight> shown in the processing flow shown in <cross-reference target="DRAWINGS">FIG. 25, a</cross-reference> straight line which passes through this position and which is parallel to the direction indicated by the direction data is established, the position of the intersections of that straight line with objects being determined. In this example, because the direction of the force of gravity is used as the direction data, in accordance with the processing of step <highlight><bold>2</bold></highlight>, the determination will be made of the position of intersections with objects in the vertical direction. </paragraph>
<paragraph id="P-0178" lvl="0"><number>&lsqb;0178&rsqb;</number> Next, at step <highlight><bold>3</bold></highlight> (v<highlight><bold>3</bold></highlight>), of the positions of intersections which were determined at step <highlight><bold>2</bold></highlight>, the intersecting position which is positioned in the direction indicated by the direction data and which is closest to the position of the moving object which was determined in step <highlight><bold>1</bold></highlight> is determined. In this example, because the direction of the force of gravity is used as the direction data, in accordance with the processing of step <highlight><bold>3</bold></highlight>, the determination will be made of the intersecting point among the object intersecting points that were determined at step <highlight><bold>2</bold></highlight> which is downward (that is, in the direction of the force of gravity) and which also is uppermost in position. Whether or not an object determined at step <highlight><bold>2</bold></highlight> is downward and whether or not it is upward is judged as follows. If the position of the object determined at step <highlight><bold>1</bold></highlight> is (x<highlight><subscript>f</subscript></highlight>, y<highlight><subscript>f</subscript></highlight>, z<highlight><subscript>f</subscript></highlight>) and the position of the intersection point with the object is (x<highlight><subscript>g</subscript></highlight>, y<highlight><subscript>g</subscript></highlight>, z<highlight><subscript>g</subscript></highlight>), if all the conditions </paragraph>
<paragraph id="P-0179" lvl="1"><number>&lsqb;0179&rsqb;</number> (x<highlight><subscript>g</subscript></highlight>&minus;x<highlight><subscript>f</subscript></highlight>)&times;x<highlight><subscript>d</subscript></highlight>&gE;0, </paragraph>
<paragraph id="P-0180" lvl="1"><number>&lsqb;0180&rsqb;</number> (x<highlight><subscript>g</subscript></highlight>&minus;x<highlight><subscript>f</subscript></highlight>)&times;x<highlight><subscript>d</subscript></highlight>&gE;0, and </paragraph>
<paragraph id="P-0181" lvl="1"><number>&lsqb;0181&rsqb;</number> (x<highlight><subscript>g</subscript></highlight>&minus;x<highlight><subscript>f</subscript></highlight>)&times;x<highlight><subscript>d</subscript></highlight>&gE;0 </paragraph>
<paragraph id="P-0182" lvl="7"><number>&lsqb;0182&rsqb;</number> are satisfied, the object is in the downward direction. If they are-not-satisfied, the object is in the upward direction. </paragraph>
<paragraph id="P-0183" lvl="0"><number>&lsqb;0183&rsqb;</number> Then, at step <highlight><bold>4</bold></highlight> (v<highlight><bold>4</bold></highlight>), from the position of the intersection which was determined at step <highlight><bold>3</bold></highlight>, the position data which is derived by displacing this position in a direction opposite that indicated by the direction data by an amount equal to the displacement data which is stored in the animation data management mechanism <highlight><bold>54</bold></highlight>, correction being made by replacing the displacement data in the direction of the position data of the moving object determined at step <highlight><bold>1</bold></highlight> with the determined position data. In this embodiment, because the direction of the force of gravity is used as the direction data, in accordance with the processing of step <highlight><bold>4</bold></highlight>, the position data in the direction of the force of gravity of the position data of the moving object determined at step <highlight><bold>1</bold></highlight> is corrected to the position data an amount H above the position determined at step <highlight><bold>2</bold></highlight>. </paragraph>
<paragraph id="P-0184" lvl="0"><number>&lsqb;0184&rsqb;</number> Next, at step <highlight><bold>5</bold></highlight> (v<highlight><bold>5</bold></highlight>), the animation path which is specified from the corrected position determined at step <highlight><bold>4</bold></highlight> is stored in the display data storage mechanism <highlight><bold>62</bold></highlight>, and processing is ended. </paragraph>
<paragraph id="P-0185" lvl="0"><number>&lsqb;0185&rsqb;</number> As described above, the animation generating mechanism <highlight><bold>60</bold></highlight> uses the animation path which is generated by the animation path generating mechanism <highlight><bold>58</bold></highlight> to generate an animation and store this in the display data storage mechanism <highlight><bold>62</bold></highlight>, thereby performing the processing for presenting the animation on a display screen. </paragraph>
<paragraph id="P-0186" lvl="0"><number>&lsqb;0186&rsqb;</number> In this manner, when the animation path input mechanism <highlight><bold>56</bold></highlight> sets the animation path of a moving object by using a perspective which is projected onto a plane which is perpendicular to the force of gravity, this being done by means of interaction with the user, the animation path generating mechanism <highlight><bold>58</bold></highlight>, while maintaining the path position on the plane which intersects perpendicularly with the direction of the force of gravity as is, corrects the animation path so that it is higher by the displacement data than the highest object, thereby generating the animation path which is used by the animation generating mechanism <highlight><bold>60</bold></highlight>. </paragraph>
<paragraph id="P-0187" lvl="0"><number>&lsqb;0187&rsqb;</number> In this manner, it is possible to generate an animation which moves a moving object along a floor or ceiling, even without setting the path of the moving object 3-dimensionally. </paragraph>
<paragraph id="P-0188" lvl="0"><number>&lsqb;0188&rsqb;</number> In <cross-reference target="DRAWINGS">FIG. 23</cross-reference>, the disclosure is of a configuration in which the direction data and the displacement data managed by the animation data management mechanism <highlight><bold>54</bold></highlight> are common to all moving objects, and it is normally necessary to store this direction data and displacement data in accordance with the attributes of the moving objects. </paragraph>
<paragraph id="P-0189" lvl="0"><number>&lsqb;0189&rsqb;</number> In the case of a plurality of moving objects, as shown in <cross-reference target="DRAWINGS">FIG. 29</cross-reference>, the configuration to be used is one in which binding vectors, which are formed from this direction data and displacement data, are stored separately for each moving object. By adopting this configuration, it is possible to implement an animation in which there is one object moving at a fixed distance from another object, and another object moving at a fixed lateral distance from yet another object. </paragraph>
<paragraph id="P-0190" lvl="0"><number>&lsqb;0190&rsqb;</number> In the processing flows shown in <cross-reference target="DRAWINGS">FIG. 25</cross-reference> and <cross-reference target="DRAWINGS">FIG. 28</cross-reference>, although the disclosure is of a configuration in which when the position of a moving object at time T is determined at step <highlight><bold>1</bold></highlight>, at step <highlight><bold>2</bold></highlight> a straight line is determined which passes through this position and is parallel to the direction indicated by the direction data, all objects which intersect with that straight line being searched, it is also possible to remove small objects from the search operation to have a configuration which enables high-speed processing. </paragraph>
<paragraph id="P-0191" lvl="0"><number>&lsqb;0191&rsqb;</number> When using this configuration, as shown in <cross-reference target="DRAWINGS">FIG. 30</cross-reference>, an object list indicating exactly which objects are to be searched for among the objects which make up the virtual world is prepared, the objects in this object list being taken as the ones for searching. </paragraph>
<paragraph id="P-0192" lvl="0"><number>&lsqb;0192&rsqb;</number> When following the processing flows as shown in <cross-reference target="DRAWINGS">FIG. 25</cross-reference> and <cross-reference target="DRAWINGS">FIG. 28</cross-reference>, there are cases in which a sudden change occurs in a corrected path. For example, when the corrected path that is shown in <cross-reference target="DRAWINGS">FIG. 31A</cross-reference> is determined, during execution of the animation from position a to position b, there is an instantaneous motion, as there is also when executing the animation from the position c to the position d. </paragraph>
<paragraph id="P-0193" lvl="0"><number>&lsqb;0193&rsqb;</number> To solve the problem that the above condition presents, in the case in which the difference between the corrected data of the previous frame and the current frame exceeds a threshold value, the configuration can be made such that data for an interpolated path between the previous frame and the current frame is generated. By adopting this configuration, the corrected path shown in <cross-reference target="DRAWINGS">FIG. 32A</cross-reference> is changed to the path shown in <cross-reference target="DRAWINGS">FIG. 32</cross-reference>B, thereby making it conform to reality. In this configuration, the underlined position correction part of the equation  
<math-cwu id="MATH-US-00004">
<number>4</number>
<math>
<mtable>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mo>(</mo>
          <mrow>
            <msub>
              <mi>x</mi>
              <mi>h</mi>
            </msub>
            <mo>,</mo>
            <msub>
              <mi>y</mi>
              <mi>h</mi>
            </msub>
            <mo>,</mo>
            <msub>
              <mi>z</mi>
              <mi>h</mi>
            </msub>
          </mrow>
          <mo>)</mo>
        </mrow>
        <mo>=</mo>
        <mstyle>
          <mtext>&emsp;</mtext>
        </mstyle>
        <mo>&it;</mo>
        <mrow>
          <mrow>
            <mo>(</mo>
            <mrow>
              <msub>
                <mi>x</mi>
                <mi>f</mi>
              </msub>
              <mo>,</mo>
              <msub>
                <mi>y</mi>
                <mi>f</mi>
              </msub>
              <mo>,</mo>
              <msub>
                <mi>z</mi>
                <mi>f</mi>
              </msub>
            </mrow>
            <mo>)</mo>
          </mrow>
          <mo>-</mo>
          <mrow>
            <mo>{</mo>
            <mrow>
              <mo>(</mo>
              <mrow>
                <mo>(</mo>
                <mrow>
                  <mi>Distance</mi>
                  <mo>&it;</mo>
                  <mstyle>
                    <mtext>&emsp;</mtext>
                  </mstyle>
                  <mo>&it;</mo>
                  <mi>between</mi>
                  <mo>&it;</mo>
                  <mstyle>
                    <mtext>&emsp;</mtext>
                  </mstyle>
                  <mo>&it;</mo>
                  <mrow>
                    <mo>(</mo>
                    <mrow>
                      <msub>
                        <mi>x</mi>
                        <mi>f</mi>
                      </msub>
                      <mo>,</mo>
                      <msub>
                        <mi>y</mi>
                        <mi>f</mi>
                      </msub>
                      <mo>,</mo>
                      <msub>
                        <mi>z</mi>
                        <mi>f</mi>
                      </msub>
                    </mrow>
                    <mo>)</mo>
                  </mrow>
                  <mo>&it;</mo>
                  <mstyle>
                    <mtext>&emsp;</mtext>
                  </mstyle>
                  <mo>&it;</mo>
                  <mi>and</mi>
                </mrow>
              </mrow>
            </mrow>
          </mrow>
        </mrow>
      </mrow>
    </mtd>
  </mtr>
  <mtr>
    <mtd>
      <mrow>
        <mrow>
          <mrow>
            <mrow>
              <mstyle>
                <mtext>&emsp;</mtext>
              </mstyle>
              <mo>&it;</mo>
              <mrow>
                <mrow>
                  <mo>(</mo>
                  <mrow>
                    <msub>
                      <mi>x</mi>
                      <mi>q</mi>
                    </msub>
                    <mo>,</mo>
                    <msub>
                      <mi>y</mi>
                      <mi>q</mi>
                    </msub>
                    <mo>,</mo>
                    <msub>
                      <mi>z</mi>
                      <mi>q</mi>
                    </msub>
                  </mrow>
                  <mo>)</mo>
                </mrow>
                <mo>)</mo>
              </mrow>
            </mrow>
            <mo>-</mo>
            <mi>H</mi>
          </mrow>
          <mo>}</mo>
        </mrow>
        <mo>&times;</mo>
        <mrow>
          <mo>(</mo>
          <mrow>
            <msub>
              <mi>x</mi>
              <mi>d</mi>
            </msub>
            <mo>,</mo>
            <msub>
              <mi>y</mi>
              <mi>d</mi>
            </msub>
            <mo>,</mo>
            <msub>
              <mi>z</mi>
              <mi>d</mi>
            </msub>
          </mrow>
          <mo>)</mo>
        </mrow>
      </mrow>
    </mtd>
  </mtr>
</mtable>
</math>
<mathematica-file id="MATHEMATICA-00004" file="US20030001843A1-20030102-M00004.NB"/>
<image id="EMI-M00004" wi="216.027" he="24.01245" file="US20030001843A1-20030102-M00004.TIF" imf="TIFF" ti="MF"/>
</math-cwu>
</paragraph>
<paragraph id="P-0194" lvl="7"><number>&lsqb;0194&rsqb;</number> is prepared for each frame, the above scheme of interpolation being employed by making a test as to whether or not the difference between the prepared value for the previous frame and the prepared value for the current frame exceeds a threshold value. </paragraph>
</section>
</detailed-description>
</subdoc-description>
<subdoc-claims>
<heading lvl="1">What is claimed is: </heading>
<claim id="CLM-00001">
<claim-text><highlight><bold>1</bold></highlight>. A computer graphics (CG) data generating apparatus for generating CG data for a CG screen from computer aided design (CAD) data that defines a virtual object made up of polygons, said CG data generating apparatus comprising: 
<claim-text>a light ray generating/intersecting judgment section for virtually generating a plurality of light rays and for making a judgment as to whether or not each of said light rays intersects with said polygons which make up said object, and </claim-text>
<claim-text>a data deleting section for eliminating, from said CAD data, data with respect to a polygon for which said judgment of the light ray generating/intersecting judgment section is that none of said light rays intersects therewith, thereby generating CG data. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00002">
<claim-text><highlight><bold>2</bold></highlight>. A CG data generating apparatus according to <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein the plurality of generated light rays are aimed from outside said object to the inside of said object. </claim-text>
</claim>
<claim id="CLM-00003">
<claim-text><highlight><bold>3</bold></highlight>. A CG data generating apparatus according to <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein with regard to an object which is viewed from the outside, said light ray generating/intersecting judgment section generates light rays aimed from outside said object to the inside of said object, wherein with regard to an object which is viewed from inside only, said light ray generating/intersecting judgment section generates light rays aimed from inside said object to the outside of said object, and wherein with regard to an object that has the possibility of being viewed from both inside and outside, said light ray generating/intersecting judgment section generates both light rays aimed from outside said object to the inside of said object and light rays aimed from inside said object to the outside of said object. </claim-text>
</claim>
<claim id="CLM-00004">
<claim-text><highlight><bold>4</bold></highlight>. A CG data generating apparatus according to <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein said light ray generating/intersecting judgment section includes a first judgment section for performing a judgment concerning light rays aimed from outside said object to the inside of said object, and a second judgment section for performing a judgment concerning light rays aimed from inside said object to the outside of said object, and wherein said data deleting section includes a first deleting section for eliminating data corresponding to a polygon which is judged by said first judgment section to intersect with none of the plurality of light rays, thereby generating first CG data suitable for making a CG image of said object when viewed from outside, and a second deleting section for eliminating data corresponding to a polygon which is judged by said first judgment section to intersect with none of the plurality of light rays, thereby generating second CG data suitable for making a CG image of said object when viewed from inside. </claim-text>
</claim>
<claim id="CLM-00005">
<claim-text><highlight><bold>5</bold></highlight>. A CG animation editing apparatus for generating CG animation of a virtual object from CAD data defining the object made up of polygons, said CG animation editing apparatus comprising: 
<claim-text>a light ray generating/intersecting judgment section for virtually generating a plurality of light rays and for making a judgment as to whether or not each of said light rays intersects with said polygons which make up said object; </claim-text>
<claim-text>a data deleting section for eliminating, from said CAD data, data corresponding to a polygon for which the judgment of said light ray generating/intersecting judgment section is that no light ray intersects therewith, thereby generating CG data which are suitable for the generation of a computer graphics image; and </claim-text>
<claim-text>a CC animation generating section which generates CC animation by using said CG data generated by said data deleting section. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00006">
<claim-text><highlight><bold>6</bold></highlight>. A CG animation editing apparatus according to <dependent-claim-reference depends_on="CLM-00005">claim 5</dependent-claim-reference>, wherein the plurality of generated light rays are aimed from outside said object to the inside of said object. </claim-text>
</claim>
<claim id="CLM-00007">
<claim-text><highlight><bold>7</bold></highlight>. A CG animation editing apparatus according to <dependent-claim-reference depends_on="CLM-00005">claim 5</dependent-claim-reference>, wherein with regard to an object which is viewed from the outside, said light ray generating/intersecting judgment section generates light rays aimed from outside said object to the inside of said object, wherein with regard to an object which is viewed from inside only, said light ray generating/intersecting judgment section generates light rays aimed from inside said object to the outside of said object, and wherein with regard to an object that has the possibility of being viewed from both inside and outside, said light ray generating/intersecting judgment section generates both light rays aimed from outside said object to the inside of said object and light rays aimed from inside said object to the outside of said object. </claim-text>
</claim>
<claim id="CLM-00008">
<claim-text><highlight><bold>8</bold></highlight>. A CG animation editing apparatus according to <dependent-claim-reference depends_on="CLM-00005">claim 5</dependent-claim-reference>, wherein said light ray generating/intersecting judgment section includes a first judgment section for performing a judgment concerning light rays aimed from outside said object to the inside of said object, and a second judgment section for performing a judgment concerning light rays aimed from inside said object to the outside of said object, and wherein said data deleting section includes a first deleting section for eliminating data corresponding to a polygon which is judged by said first judgment section to intersect with none of the plurality of light rays, thereby generating first CG data suitable for making a CG image of said object when viewed from outside, and a second deleting section for eliminating data corresponding to a polygon which is judged by said first judgment section to intersect with none of the plurality of light rays, thereby generating a second CG data suitable for making a CG image of said object when viewed from inside, said CG animation editing apparatus further comprising a viewing point judgment section which judges whether the current viewing point is inside an object or outside an object, and said CG animation generating section selecting one of said first and second CG data, in accordance with judgement of said viewing point judgment section, thereby generating CG animation. </claim-text>
</claim>
<claim id="CLM-00009">
<claim-text><highlight><bold>9</bold></highlight>. An animation path generating apparatus comprising: 
<claim-text>means for interactively setting a plurality of path points through which a path passes within a virtual space inside which an object exists; </claim-text>
<claim-text>means for judging, when a path which joins adjacent path points is set, whether or not there exists an object which interferes, either directly or indirectly, with the path; </claim-text>
<claim-text>means for setting a path which joins said adjacent path points when said judging means judges that an interfering object does not exist; and </claim-text>
<claim-text>means for automatically setting an alternate route path between said adjacent path points when said judging means judges that an interfering object does exist. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00010">
<claim-text><highlight><bold>10</bold></highlight>. An animation path generating apparatus according to <dependent-claim-reference depends_on="CLM-00009">claim 9</dependent-claim-reference>, further comprising means for determining a bounding circle for the object, by slicing a bounding sphere which includes said object using a plane which includes the adjacent path points, said alternate route path setting means setting a path within said plane which detours around the bounding circle, thereby setting a detour path around said object. </claim-text>
</claim>
<claim id="CLM-00011">
<claim-text><highlight><bold>11</bold></highlight>. An animation path generating apparatus according to <dependent-claim-reference depends_on="CLM-00009">claim 9</dependent-claim-reference>, further comprising means for determining a cross section for the object, by slicing the object, using a plane which includes the adjacent path points, said alternate route path setting means setting a path within said plane which detours around the cross section, thereby setting a detour path around said object. </claim-text>
</claim>
<claim id="CLM-00012">
<claim-text><highlight><bold>12</bold></highlight>. An animation path generating apparatus according to <dependent-claim-reference depends_on="CLM-00009">claim 9</dependent-claim-reference>, further comprising a means for determining an expanded cross section for the object, by slicing the object, using a plane which includes the adjacent path points, and expanding it by a prescribed multiplier, said alternate route path setting means setting a path within said plane which detours around the expanded cross section, thereby setting a detour path around said object. </claim-text>
</claim>
<claim id="CLM-00013">
<claim-text><highlight><bold>13</bold></highlight>. An animation path generating apparatus according to <dependent-claim-reference depends_on="CLM-00009">claim 9</dependent-claim-reference>, further comprising means for locating an emergence and an intake of a fluid at said adjacent path points, and means for locating an emergence and an intake of a fluid at position of the object, intensity of the emergence and intake being responsive to a size of the object, wherein said alternate route path setting means establishes one of the flow lines between adjacent path points determined by fluid dynamics as the alternate route path. </claim-text>
</claim>
<claim id="CLM-00014">
<claim-text><highlight><bold>14</bold></highlight>. An animation path generating apparatus comprising: 
<claim-text>means for interactively setting a path of a moving object in a perspective view of a virtual world, from a selected direction; </claim-text>
<claim-text>means for calculating a position in said virtual world of said path set by the interactive setting means; </claim-text>
<claim-text>means for correcting along the selected direction the position of the path at each time to a position which is removed from an object by a prescribed distance. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00015">
<claim-text><highlight><bold>15</bold></highlight>. An animation path generating apparatus according to <dependent-claim-reference depends_on="CLM-00011">claim 14</dependent-claim-reference>, wherein said position correcting means corrects the path position to a position which is removed from an object which is closest to a viewing point of the perspective view by a prescribed distance toward the viewing point. </claim-text>
</claim>
<claim id="CLM-00016">
<claim-text><highlight><bold>16</bold></highlight>. An animation path generating apparatus according to <dependent-claim-reference depends_on="CLM-00011">claim 14</dependent-claim-reference>, wherein said position correcting means corrects the path position to a position which is removed from an object which is closest to the viewing point, of objects which are more distant from the viewing point than the position of the path before correction, by a prescribed distance toward the viewing point. </claim-text>
</claim>
<claim id="CLM-00017">
<claim-text><highlight><bold>17</bold></highlight>. An animation path generating apparatus according to <dependent-claim-reference depends_on="CLM-00011">claim 14</dependent-claim-reference>, wherein said selected direction and prescribed distance are set separately for each moving object. </claim-text>
</claim>
<claim id="CLM-00018">
<claim-text><highlight><bold>18</bold></highlight>. An animation path generating apparatus according to <dependent-claim-reference depends_on="CLM-00011">claim 14</dependent-claim-reference>, wherein said position correcting means corrects the path position considering only specified objects existing in said virtual world. </claim-text>
</claim>
<claim id="CLM-00019">
<claim-text><highlight><bold>19</bold></highlight>. An animation path generating apparatus according to <dependent-claim-reference depends_on="CLM-00011">claim 14</dependent-claim-reference>, wherein when the difference between the correction at a previous time and the correction at the current time is greater than a prescribed value, said position correcting means performs interpolation between the correction at a previous time and the correction at the current time.</claim-text>
</claim>
</subdoc-claims>
<subdoc-drawings id="DRAWINGS">
<heading lvl="0" align="CENTER">Drawings</heading>
<representative-figure>1</representative-figure>
<figure id="figure-D00000">
<image id="EMI-D00000" file="US20030001843A1-20030102-D00000.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00001">
<image id="EMI-D00001" file="US20030001843A1-20030102-D00001.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00002">
<image id="EMI-D00002" file="US20030001843A1-20030102-D00002.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00003">
<image id="EMI-D00003" file="US20030001843A1-20030102-D00003.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00004">
<image id="EMI-D00004" file="US20030001843A1-20030102-D00004.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00005">
<image id="EMI-D00005" file="US20030001843A1-20030102-D00005.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00006">
<image id="EMI-D00006" file="US20030001843A1-20030102-D00006.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00007">
<image id="EMI-D00007" file="US20030001843A1-20030102-D00007.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00008">
<image id="EMI-D00008" file="US20030001843A1-20030102-D00008.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00009">
<image id="EMI-D00009" file="US20030001843A1-20030102-D00009.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00010">
<image id="EMI-D00010" file="US20030001843A1-20030102-D00010.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00011">
<image id="EMI-D00011" file="US20030001843A1-20030102-D00011.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00012">
<image id="EMI-D00012" file="US20030001843A1-20030102-D00012.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00013">
<image id="EMI-D00013" file="US20030001843A1-20030102-D00013.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00014">
<image id="EMI-D00014" file="US20030001843A1-20030102-D00014.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00015">
<image id="EMI-D00015" file="US20030001843A1-20030102-D00015.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00016">
<image id="EMI-D00016" file="US20030001843A1-20030102-D00016.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00017">
<image id="EMI-D00017" file="US20030001843A1-20030102-D00017.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00018">
<image id="EMI-D00018" file="US20030001843A1-20030102-D00018.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00019">
<image id="EMI-D00019" file="US20030001843A1-20030102-D00019.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00020">
<image id="EMI-D00020" file="US20030001843A1-20030102-D00020.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00021">
<image id="EMI-D00021" file="US20030001843A1-20030102-D00021.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00022">
<image id="EMI-D00022" file="US20030001843A1-20030102-D00022.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00023">
<image id="EMI-D00023" file="US20030001843A1-20030102-D00023.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00024">
<image id="EMI-D00024" file="US20030001843A1-20030102-D00024.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00025">
<image id="EMI-D00025" file="US20030001843A1-20030102-D00025.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00026">
<image id="EMI-D00026" file="US20030001843A1-20030102-D00026.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00027">
<image id="EMI-D00027" file="US20030001843A1-20030102-D00027.TIF" imf="TIFF" ti="DR"/>
</figure>
</subdoc-drawings>
</patent-application-publication>
