<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE patent-application-publication SYSTEM "pap-v16-2002-01-01.dtd" [
<!ENTITY US20030002724A1-20030102-D00000.TIF SYSTEM "US20030002724A1-20030102-D00000.TIF" NDATA TIF>
<!ENTITY US20030002724A1-20030102-D00001.TIF SYSTEM "US20030002724A1-20030102-D00001.TIF" NDATA TIF>
<!ENTITY US20030002724A1-20030102-D00002.TIF SYSTEM "US20030002724A1-20030102-D00002.TIF" NDATA TIF>
<!ENTITY US20030002724A1-20030102-D00003.TIF SYSTEM "US20030002724A1-20030102-D00003.TIF" NDATA TIF>
<!ENTITY US20030002724A1-20030102-D00004.TIF SYSTEM "US20030002724A1-20030102-D00004.TIF" NDATA TIF>
<!ENTITY US20030002724A1-20030102-D00005.TIF SYSTEM "US20030002724A1-20030102-D00005.TIF" NDATA TIF>
<!ENTITY US20030002724A1-20030102-D00006.TIF SYSTEM "US20030002724A1-20030102-D00006.TIF" NDATA TIF>
<!ENTITY US20030002724A1-20030102-D00007.TIF SYSTEM "US20030002724A1-20030102-D00007.TIF" NDATA TIF>
<!ENTITY US20030002724A1-20030102-D00008.TIF SYSTEM "US20030002724A1-20030102-D00008.TIF" NDATA TIF>
<!ENTITY US20030002724A1-20030102-D00009.TIF SYSTEM "US20030002724A1-20030102-D00009.TIF" NDATA TIF>
<!ENTITY US20030002724A1-20030102-D00010.TIF SYSTEM "US20030002724A1-20030102-D00010.TIF" NDATA TIF>
<!ENTITY US20030002724A1-20030102-D00011.TIF SYSTEM "US20030002724A1-20030102-D00011.TIF" NDATA TIF>
<!ENTITY US20030002724A1-20030102-D00012.TIF SYSTEM "US20030002724A1-20030102-D00012.TIF" NDATA TIF>
<!ENTITY US20030002724A1-20030102-D00013.TIF SYSTEM "US20030002724A1-20030102-D00013.TIF" NDATA TIF>
<!ENTITY US20030002724A1-20030102-D00014.TIF SYSTEM "US20030002724A1-20030102-D00014.TIF" NDATA TIF>
<!ENTITY US20030002724A1-20030102-D00015.TIF SYSTEM "US20030002724A1-20030102-D00015.TIF" NDATA TIF>
<!ENTITY US20030002724A1-20030102-D00016.TIF SYSTEM "US20030002724A1-20030102-D00016.TIF" NDATA TIF>
<!ENTITY US20030002724A1-20030102-D00017.TIF SYSTEM "US20030002724A1-20030102-D00017.TIF" NDATA TIF>
<!ENTITY US20030002724A1-20030102-D00018.TIF SYSTEM "US20030002724A1-20030102-D00018.TIF" NDATA TIF>
<!ENTITY US20030002724A1-20030102-D00019.TIF SYSTEM "US20030002724A1-20030102-D00019.TIF" NDATA TIF>
<!ENTITY US20030002724A1-20030102-D00020.TIF SYSTEM "US20030002724A1-20030102-D00020.TIF" NDATA TIF>
]>
<patent-application-publication>
<subdoc-bibliographic-information>
<document-id>
<doc-number>20030002724</doc-number>
<kind-code>A1</kind-code>
<document-date>20030102</document-date>
</document-id>
<publication-filing-type>new</publication-filing-type>
<domestic-filing-data>
<application-number>
<doc-number>10181783</doc-number>
</application-number>
<application-number-series-code>10</application-number-series-code>
<filing-date>20020722</filing-date>
</domestic-filing-data>
<foreign-priority-data>
<priority-application-number>
<doc-number>2000-358420</doc-number>
</priority-application-number>
<filing-date>20001124</filing-date>
<country-code>JP</country-code>
</foreign-priority-data>
<technical-information>
<classification-ipc>
<classification-ipc-primary>
<ipc>G06T005/00</ipc>
</classification-ipc-primary>
<classification-ipc-secondary>
<ipc>G06T005/40</ipc>
</classification-ipc-secondary>
<classification-ipc-secondary>
<ipc>G06T017/00</ipc>
</classification-ipc-secondary>
<classification-ipc-edition>07</classification-ipc-edition>
</classification-ipc>
<classification-us>
<classification-us-primary>
<uspc>
<class>382</class>
<subclass>131000</subclass>
</uspc>
</classification-us-primary>
<classification-us-secondary>
<uspc>
<class>382</class>
<subclass>275000</subclass>
</uspc>
</classification-us-secondary>
<classification-us-secondary>
<uspc>
<class>382</class>
<subclass>169000</subclass>
</uspc>
</classification-us-secondary>
<classification-us-secondary>
<uspc>
<class>382</class>
<subclass>285000</subclass>
</uspc>
</classification-us-secondary>
</classification-us>
<title-of-invention>Image processing method</title-of-invention>
</technical-information>
<inventors>
<first-named-inventor>
<name>
<given-name>Shinobu</given-name>
<family-name>Befu</family-name>
</name>
<residence>
<residence-non-us>
<city>Ibaraki</city>
<country-code>JP</country-code>
</residence-non-us>
</residence>
<authority-applicant>INV</authority-applicant>
</first-named-inventor>
<inventor>
<name>
<given-name>Yoshinori</given-name>
<family-name>Arai</family-name>
</name>
<residence>
<residence-non-us>
<city>Tokyo</city>
<country-code>JP</country-code>
</residence-non-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
<inventor>
<name>
<given-name>Hitoshi</given-name>
<family-name>Tsunashima</family-name>
</name>
<residence>
<residence-non-us>
<city>Chiba</city>
<country-code>JP</country-code>
</residence-non-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
<inventor>
<name>
<given-name>Masakazu</given-name>
<family-name>Suzuki</family-name>
</name>
<residence>
<residence-non-us>
<city>Kyoto</city>
<country-code>JP</country-code>
</residence-non-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
</inventors>
<correspondence-address>
<name-1>OBLON SPIVAK MCCLELLAND MAIER &amp; NEUSTADT PC</name-1>
<name-2>FOURTH FLOOR</name-2>
<address>
<address-1>1755 JEFFERSON DAVIS HIGHWAY</address-1>
<city>ARLINGTON</city>
<state>VA</state>
<postalcode>22202</postalcode>
<country>
<country-code>US</country-code>
</country>
</address>
</correspondence-address>
<international-conventions>
<pct-application>
<document-id>
<doc-number>PCT/JP01/10211</doc-number>
<document-date>20011122</document-date>
<country-code>WO</country-code>
</document-id>
</pct-application>
</international-conventions>
</subdoc-bibliographic-information>
<subdoc-abstract>
<paragraph id="A-0001" lvl="0">An image processing method comprises an extracting step for extracting a plurality of images of different depths of a 3-dimensional object, the axes forming different angles with the 3-dimensional object, a binary coding step for eliminating a gray level range of the plurality of images obtained by the extracting step, the gray level range containing a small amount of components of the 3-dimensional object, remapping the other portion to a predetermined gray level value range, and putting the plurality of images in binary form, and a step for forming a 3-dimensional image of the 3-dimensional object based on the plurality of images put in binary form by the binary coding step. </paragraph>
</subdoc-abstract>
<subdoc-description>
<summary-of-invention>
<section>
<heading lvl="1">TECHNICAL FIELD </heading>
<paragraph id="P-0001" lvl="0"><number>&lsqb;0001&rsqb;</number> The invention relates to an image processing method and particularly to an image processing method to process 3-dimensional images. </paragraph>
</section>
<section>
<heading lvl="1">BACKGROUND ART </heading>
<paragraph id="P-0002" lvl="0"><number>&lsqb;0002&rsqb;</number> Recently, with development of a computer technique that is applied to the medical field, the diagnosis of a human body and structural analysis are commonly performed based on 3-dimensional data obtained by CT and MRI. Additionally, in the field of dentistry, a 3-dimensional imaging technique is used for a study to form a 3-dimensional model of the temporomandibular joint from data filmed by CT, for example, Yoshinori Arai, Koji Hashimoto, Hiroshi Shinoda, &ldquo;Development in 3D imaging program for small field-sized X-ray computed tomography (an Ortho-CT image) for dentistry use&rdquo;, Dentistry Radioactive Rays, <highlight><bold>39</bold></highlight> &ldquo;4&rdquo; P224-P229, 2000. </paragraph>
<paragraph id="P-0003" lvl="0"><number>&lsqb;0003&rsqb;</number> However, there still remain lots of things in which the operator is involved to process the CT image in the conventional study. Therefore, it is desired that a system can semi-automatically create, based on the data obtained by CT, a 3-dimensional model that the operator can process afterward. </paragraph>
<paragraph id="P-0004" lvl="0"><number>&lsqb;0004&rsqb;</number> Additionally, a CT image of the temporomandibular joint requires a plurality of transmitted images or reflective images taken by applying radiation to a body. The amount of radiation applied to the body must be minimized so as to not expose the body to excessive radiation. </paragraph>
<paragraph id="P-0005" lvl="0"><number>&lsqb;0005&rsqb;</number> However, the CT image obtained by weak radiation is noisy. In the case of a CT apparatus for the temporomandibular joint, the amount of X-ray radiation is limited to about {fraction (1/100)} of that of a conventional CT apparatus for general medical use. The 3-dimensional image obtained by such a weak X-ray radiation sometimes is partially not clear enough for dental use. </paragraph>
</section>
<section>
<heading lvl="1">DISCLOSURE OF INVENTION </heading>
<paragraph id="P-0006" lvl="0"><number>&lsqb;0006&rsqb;</number> Accordingly, it is a general object of the present invention to provide a novel and useful image processing method in which one or more of the problems described above are eliminated. </paragraph>
<paragraph id="P-0007" lvl="0"><number>&lsqb;0007&rsqb;</number> Another and more specific object of the present invention is to provide a high speed image processing method to create a low noise image even using weak radiation. </paragraph>
<paragraph id="P-0008" lvl="0"><number>&lsqb;0008&rsqb;</number> To achieve this object, the present invention is configured to form a 3-dimensional image of a <highlight><bold>3</bold></highlight>dimensional object by an extracting step for extracting a plurality of images of a 3-dimensional object along one or more axes at different depths, said axes forming different angles with said 3-dimensional object, a binary coding step for eliminating a gray level range containing small components of said object, remapping the other portion of the gray level range to a predetermined gray level value range, and putting the plurality of images in binary form, and a step for forming a 3-dimensional image of said 3-dimensional object based on the plurality of images put in binary form by said binary coding step. </paragraph>
<paragraph id="P-0009" lvl="0"><number>&lsqb;0009&rsqb;</number> To achieve this object, another configuration of the present invention is configured to form a 3-dimensional image of a 3-dimensional object by an extracting step for extracting, from a 3-dimensional object, a plurality of images along a plurality of axes, wherein each axis is in a different direction and each image is at a different depth on an axis, a binary coding step for eliminating a gray level range containing small components of said object, remapping the other portion of the gray level range to a predetermined gray level value range, and putting the plurality of images in binary form, and a step for forming a 3-dimensional image of said 3-dimensional object based on the plurality of images put in binary form by said binary coding step. </paragraph>
<paragraph id="P-0010" lvl="0"><number>&lsqb;0010&rsqb;</number> The present invention, configured as described above, can provide an image processing method with which low noise images are obtained by high speed processing even with a small amount of radiation, and one can surely obtain an image of the 3-dimensional object even if a gray level range containing a small amount of the 3-dimensional object components is eliminated, since a plurality of images of different depths of the 3-dimensional object are extracted for a plurality of angles and many images are obtained. Images with small influence of noise are also obtainable since noise components can be diffused by eliminating a gray level range containing a small amount of the 3-dimensional object components, remapping the other portion to a predetermined gray level range, and binary coding. </paragraph>
<paragraph id="P-0011" lvl="0"><number>&lsqb;0011&rsqb;</number> In order to reduce the noise component without damaging the accuracy of the image, the present invention can be configured to include a step for extracting averaged images from the average of a predetermined number of images consecutive in the depth direction, or the present invention can be configured to include a step of extracting images by averaging a predetermined number of images, each averaging performed on a predetermined number of images shifted by one image. </paragraph>
<paragraph id="P-0012" lvl="0"><number>&lsqb;0012&rsqb;</number> Further, in order to perform faster processing, the present invention can be configured to include a step for averaging one image for every plurality of images and performing extraction. </paragraph>
<paragraph id="P-0013" lvl="0"><number>&lsqb;0013&rsqb;</number> In addition, the degrading of accuracy is negligible since the present invention uses a great number of images even when configured as described above. </paragraph>
<paragraph id="P-0014" lvl="0"><number>&lsqb;0014&rsqb;</number> Furthermore, in order to reduce the noise component and extract the image of the 3-dimensional object for sure, the present invention can be configured to include a step for binary coding after remapping a gray level value range, in which noise component is negligible, in the original gray level distribution containing a great amount of noise components in the histogram of gray level of the 3-dimensional object. </paragraph>
<paragraph id="P-0015" lvl="0"><number>&lsqb;0015&rsqb;</number> Furthermore, in order to surely extract an image containing the components of the 3-dimensional object from the image containing the background components or the imaging object component, the present invention can be configured to include a step for performing remapping depending on a peak gray level, multiplied by a predetermined coefficient, in a gray level range containing a great amount of the background components or a peak gray level, multiplied by a predetermined coefficient, in a gray level range containing a great amount of the 3-dimensional object and for binary coding. </paragraph>
<paragraph id="P-0016" lvl="0"><number>&lsqb;0016&rsqb;</number> Furthermore, in order to obtain the most suitable image in consideration of gray level distribution of the peripheral pixels, the present invention can be configured to include a step for statistically performing the remapping based on the gray level distribution of peripheral pixels and binary coding.</paragraph>
</section>
</summary-of-invention>
<brief-description-of-drawings>
<section>
<heading lvl="1">BRIEF DESCRIPTION OF DRAWINGS </heading>
<paragraph id="P-0017" lvl="0"><number>&lsqb;0017&rsqb;</number> Other objects, features, and advantages of the present invention will become more apparent from the following detailed description when read in conjunction with the accompanying drawings. </paragraph>
<paragraph id="P-0018" lvl="0"><number>&lsqb;0018&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 1</cross-reference> is schematic block diagram of an imaging apparatus (an Ortho-CT). </paragraph>
<paragraph id="P-0019" lvl="0"><number>&lsqb;0019&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 2</cross-reference> is a schematic diagram for explaining a method of image extraction. </paragraph>
<paragraph id="P-0020" lvl="0"><number>&lsqb;0020&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 3</cross-reference> is a photograph that shows an original picture image of a temporomandibular joint. </paragraph>
<paragraph id="P-0021" lvl="0"><number>&lsqb;0021&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 4</cross-reference> is a flow diagram of image processing according to the first embodiment. </paragraph>
<paragraph id="P-0022" lvl="0"><number>&lsqb;0022&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 5</cross-reference> is a schematic diagram for explaining the averaging process according to the present invention. </paragraph>
<paragraph id="P-0023" lvl="0"><number>&lsqb;0023&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 6</cross-reference> is a photograph that shows the image obtained by performing the averaging process on the temporomandibular joint image showed in <cross-reference target="DRAWINGS">FIG. 3</cross-reference>. </paragraph>
<paragraph id="P-0024" lvl="0"><number>&lsqb;0024&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 7</cross-reference> is a histogram of a temporomandibular joint image. </paragraph>
<paragraph id="P-0025" lvl="0"><number>&lsqb;0025&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 8</cross-reference> is a photograph that shows a temporomandibular joint image having a histogram characteristic of <cross-reference target="DRAWINGS">FIG. 7</cross-reference>. </paragraph>
<paragraph id="P-0026" lvl="0"><number>&lsqb;0026&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 9</cross-reference> is a photograph that shows a temporomandibular joint image having a histogram characteristic of <cross-reference target="DRAWINGS">FIG. 10</cross-reference>. </paragraph>
<paragraph id="P-0027" lvl="0"><number>&lsqb;0027&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 10</cross-reference> is a histogram in which gray levels 100-255 showed in <cross-reference target="DRAWINGS">FIG. 7</cross-reference> are remapped to gray levels 0-255 according to the first embodiment. </paragraph>
<paragraph id="P-0028" lvl="0"><number>&lsqb;0028&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 11</cross-reference> is a binary coded image after partial emphasizing according to the first embodiment. </paragraph>
<paragraph id="P-0029" lvl="0"><number>&lsqb;0029&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 12</cross-reference> is a binary coded image of the original picture image according to the first embodiment. </paragraph>
<paragraph id="P-0030" lvl="0"><number>&lsqb;0030&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 13</cross-reference> is a photograph that shows the 3-dimensional image which is formed based on the image provided by image processing according to the first embodiment. </paragraph>
<paragraph id="P-0031" lvl="0"><number>&lsqb;0031&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 14</cross-reference> is a figure for explaining the angle of images about the <highlight><bold>3</bold></highlight> axes X, Y, and Z. </paragraph>
<paragraph id="P-0032" lvl="0"><number>&lsqb;0032&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 15</cross-reference> is a diagram for explaining the processing procedure according to the second embodiment. </paragraph>
<paragraph id="P-0033" lvl="0"><number>&lsqb;0033&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 16</cross-reference> is a diagram for explaining the angle of images about the five axes. </paragraph>
<paragraph id="P-0034" lvl="0"><number>&lsqb;0034&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 17</cross-reference> shows regular polyhedrons. </paragraph>
<paragraph id="P-0035" lvl="0"><number>&lsqb;0035&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 18</cross-reference> is a photograph that shows the 3-dimensional image that is obtained by conventional image processing. </paragraph>
<paragraph id="P-0036" lvl="0"><number>&lsqb;0036&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 19</cross-reference> is a photograph that shows the 3-dimensional image that is obtained by the first embodiment. </paragraph>
<paragraph id="P-0037" lvl="0"><number>&lsqb;0037&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 20</cross-reference> is a photograph that shows the <highlight><bold>3</bold></highlight>dimensional image that is obtained by image processing in 3 directions according to the second embodiment. </paragraph>
<paragraph id="P-0038" lvl="0"><number>&lsqb;0038&rsqb;</number> Main reference marks used in the above figures are explained as follows. </paragraph>
<paragraph id="P-0039" lvl="0"><number>&lsqb;0039&rsqb;</number> <highlight><bold>1</bold></highlight> is a system; <highlight><bold>11</bold></highlight> is a radiation source; <highlight><bold>12</bold></highlight> is a detector; <highlight><bold>13</bold></highlight> is an analog-to-digital converter; <highlight><bold>14</bold></highlight> is a general purpose computer; and <highlight><bold>21</bold></highlight> is an imaged 3-dimensional object.</paragraph>
</section>
</brief-description-of-drawings>
<detailed-description>
<section>
<heading lvl="1">BEST MODE FOR CARRYING OUT THE INVENTION </heading>
<paragraph id="P-0040" lvl="0"><number>&lsqb;0040&rsqb;</number> A description of the preferred embodiment of the present invention will be given below. </paragraph>
<paragraph id="P-0041" lvl="7"><number>&lsqb;0041&rsqb;</number> (The first embodiment) </paragraph>
<paragraph id="P-0042" lvl="0"><number>&lsqb;0042&rsqb;</number> This embodiment is the case such that 2-dimensional image data of the 3-dimensional object are extracted from 3-dimensional data obtained by an Ortho-CT apparatus. </paragraph>
<paragraph id="P-0043" lvl="0"><number>&lsqb;0043&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 1</cross-reference> is a block diagram showing an Ortho-CT apparatus. </paragraph>
<paragraph id="P-0044" lvl="0"><number>&lsqb;0044&rsqb;</number> Imaging apparatus <highlight><bold>1</bold></highlight> is an Ortho-CT apparatus configured by a radiation source <highlight><bold>11</bold></highlight>, detector <highlight><bold>12</bold></highlight>, analog-to-digital converter <highlight><bold>13</bold></highlight>, and a general purpose computer <highlight><bold>14</bold></highlight>. Radiation source <highlight><bold>11</bold></highlight> emits radiation, and the radiation emitted by radiation source <highlight><bold>11</bold></highlight> irradiates the 3-dimensional object <highlight><bold>21</bold></highlight>. The radiation is transmitted through the 3-dimensional object <highlight><bold>21</bold></highlight> and is incident on detector <highlight><bold>12</bold></highlight>. Detector <highlight><bold>12</bold></highlight> outputs detection signals in response to the strength of the incident radiation. </paragraph>
<paragraph id="P-0045" lvl="0"><number>&lsqb;0045&rsqb;</number> In addition, general purpose computer <highlight><bold>14</bold></highlight> may perform image processing by installing and running an image processing program stored in recording media such as HDD, CD-ROM, CD-R, and FDD. </paragraph>
<paragraph id="P-0046" lvl="0"><number>&lsqb;0046&rsqb;</number> Additionally, general purpose computer <highlight><bold>14</bold></highlight> may operate as the analog-to-digital converter <highlight><bold>13</bold></highlight> by running a software program. In this case, a separate analog-to-digital converter <highlight><bold>13</bold></highlight> may not be required. </paragraph>
<paragraph id="P-0047" lvl="0"><number>&lsqb;0047&rsqb;</number> Radiation source <highlight><bold>11</bold></highlight> and detector <highlight><bold>12</bold></highlight> are positioned facing each other with the 3-dimensional object <highlight><bold>21</bold></highlight> in between, and can rotate around the Z-axis at least 180 degrees. The (analog) signal detected by detector <highlight><bold>12</bold></highlight> is provided to analog-to-digital converter <highlight><bold>13</bold></highlight>, and converted into digital data. The data that are converted by analog-to-digital converter <highlight><bold>13</bold></highlight> are provided to general purpose computer <highlight><bold>14</bold></highlight> for image processing. The 3-dimensional data of the 3-dimensional object <highlight><bold>21</bold></highlight> are obtained in this manner. As showed in <cross-reference target="DRAWINGS">FIG. 2</cross-reference>, the 3-dimensional projection data obtained by the Ortho-CT has a cylinder shaped imaging region of 240*300*300 pixels. </paragraph>
<paragraph id="P-0048" lvl="0"><number>&lsqb;0048&rsqb;</number> In the image processing according to this embodiment, 3-dimensional data are first converted into 2-dimensional data, and then converted again into 3-dimensional data so that the image processing becomes simple. In other words, general purpose computer <highlight><bold>14</bold></highlight> extracts a 2-dimensional image from cylinder-shaped 3-dimensional data directly provided by an Ortho-CT. General purpose computer <highlight><bold>14</bold></highlight> processes the <highlight><bold>2</bold></highlight>-dimensional image, the details of which will be described later, to obtain binary images with reduced noise and converts the binary images into 3-dimensional data again. </paragraph>
<paragraph id="P-0049" lvl="0"><number>&lsqb;0049&rsqb;</number> An Ortho-CT is described in detail in &ldquo;Development of Ortho Cubic Super High Resolution CT (Ortho-CT)&rdquo;, Car &apos;<highlight><bold>98</bold></highlight>, P780- P785 (proc.), 1998, written by Arai Y, Tammisalo E, Iwai K, et al. </paragraph>
<paragraph id="P-0050" lvl="0"><number>&lsqb;0050&rsqb;</number> Next, the method for extracting 2-dimensional images from 3-dimensional data directly provided by an Ortho-CT will be described. </paragraph>
<paragraph id="P-0051" lvl="0"><number>&lsqb;0051&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 2</cross-reference> is a figure convenient for explaining the image extraction method according to an embodiment of the present invention. </paragraph>
<paragraph id="P-0052" lvl="0"><number>&lsqb;0052&rsqb;</number> Three dimensional data are obtained by taking images of the temporomandibular joint with the Ortho-CT showed in <cross-reference target="DRAWINGS">FIG. 1</cross-reference>. Two-dimensional images are obtained (extracted) from the 3-dimensional data. In order to get a temporomandibular joint image with a relatively clear outline, 4,416 images (276 images&times;16 directions), for example, are taken. In other words, as shown in <cross-reference target="DRAWINGS">FIG. 2, </cross-reference>276 images of 300 pixels wide X 240 pixels long, each having different depth in a direction, are taken for 16 directions. In addition, each pixel of extracted image data is expressed in 8 bits, 256 steps of gray scale, for example. </paragraph>
<paragraph id="P-0053" lvl="0"><number>&lsqb;0053&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 3</cross-reference> is an original image of a temporomandibular joint extracted as showed in <cross-reference target="DRAWINGS">FIG. 2</cross-reference>. The original image showed in <cross-reference target="DRAWINGS">FIG. 3</cross-reference> is noisy since image processing has not been performed yet. </paragraph>
<paragraph id="P-0054" lvl="0"><number>&lsqb;0054&rsqb;</number> General purpose computer <highlight><bold>14</bold></highlight> extracts the 2-dimensional images from the 3-dimensional data detected by detector <highlight><bold>12</bold></highlight> as described above, and stores the 2-dimensional images in internal memory. Image processing of the present embodiment is performed using 276 two-dimensional images stored in internal memory. </paragraph>
<paragraph id="P-0055" lvl="0"><number>&lsqb;0055&rsqb;</number> A detailed description of the image processing according to the present embodiment will be given below. </paragraph>
<paragraph id="P-0056" lvl="0"><number>&lsqb;0056&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 4</cross-reference> is a flow diagram showing the image processing according to the first embodiment of the present invention. </paragraph>
<paragraph id="P-0057" lvl="0"><number>&lsqb;0057&rsqb;</number> Image processing of the first embodiment includes steps S<highlight><bold>1</bold></highlight> through S<highlight><bold>4</bold></highlight>. Step S<highlight><bold>1</bold></highlight> is a step for averaging a plurality of images in order to reduce noise. Step S<highlight><bold>2</bold></highlight> is a step for appropriately remapping the plurality of images that were averaged in Step S<highlight><bold>2</bold></highlight>. In step S<highlight><bold>2</bold></highlight>, one may use a histogram emphasis method, for example. Step S<highlight><bold>3</bold></highlight> is a step for binary coding an image mapped in step S<highlight><bold>2</bold></highlight>. Step S<highlight><bold>4</bold></highlight> is a step for forming a 3-dimensional image of the imaging object based on the image binary coded in step S<highlight><bold>2</bold></highlight>. </paragraph>
<paragraph id="P-0058" lvl="0"><number>&lsqb;0058&rsqb;</number> By the way, a clear 2-dimensional image having low noise is available in step S<highlight><bold>3</bold></highlight> since the image data are processed in steps S<highlight><bold>1</bold></highlight> and S<highlight><bold>2</bold></highlight> prior to step S<highlight><bold>3</bold></highlight>. Therefore, one may regard steps S<highlight><bold>1</bold></highlight> and S<highlight><bold>2</bold></highlight> as preparatory steps of step S<highlight><bold>3</bold></highlight> included therein. </paragraph>
<paragraph id="P-0059" lvl="0"><number>&lsqb;0059&rsqb;</number> Averaging processing in step S<highlight><bold>1</bold></highlight> will be described first. <cross-reference target="DRAWINGS">FIG. 5</cross-reference> is a figure convenient for explaining the operation of averaging processing according to the embodiment of the present invention. </paragraph>
<paragraph id="P-0060" lvl="0"><number>&lsqb;0060&rsqb;</number> As showed in <cross-reference target="DRAWINGS">FIG. 5</cross-reference>(A), the averaging processing forms one 2-dimensional image by averaging every eight 2-dimensional images, for example, each pixel of the one 2-dimensional image being the average of corresponding pixels of the eight 2-dimensional images. For example, the first screen P<highlight><bold>1</bold></highlight> is the average of eight screens p<highlight><bold>1</bold></highlight>-p<highlight><bold>8</bold></highlight>; the second screen P<highlight><bold>2</bold></highlight> is the average of eight screens p<highlight><bold>2</bold></highlight>-p<highlight><bold>9</bold></highlight>; and the third screen P<highlight><bold>3</bold></highlight> is the average of eight screens p<highlight><bold>3</bold></highlight>-p<highlight><bold>10</bold></highlight>. </paragraph>
<paragraph id="P-0061" lvl="0"><number>&lsqb;0061&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 6</cross-reference> is a figure showing the averaged picture of the temporomandibular joint image shown in <cross-reference target="DRAWINGS">FIG. 5</cross-reference>. </paragraph>
<paragraph id="P-0062" lvl="0"><number>&lsqb;0062&rsqb;</number> Since noise is random, the noise approaches a certain value when it is added to each other. As showed in <cross-reference target="DRAWINGS">FIG. 6</cross-reference>, one may notice that the noise is reduced by averaging processing. </paragraph>
<paragraph id="P-0063" lvl="0"><number>&lsqb;0063&rsqb;</number> As described above, one can reduce the noise by only the averaging processing, even without any special image processing technique. In this embodiment, the next combination of eight screens for averaging processing is shifted by one screen from the preceding combination of eight screens, but one can select the next combination of eight screens next to the preceding combination of eight screens as shown in <cross-reference target="DRAWINGS">FIG. 5</cross-reference>(B). Additionally, the number of screens that are averaged is not limited to eight. Furthermore, averaging is not limited to a simple mean, but can be another statistical process that can reproduce images without distortion. For example, one can use an arithmetic mean depending on the characteristics of the noise. </paragraph>
<paragraph id="P-0064" lvl="0"><number>&lsqb;0064&rsqb;</number> Next, emphasizing processing in compliance with the histogram emphasis method of step S<highlight><bold>2</bold></highlight> is executed. </paragraph>
<paragraph id="P-0065" lvl="0"><number>&lsqb;0065&rsqb;</number> The histogram emphasis method emphasizes only the histograms in the gray level between a-b by applying the following expression (1) to an image. </paragraph>
<paragraph lvl="0"><in-line-formula>Y&equals;255* (X&minus;<highlight><italic>a</italic></highlight>) / (<highlight><italic>b&minus;a</italic></highlight>) &emsp;&emsp;(1) </in-line-formula></paragraph>
<paragraph id="P-0066" lvl="0"><number>&lsqb;0066&rsqb;</number> In this embodiment, some density ranges are emphasized first. For example, a gray scale range (a, b) that includes a large portion of the outline of temporomandibular joint, and then, the gray scale range a-b is remapped to the gray scale range 0-255, and accordingly partial emphasis is made. </paragraph>
<paragraph id="P-0067" lvl="0"><number>&lsqb;0067&rsqb;</number> In addition, the details of the histogram emphasis method are described in &ldquo;Introduction to image processing by C language, Shoko-Do, 2000&rdquo;. </paragraph>
<paragraph id="P-0068" lvl="0"><number>&lsqb;0068&rsqb;</number> In consideration of histograms of the gray level of an image, an image suitable for binary coding generally has 2 peaks. That is, the image suitable for binary coding is an image such that, when binary coded, an object and a non-object form respective peaks, of which difference is clear. This situation is desirable. </paragraph>
<paragraph id="P-0069" lvl="0"><number>&lsqb;0069&rsqb;</number> Histogram emphasizing will be described in more detail as follows. </paragraph>
<paragraph id="P-0070" lvl="0"><number>&lsqb;0070&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 7</cross-reference> is a histogram of gray levels of a temporomandibular joint image. Histogram emphasis processing reduces influence of noise components by removing gray level ranges that do not include the gray level of the temporomandibular joint that is an object (the range that mainly includes noise components and does not include many components of temporomandibular joint) and remapping the other gray level range to the original range. </paragraph>
<paragraph id="P-0071" lvl="0"><number>&lsqb;0071&rsqb;</number> In a temporomandibular joint image shown in <cross-reference target="DRAWINGS">FIG. 7</cross-reference>, as the result of trial and error, it is known that the gray level range less than 100, for example, does not include much information of the temporomandibular joint. Therefore, the histogram emphasis processing reduces noise by eliminating the gray level range less than 100 and remapping the remaining range 100-255 to the original range to emphasize the gray level range 100-255, and creates images with reduced effect of noise. </paragraph>
<paragraph id="P-0072" lvl="0"><number>&lsqb;0072&rsqb;</number> In addition, even if the histogram emphasis processing cuts a portion of uncertain data, the cut causes no problem in forming a 3-dimensional image of temporomandibular joint. In other words, since the present invention treats a lot of temporomandibular joint images and, even if the present invention cuts a portion of uncertain data, temporomandibular joint images of the other angles indicate the portion clearly, and there is no problem in forming the temporomandibular joint image. For example, since an unclear region that is located at the edge of a certain image is, in the case of another image of a different angle, located at the center, ignoring the unclear data at the edge position causes no problem. </paragraph>
<paragraph id="P-0073" lvl="0"><number>&lsqb;0073&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 8</cross-reference> is an original image of a temporomandibular joint having histogram characteristics shown in <cross-reference target="DRAWINGS">FIG. 7</cross-reference>; <cross-reference target="DRAWINGS">FIG. 9</cross-reference> is a temporomandibular joint image of which gray levels less than 100 are eliminated and gray levels 100-255 are emphasized; and <cross-reference target="DRAWINGS">FIG. 10</cross-reference> is a histogram of a temporomandibular joint image of <cross-reference target="DRAWINGS">FIG. 9</cross-reference>. As to the image after emphasizing shown in <cross-reference target="DRAWINGS">FIG. 9</cross-reference>, in comparison with the original image shown in <cross-reference target="DRAWINGS">FIG. 8</cross-reference>, one may notice that the noise component is reduced, and the contrast between the portion of the temporomandibular joint and the background is clear. </paragraph>
<paragraph id="P-0074" lvl="0"><number>&lsqb;0074&rsqb;</number> In addition, as to mapping, various kinds of methods are considerable. In the gray level histogram shown in <cross-reference target="DRAWINGS">FIG. 7</cross-reference>, among three peaks, it is supposed that the left peak is the black level, the middle is the background level, and the right is the white level. Thus, since there is no problem for the temporomandibular joint image information even if the background portion in the center does not exist, for example, one can eliminate the background portion in the center and use gray levels near the white level and the black level for emphasizing. </paragraph>
<paragraph id="P-0075" lvl="0"><number>&lsqb;0075&rsqb;</number> Furthermore, one may perform remapping based on, in the histogram of density shown in <cross-reference target="DRAWINGS">FIG. 7</cross-reference>, the peak density of a density range that includes a lot of the background components or the peak density of a density range that include a lot of the 3-dimensional object components multiplied by a predetermined coefficient. </paragraph>
<paragraph id="P-0076" lvl="0"><number>&lsqb;0076&rsqb;</number> Moreover, one may select, from the peripheral density distribution, the density range by which the best image is statistically available as the density range of remapping, and the important thing is to remap in the density range where the influence of noise can be reduced. </paragraph>
<paragraph id="P-0077" lvl="0"><number>&lsqb;0077&rsqb;</number> Following step <highlight><bold>2</bold></highlight>, in step S<highlight><bold>3</bold></highlight>, binary coding processing is performed by the Canny method, for example. The binary coding processing by the Canny method of step S<highlight><bold>3</bold></highlight> will be explained next. </paragraph>
<paragraph id="P-0078" lvl="0"><number>&lsqb;0078&rsqb;</number> The binary coding processing by the Canny method is binary coding processing that detects an edge by obtaining a local maximum of a gradient of the image. </paragraph>
<paragraph id="P-0079" lvl="0"><number>&lsqb;0079&rsqb;</number> At first, using <highlight><bold>2</bold></highlight> thresholds, the binary coding processing detects a strong edge and a weak edge. And, only in the case such that the weak edge is connected to the strong edge, the binary coding processing puts the weak edge in binary form by including the weak edge in an output edge. In addition, the details of the Canny method are indicated in &ldquo;CANNY, A Computational Approach to Edge Detection, IEEE TRANSACTION ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, 1986&rdquo;. </paragraph>
<paragraph id="P-0080" lvl="0"><number>&lsqb;0080&rsqb;</number> Additionally, depending on an extraction image portion, one may set the threshold at an appropriate value at which the best image is empirically obtained. Furthermore, binary coding processing is not limited to the Canny method, and one can binary code using another binary coding method. </paragraph>
<paragraph id="P-0081" lvl="0"><number>&lsqb;0081&rsqb;</number> Additionally, the emphasized image shown in <cross-reference target="DRAWINGS">FIG. 9</cross-reference> is binary coded by the Canny method, which is the binary coded image shown in <cross-reference target="DRAWINGS">FIG. 11</cross-reference>, and the original image showed in <cross-reference target="DRAWINGS">FIG. 6</cross-reference> is binary coded &ldquo;as is&rdquo; by the Canny method, which is the binary coded image showed in <cross-reference target="DRAWINGS">FIG. 12</cross-reference>. According to these binary coded images, in the binary coded image showed in <cross-reference target="DRAWINGS">FIG. 11</cross-reference>, in comparison with the binary coded image shown in <cross-reference target="DRAWINGS">FIG. 12</cross-reference>, it is evident that noise is reduced. Even in <cross-reference target="DRAWINGS">FIG. 12</cross-reference>, the outline of the temporomandibular joint appears, but, it is evident that the image is noisier. This indicates that, in the histogram of the original image shown in <cross-reference target="DRAWINGS">FIG. 7</cross-reference>, there exist a plurality of mountains, and the noise component is not concentrated on a certain gray level, but is distributed in the gray levels 100-255 too. </paragraph>
<paragraph id="P-0082" lvl="0"><number>&lsqb;0082&rsqb;</number> In this embodiment, by mapping the gray levels 100-255 in the gray level 0-255 by the histogram emphasizing method, one can scatter the noise component existing in the gray level 100-255, and reduce the frequency of detecting a noise having a small gradient as an edge in the case of binary coding by the Canny method, and, as a result, one can further reduce the noise component. </paragraph>
<paragraph id="P-0083" lvl="0"><number>&lsqb;0083&rsqb;</number> As described above, one can effectively reduce noise by binary coding the image by the Canny method after performing the histogram emphasis method. </paragraph>
<paragraph id="P-0084" lvl="0"><number>&lsqb;0084&rsqb;</number> As described above, one can obtain a clear 3-dimensional image by forming the 3-dimensional image using binary coded 2-dimensional images. In addition, a 3-dimensional image can be formed, using the opposite algorithm for the cutting of an image, for example. </paragraph>
<paragraph id="P-0085" lvl="0"><number>&lsqb;0085&rsqb;</number> In <cross-reference target="DRAWINGS">FIG. 13</cross-reference> is the 3-dimensional image formed based on an image provided by the above image processing. </paragraph>
<paragraph id="P-0086" lvl="0"><number>&lsqb;0086&rsqb;</number> According to the present embodiment, it is possible to obtain a clear 3-dimensional image from noisy 2-dimensional images taken with a small amount of radiation exposure. In this embodiment, a plurality of images processed by averaging processing are processed by the histogram emphasizing method and binary coded, so that one can lower the influence of noise and detect an edge without performing complicated processing. That is how one can minimize the influence of noise. </paragraph>
<paragraph id="P-0087" lvl="0"><number>&lsqb;0087&rsqb;</number> In addition, in this embodiment, the same processing is performed on 4,416 2-dimensional images, but it is possible to apply, depending on the extracted portion, averaging processing and binary coding processing that are most suitable for the portion. For example, one may obtain extraction results with varying thresholds and switching thresholds, depending on the portion of images by setting a threshold for each portion with which the best image is obtained. </paragraph>
<paragraph id="P-0088" lvl="0"><number>&lsqb;0088&rsqb;</number> Moreover, in this embodiment, an example wherein the present invention is applied to the temporomandibular joint is explained, but the image processing according to the present invention is not limited to the temporomandibular joint, but applicable to an imaging object that requires imaging with little radiation due to a problem of radiation exposure. </paragraph>
<paragraph id="P-0089" lvl="0"><number>&lsqb;0089&rsqb;</number> Moreover, the image processing method according to this embodiment cuts images of various angles around the Z-axis, and performs interpolation on edges that cannot be obtained in a direction. </paragraph>
<paragraph id="P-0090" lvl="0"><number>&lsqb;0090&rsqb;</number> It is also possible to cut images of various angles around the X-axis or Y-axis instead of the Z-axis. </paragraph>
<paragraph id="P-0091" lvl="0"><number>&lsqb;0091&rsqb;</number> Moreover, it is possible to cut images around: </paragraph>
<paragraph id="P-0092" lvl="2"><number>&lsqb;0092&rsqb;</number> (i) X-axis and Y-axis that are rotated by 45 degrees around the origin in the XY plane; </paragraph>
<paragraph id="P-0093" lvl="2"><number>&lsqb;0093&rsqb;</number> (ii) Y-axis and Z-axis that are rotated by 45 degrees around the origin in the YZ plane; and </paragraph>
<paragraph id="P-0094" lvl="2"><number>&lsqb;0094&rsqb;</number> (iii) Z-axis and X-axis that are rotated by 45 degrees around the origin in the ZX plane. </paragraph>
<paragraph id="P-0095" lvl="0"><number>&lsqb;0095&rsqb;</number> Moreover, it is possible to cut images in various angles around a plurality of axes. </paragraph>
<paragraph id="P-0096" lvl="7"><number>&lsqb;0096&rsqb;</number> (The Second Embodiment) </paragraph>
<paragraph id="P-0097" lvl="0"><number>&lsqb;0097&rsqb;</number> By the way, as to the image processing method described above, as showed in <cross-reference target="DRAWINGS">FIG. 14</cross-reference>, since the image processing is applied to many images taken in various directions around the Z-axis, for example, there exists a problem that the image processing takes a long time. </paragraph>
<paragraph id="P-0098" lvl="0"><number>&lsqb;0098&rsqb;</number> For example, when the present inventors measured the time required for processing in the case of 4,416 images in total, that is, 276 images per direction, 16 directions, using a computer operated at 800 MHz and having 256 MB RAM memory, for example, it took 15 minutes for cutting images, 35 minutes for image processing and forming, and 5 minutes for displaying in 3-dimension, in total 55 minutes. It is said that the maximum time that can be spent for clinical application is about 10 minutes, and the use of the image processing method is not practical. </paragraph>
<paragraph id="P-0099" lvl="0"><number>&lsqb;0099&rsqb;</number> Therefore, in the embodiment described above, processing in various directions around one axis is performed 3-dimensionally, but in the second embodiment as shown in <cross-reference target="DRAWINGS">FIG. 15, </cross-reference>2-dimensional images (B) in each direction of X-axis, Y-axis, and Z-axis, 3 directions in total, are cut from 3-dimensional data (A) provided by means of the imaging apparatus. The 2-dimensional images that are cut are processed by the averaging processing, histogram emphasizing, and binary coding processing (C) as shown in <cross-reference target="DRAWINGS">FIG. 4</cross-reference>. Using these 2-dimensional images provided, 3-dimensional images are formed and stored in the internal memory (D). The 3-dimensional image stored in the internal memory is displayed. In addition, in this embodiment, besides the cutting of an image, the image processing method according to the first embodiment described above can be used. </paragraph>
<paragraph id="P-0100" lvl="0"><number>&lsqb;0100&rsqb;</number> In addition, as to the above embodiment, the case such that images are cut from 3 directions of X-, Y- and Z-axes is described, but even a case other than this can be embodied. </paragraph>
<paragraph id="P-0101" lvl="0"><number>&lsqb;0101&rsqb;</number> For example, one may add a part or all of the following axes to X-, Y-, and Z-axes described previously: </paragraph>
<paragraph id="P-0102" lvl="2"><number>&lsqb;0102&rsqb;</number> (i) X-axis and Y-axis rotated 45 degrees in the XY plane around the origin; </paragraph>
<paragraph id="P-0103" lvl="2"><number>&lsqb;0103&rsqb;</number> (ii) Y-axis and Z-axis rotated 45 degrees in the YZ plane around the origin; and </paragraph>
<paragraph id="P-0104" lvl="2"><number>&lsqb;0104&rsqb;</number> (iii) Z-axis and X-axis rotated 45 degrees in the ZX plane around the origin and X-axis. </paragraph>
<paragraph id="P-0105" lvl="0"><number>&lsqb;0105&rsqb;</number> For example, in <cross-reference target="DRAWINGS">FIG. 16</cross-reference>, an example such that images are cut in the directions of five axes consisting of X&prime; axis and Y&prime; axis that are rotated by 45 degrees in the XY plane in addition to X-, Y- and Z-axes. </paragraph>
<paragraph id="P-0106" lvl="0"><number>&lsqb;0106&rsqb;</number> Moreover, it is possible to use, as an axis, a part or all of the lines connecting the center of each face of regular polyhedrons and the center of the regular polyhedrons, regular tetrahedron (A), regular hexahedron (B), regular octahedron (C), regular dodecahedron (D), and regular icosahedron (E), as showed in <cross-reference target="DRAWINGS">FIG. 17</cross-reference>. </paragraph>
<paragraph id="P-0107" lvl="0"><number>&lsqb;0107&rsqb;</number> A 3-dimensional image processed in 3 directions actually using the method showed in <cross-reference target="DRAWINGS">FIG. 15</cross-reference> is showed in <cross-reference target="DRAWINGS">FIG. 20</cross-reference>. A 3-dimensional image obtained by a conventional method is showed in <cross-reference target="DRAWINGS">FIG. 18</cross-reference>, and a 3-dimensional image obtained by the method of the first embodiment is showed in <cross-reference target="DRAWINGS">FIG. 19</cross-reference>. The 3-dimensional image showed in <cross-reference target="DRAWINGS">FIG. 20</cross-reference>, in comparison with the 3-dimensional image showed in <cross-reference target="DRAWINGS">FIG. 19</cross-reference>, is a 3-dimensional image in which some noises are observable but the shape of the object is recognizable. Moreover, the 3-dimensional image showed in <cross-reference target="DRAWINGS">FIG. 20</cross-reference>, in comparison with the 3-dimensional image showed in <cross-reference target="DRAWINGS">FIG. 18</cross-reference>, is a 3-dimensional image in which no deficit exists and the shape of the object is recognizable. This situation is understandable. </paragraph>
<paragraph id="P-0108" lvl="0"><number>&lsqb;0108&rsqb;</number> Moreover, as to processing time, the method of the second embodiment took about 14 minutes from the reading of 3-dimensional projection data to the completion of a file making. In comparison with that it takes about 55 minutes in the case of the conventional 16-direction processing, so a 3-dimensional image can be obtained in about one forth of the time with the second embodiment. </paragraph>
<paragraph id="P-0109" lvl="0"><number>&lsqb;0109&rsqb;</number> As described above, as to the image showed in <cross-reference target="DRAWINGS">FIG. 20</cross-reference>, in comparison with the 3-dimensional image of 16 directions, a portion that is inferior to the 3-dimensional image of 16 directions is more or less observable, but the 3-dimensional image obtained by image processing in 3 directions is considered to be effective in consideration of computation time, as long as the objective is grasping shapes. </paragraph>
<paragraph id="P-0110" lvl="0"><number>&lsqb;0110&rsqb;</number> As described above, as to the first and the second embodiments, since many images are obtained by extracting them, in a plurality of angles or around one or more axes, from a plurality of images having different depths of a 3-dimensional object, it is possible to surely obtain an image of the imaging object even if a density range containing a few components ob the 3-dimensional object is eliminated, and, since noise components can be diffused by eliminating the density ranges containing a few components of the 3-dimensional object, remapping the other portion to a predetermined density range, and binary coding the first and the second embodiments are characterized in that, for example, images having less influence of noise are obtainable. </paragraph>
<paragraph id="P-0111" lvl="0"><number>&lsqb;0111&rsqb;</number> Moreover, the first and the second embodiments are characterized in that, for example, images having less influence of noise can be obtained since they can reduce noise components without damaging the accuracy of images by extracting averaged images from the average of a predetermined number of images consecutive in the direction of depth. </paragraph>
<paragraph id="P-0112" lvl="0"><number>&lsqb;0112&rsqb;</number> Moreover, the first and the second embodiments are characterized in that they can obtain images having less influence of noise since they can reduce noise components without losing the accuracy of images by averaging and extracting images while shifting by one image in the direction of depth for averaging. </paragraph>
<paragraph id="P-0113" lvl="0"><number>&lsqb;0113&rsqb;</number> Moreover, the present invention can perform image processing at a high speed by averaging a plurality of images and extracting one image by the plurality of images, and then, the present invention is characterized in that, for example, it can reduce the loss in accuracy by treating many images that are imaged. </paragraph>
<paragraph id="P-0114" lvl="0"><number>&lsqb;0114&rsqb;</number> Moreover, the first and the second embodiments, in the case of mapping, can reduce noise without including noise components and surely extract images of the 3-dimensional object by selecting, as a threshold, a density in the density distribution containing great noise components in the histogram of density level of the images of the 3-dimensional object, at which the noise component is negligible. </paragraph>
<paragraph id="P-0115" lvl="0"><number>&lsqb;0115&rsqb;</number> Moreover, the first and the second embodiments can surely extract the density component containing the background components or the 3-dimensional object components by performing remapping based on the peak density of the density range containing the background components or the peak density of the density range containing the 3-dimensional object components, both multiplied by a predetermined coefficient. </paragraph>
<paragraph id="P-0116" lvl="0"><number>&lsqb;0116&rsqb;</number> Moreover, the first and the second embodiments are characterized in that they can provide the most suitable image for forming a 3-dimensional image since a threshold with which the statistically optimum image is obtained is selected. </paragraph>
<paragraph id="P-0117" lvl="0"><number>&lsqb;0117&rsqb;</number> Moreover, it is possible that, in the general purpose computer <highlight><bold>14</bold></highlight> showed in <cross-reference target="DRAWINGS">FIG. 1, a</cross-reference> computer program that causes the general purpose computer <highlight><bold>14</bold></highlight> to perform an extraction step to extract a plurality of images having different depths of a 3-dimensional object around one or more axes over a plurality of angles, a binary coding step that eliminates the density range of the plurality of images obtained in the extraction step, in which few components of the object are contained, and remaps the other portion to a certain density range, and puts the image in a binary form, and a step to form a 3-dimensional image of the 3-dimensional object based on the images put in binary form in the binary coding step, and image processing according to the first and second embodiments can be performed by this program. </paragraph>
<paragraph id="P-0118" lvl="0"><number>&lsqb;0118&rsqb;</number> In addition, needless to say, the present invention is not limited to the above embodiments and, without deviating from the scope of claims, various variations and modifications are possible. </paragraph>
</section>
</detailed-description>
</subdoc-description>
<subdoc-claims>
<claim id="CLM-00001">
<claim-text><highlight><bold>1</bold></highlight>. An image processing method, comprising: 
<claim-text>an extracting step for extracting a plurality of images of a 3-dimensional object along one or more axes at different depths, said axes forming different angles with said 3-dimensional object; </claim-text>
<claim-text>a binary coding step for eliminating a gray level range containing few components of said 3-dimensional object, remapping the other portion of the gray level range to a predetermined gray level value range, and putting the plurality of images in binary form; and </claim-text>
<claim-text>a step for forming a 3-dimensional image of said 3-dimensional object based on the plurality of images put in binary form by said binary coding step. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00002">
<claim-text><highlight><bold>2</bold></highlight>. An image processing method comprising: 
<claim-text>an extracting step for extracting, from a 3-dimensional object, a plurality of images along a plurality of axes, wherein each axis is in a different direction and each image is at a different depth on an axis; </claim-text>
<claim-text>a binary coding step for eliminating a gray level range containing few components of said 3-dimensional object, remapping the other portion of the gray level range to a predetermined gray level value range, and putting the plurality of images in binary form; and </claim-text>
<claim-text>a step for forming a 3-dimensional image of said 3-dimensional object based on the plurality of images put in binary form by said binary coding step. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00003">
<claim-text><highlight><bold>3</bold></highlight>. The image processing method claimed in <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference> or <dependent-claim-reference depends_on="CLM-00002">claim 2</dependent-claim-reference>, characterized in that said binary coding step comprises an averaging step for extracting an averaged image from a predetermined number of images consecutive in the direction of depth. </claim-text>
</claim>
<claim id="CLM-00004">
<claim-text><highlight><bold>4</bold></highlight>. The image processing method as claimed in <dependent-claim-reference depends_on="CLM-00003">claim 3</dependent-claim-reference>, characterized in that said averaging step extracts images by shifting the predetermined number of images by one image in the direction of depth. </claim-text>
</claim>
<claim id="CLM-00005">
<claim-text><highlight><bold>5</bold></highlight>. The image processing method as claimed in <dependent-claim-reference depends_on="CLM-00004">claim 4</dependent-claim-reference>, characterized in that said averaging step extracts images by averaging an image every plurality of images. </claim-text>
</claim>
<claim id="CLM-00006">
<claim-text><highlight><bold>6</bold></highlight>. The image processing method as claimed in one of claims <highlight><bold>1</bold></highlight> through <highlight><bold>5</bold></highlight>, characterized in that said binary coding step puts the plurality of images in binary form after remapping the gray level range, the remapped gray level range having a noise component that is negligible, the original gray level range containing a great noise component in a gray level histogram of said extraction object. </claim-text>
</claim>
<claim id="CLM-00007">
<claim-text><highlight><bold>7</bold></highlight>. The image processing method as claimed in one of claims <highlight><bold>1</bold></highlight> through <highlight><bold>5</bold></highlight>, characterized in that said binary coding step puts the plurality of images in binary form by performing remapping depending on a peak gray level, multiplied with a predetermined coefficient, of a gray level range containing a great amount of background components or a peak gray level, multiplied with a predetermined coefficient, of a gray level range containing a great amount of said 3-dimensional object components, in a gray level histogram of said 3-dimensional object. </claim-text>
</claim>
<claim id="CLM-00008">
<claim-text><highlight><bold>8</bold></highlight>. The image processing method as claimed in one of claims <highlight><bold>1</bold></highlight> through <highlight><bold>5</bold></highlight>, characterized in that said binary coding step puts the plurality of images in binary form by statistically performing remapping based on gray level distribution of peripheral pixels. </claim-text>
</claim>
<claim id="CLM-00009">
<claim-text><highlight><bold>9</bold></highlight>. An image processing program that causes a computer to perform: 
<claim-text>an extracting step for extracting a plurality of images of a 3-dimensional object along one or more axes at different depths, said axes forming different angles with said 3-dimensional object; </claim-text>
<claim-text>a binary coding step for eliminating a gray level range containing few components of said 3-dimensional object, remapping the other portion of the gray scale range to a predetermined gray level value range, and putting the plurality of images in binary form; and </claim-text>
<claim-text>a step for forming a 3-dimensional image of said 3-dimensional object based on the plurality of images put in binary form by said binary coding step. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00010">
<claim-text><highlight><bold>10</bold></highlight>. A computer readable recording medium in which the image processing program claimed in <dependent-claim-reference depends_on="CLM-00009">claim 9</dependent-claim-reference> is stored therein.</claim-text>
</claim>
</subdoc-claims>
<subdoc-drawings id="DRAWINGS">
<heading lvl="0" align="CENTER">Drawings</heading>
<representative-figure>4</representative-figure>
<figure id="figure-D00000">
<image id="EMI-D00000" file="US20030002724A1-20030102-D00000.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00001">
<image id="EMI-D00001" file="US20030002724A1-20030102-D00001.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00002">
<image id="EMI-D00002" file="US20030002724A1-20030102-D00002.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00003">
<image id="EMI-D00003" file="US20030002724A1-20030102-D00003.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00004">
<image id="EMI-D00004" file="US20030002724A1-20030102-D00004.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00005">
<image id="EMI-D00005" file="US20030002724A1-20030102-D00005.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00006">
<image id="EMI-D00006" file="US20030002724A1-20030102-D00006.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00007">
<image id="EMI-D00007" file="US20030002724A1-20030102-D00007.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00008">
<image id="EMI-D00008" file="US20030002724A1-20030102-D00008.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00009">
<image id="EMI-D00009" file="US20030002724A1-20030102-D00009.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00010">
<image id="EMI-D00010" file="US20030002724A1-20030102-D00010.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00011">
<image id="EMI-D00011" file="US20030002724A1-20030102-D00011.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00012">
<image id="EMI-D00012" file="US20030002724A1-20030102-D00012.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00013">
<image id="EMI-D00013" file="US20030002724A1-20030102-D00013.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00014">
<image id="EMI-D00014" file="US20030002724A1-20030102-D00014.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00015">
<image id="EMI-D00015" file="US20030002724A1-20030102-D00015.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00016">
<image id="EMI-D00016" file="US20030002724A1-20030102-D00016.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00017">
<image id="EMI-D00017" file="US20030002724A1-20030102-D00017.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00018">
<image id="EMI-D00018" file="US20030002724A1-20030102-D00018.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00019">
<image id="EMI-D00019" file="US20030002724A1-20030102-D00019.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00020">
<image id="EMI-D00020" file="US20030002724A1-20030102-D00020.TIF" imf="TIFF" ti="DR"/>
</figure>
</subdoc-drawings>
</patent-application-publication>
