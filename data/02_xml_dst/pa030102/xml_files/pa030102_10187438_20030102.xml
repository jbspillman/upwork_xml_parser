<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE patent-application-publication SYSTEM "pap-v16-2002-01-01.dtd" [
<!ENTITY US20030002712A1-20030102-D00000.TIF SYSTEM "US20030002712A1-20030102-D00000.TIF" NDATA TIF>
<!ENTITY US20030002712A1-20030102-D00001.TIF SYSTEM "US20030002712A1-20030102-D00001.TIF" NDATA TIF>
<!ENTITY US20030002712A1-20030102-D00002.TIF SYSTEM "US20030002712A1-20030102-D00002.TIF" NDATA TIF>
<!ENTITY US20030002712A1-20030102-D00003.TIF SYSTEM "US20030002712A1-20030102-D00003.TIF" NDATA TIF>
<!ENTITY US20030002712A1-20030102-D00004.TIF SYSTEM "US20030002712A1-20030102-D00004.TIF" NDATA TIF>
<!ENTITY US20030002712A1-20030102-D00005.TIF SYSTEM "US20030002712A1-20030102-D00005.TIF" NDATA TIF>
<!ENTITY US20030002712A1-20030102-D00006.TIF SYSTEM "US20030002712A1-20030102-D00006.TIF" NDATA TIF>
<!ENTITY US20030002712A1-20030102-D00007.TIF SYSTEM "US20030002712A1-20030102-D00007.TIF" NDATA TIF>
<!ENTITY US20030002712A1-20030102-D00008.TIF SYSTEM "US20030002712A1-20030102-D00008.TIF" NDATA TIF>
<!ENTITY US20030002712A1-20030102-D00009.TIF SYSTEM "US20030002712A1-20030102-D00009.TIF" NDATA TIF>
<!ENTITY US20030002712A1-20030102-D00010.TIF SYSTEM "US20030002712A1-20030102-D00010.TIF" NDATA TIF>
<!ENTITY US20030002712A1-20030102-D00011.TIF SYSTEM "US20030002712A1-20030102-D00011.TIF" NDATA TIF>
]>
<patent-application-publication>
<subdoc-bibliographic-information>
<document-id>
<doc-number>20030002712</doc-number>
<kind-code>A1</kind-code>
<document-date>20030102</document-date>
</document-id>
<publication-filing-type>new</publication-filing-type>
<domestic-filing-data>
<application-number>
<doc-number>10187438</doc-number>
</application-number>
<application-number-series-code>10</application-number-series-code>
<filing-date>20020702</filing-date>
</domestic-filing-data>
<technical-information>
<classification-ipc>
<classification-ipc-primary>
<ipc>G06K009/00</ipc>
</classification-ipc-primary>
<classification-ipc-edition>07</classification-ipc-edition>
</classification-ipc>
<classification-us>
<classification-us-primary>
<uspc>
<class>382</class>
<subclass>103000</subclass>
</uspc>
</classification-us-primary>
<classification-us-secondary>
<uspc>
<class>382</class>
<subclass>154000</subclass>
</uspc>
</classification-us-secondary>
</classification-us>
<title-of-invention>Method and apparatus for measuring dwell time of objects in an environment</title-of-invention>
</technical-information>
<continuity-data>
<non-provisional-of-provisional>
<document-id>
<doc-number>60301828</doc-number>
<document-date>20010702</document-date>
<country-code>US</country-code>
</document-id>
</non-provisional-of-provisional>
</continuity-data>
<inventors>
<first-named-inventor>
<name>
<given-name>Malcolm</given-name>
<family-name>Steenburgh</family-name>
</name>
<residence>
<residence-non-us>
<city>Vancouver</city>
<country-code>CA</country-code>
</residence-non-us>
</residence>
<authority-applicant>INV</authority-applicant>
</first-named-inventor>
<inventor>
<name>
<given-name>Don</given-name>
<family-name>Murray</family-name>
</name>
<residence>
<residence-non-us>
<city>Vancouver</city>
<country-code>CA</country-code>
</residence-non-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
<inventor>
<name>
<given-name>Vladimir</given-name>
<family-name>Tucakov</family-name>
</name>
<residence>
<residence-non-us>
<city>Vancouver</city>
<country-code>CA</country-code>
</residence-non-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
<inventor>
<name>
<given-name>Shyan</given-name>
<family-name>Ku</family-name>
</name>
<residence>
<residence-non-us>
<city>Vancouver</city>
<country-code>CA</country-code>
</residence-non-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
<inventor>
<name>
<given-name>Rod</given-name>
<family-name>Barman</family-name>
</name>
<residence>
<residence-non-us>
<city>Vancouver</city>
<country-code>CA</country-code>
</residence-non-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
</inventors>
<correspondence-address>
<name-1>OYEN, WIGGS, GREEN &amp; MUTALA</name-1>
<name-2>480 - THE STATION</name-2>
<address>
<address-1>601 WEST CORDOVA STREET</address-1>
<city>VANCOUVER</city>
<state>BC</state>
<postalcode>V6B 1G1</postalcode>
<country>
<country-code>CA</country-code>
</country>
</address>
</correspondence-address>
</subdoc-bibliographic-information>
<subdoc-abstract>
<paragraph id="A-0001" lvl="0">A method for tracking objects entering and exiting an environment comprises providing stereo vision cameras positioned proximate to the paths to enter the environment, such that the stereo vision fields associated with the cameras substantially cover the paths. Using image data obtained by the stereo vision cameras, entry signatures are determined for objects entering the environment and exit signatures are obtained for objects exiting the environment. At any time after the determination of at least one exit signature, exit signatures are matched with the set of available entry signatures. Matching may be performed according to a variety of techniques. </paragraph>
</subdoc-abstract>
<subdoc-description>
<cross-reference-to-related-applications>
<heading lvl="1">CROSS-REFERENCE TO RELATED APPLICATION </heading>
<paragraph id="P-0001" lvl="0"><number>&lsqb;0001&rsqb;</number> This application claims the benefit of the filing date of U.S. Application No. <highlight><bold>60</bold></highlight>/<highlight><bold>301</bold></highlight>,<highlight><bold>828</bold></highlight> filed Jul. 2, 2001, which is hereby incorporated by reference.</paragraph>
</cross-reference-to-related-applications>
<summary-of-invention>
<section>
<heading lvl="1">TECHNICAL FIELD </heading>
<paragraph id="P-0002" lvl="0"><number>&lsqb;0002&rsqb;</number> This invention relates to machine and computer vision and to recognition and tracking of objects. Particular embodiments of the invention provide methods and apparatus for determining the lengths of time that objects spend in an environment. </paragraph>
</section>
<section>
<heading lvl="1">BACKGROUND </heading>
<paragraph id="P-0003" lvl="0"><number>&lsqb;0003&rsqb;</number> There exists a need to identify, recognize, count and/or track the movement of objects in an environment. For example, there is a need to determine how long people spend in retail stores or other commercial environments. When an object (e.g. a person, animal, vehicle or shopping cart) is tracked, useful information can be generated. For example, in a retail store application, useful information may be generated by tracking the movements of individuals through the store. Such useful information may include, for example: time that people spend viewing an advertisement or promotional display; the ratio of adults to children in a particular area; the number of people that view a promotional display today as compared to the same promotional display yesterday or last week; the duration of time that shoppers spend in a particular area; and the like. Such information can be helpful to generate additional information, such as desirable staffing allocations, traffic counts, shopper-to-purchaser ratios, etc. </paragraph>
<paragraph id="P-0004" lvl="0"><number>&lsqb;0004&rsqb;</number> Various prior art techniques have been proposed and developed to automate the counting and/or tracking of objects. Often, the applicability of these techniques depends on the environment in which the counting and/or tracking takes place. </paragraph>
<paragraph id="P-0005" lvl="0"><number>&lsqb;0005&rsqb;</number> One common technique for counting the movement of objects into or out of an environment involves the use of infrared beams. An infrared beam is directed from a source located on one side of an opening, such as a door, to a detector positioned on an opposite side of the opening. When the beam is interrupted by the movement of an object through its path, the system detects the passage of an object through the environment. Techniques of this type are not capable of detecting the direction of movement of objects through the environment, or identifying, recognizing or tracking particular objects. </paragraph>
<paragraph id="P-0006" lvl="0"><number>&lsqb;0006&rsqb;</number> Monocular vision systems comprising a video camera and a video processor can identify, detect and track the movements of objects. Such monocular vision systems identify and detect motion by monitoring successive image frames and recording the positions of distinctive groups of pixels. These distinctive groups of pixels may be identified as objects of interest that are found in successive image frames. If the position of a particular group of pixels changes between image frames, then the vision system detects movement of the corresponding object. Such systems can also track the motion of an object by analyzing successive image frames. The relative position of the group of pixels between successive image frames can be interpreted as movement of the associated object relative to the stationary background. The movement of the object may be tracked by recording the position of its group of pixels between successive image frames. </paragraph>
<paragraph id="P-0007" lvl="0"><number>&lsqb;0007&rsqb;</number> In operation, however, monocular vision systems are not very robust and can make mistakes in relation to the movement and/or identification of objects. In addition, monocular vision systems are limited to the detection of two-dimensional object features. </paragraph>
<paragraph id="P-0008" lvl="0"><number>&lsqb;0008&rsqb;</number> Stereo vision cameras, which generally comprise two (or more) cameras and a video processor, are also used for object identification, recognition and tracking applications. Such stereo vision cameras can detect three-dimensional properties of objects in their stereo vision field of view. Examples of commercially available stereo vision cameras include the DIGICLOPS&trade; and the BUMBLEBEE&trade; camera systems available from Point Grey Research Inc. of Vancouver, Canada. In addition to object identification, recognition and tracking, stereo vision cameras are used for a wide variety of applications, such as computer vision and object dimensioning. </paragraph>
<paragraph id="P-0009" lvl="0"><number>&lsqb;0009&rsqb;</number> A typical stereo vision camera comprises two spaced-apart monocular cameras. Some prior art stereo vision cameras have three or more monocular cameras. <cross-reference target="DRAWINGS">FIG. 1</cross-reference> shows a stereo vision camera <highlight><bold>10</bold></highlight> having two monocular cameras <highlight><bold>11</bold></highlight>A and <highlight><bold>11</bold></highlight>B. Preferably, although not necessarily, monocular cameras <highlight><bold>11</bold></highlight>A and <highlight><bold>11</bold></highlight>B are digital cameras. The distance b between camera <highlight><bold>11</bold></highlight>A and camera <highlight><bold>11</bold></highlight>B is referred to as the &ldquo;baseline&rdquo;. Each of cameras <highlight><bold>11</bold></highlight>A and <highlight><bold>11</bold></highlight>B has an associated optical axis <highlight><bold>16</bold></highlight>A and <highlight><bold>16</bold></highlight>B and an associated field of view <highlight><bold>12</bold></highlight>A and <highlight><bold>12</bold></highlight>B. These fields of view <highlight><bold>12</bold></highlight>A and <highlight><bold>12</bold></highlight>B overlap one another in region <highlight><bold>13</bold></highlight>, which is referred to as the &ldquo;stereo vision field&rdquo; or the &ldquo;stereo vision field of view&rdquo;. In a stereo vision camera having three or more monocular cameras, the system&apos;s stereo vision field includes any region where the fields of view of two or more monocular cameras overlap. Stereo vision camera <highlight><bold>10</bold></highlight> comprises a processor <highlight><bold>14</bold></highlight>, which receives image data from each monocular camera <highlight><bold>11</bold></highlight>A and <highlight><bold>11</bold></highlight>B. Using standard triangulation techniques and/or other well-known stereo vision techniques, processor <highlight><bold>14</bold></highlight> can determine three-dimensional features of an object (i.e. object <highlight><bold>15</bold></highlight>) in the stereo vision field. </paragraph>
<paragraph id="P-0010" lvl="0"><number>&lsqb;0010&rsqb;</number> Stereo vision camera systems have been used to implement prior art tracking techniques. According to these prior art techniques, an environment is populated with a plurality of cameras having overlapping fields of view, such that the entire environment of interest is located within the system&apos;s stereo vision field of view. Tracking is then performed in a manner similar to that of monocular vision systems, except that three-dimensional features of the image data and three-dimensional object features may be used to identify, recognize and track the motion of objects. </paragraph>
<paragraph id="P-0011" lvl="0"><number>&lsqb;0011&rsqb;</number> These prior art stereo vision tracking techniques suffer from the disadvantage that the entire environment of interest must be within the system&apos;s stereo vision field. When the environment of interest is large, such tracking techniques can be prohibitively expensive, because a large number of cameras is required to provide adequate coverage of the environment of interest. </paragraph>
<paragraph id="P-0012" lvl="0"><number>&lsqb;0012&rsqb;</number> There is a need for cost effective techniques for tracking the movement of objects, such as people, animals, vehicles and/or other moveable objects, into and out of environments. </paragraph>
</section>
<section>
<heading lvl="1">SUMMARY OF INVENTION </heading>
<paragraph id="P-0013" lvl="0"><number>&lsqb;0013&rsqb;</number> The invention provides a method for tracking objects entering and exiting an environment. The method comprises providing cameras positioned proximate to one or more paths into and/or out of the environment, such that the fields of view associated with the cameras substantially cover the paths. Using image data obtained by the cameras, entry signatures are determined for objects entering the environment and exit signatures are obtained for objects exiting the environment. At any time after obtaining at least one exit signature, one or more exit signatures are matched with the set of available entry signatures. </paragraph>
<paragraph id="P-0014" lvl="0"><number>&lsqb;0014&rsqb;</number> The cameras may be stereo vision cameras and the fields of view associated with the cameras which cover the paths may be stereo vision fields of view. </paragraph>
<paragraph id="P-0015" lvl="0"><number>&lsqb;0015&rsqb;</number> The objects tracked may be people, animals, vehicles and/or other moveable objects, such as shopping carts. </paragraph>
<paragraph id="P-0016" lvl="0"><number>&lsqb;0016&rsqb;</number> After matching entry and exit signatures, dwell times of objects in the environment may be determined. </paragraph>
<paragraph id="P-0017" lvl="0"><number>&lsqb;0017&rsqb;</number> Further aspects of the invention and features of specific embodiments of the invention are described below.</paragraph>
</section>
</summary-of-invention>
<brief-description-of-drawings>
<section>
<heading lvl="1">BRIEF DESCRIPTION OF DRAWINGS </heading>
<paragraph id="P-0018" lvl="0"><number>&lsqb;0018&rsqb;</number> In drawings, which illustrate non-limiting embodiments of the invention: </paragraph>
<paragraph id="P-0019" lvl="0"><number>&lsqb;0019&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 1</cross-reference> is a schematic illustration of a stereo vision camera; </paragraph>
<paragraph id="P-0020" lvl="0"><number>&lsqb;0020&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 2</cross-reference> shows a schematic top-down illustration of a particular embodiment of the invention which is deployed in an environment having a single entry/exit; </paragraph>
<paragraph id="P-0021" lvl="0"><number>&lsqb;0021&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 3</cross-reference> is a top-down schematic illustration of another particular embodiment of the invention which is deployed in an environment having multiple entries/exits; </paragraph>
<paragraph id="P-0022" lvl="0"><number>&lsqb;0022&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 4</cross-reference> is a particular embodiment of a stereo vision camera that may be used in the apparatus of <cross-reference target="DRAWINGS">FIG. 2</cross-reference> or <cross-reference target="DRAWINGS">FIG. 3</cross-reference>; </paragraph>
<paragraph id="P-0023" lvl="0"><number>&lsqb;0023&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 5</cross-reference> is another embodiment of a stereo vision camera that may be used in the apparatus of <cross-reference target="DRAWINGS">FIG. 2</cross-reference> or <cross-reference target="DRAWINGS">FIG. 3</cross-reference>; </paragraph>
<paragraph id="P-0024" lvl="0"><number>&lsqb;0024&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 6</cross-reference> is a flowchart illustrating a method for tracking the dwell time of objects in an environment according to one embodiment of the invention; </paragraph>
<paragraph id="P-0025" lvl="0"><number>&lsqb;0025&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 7</cross-reference> is a flowchart illustrating a method for localization of poeple in accordance with one embodiment of the invention;; </paragraph>
<paragraph id="P-0026" lvl="0"><number>&lsqb;0026&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 8</cross-reference> is a flowchart illustrating a method for signature vector matching in accordance with one embodiment of the invention; </paragraph>
<paragraph id="P-0027" lvl="0"><number>&lsqb;0027&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 9</cross-reference> is a flowchart illustrating a method for tracking the dwell time of objects in an environment according to another embodiment of the invention; </paragraph>
<paragraph id="P-0028" lvl="0"><number>&lsqb;0028&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 10</cross-reference> is a top plan schematic view showing an arrangement of cameras in a particular embodiment of the invention which is deployed in an open environment; and </paragraph>
<paragraph id="P-0029" lvl="0"><number>&lsqb;0029&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 11</cross-reference> is an example of a signature vector that could be used in accordance with the present invention.</paragraph>
</section>
</brief-description-of-drawings>
<detailed-description>
<section>
<heading lvl="1">DESCRIPTION </heading>
<paragraph id="P-0030" lvl="0"><number>&lsqb;0030&rsqb;</number> Throughout the following description, specific details are set forth in order to provide a more thorough understanding of the invention. However, the invention may be practiced without these particulars. In other instances, well known elements have not been shown or described in detail to avoid unnecessarily obscuring the invention. Accordingly, the specification and drawings are to be regarded in an illustrative, rather than a restrictive, sense. </paragraph>
<paragraph id="P-0031" lvl="0"><number>&lsqb;0031&rsqb;</number> The invention disclosed herein relates to methods and apparatus for identifying, counting, recognizing, timing and/or tracking the movement of objects, which may include people, animals, vehicles and/or other moveable objects, into and out of an environment. The following description gives examples of the application of the invention to monitoring movements of people. The environment of interest may be a restricted access environment, where all of the possible paths by way of which an object can move into or out of the environment (or at least the paths of interest) are known. Throughout this description, the terms &ldquo;path&rdquo; and &ldquo;paths&rdquo; are used to describe entrances into an environment, exits out of the environment, and any passage or space that can function as either an entry to the environment, an exit from the environment or both. A doorway leading into or out of a store is one example of a path of the store, where the store is one example of an environment. </paragraph>
<paragraph id="P-0032" lvl="0"><number>&lsqb;0032&rsqb;</number> A tracking system employs a camera to monitor one or more paths of an environment of interest. The system counts the number of people entering the environment and records information characteristic of each of those people. The system also obtains information characteristic of the people exiting the environment and then uses the recorded information to match people exiting the environment with those that entered. Using information related to the entry and exit of people from the environment, the tracking system may determine other useful information, such as the time that individual people spent in the environment or other statistical information relating to the stays of people in the environment. The length of time that a person (or other object) spends in an environment may be called the &ldquo;dwell time&rdquo; of the person in the environment. The tracking system need not monitor the entirety of an environment of interest. The system may be implemented with a reduced amount of hardware and at a correspondingly lower cost relative to prior art stereo vision tracking systems, which provide cameras covering all of the environment. </paragraph>
<paragraph id="P-0033" lvl="0"><number>&lsqb;0033&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 2</cross-reference> is a top-down schematic illustration of a particular embodiment of a tracking system <highlight><bold>50</bold></highlight> according to the invention. Tracking system <highlight><bold>50</bold></highlight> is deployed in an environment <highlight><bold>60</bold></highlight> having a single path <highlight><bold>52</bold></highlight> by way of which people can enter or leave environment <highlight><bold>60</bold></highlight>. Tracking system <highlight><bold>50</bold></highlight> comprises a stereo vision camera <highlight><bold>54</bold></highlight> which is controlled by a processor <highlight><bold>56</bold></highlight>. Stereo vision camera <highlight><bold>54</bold></highlight> contains a plurality of monocular video cameras (not shown) and may be implemented in a manner the same or similar to that of stereo vision camera <highlight><bold>10</bold></highlight> depicted in <cross-reference target="DRAWINGS">FIG. 1</cross-reference>. Preferably, the monocular video cameras used in stereo vision camera <highlight><bold>54</bold></highlight> are digital video cameras, but the invention may also be implemented using analog video cameras, whose images are converted to digital form. </paragraph>
<paragraph id="P-0034" lvl="0"><number>&lsqb;0034&rsqb;</number> Area <highlight><bold>58</bold></highlight> schematically depicts the stereo vision field of view of stereo vision camera <highlight><bold>54</bold></highlight>. Stereo vision field <highlight><bold>58</bold></highlight> substantially covers path <highlight><bold>52</bold></highlight>, such that no person may enter or leave environment <highlight><bold>60</bold></highlight> by way of path <highlight><bold>52</bold></highlight> without traveling through stereo vision field <highlight><bold>58</bold></highlight>. In preferred embodiments, stereo vision camera <highlight><bold>54</bold></highlight> is mounted above path <highlight><bold>52</bold></highlight> in a downwardly pointing orientation. In general, however, stereo vision camera <highlight><bold>54</bold></highlight> may be mounted in any suitable location, provided that its stereo vision field <highlight><bold>58</bold></highlight> substantially covers path <highlight><bold>52</bold></highlight>. It is preferable that camera <highlight><bold>54</bold></highlight> be located in a position such that one person standing in path <highlight><bold>52</bold></highlight> will not occlude another person or persons in path <highlight><bold>52</bold></highlight> from the view of camera <highlight><bold>54</bold></highlight>. </paragraph>
<paragraph id="P-0035" lvl="0"><number>&lsqb;0035&rsqb;</number> Processor <highlight><bold>56</bold></highlight> processes information from stereo vision camera <highlight><bold>54</bold></highlight>. Processor <highlight><bold>56</bold></highlight> may comprise a memory (not shown). In general, processor <highlight><bold>56</bold></highlight> may be implemented in many forms. For example, processor <highlight><bold>56</bold></highlight> may comprise any of: a computer executing software; a processor embedded in stereo vision camera <highlight><bold>54</bold></highlight> that is configured to execute instructions contained in a memory associated with the embedded processor; and a detachable processor that is attachable to, and detachable from, stereo vision camera <highlight><bold>54</bold></highlight> and that is configured to execute instructions contained in a memory associated with the detachable processor. Processor <highlight><bold>56</bold></highlight> may comprise image processing hardware and software. Processor <highlight><bold>56</bold></highlight> may also comprise a plurality or a combination of any such processors. Processor <highlight><bold>56</bold></highlight> may be implemented in any suitable manner and may receive image data from stereo vision camera <highlight><bold>54</bold></highlight> in any suitable manner. </paragraph>
<paragraph id="P-0036" lvl="0"><number>&lsqb;0036&rsqb;</number> The monocular cameras of stereo vision camera <highlight><bold>54</bold></highlight> each generate image data. In the case of digital cameras, the image data is captured directly in digital form. Otherwise analog image data may be converted to digital form. Processor <highlight><bold>56</bold></highlight> may receive the image data directly from the monocular cameras. Alternatively or additionally, some processing of the image data may occur prior to its receipt by processor <highlight><bold>56</bold></highlight>. The image data may comprise a plurality of image frames, each of which represents image data captured at a particular time. </paragraph>
<paragraph id="P-0037" lvl="0"><number>&lsqb;0037&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 3</cross-reference> is a top-down schematic illustration of tracking system <highlight><bold>70</bold></highlight> according to another embodiment of the invention. Tracking system <highlight><bold>70</bold></highlight> is deployed in an environment <highlight><bold>86</bold></highlight> having a plurality of paths <highlight><bold>72</bold></highlight> and <highlight><bold>74</bold></highlight> by way of which people can enter or leave environment <highlight><bold>86</bold></highlight>. Tracking system <highlight><bold>70</bold></highlight> comprises a first stereo vision camera <highlight><bold>76</bold></highlight>, associated with first path <highlight><bold>72</bold></highlight>, and a second stereo vision camera <highlight><bold>78</bold></highlight>, associated with second path <highlight><bold>74</bold></highlight>. Area <highlight><bold>80</bold></highlight> is the stereo vision field of view of first stereo vision camera <highlight><bold>76</bold></highlight> and area <highlight><bold>82</bold></highlight> is the stereo vision field of second stereo vision camera <highlight><bold>78</bold></highlight>. Stereo vision field <highlight><bold>80</bold></highlight> of first stereo vision camera <highlight><bold>76</bold></highlight> substantially covers path <highlight><bold>72</bold></highlight>, such that no person may pass in either direction through path <highlight><bold>72</bold></highlight> without traveling through stereo vision field <highlight><bold>80</bold></highlight>. Similarly, stereo vision field <highlight><bold>82</bold></highlight> of second stereo vision camera <highlight><bold>78</bold></highlight> substantially covers path <highlight><bold>74</bold></highlight>, such that no person may pass in either direction through path <highlight><bold>74</bold></highlight> without traveling through stereo vision field <highlight><bold>82</bold></highlight>. </paragraph>
<paragraph id="P-0038" lvl="0"><number>&lsqb;0038&rsqb;</number> In preferred embodiments, first and second stereo vision cameras <highlight><bold>76</bold></highlight> and <highlight><bold>78</bold></highlight> are mounted above their respective paths <highlight><bold>72</bold></highlight> and <highlight><bold>74</bold></highlight> in downwardly pointing orientations. In general, however, stereo vision cameras <highlight><bold>76</bold></highlight> and <highlight><bold>78</bold></highlight> may be mounted in any location, provided that their stereo vision fields <highlight><bold>80</bold></highlight> and <highlight><bold>82</bold></highlight> substantially cover their associated paths <highlight><bold>72</bold></highlight> and <highlight><bold>74</bold></highlight>. </paragraph>
<paragraph id="P-0039" lvl="0"><number>&lsqb;0039&rsqb;</number> Each stereo vision camera <highlight><bold>76</bold></highlight>, <highlight><bold>78</bold></highlight> may comprise a plurality of monocular video cameras (not shown) and may be implemented in a manner the same or similar to that of stereo vision camera <highlight><bold>10</bold></highlight> depicted in <cross-reference target="DRAWINGS">FIG. 1</cross-reference>. Preferably, the monocular video cameras used in stereo vision camera <highlight><bold>76</bold></highlight> and <highlight><bold>78</bold></highlight> are digital video cameras, but the invention may also be implemented using analog video cameras, whose images are converted to digital form. </paragraph>
<paragraph id="P-0040" lvl="0"><number>&lsqb;0040&rsqb;</number> In the embodiment of <cross-reference target="DRAWINGS">FIG. 3</cross-reference>, both stereo vision cameras <highlight><bold>76</bold></highlight> and <highlight><bold>78</bold></highlight> are in communication with a processor <highlight><bold>84</bold></highlight>. Alternatively or in addition, stereo vision cameras <highlight><bold>76</bold></highlight> and <highlight><bold>78</bold></highlight> may each have their own processor (not shown), which may be in communication with the processor of the other stereo vision camera. </paragraph>
<paragraph id="P-0041" lvl="0"><number>&lsqb;0041&rsqb;</number> It will be appreciated by those skilled in the art that by adding more stereo vision cameras, the apparatus of <cross-reference target="DRAWINGS">FIG. 3</cross-reference> may be extended to environments having more than two paths. In addition, if the stereo vision field of a single stereo vision camera is sufficiently large, then the single stereo vision camera may be used to cover more than one path of an environment. </paragraph>
<paragraph id="P-0042" lvl="0"><number>&lsqb;0042&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 4</cross-reference> is an isometric view of one possible embodiment of a stereo vision camera <highlight><bold>90</bold></highlight> that could be used as stereo vision camera <highlight><bold>54</bold></highlight> in the apparatus of <cross-reference target="DRAWINGS">FIG. 2</cross-reference> or as either one of stereo vision cameras <highlight><bold>76</bold></highlight> and <highlight><bold>78</bold></highlight> in the apparatus of <cross-reference target="DRAWINGS">FIG. 3</cross-reference>. Stereo vision camera <highlight><bold>90</bold></highlight> comprises three monocular digital video cameras <highlight><bold>92</bold></highlight>A, <highlight><bold>92</bold></highlight>B and <highlight><bold>92</bold></highlight>C. Stereo vision camera <highlight><bold>90</bold></highlight> may be mounted above a path (i.e. such as path <highlight><bold>52</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 2</cross-reference>) to a ceiling or to some other overhead fixture, such that its stereo vision field (not shown) extends downward, substantially covering the path. <cross-reference target="DRAWINGS">FIG. 5</cross-reference> is an isometric view of another possible embodiment of a stereo vision camera <highlight><bold>90</bold></highlight>&prime; that could be used as stereo vision camera <highlight><bold>54</bold></highlight> in the apparatus of <cross-reference target="DRAWINGS">FIG. 2</cross-reference> or as either one of stereo vision cameras <highlight><bold>76</bold></highlight> and <highlight><bold>78</bold></highlight> in the apparatus of <cross-reference target="DRAWINGS">FIG. 3</cross-reference>. Stereo vision camera <highlight><bold>90</bold></highlight>&prime; comprises two monocular digital video cameras <highlight><bold>94</bold></highlight>A and <highlight><bold>94</bold></highlight>B. Stereo vision camera <highlight><bold>90</bold></highlight>&prime; may be mounted above a path (i.e. such as path <highlight><bold>52</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 2</cross-reference>) to a ceiling or to some other overhead fixture, such that its stereo vision field (not shown) extends downward, substantially covering the path. </paragraph>
<paragraph id="P-0043" lvl="0"><number>&lsqb;0043&rsqb;</number> In another embodiment (not depicted), a stereo vision camera system (suitable for use as stereo vision camera <highlight><bold>54</bold></highlight> in the apparatus of <cross-reference target="DRAWINGS">FIG. 2</cross-reference> or as either one of stereo vision cameras <highlight><bold>76</bold></highlight> and <highlight><bold>78</bold></highlight> in the apparatus of <cross-reference target="DRAWINGS">FIG. 3</cross-reference>) may be implemented using two (or more) distinct monocular cameras that are each in communication with a processor. For example, one monocular camera could be located on one side of a door frame, while a second monocular camera could be mounted on the opposing side of the door frame. Together, the two monocular cameras could provide a stereo vision camera system. </paragraph>
<paragraph id="P-0044" lvl="0"><number>&lsqb;0044&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 6</cross-reference> illustrates a method <highlight><bold>100</bold></highlight> according to one particular embodiment of the invention. Method <highlight><bold>100</bold></highlight> tracks objects that enter and exit from an environment of interest. In particular, method <highlight><bold>100</bold></highlight> may be used to track the dwell time of people, animals, vehicles and/or other moveable objects, such as shopping carts, in the environment of interest. The following description of method <highlight><bold>100</bold></highlight> assumes that tracking system <highlight><bold>70</bold></highlight> is deployed in a restricted access environment <highlight><bold>86</bold></highlight> having two paths <highlight><bold>72</bold></highlight> and <highlight><bold>74</bold></highlight> as depicted in <cross-reference target="DRAWINGS">FIG. 3</cross-reference>. Throughout tracking method <highlight><bold>100</bold></highlight>, stereo vision camera <highlight><bold>76</bold></highlight> monitors path <highlight><bold>72</bold></highlight> and stereo vision camera <highlight><bold>78</bold></highlight> monitors path <highlight><bold>74</bold></highlight>. It will be appreciated by those skilled in the art that the principles of method <highlight><bold>100</bold></highlight> may be easily extended to environments having a single path or to environments having three or more paths by varying the number of stereo vision cameras connected to processor <highlight><bold>84</bold></highlight>. </paragraph>
<paragraph id="P-0045" lvl="0"><number>&lsqb;0045&rsqb;</number> In block <highlight><bold>120</bold></highlight>, tracking system <highlight><bold>70</bold></highlight> identifies and localizes objects entering environment <highlight><bold>86</bold></highlight> by way of either of paths <highlight><bold>72</bold></highlight> and <highlight><bold>74</bold></highlight>. For at least each entering object that is an object of interest, an entry signature vector is generated and an entry time is recorded in block <highlight><bold>140</bold></highlight>. System <highlight><bold>70</bold></highlight> simultaneously monitors paths <highlight><bold>72</bold></highlight> and <highlight><bold>74</bold></highlight> in block <highlight><bold>160</bold></highlight> to identify and localize objects that are exiting environment <highlight><bold>86</bold></highlight> by way of either of paths <highlight><bold>72</bold></highlight> and <highlight><bold>74</bold></highlight>. For at least each exiting object that is an object of interest, an exit signature vector is generated and an exit time is recorded in block <highlight><bold>180</bold></highlight>. At any time, system <highlight><bold>70</bold></highlight> may execute a matching procedure in block <highlight><bold>200</bold></highlight> to obtain an optimum match between one or more of the exit signature vectors generated in block <highlight><bold>180</bold></highlight> and entry signature vectors generated in block <highlight><bold>140</bold></highlight>. </paragraph>
<paragraph id="P-0046" lvl="0"><number>&lsqb;0046&rsqb;</number> After the entry signature vectors generated in block <highlight><bold>140</bold></highlight> are matched with the exit signature vectors generated in block <highlight><bold>180</bold></highlight>, statistics may be generated in block <highlight><bold>250</bold></highlight>. Among other useful statistics calculated in block <highlight><bold>250</bold></highlight>, the dwell time of people in environment <highlight><bold>86</bold></highlight> may be determined by comparing the entry and exit times of each matched pair of signature vectors. </paragraph>
<paragraph id="P-0047" lvl="0"><number>&lsqb;0047&rsqb;</number> Block <highlight><bold>120</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 6</cross-reference> involves identification and localization of objects that enter environment <highlight><bold>86</bold></highlight> through either one of paths <highlight><bold>72</bold></highlight> and <highlight><bold>74</bold></highlight>. A particular embodiment of block <highlight><bold>120</bold></highlight> is depicted in <cross-reference target="DRAWINGS">FIG. 7</cross-reference>. Stereo vision cameras <highlight><bold>76</bold></highlight> and <highlight><bold>78</bold></highlight> monitor paths <highlight><bold>72</bold></highlight> and <highlight><bold>74</bold></highlight> respectively and generate image data. Such image data may include three-dimensional image data relating to objects within stereo vision fields <highlight><bold>80</bold></highlight> and <highlight><bold>82</bold></highlight> of cameras <highlight><bold>76</bold></highlight> and <highlight><bold>78</bold></highlight> respectively. Using this image data, processor <highlight><bold>84</bold></highlight> executes software instructions which identify a moving object as a person in block <highlight><bold>122</bold></highlight>. Such software may use various features of the image data to identify an object as a person. For example, it may be assumed that persons have heights in excess of 18 inches. Alternatively or in addition, such software may use two-dimensional or three-dimensional template matching techniques to identify an object as a person. An identification time may also be recorded in block <highlight><bold>122</bold></highlight> indicating the time that the person was identified. </paragraph>
<paragraph id="P-0048" lvl="0"><number>&lsqb;0048&rsqb;</number> The location of an identified person is obtained in block <highlight><bold>124</bold></highlight>. The information obtained in block <highlight><bold>124</bold></highlight> may include a person&apos;s three-dimensional location and other three-dimensional image features. The three-dimensional location and other three-dimensional features of the person may be derived from the image data using well known stereo vision and image processing techniques, such as triangulation. If a person is identified in stereo vision field <highlight><bold>80</bold></highlight> associated with camera <highlight><bold>76</bold></highlight>, then the location of the person and various other features of the person may be determined in the three-dimensional coordinate system of stereo vision camera <highlight><bold>76</bold></highlight>. Similarly, if a person is identified in stereo vision field <highlight><bold>82</bold></highlight> associated with camera <highlight><bold>78</bold></highlight>, then the location of the person and various other features of the person may be determined in the three-dimensional coordinate system of stereo vision camera <highlight><bold>78</bold></highlight>. </paragraph>
<paragraph id="P-0049" lvl="0"><number>&lsqb;0049&rsqb;</number> The process of obtaining the person&apos;s location and various other features in block <highlight><bold>124</bold></highlight> may be repeated over a number of image frames. The person&apos;s trajectory may then be estimated in block <highlight><bold>126</bold></highlight>. The trajectory obtained in block <highlight><bold>126</bold></highlight> may be a three-dimensional trajectory. The process of estimating the person&apos;s trajectory involves comparing the person&apos;s location over a number of image frames. The minimum number of image frames required to estimate a trajectory is two, but the accuracy of the calculated trajectory may be improved by incorporating the person&apos;s location from a larger number of image frames into the trajectory calculation. At a most basic level, estimating a person&apos;s trajectory comprises enough information to determine whether the person is entering or exiting environment <highlight><bold>86</bold></highlight>. </paragraph>
<paragraph id="P-0050" lvl="0"><number>&lsqb;0050&rsqb;</number> In the field of machine and computer vision, there are a variety of known software and hardware processes and devices for object identification and for obtaining the trajectory of moving objects (as performed in blocks <highlight><bold>122</bold></highlight>, <highlight><bold>124</bold></highlight> and <highlight><bold>126</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 7</cross-reference>). An example of a system that may be used to identify and obtain the trajectory of moving objects is the CENSYS3D&trade; system available from Point Grey Research, Inc. of Vancouver, Canada. </paragraph>
<paragraph id="P-0051" lvl="0"><number>&lsqb;0051&rsqb;</number> In block <highlight><bold>128</bold></highlight>, the location and/or trajectory of a person may be used to decide whether the person is entering or exiting environment <highlight><bold>86</bold></highlight>. In general, the person will enter or exit environment <highlight><bold>86</bold></highlight> through one of paths <highlight><bold>72</bold></highlight> and <highlight><bold>74</bold></highlight> and will, therefore, travel through one of stereo vision fields <highlight><bold>80</bold></highlight> and <highlight><bold>82</bold></highlight> associated with stereo vision cameras <highlight><bold>76</bold></highlight> and <highlight><bold>78</bold></highlight> respectively. In some embodiments, the location and trajectory of an identified person may be continuously tracked in blocks <highlight><bold>124</bold></highlight> and <highlight><bold>126</bold></highlight>, until such time as that person exits the stereo vision field of the particular stereo vision camera. The decision in block <highlight><bold>128</bold></highlight> as to whether that person is entering or exiting environment <highlight><bold>86</bold></highlight> may then be made, at least in part, on the basis of the location where the person exits the stereo vision field, the person&apos;s trajectory when they exit the stereo vision field or both. In an alternative embodiment, the location and trajectory of a person may be determined relatively quickly in blocks <highlight><bold>124</bold></highlight> and <highlight><bold>126</bold></highlight> over a relatively small number of image frames. In such embodiments, the decision as to whether a person is entering or exiting environment <highlight><bold>86</bold></highlight> may also be made over a relatively small number of image frames. In particular, the determination as to whether a person is entering or exiting environment <highlight><bold>86</bold></highlight> may be made prior to the person leaving the relevant stereo vision field. After the entry or exit determination is made in block <highlight><bold>128</bold></highlight>, system <highlight><bold>70</bold></highlight> proceeds to the signature generation procedure in block <highlight><bold>130</bold></highlight>. </paragraph>
<paragraph id="P-0052" lvl="0"><number>&lsqb;0052&rsqb;</number> System <highlight><bold>70</bold></highlight> obtains entry signature vectors for people entering the environment in block <highlight><bold>140</bold></highlight> (see <cross-reference target="DRAWINGS">FIG. 6</cross-reference>), such that each entering person has an associated entry signature vector. Such entry signature vectors may be generated from any available image data. Preferably, such entry signature vectors may be generated from features of three-dimensional image data that may be obtained while the person travels through one of stereo vision fields <highlight><bold>80</bold></highlight> and <highlight><bold>82</bold></highlight> on their way into environment <highlight><bold>86</bold></highlight>. Examples of features of the image data that may be used to form a signature vector include, without limitation: the location of the person (as determined in block <highlight><bold>124</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 7</cross-reference>); the trajectory of the person (as determined in block <highlight><bold>126</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 7</cross-reference>); the time at which the person was identified (as determined in block <highlight><bold>122</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 7</cross-reference>); the height of the person; a color histogram generated from one or more images of the person; a shoulder width of the person; a two-dimensional image template of the person which may be based on a portion of one or more image frames containing the person; hair color of the person; iris characteristics of the person; a pattern of motion of the person; and face characteristics of the person. Such image features may be obtained from single frames or over a plurality of image frames. </paragraph>
<paragraph id="P-0053" lvl="0"><number>&lsqb;0053&rsqb;</number> Signature vectors may be generated from image data taken from multiple image frames. The use of multiple image frames allows for confidence measures to be placed on certain features of the image data. Confidence measures may be used to characterize certain features. Confidence measures may relate to the variance of a particular feature over a number of image frames. Confidence features may be included as additional component(s) of a signature vector. For example, if the height of a person is measured to be between 5&prime;10&Prime; and 6&prime;1&Prime; over a number of image frames (for example, because a person has a particular gait when they walk), then an average height of 5&prime;11 &frac12;&Prime; may be incorporated as a component of the signature vector for that person and a confidence measure of the height of that person may be incorporated as another component of the signature vector to indicate that the expected variance of the height of that person may be <highlight><bold>3</bold></highlight>&Prime;. In addition or as an alternative to providing a measure of the variance, confidence measures may be used to weight certain component(s) of a signature vector. For example, if a particular feature, such as the color histogram of a person, is detected to be relatively constant over successive image frames, then the component(s) of the entry signature vector resulting from the color histogram of the person may be more heavily weighted during the calculation of the signature vector than other components which are related to features that vary significantly between frames. Confidence measures may be assigned to selective image features, but not others. Confidence measures may be used, additionally or alternatively, during the matching process of block <highlight><bold>200</bold></highlight> as is discussed further below. </paragraph>
<paragraph id="P-0054" lvl="0"><number>&lsqb;0054&rsqb;</number> In addition to generating an entry signature vector, the time of entry of the person into environment <highlight><bold>86</bold></highlight> is recorded in block <highlight><bold>140</bold></highlight>. The entry time may also be associated with, or included as a part of, each person&apos;s entry signature vector. </paragraph>
<paragraph id="P-0055" lvl="0"><number>&lsqb;0055&rsqb;</number> An example of a signature vector <highlight><bold>400</bold></highlight> is shown in <cross-reference target="DRAWINGS">FIG. 11</cross-reference>. Signature vector <highlight><bold>400</bold></highlight> has a number of components (<highlight><bold>410</bold></highlight>, <highlight><bold>412</bold></highlight>, <highlight><bold>414</bold></highlight> . . . , <highlight><bold>426</bold></highlight>, <highlight><bold>428</bold></highlight>). In the illustrated example of vector <highlight><bold>400</bold></highlight>: component <highlight><bold>410</bold></highlight> is a representation of the height of the person; component <highlight><bold>412</bold></highlight> is a representation of the variance of the height of the person; component <highlight><bold>414</bold></highlight> is a representation of a color histogram of the person; component <highlight><bold>416</bold></highlight> is a representation of the variance of the color histogram of the person; component <highlight><bold>418</bold></highlight> is a representation of the two-dimensional image template of the person; component <highlight><bold>420</bold></highlight> is a representation of the variance in the two dimensional image template of the person; component <highlight><bold>422</bold></highlight> is a representation of the hair color of the person; component <highlight><bold>424</bold></highlight> is a representation of the variance of the hair color of the person; component <highlight><bold>426</bold></highlight> is a representation of the time at which the person entered the environment of interest; and component <highlight><bold>428</bold></highlight> is a representation of the location of the person when they entered the environment of interest. In the illustrated embodiment, some of the measured features derived from the image data (i.e. components <highlight><bold>410</bold></highlight>, <highlight><bold>414</bold></highlight>, <highlight><bold>418</bold></highlight>, and <highlight><bold>422</bold></highlight>) are accompanied by confidence measures (i.e. components <highlight><bold>412</bold></highlight>, <highlight><bold>416</bold></highlight>, <highlight><bold>420</bold></highlight>, and <highlight><bold>424</bold></highlight>) indicating the variance of the measured feature. Other components (i.e. components <highlight><bold>426</bold></highlight> and <highlight><bold>428</bold></highlight>), do not have associated confidence measures. Additionally or alternatively, certain components (not shown) of vector <highlight><bold>400</bold></highlight> may comprise functions of certain measured features, rather than the measured features themselves. </paragraph>
<paragraph id="P-0056" lvl="0"><number>&lsqb;0056&rsqb;</number> It will be appreciated by those skilled in the art that signature vector <highlight><bold>400</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 11</cross-reference> is merely an example of a signature vector that may be used in accordance with the present invention. As discussed throughout this description, many other forms of signature vectors comprising many other types of components and components representing many other types of image and non-image features may be envisaged as part of the present invention. </paragraph>
<paragraph id="P-0057" lvl="0"><number>&lsqb;0057&rsqb;</number> As a part of block <highlight><bold>120</bold></highlight> and/or block <highlight><bold>140</bold></highlight>, system <highlight><bold>70</bold></highlight> may optionally perform one or more checks on the image data. For example, in the case where system <highlight><bold>70</bold></highlight> is tracking more generalized objects, method <highlight><bold>100</bold></highlight> may involve verifying that an object in the image data has characteristics expected of an object of interest. Such a determination could be made on the basis of one or more features of the object (derived from the image data), prior to obtaining an entry signature vector for the object or recording the entry time for the object in block <highlight><bold>140</bold></highlight>. Alternatively or additionally, this determination may be made on the basis of one or more characteristics of the entry signature vector determined in block <highlight><bold>140</bold></highlight>. For objects not identified as being objects of interest, entry signature vectors may optionally not be obtained in block <highlight><bold>140</bold></highlight> or may simply not be recorded. In the alternative, data may be stored that is indicative of whether or not certain entry signature vectors obtained in block <highlight><bold>140</bold></highlight> correspond to objects of interest. </paragraph>
<paragraph id="P-0058" lvl="0"><number>&lsqb;0058&rsqb;</number> System <highlight><bold>70</bold></highlight> also monitors paths <highlight><bold>72</bold></highlight> and <highlight><bold>74</bold></highlight> for people that leave environment <highlight><bold>86</bold></highlight>. The monitoring of paths <highlight><bold>72</bold></highlight> and <highlight><bold>74</bold></highlight> for people leaving environment <highlight><bold>86</bold></highlight> may occur simultaneously with the monitoring of paths <highlight><bold>72</bold></highlight> and <highlight><bold>74</bold></highlight> for people entering environment <highlight><bold>86</bold></highlight>. People that leave environment <highlight><bold>86</bold></highlight> are identified and localized in block <highlight><bold>160</bold></highlight> (see <cross-reference target="DRAWINGS">FIG. 6</cross-reference>) and exit signature vectors and exit times are generated for those people in block <highlight><bold>180</bold></highlight>. The block <highlight><bold>160</bold></highlight> identification and localization process for people exiting environment <highlight><bold>86</bold></highlight> may be implemented in a manner that is substantially similar to the identification and localization process <highlight><bold>120</bold></highlight> for people entering environment <highlight><bold>86</bold></highlight> (see <cross-reference target="DRAWINGS">FIG. 7</cross-reference>). Clearly, however, when identifying and localizing people that exit environment <highlight><bold>86</bold></highlight>, the decision of block <highlight><bold>128</bold></highlight> should determine that the person is exiting. In block <highlight><bold>180</bold></highlight>, exit signature vectors are calculated for each person that leaves environment <highlight><bold>86</bold></highlight>. The process of exit signature vector calculation may be substantially similar to the calculation of entry signature vectors in block <highlight><bold>140</bold></highlight>. In block <highlight><bold>180</bold></highlight>, system <highlight><bold>70</bold></highlight> also records an exit time for each person. The exit time may also be associated with, or included as part of, the exit signature vector for that person. As a part of block <highlight><bold>160</bold></highlight> and/or block <highlight><bold>180</bold></highlight>, system <highlight><bold>70</bold></highlight> may optionally perform one or more checks on the image data to determine whether objects exiting the environment are objects of interest. Such a determination may be done in a manner substantially similar to that described above in relation to the entry of objects into the environment. </paragraph>
<paragraph id="P-0059" lvl="0"><number>&lsqb;0059&rsqb;</number> At any given time, system <highlight><bold>70</bold></highlight> may execute a matching procedure in block <highlight><bold>200</bold></highlight>. The matching procedure of block <highlight><bold>200</bold></highlight> attempts to match at least one exit signature vector obtained in block <highlight><bold>180</bold></highlight> with the set of entry signature vectors obtained in block <highlight><bold>140</bold></highlight>. </paragraph>
<paragraph id="P-0060" lvl="0"><number>&lsqb;0060&rsqb;</number> The matching procedure of block <highlight><bold>200</bold></highlight> may be implemented via a variety of different techniques. The matching procedure of block <highlight><bold>200</bold></highlight> may be done in &ldquo;real time&rdquo;, whereby as people exit from environment <highlight><bold>86</bold></highlight>, their exit signature vectors (generated in block <highlight><bold>180</bold></highlight>) are matched with the closest matching entry signature vector from among the set of available entry signature vectors (generated in block <highlight><bold>140</bold></highlight>). The two matched signature vectors may then be excluded from further matching. </paragraph>
<paragraph id="P-0061" lvl="0"><number>&lsqb;0061&rsqb;</number> Alternatively or in addition, the matching procedure of block <highlight><bold>200</bold></highlight> may be done &ldquo;off-line&rdquo;. Off-line matching may be performed after a pre-determined period of time, after a number of entry signature vectors and a number of exit signature vectors have been recorded. For example, such an off-line matching process may be performed in a retail store application at the end of the shopping day, when all of the customers and employees have left. In an off-line matching procedure, each of the available exit signature vectors may be matched with all of the available entry signature vectors to provide the best overall matching. </paragraph>
<paragraph id="P-0062" lvl="0"><number>&lsqb;0062&rsqb;</number> Alternatively or in addition, the matching procedure of block <highlight><bold>200</bold></highlight> may be performed &ldquo;recursively&rdquo;, whereby as a person exits, that person&apos;s exit signature vector may be matched with an entry signature vector from among the set of available entry signature vectors (i.e. in real time). However, rather than removing the matched pair from further consideration, the quality of the matching of all of the previously matched pairs may be scrutinized as each person exits. If the overall matching of entry signature vectors and exit signature vectors could be improved, then a previously matched pair of signature vectors may be uncoupled and new matches may be made to provide the best overall match for the entire set of available entry signature vectors and the entire set of available exit signature vectors. </paragraph>
<paragraph id="P-0063" lvl="0"><number>&lsqb;0063&rsqb;</number> Alternatively or in addition, the matching procedure of block <highlight><bold>200</bold></highlight> may be performed at any time when there is a plurality of exit signature vectors and a plurality of entry signature vectors. In such a situation, the matching procedure of block <highlight><bold>200</bold></highlight> may obtain the best overall match between the set of available exit signature vectors and the set of available entry signature vectors at the time that it is executed. This &ldquo;estimate&rdquo; of the matching may be re-evaluated at a later time using an off-line matching procedure. </paragraph>
<paragraph id="P-0064" lvl="0"><number>&lsqb;0064&rsqb;</number> Preferably, at the end of a predetermined period of time (i.e. such as the end of a day), a final matching procedure <highlight><bold>200</bold></highlight> may be executed in an off-line mode, such that all of the available exit signature vectors may be matched with all of the available entry signature vectors. After the execution of such an off-line matching procedure, the entry and exit signature vectors may be purged from memory, so that the memory may be made available to accommodate additional entry and exit signature vectors when system <highlight><bold>70</bold></highlight> goes online again. </paragraph>
<paragraph id="P-0065" lvl="0"><number>&lsqb;0065&rsqb;</number> In some circumstances, such as when matching procedure <highlight><bold>200</bold></highlight> is called in an off-line scenario, the number of entry signature vectors may be the same as the number of exit signature vectors. An example of such a circumstance is when environment <highlight><bold>86</bold></highlight> is a retail store, paths <highlight><bold>72</bold></highlight> and <highlight><bold>74</bold></highlight> represent the only two ways into or out of environment <highlight><bold>86</bold></highlight> and matching procedure <highlight><bold>200</bold></highlight> is executed after all of the shoppers and employees have left the store. In such a situation, system <highlight><bold>70</bold></highlight> would have generated entry signature vectors (in block <highlight><bold>140</bold></highlight>) for all of the employees and shoppers at the time that they entered the store and system <highlight><bold>70</bold></highlight> also would have generated exit signature vectors (in block <highlight><bold>180</bold></highlight>) for all of the same people at the time that they left the store. As a result, the number of entry signature vectors should ideally be equal to the number of exit signature vectors when matching procedure <highlight><bold>200</bold></highlight> is performed. </paragraph>
<paragraph id="P-0066" lvl="0"><number>&lsqb;0066&rsqb;</number> In other circumstances, such as when matching procedure <highlight><bold>200</bold></highlight> is called in a real time or a recursive scenario, there may be different numbers of entry signature vectors and exit signature vectors. Taking the same retail store application for example, if matching procedure <highlight><bold>200</bold></highlight> is executed in the middle of the shopping day, then there may still be employees and shoppers in the store. At the time of execution of matching procedure <highlight><bold>200</bold></highlight>, system <highlight><bold>70</bold></highlight> would have generated entry signature vectors for the people in the store (in block <highlight><bold>140</bold></highlight>) at the time that they entered the store and also would have generated exit signature vectors (in block <highlight><bold>180</bold></highlight>) for those people who have left the store. However, system <highlight><bold>70</bold></highlight> would not yet have generated exit signature vectors for those people who remain in the store. In these types of circumstances, matching procedure <highlight><bold>200</bold></highlight> may attempt to match any or all of the available exit signatures with a subset of the available entry signatures to obtain the best match for the available entry and exit signatures. Other examples of circumstances when the number of exit signature vectors may differ from the number of input signature vectors include situations where a moving object is improperly identified as a person during one of the identification procedures of block <highlight><bold>120</bold></highlight> or block <highlight><bold>160</bold></highlight> and situations where not all of the paths of the environment of interest are monitored. </paragraph>
<paragraph id="P-0067" lvl="0"><number>&lsqb;0067&rsqb;</number> Matching procedure <highlight><bold>200</bold></highlight> involves finding the best matches between one or more exit signature vectors and the available entry signature vectors. In real applications, matching may not be perfect. There may be cases which cause exit signatures to be matched with the wrong entrance signatures. These cases include situations where: (i) two or more entry signature vectors or two or more exit signature vectors may not be distinguishable from one another; (ii) entry signature vectors of a particular person may not match exactly with exit signature vectors of the same person (i.e. the entry and exit signature vectors may not be perfectly repeatable and/or the appearance of a person or other features of a person may change while the person is in the environment). For example, the person might put on or take off a hat or coat; (iii) the metric used to match entry signature vectors to exit signature vectors may be imperfect; and (iv) an entry signature or an exit signature is improperly generated for an object that is not an object of interest (i.e. one or more moving objects are falsely identified as people). Such deviations from ideal behavior will typically be acceptable if, overall, the system can generate reasonably reliable dwell time statistics. </paragraph>
<paragraph id="P-0068" lvl="0"><number>&lsqb;0068&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 8</cross-reference> shows a particular embodiment of a matching procedure <highlight><bold>200</bold></highlight> in accordance with the invention. In block <highlight><bold>210</bold></highlight>, matching procedure <highlight><bold>200</bold></highlight> involves obtaining a metric indicative of a distance between one or more exit signature vectors and each of a set of entry signature vectors. The metric may comprise, for example, a non-uniformly weighted Euclidean distance between the set of entry signature vectors and the set of exit signature vectors to be matched. Depending on how matching procedure <highlight><bold>200</bold></highlight> is implemented, the set of exit signature vectors to be matched may include one or more exit signature vectors (i.e. in recursive or off-line matching procedures) or the set of exit signature vectors to be matched may include only one exit signature vector (i.e. real time matching procedures). </paragraph>
<paragraph id="P-0069" lvl="0"><number>&lsqb;0069&rsqb;</number> As discussed above, the entry and exit signature vectors may be based on a number of features obtained from captured image data. These features of the entry and exit signature vectors may be non-uniformly weighted during matching procedure <highlight><bold>200</bold></highlight> to optimize matching procedure <highlight><bold>200</bold></highlight> for different circumstances, for different viewing conditions and/or to emphasize particular distinctions between features. In addition, the matching procedure may be constrained by information that is known about certain image features or by the result that is sought to be achieved by matching. For example, in situations where one wants to distinguish between the entry or exit of children from adults, any height information that goes into the entry and exit signature vectors may be given increased weight when performing the matching procedure <highlight><bold>200</bold></highlight>. In another example, it may be known or assumed that a person will only stay in the environment of interest for a time period between 1 minute and 1 hour. In such a situation, the time components of entry and exit signature vectors may be weighted to discourage matching of entry and exit signatures that fall outside of this time frame. Specific weighting techniques that may be used include, without limitation, prior modeling and/or weighting based on a Mahalanobis distance. </paragraph>
<paragraph id="P-0070" lvl="0"><number>&lsqb;0070&rsqb;</number> The confidence measures used in relation to signature vector generation (see above discussion) may also be used to provide non-uniform weighting of certain components of entry and exit signature vectors during matching procedure <highlight><bold>200</bold></highlight>. For example, if it is determined that the level of confidence in the color histogram of an entry signature is fairly high, then component(s) of the signature vector related to color histogram may be weighted relatively highly during matching procedure <highlight><bold>200</bold></highlight>. </paragraph>
<paragraph id="P-0071" lvl="0"><number>&lsqb;0071&rsqb;</number> Block <highlight><bold>220</bold></highlight> of matching procedure <highlight><bold>200</bold></highlight> involves minimizing the non-uniformly weighted Euclidean distance between the set of entry signature vectors and the set of exit signature vectors to be matched. This minimization process may be performed in various ways. In a brute force approach, every exit signature vector is compared to every entrance signature vector and the exit signature vectors are each paired with an entrance signature vector starting with the closest match. </paragraph>
<paragraph id="P-0072" lvl="0"><number>&lsqb;0072&rsqb;</number> Matching procedure <highlight><bold>200</bold></highlight> may attempt to provide a globally optimum solution over the set of available entry and exit signature vectors. That is, the preferred solution of matching process <highlight><bold>200</bold></highlight> may occur when the solution is optimized for all of the available entry and exit signature vectors, as opposed to any individual pairs of entry and exit signature vectors. </paragraph>
<paragraph id="P-0073" lvl="0"><number>&lsqb;0073&rsqb;</number> Certain implementations of the invention comprise computer processors which execute software instructions that cause the processors to perform a method of the invention. The invention may also be provided in the form of a program product. The program product may comprise any medium which carries a set of computer-readable signals comprising instructions which, when executed by a data processor, cause the data processor to execute a method of the invention. The program product may be in any of a wide variety of forms. The program product may comprise, for example, physical media such as magnetic data storage media including floppy diskettes, hard disk drives, optical data storage media including CD ROMs, DVDs, electronic data storage media including ROMs, flash RAM, or the like or transmission-type media such as digital or analog communication links. </paragraph>
<paragraph id="P-0074" lvl="0"><number>&lsqb;0074&rsqb;</number> Where a component (e.g. a software module, processor, assembly, device, circuit, etc.) is referred to above, unless otherwise indicated, reference to that component (including a reference to a &ldquo;means&rdquo;) should be interpreted as including, as equivalents of that component, any component which performs the function of the described component (i.e., that is functionally equivalent), including components which are not structurally equivalent to the disclosed structure which performs the function in the illustrated exemplary embodiments of the invention. </paragraph>
<paragraph id="P-0075" lvl="0"><number>&lsqb;0075&rsqb;</number> As will be apparent to those skilled in the art in the light of the foregoing disclosure, many alterations and modifications are possible in the practice of this invention without departing from the spirit or scope thereof. For example: </paragraph>
<paragraph id="P-0076" lvl="2"><number>&lsqb;0076&rsqb;</number> The description of the tracking system discussed above relates to the monitoring of people. It will be understood by those skilled in the art that the methods and apparatus described above are equally suitable for monitoring animals, vehicles or other moveable objects, such as shopping carts. The invention disclosed herein should be understood to have application to the identifying, counting, recognizing, timing and/or tracking the movement of generalized objects into and out of an environment. </paragraph>
<paragraph id="P-0077" lvl="2"><number>&lsqb;0077&rsqb;</number> The above description only discusses the use of stereo vision cameras and stereo vision camera systems to generate the image data and signature vectors used to implement the invention. More generally, however, a monocular camera could be located in the vicinity of a path, such that its field of view substantially covered the path. Such a monocular camera could be used to generate two-dimensional image data about objects entering and exiting the environment. Other sensors, which may be non-imaging sensors, such as one or more laser range finders, ultrasonic range finders, weight sensors or the like, could be positioned to obtain other information, such as height information, about objects entering and exiting the environment. Together, the data obtained from these devices could be used to generate suitable signature vectors representative of the objects entering and exiting the environment. </paragraph>
<paragraph id="P-0078" lvl="2"><number>&lsqb;0078&rsqb;</number> The system and method described above involve the use of stereo vision cameras and stereo vision camera systems to generate the image data and signature vectors used to implement the invention. Non-imaging sensors may be added to the stereo vision system described above to provide additional data that may be used to generate the signature vectors used to implement the invention. Non-imaging sensors may include, for example, laser range finders, ultrasonic range finders, weight sensors, or the like. </paragraph>
<paragraph id="P-0079" lvl="2"><number>&lsqb;0079&rsqb;</number> Method <highlight><bold>100</bold></highlight>, depicted in <cross-reference target="DRAWINGS">FIG. 6</cross-reference> and described above, relates to a restricted access environment <highlight><bold>86</bold></highlight> having two paths <highlight><bold>72</bold></highlight> and <highlight><bold>74</bold></highlight> (see <cross-reference target="DRAWINGS">FIG. 3</cross-reference>). It should be appreciated by those skilled in the art, that the invention described herein can easily be extended to environments having more than two paths or fewer than two paths. </paragraph>
<paragraph id="P-0080" lvl="2"><number>&lsqb;0080&rsqb;</number> In some applications, it may not be required to monitor all of the paths associated with a restricted access environment. For example, a retail environment may have a path to a stockroom and the entry and exit of people from the stockroom may not be of interest. In some applications, there may be a number of paths, some of which may be heavily used and others of which may be much more lightly used. In these and other types of applications, acceptable statistics may be obtained by monitoring certain path(s) (i.e. the heavily used path(s)) and ignoring other paths. Those skilled in the art will appreciate that the invention may be deployed over a number of paths associated with an environment, but that number of paths need not include all of the paths associated with the environment. </paragraph>
<paragraph id="P-0081" lvl="2"><number>&lsqb;0081&rsqb;</number> The embodiment of tracking method <highlight><bold>100</bold></highlight> depicted in <cross-reference target="DRAWINGS">FIG. 6</cross-reference> represents one possible embodiment of the method according to the present invention. Those skilled in the art will appreciate that there are many other techniques to implement a tracking method according to the present invention. <cross-reference target="DRAWINGS">FIG. 9</cross-reference> depicts an alternative method <highlight><bold>300</bold></highlight> for tracking the entry and exit of a person from an environment of interest. In method <highlight><bold>300</bold></highlight>, the paths of the environment of interest are monitored constantly in block <highlight><bold>310</bold></highlight>. Method <highlight><bold>300</bold></highlight> involves identifying moving objects as people and determining their locations and trajectories as part of block <highlight><bold>320</bold></highlight>. A signature vector is generated for each entering and exiting person in block <highlight><bold>330</bold></highlight>. In block <highlight><bold>340</bold></highlight>, a decision is made as to whether the person is entering or exiting the environment of interest. The signature vectors for the exiting people may be matched with the signature vectors for the entering people at any time as a part of block <highlight><bold>350</bold></highlight>. Useful statistics, such as the dwell time of a person in the environment may be determined in block <highlight><bold>360</bold></highlight>. </paragraph>
<paragraph id="P-0082" lvl="2"><number>&lsqb;0082&rsqb;</number> The description of the invention discussed above relates to a restricted access environment. In general, however, the environment of interest need not be a restricted access environment if all or most possible paths to enter or exit the environment are covered by the stereo vision fields of stereo vision cameras. For example, <cross-reference target="DRAWINGS">FIG. 10</cross-reference> depicts an apparatus <highlight><bold>500</bold></highlight> according to an embodiment of the invention that is deployed to monitor an open environment <highlight><bold>510</bold></highlight>. Open environment <highlight><bold>510</bold></highlight> is surrounded by a plurality of stereo vision cameras (<highlight><bold>504</bold></highlight>A, <highlight><bold>504</bold></highlight>B, . . . <highlight><bold>504</bold></highlight>H), each of which has an associated stereo vision field (<highlight><bold>502</bold></highlight>A, <highlight><bold>502</bold></highlight>B, . . . <highlight><bold>502</bold></highlight>H). Stereo vision fields (<highlight><bold>502</bold></highlight>A, <highlight><bold>502</bold></highlight>B, . . . <highlight><bold>502</bold></highlight>H) overlap one another and surround open environment <highlight><bold>510</bold></highlight>, such that they may monitor the entrance and exit of people in a manner similar to that described above. </paragraph>
<paragraph id="P-0083" lvl="2"><number>&lsqb;0083&rsqb;</number> The methods described above need not be implemented in a particular order. For example, the methods depicted in <cross-reference target="DRAWINGS">FIGS. 6 and 7</cross-reference> suggest that a decision about whether a person is exiting or entering an environment of interest (block <highlight><bold>128</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 7</cross-reference>) is made prior to generating the signature vector for that person (block <highlight><bold>140</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 6</cross-reference>). Similarly, the method depicted in <cross-reference target="DRAWINGS">FIG. 9</cross-reference> shows that a signature vector is generated for each person (block <highlight><bold>330</bold></highlight>) before a decision is made as to whether the person is entering or exiting the environment (block <highlight><bold>340</bold></highlight>). The invention should be understood to be independent of the order in which these and other blocks are executed. </paragraph>
<paragraph id="P-0084" lvl="2"><number>&lsqb;0084&rsqb;</number> The use of entry signature &ldquo;vectors&rdquo; and exit signature &ldquo;vectors&rdquo; corresponds to particular embodiments of the invention where entry signatures and exit signatures take the form of vectors. In general, entry signatures and exit signatures need not be vectors and may take any form that is representative of the objects entering and exiting the environment of interest. For example, vector <highlight><bold>400</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 11</cross-reference> need not necessarily be a &ldquo;vector&rdquo; by the precise mathematical definition. The components (<highlight><bold>410</bold></highlight>, <highlight><bold>412</bold></highlight>, <highlight><bold>414</bold></highlight> . . . , <highlight><bold>422</bold></highlight>, <highlight><bold>424</bold></highlight>) of vector <highlight><bold>400</bold></highlight> may be non-scalar quantities. For example, a component of vector <highlight><bold>400</bold></highlight>, such as component <highlight><bold>410</bold></highlight> representing the height of a person may itself be a vector representing the precise height measurements made from each image frame when the person was in the stereo vision field. </paragraph>
<paragraph id="P-0085" lvl="2"><number>&lsqb;0085&rsqb;</number> In some circumstances, an object entering or exiting the environment will be improperly identified as a person, or, more generally, as an object of interest. Such a circumstance is referred to as a &ldquo;false positive&rdquo; identification. In many cases, the present invention is used to acquire statistical data over a relatively large sample size. Consequently, a relatively small number of false positives does not generally cause significant problems. </paragraph>
<paragraph id="P-0086" lvl="2"><number>&lsqb;0086&rsqb;</number> Many of the examples provided above relate to retail store applications. It should be appreciated by those skilled in the art, that the invention described above has many other applications, such as: security applications, the monitoring of other public and private areas, monitoring vehicles in traffic congested environments, the monitoring of farm animals. </paragraph>
<paragraph id="P-0087" lvl="0"><number>&lsqb;0087&rsqb;</number> Accordingly, the scope of the invention is to be construed in accordance with the substance defined by the following claims. </paragraph>
</section>
</detailed-description>
</subdoc-description>
<subdoc-claims>
<heading lvl="1">What is claimed is: </heading>
<claim id="CLM-00001">
<claim-text><highlight><bold>1</bold></highlight>. A method for tracking objects entering and exiting an environment, the method comprising: 
<claim-text>obtaining image data for one or more fields of view covering one or more paths by way of which objects can enter or exit the environment; </claim-text>
<claim-text>identifying objects of interest represented in the image data; </claim-text>
<claim-text>for each of the objects, determining from the image data whether the object is entering or exiting the environment and generating a signature for the object based, at least in part, on the image data; and </claim-text>
<claim-text>after generating one or more entry signatures for objects determined to be entering the environment and at least one exit signature for objects determined to be exiting the environment, matching the at least one exit signature to the one or more entry signatures, such that each matched exit signature corresponds with one matched entry signature. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00002">
<claim-text><highlight><bold>2</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein determining from the image data whether the object is entering or exiting the environment comprises tracking a position of the object over a plurality of image frames to determine a trajectory of the object. </claim-text>
</claim>
<claim id="CLM-00003">
<claim-text><highlight><bold>3</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00002">claim 2</dependent-claim-reference>, wherein determining from the image data whether the object is entering or exiting the environment is based, at least in part, on the position of the object when it leaves the one or more fields of view. </claim-text>
</claim>
<claim id="CLM-00004">
<claim-text><highlight><bold>4</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein generating a signature for the object comprises assigning one or more confidence measures to features of the image data, each such confidence measure being based on the uniformity of an associated feature of the image data over a plurality image frames. </claim-text>
</claim>
<claim id="CLM-00005">
<claim-text><highlight><bold>5</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00004">claim 4</dependent-claim-reference>, wherein each of the one or more confidence measures is representative of a variance of the associated feature of the image data. </claim-text>
</claim>
<claim id="CLM-00006">
<claim-text><highlight><bold>6</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00005">claim 5</dependent-claim-reference>, wherein generating a signature for the object comprises including one or more confidence measures as components of the signature for the object. </claim-text>
</claim>
<claim id="CLM-00007">
<claim-text><highlight><bold>7</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein matching the at least one exit signature to the one or more entry signatures comprises, after generating each of the at least one exit signatures for objects determined to be exiting the environment, matching the exit signature to the one or more entry signatures to identify a corresponding entry signature that provides a best match to the exit signature. </claim-text>
</claim>
<claim id="CLM-00008">
<claim-text><highlight><bold>8</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00007">claim 7</dependent-claim-reference>, wherein matching the exit signature to the one or more entry signatures comprises removing the exit signature and the corresponding entry signature from any subsequent matching. </claim-text>
</claim>
<claim id="CLM-00009">
<claim-text><highlight><bold>9</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00007">claim 7</dependent-claim-reference> comprising, after identifying the corresponding entry signature, checking a plurality of previously matched exit signatures and corresponding previously matched entry signatures to determine whether any of the previously matched entry signatures would be a better match to the exit signature than the corresponding entry signature. </claim-text>
</claim>
<claim id="CLM-00010">
<claim-text><highlight><bold>10</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00007">claim 7</dependent-claim-reference> comprising, after identifying the particular entry signature, checking a plurality of previously matched exit signatures and corresponding previously matched entry signatures to determine whether rematching one or more of the previously matched exit signatures would provide better overall matching between a plurality of available exit signatures and the one or more entry signatures. </claim-text>
</claim>
<claim id="CLM-00011">
<claim-text><highlight><bold>11</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein matching the at least one exit signature to the one or more entry signatures comprises matching a plurality of exit signatures to the one or more entry signatures to determine a plurality of matched entry and exit signatures, each matched exit signature corresponding to one matched entry signature. </claim-text>
</claim>
<claim id="CLM-00012">
<claim-text><highlight><bold>12</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00011">claim 11</dependent-claim-reference>, wherein matching a plurality of exit signatures to the one or more entry signatures comprises optimizing an overall match over the plurality of exit signatures, as opposed to a match over any one individual exit signature. </claim-text>
</claim>
<claim id="CLM-00013">
<claim-text><highlight><bold>13</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00011">claim 11</dependent-claim-reference>, wherein a number of exit signatures in the plurality of exit signatures is equal to a number of the one or more entry signatures. </claim-text>
</claim>
<claim id="CLM-00014">
<claim-text><highlight><bold>14</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00011">claim 11</dependent-claim-reference>, wherein a number of exit signatures in the plurality of exit signatures is different from a number of the one or more entry signatures. </claim-text>
</claim>
<claim id="CLM-00015">
<claim-text><highlight><bold>15</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein matching the at least one exit signature to the one or more entry signatures comprises weighting each of a plurality of components of the entry and exit signatures. </claim-text>
</claim>
<claim id="CLM-00016">
<claim-text><highlight><bold>16</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00011">claim 15</dependent-claim-reference>, wherein weighting each of a plurality of components of the entry and exit signatures comprises non-uniformly weighting components of the entry and exit signatures. </claim-text>
</claim>
<claim id="CLM-00017">
<claim-text><highlight><bold>17</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein matching the at least one exit signature to the one or more entry signatures comprises minimizing a metric representative of a Euclidean distance between the at least one exit signature and the one or more entry signatures. </claim-text>
</claim>
<claim id="CLM-00018">
<claim-text><highlight><bold>18</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00011">claim 17</dependent-claim-reference>, wherein minimizing a metric representative of a Euclidean distance between the at least one exit signature and the one or more entry signatures comprises applying non-uniform weighting to components of the metric. </claim-text>
</claim>
<claim id="CLM-00019">
<claim-text><highlight><bold>19</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00011">claim 17</dependent-claim-reference>, wherein minimizing a metric representative of a Euclidean distance between the at least one exit signature and the one or more entry signatures comprises applying non-uniform weighting to components of the entry and exit signatures. </claim-text>
</claim>
<claim id="CLM-00020">
<claim-text><highlight><bold>20</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference> comprising: 
<claim-text>recording an entry time for each of the objects determined to be entering the environment and associating the entry time with the entry signature for that object; </claim-text>
<claim-text>recording an exit time for each of the objects determined to be exiting the environment and associating the exit time with the exit signature for that object; and </claim-text>
<claim-text>after matching the at least one exit signature to the one or more entry signatures, determining a dwell time for one or more objects corresponding to matched pairs of entry and exit signatures. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00021">
<claim-text><highlight><bold>21</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein the image data comprises stereoscopic image data and the fields of view are stereo vision fields. </claim-text>
</claim>
<claim id="CLM-00022">
<claim-text><highlight><bold>22</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00002">claim 2</dependent-claim-reference>, wherein the image data comprises stereoscopic image data and the fields of view are stereo vision fields. </claim-text>
</claim>
<claim id="CLM-00023">
<claim-text><highlight><bold>23</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00004">claim 4</dependent-claim-reference>, wherein the image data comprises stereoscopic image data and the fields of view are stereo vision fields. </claim-text>
</claim>
<claim id="CLM-00024">
<claim-text><highlight><bold>24</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00007">claim 7</dependent-claim-reference>, wherein the image data comprises stereoscopic image data and the fields of view are stereo vision fields. </claim-text>
</claim>
<claim id="CLM-00025">
<claim-text><highlight><bold>25</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00011">claim 11</dependent-claim-reference>, wherein the image data comprises stereoscopic image data and the fields of view are stereo vision fields. </claim-text>
</claim>
<claim id="CLM-00026">
<claim-text><highlight><bold>26</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00011">claim 15</dependent-claim-reference>, wherein the image data comprises stereoscopic image data and the fields of view are stereo vision fields. </claim-text>
</claim>
<claim id="CLM-00027">
<claim-text><highlight><bold>27</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00011">claim 17</dependent-claim-reference>, wherein the image data comprises stereoscopic image data and the fields of view are stereo vision fields. </claim-text>
</claim>
<claim id="CLM-00028">
<claim-text><highlight><bold>28</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00022">claim 20</dependent-claim-reference>, wherein the image data comprises stereoscopic image data and the fields of view are stereo vision fields. </claim-text>
</claim>
<claim id="CLM-00029">
<claim-text><highlight><bold>29</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00022">claim 21</dependent-claim-reference>, wherein generating a signature for the object is based, at least in part, on features of the image data, the features of the image data comprising one or more of: a shape of the object; a location of the object; a transverse dimension of the object; a trajectory of the object; a color histogram of the object and a two-dimensional template of the object. </claim-text>
</claim>
<claim id="CLM-00030">
<claim-text><highlight><bold>30</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein the objects of interest are persons. </claim-text>
</claim>
<claim id="CLM-00031">
<claim-text><highlight><bold>31</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00002">claim 2</dependent-claim-reference>, wherein the objects of interest are persons. </claim-text>
</claim>
<claim id="CLM-00032">
<claim-text><highlight><bold>32</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00004">claim 4</dependent-claim-reference>, wherein the objects of interest are persons. </claim-text>
</claim>
<claim id="CLM-00033">
<claim-text><highlight><bold>33</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00007">claim 7</dependent-claim-reference>, wherein the objects of interest are persons. </claim-text>
</claim>
<claim id="CLM-00034">
<claim-text><highlight><bold>34</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00011">claim 11</dependent-claim-reference>, wherein the objects of interest are persons. </claim-text>
</claim>
<claim id="CLM-00035">
<claim-text><highlight><bold>35</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00011">claim 15</dependent-claim-reference>, wherein the objects of interest are persons. </claim-text>
</claim>
<claim id="CLM-00036">
<claim-text><highlight><bold>36</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00011">claim 17</dependent-claim-reference>, wherein the objects of interest are persons. </claim-text>
</claim>
<claim id="CLM-00037">
<claim-text><highlight><bold>37</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00022">claim 20</dependent-claim-reference>, wherein the objects of interest are persons. </claim-text>
</claim>
<claim id="CLM-00038">
<claim-text><highlight><bold>38</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00022">claim 29</dependent-claim-reference>, wherein the objects of interest are persons. </claim-text>
</claim>
<claim id="CLM-00039">
<claim-text><highlight><bold>39</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00022">claim 21</dependent-claim-reference>, wherein the objects of interest are persons. </claim-text>
</claim>
<claim id="CLM-00040">
<claim-text><highlight><bold>40</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00033">claim 39</dependent-claim-reference>, wherein generating a signature for the object is based, at least in part, on features of the image data, the features of the image data comprising one or more of: a height of the person; a shoulder width of the person; a shape of the person; a trajectory of the person; a location of the person; a color histogram of the person; a two-dimensional image template of the person; hair color of the person; a sex of the person; iris characteristics of the person; and facial characteristics of the person. </claim-text>
</claim>
<claim id="CLM-00041">
<claim-text><highlight><bold>41</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein generating a signature for the object is based, at least in part, on data obtained from one or more non-image data producing sensors, which are positioned to measure characteristics of objects entering or exiting the environment by way of the paths. </claim-text>
</claim>
<claim id="CLM-00042">
<claim-text><highlight><bold>42</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00022">claim 21</dependent-claim-reference>, wherein generating a signature for the object is based, at least in part, on data obtained from one or more non-image data producing sensors, which are positioned to measure characteristics of objects entering or exiting the environment by way of the paths. </claim-text>
</claim>
<claim id="CLM-00043">
<claim-text><highlight><bold>43</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00033">claim 30</dependent-claim-reference>, wherein generating a signature for the object is based, at least in part, on data obtained from one or more non-image data producing sensors, which are positioned to measure characteristics of objects entering or exiting the environment by way of the paths. </claim-text>
</claim>
<claim id="CLM-00044">
<claim-text><highlight><bold>44</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00044">claim 42</dependent-claim-reference>, wherein the one or more non-image data producing sensors comprise at least one of: a laser range finder; a ultrasonic distance sensor; and a weight sensor. </claim-text>
</claim>
<claim id="CLM-00045">
<claim-text><highlight><bold>45</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00033">claim 39</dependent-claim-reference>, wherein generating a signature for the object is based, at least in part, on data obtained from one or more non-image data producing sensors, which are positioned to measure characteristics of objects entering or exiting the environment by way of the paths. </claim-text>
</claim>
<claim id="CLM-00046">
<claim-text><highlight><bold>46</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein the objects of interest are one or more of: animals, vehicles and shopping carts. </claim-text>
</claim>
<claim id="CLM-00047">
<claim-text><highlight><bold>47</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00022">claim 21</dependent-claim-reference>, wherein the objects of interest are one or more of: animals, vehicles and shopping carts. </claim-text>
</claim>
<claim id="CLM-00048">
<claim-text><highlight><bold>48</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00033">claim 30</dependent-claim-reference>, wherein the objects of interest are one or more of: animals, vehicles and shopping carts. </claim-text>
</claim>
<claim id="CLM-00049">
<claim-text><highlight><bold>49</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00044">claim 41</dependent-claim-reference>, wherein the objects of interest are one or more of: animals, vehicles and shopping carts. </claim-text>
</claim>
<claim id="CLM-00050">
<claim-text><highlight><bold>50</bold></highlight>. A method of tracking objects entering and exiting an environment, the method comprising: 
<claim-text>providing one or more cameras positioned in such a manner that paths of the environment are substantially covered by fields of view associated with the one or more cameras; </claim-text>
<claim-text>determining entry signatures for objects entering the environment, the entry signatures based, at least in part, on image data obtained by the one or more cameras, each entry signature associated with a particular object; </claim-text>
<claim-text>determining exit signatures for objects exiting the environment, the entry signatures based, at least in part, on image data obtained by the one or more cameras, each exit signature associated with a particular object; and </claim-text>
<claim-text>at any time after determination of at least one exit signature, matching the at least one exit signature to a set of available entry signatures. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00051">
<claim-text><highlight><bold>51</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00055">claim 50</dependent-claim-reference>, wherein the one or more cameras are stereo vision cameras and the fields of view associated with the one or more cameras are stereo vision fields. </claim-text>
</claim>
<claim id="CLM-00052">
<claim-text><highlight><bold>52</bold></highlight>. A method of tracking objects entering and exiting an environment, the method comprising: 
<claim-text>monitoring paths to enter the environment using stereo vision cameras, the stereo vision cameras having stereo vision fields that substantially cover the paths; </claim-text>
<claim-text>tracking moving objects in the stereo vision fields; </claim-text>
<claim-text>deciding whether the moving objects are entering or exiting the environment and: </claim-text>
<claim-text>if a particular moving object is entering the environment, determining an entry signature associated with that particular object; and </claim-text>
<claim-text>if a particular moving object is exiting the environment, determining an exit signature associated with that particular object; and </claim-text>
<claim-text>at any time after determining at least one exit signature, matching the at least one exit signature to a set of available entry signatures, such that each matched exit signature corresponds with one matched entry signature.. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00053">
<claim-text><highlight><bold>53</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00055">claim 52</dependent-claim-reference>, wherein matching the at least one exit signature to a set of available entry signatures comprises matching a first plurality of exit signatures with a second plurality of entry signatures. </claim-text>
</claim>
<claim id="CLM-00054">
<claim-text><highlight><bold>54</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00055">claim 52</dependent-claim-reference>, wherein the objects are people. </claim-text>
</claim>
<claim id="CLM-00055">
<claim-text><highlight><bold>55</bold></highlight>. An apparatus for tracking objects entering and exiting an environment by way of one or more paths, the apparatus comprising: 
<claim-text>means for obtaining three-dimensional digital image data from regions proximate the paths; </claim-text>
<claim-text>means for determining entry signatures for objects entering the environment and for determining exit signature vectors for objects exiting the environment; and, </claim-text>
<claim-text>means for matching one or more exit signatures to a set of available entrance signatures. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00056">
<claim-text><highlight><bold>56</bold></highlight>. An apparatus according to <dependent-claim-reference depends_on="CLM-00055">claim 55</dependent-claim-reference>, comprising means for determining a dwell time of the objects in the environment. </claim-text>
</claim>
<claim id="CLM-00057">
<claim-text><highlight><bold>57</bold></highlight>. A machine readable medium carrying a set of instructions, which when executed by a data processor cause the data processor to perform a method of tracking objects entering and exiting an environment, the method comprising: 
<claim-text>receiving image data from one or more stereo vision cameras which are positioned in such a manner that paths to enter the environment are substantially covered by stereo vision fields associated with the one or more stereo vision cameras; </claim-text>
<claim-text>determining entry signatures for objects entering the environment using the image data obtained by the one or more stereo vision cameras, each entry signature associated with a particular object; </claim-text>
<claim-text>determining exit signatures for objects exiting the environment using image data obtained by the one or more stereo vision cameras, each exit signature associated with a particular object; and </claim-text>
<claim-text>at any time after determining at least one exit signature, matching at least one exit signature to a set of available entry signatures. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00058">
<claim-text><highlight><bold>58</bold></highlight>. A method according to <dependent-claim-reference depends_on="CLM-00055">claim 57</dependent-claim-reference> comprising determining a dwell time of a person by subtracting, from an exit time associated with an exit signature for that person, an entrance time associated with a matching entrance signature for that person.</claim-text>
</claim>
</subdoc-claims>
<subdoc-drawings id="DRAWINGS">
<heading lvl="0" align="CENTER">Drawings</heading>
<representative-figure>6</representative-figure>
<figure id="figure-D00000">
<image id="EMI-D00000" file="US20030002712A1-20030102-D00000.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00001">
<image id="EMI-D00001" file="US20030002712A1-20030102-D00001.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00002">
<image id="EMI-D00002" file="US20030002712A1-20030102-D00002.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00003">
<image id="EMI-D00003" file="US20030002712A1-20030102-D00003.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00004">
<image id="EMI-D00004" file="US20030002712A1-20030102-D00004.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00005">
<image id="EMI-D00005" file="US20030002712A1-20030102-D00005.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00006">
<image id="EMI-D00006" file="US20030002712A1-20030102-D00006.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00007">
<image id="EMI-D00007" file="US20030002712A1-20030102-D00007.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00008">
<image id="EMI-D00008" file="US20030002712A1-20030102-D00008.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00009">
<image id="EMI-D00009" file="US20030002712A1-20030102-D00009.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00010">
<image id="EMI-D00010" file="US20030002712A1-20030102-D00010.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00011">
<image id="EMI-D00011" file="US20030002712A1-20030102-D00011.TIF" imf="TIFF" ti="DR"/>
</figure>
</subdoc-drawings>
</patent-application-publication>
