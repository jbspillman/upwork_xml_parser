<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE patent-application-publication SYSTEM "pap-v16-2002-01-01.dtd" [
<!ENTITY US20030002730A1-20030102-D00000.TIF SYSTEM "US20030002730A1-20030102-D00000.TIF" NDATA TIF>
<!ENTITY US20030002730A1-20030102-D00001.TIF SYSTEM "US20030002730A1-20030102-D00001.TIF" NDATA TIF>
<!ENTITY US20030002730A1-20030102-D00002.TIF SYSTEM "US20030002730A1-20030102-D00002.TIF" NDATA TIF>
<!ENTITY US20030002730A1-20030102-D00003.TIF SYSTEM "US20030002730A1-20030102-D00003.TIF" NDATA TIF>
<!ENTITY US20030002730A1-20030102-D00004.TIF SYSTEM "US20030002730A1-20030102-D00004.TIF" NDATA TIF>
<!ENTITY US20030002730A1-20030102-D00005.TIF SYSTEM "US20030002730A1-20030102-D00005.TIF" NDATA TIF>
<!ENTITY US20030002730A1-20030102-D00006.TIF SYSTEM "US20030002730A1-20030102-D00006.TIF" NDATA TIF>
<!ENTITY US20030002730A1-20030102-D00007.TIF SYSTEM "US20030002730A1-20030102-D00007.TIF" NDATA TIF>
<!ENTITY US20030002730A1-20030102-D00008.TIF SYSTEM "US20030002730A1-20030102-D00008.TIF" NDATA TIF>
<!ENTITY US20030002730A1-20030102-D00009.TIF SYSTEM "US20030002730A1-20030102-D00009.TIF" NDATA TIF>
<!ENTITY US20030002730A1-20030102-D00010.TIF SYSTEM "US20030002730A1-20030102-D00010.TIF" NDATA TIF>
<!ENTITY US20030002730A1-20030102-D00011.TIF SYSTEM "US20030002730A1-20030102-D00011.TIF" NDATA TIF>
<!ENTITY US20030002730A1-20030102-D00012.TIF SYSTEM "US20030002730A1-20030102-D00012.TIF" NDATA TIF>
<!ENTITY US20030002730A1-20030102-D00013.TIF SYSTEM "US20030002730A1-20030102-D00013.TIF" NDATA TIF>
<!ENTITY US20030002730A1-20030102-D00014.TIF SYSTEM "US20030002730A1-20030102-D00014.TIF" NDATA TIF>
<!ENTITY US20030002730A1-20030102-D00015.TIF SYSTEM "US20030002730A1-20030102-D00015.TIF" NDATA TIF>
<!ENTITY US20030002730A1-20030102-D00016.TIF SYSTEM "US20030002730A1-20030102-D00016.TIF" NDATA TIF>
<!ENTITY US20030002730A1-20030102-D00017.TIF SYSTEM "US20030002730A1-20030102-D00017.TIF" NDATA TIF>
]>
<patent-application-publication>
<subdoc-bibliographic-information>
<document-id>
<doc-number>20030002730</doc-number>
<kind-code>A1</kind-code>
<document-date>20030102</document-date>
</document-id>
<publication-filing-type>new</publication-filing-type>
<domestic-filing-data>
<application-number>
<doc-number>10188660</doc-number>
</application-number>
<application-number-series-code>10</application-number-series-code>
<filing-date>20020702</filing-date>
</domestic-filing-data>
<technical-information>
<classification-ipc>
<classification-ipc-primary>
<ipc>G06T015/50</ipc>
</classification-ipc-primary>
<classification-ipc-secondary>
<ipc>G06T015/00</ipc>
</classification-ipc-secondary>
<classification-ipc-secondary>
<ipc>G06T007/00</ipc>
</classification-ipc-secondary>
<classification-ipc-edition>07</classification-ipc-edition>
</classification-ipc>
<classification-us>
<classification-us-primary>
<uspc>
<class>382</class>
<subclass>154000</subclass>
</uspc>
</classification-us-primary>
<classification-us-secondary>
<uspc>
<class>382</class>
<subclass>284000</subclass>
</uspc>
</classification-us-secondary>
<classification-us-secondary>
<uspc>
<class>382</class>
<subclass>286000</subclass>
</uspc>
</classification-us-secondary>
<classification-us-secondary>
<uspc>
<class>345</class>
<subclass>629000</subclass>
</uspc>
</classification-us-secondary>
<classification-us-secondary>
<uspc>
<class>345</class>
<subclass>634000</subclass>
</uspc>
</classification-us-secondary>
</classification-us>
<title-of-invention>System and method for discovering and categorizing attributes of a digital image</title-of-invention>
</technical-information>
<continuity-data>
<non-provisional-of-provisional>
<document-id>
<doc-number>60302919</doc-number>
<document-date>20010702</document-date>
<country-code>US</country-code>
</document-id>
</non-provisional-of-provisional>
</continuity-data>
<inventors>
<first-named-inventor>
<name>
<given-name>David</given-name>
<middle-name>B.</middle-name>
<family-name>Petrich</family-name>
</name>
<residence>
<residence-us>
<city>Seattle</city>
<state>WA</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</first-named-inventor>
</inventors>
<correspondence-address>
<name-1>MERCHANT &amp; GOULD PC</name-1>
<name-2></name-2>
<address>
<address-1>P.O. BOX 2903</address-1>
<city>MINNEAPOLIS</city>
<state>MN</state>
<postalcode>55402-0903</postalcode>
<country>
<country-code>US</country-code>
</country>
</address>
</correspondence-address>
</subdoc-bibliographic-information>
<subdoc-abstract>
<paragraph id="A-0001" lvl="0">With respect to photography, the present invention includes a physical target device used in conjunction with computer software to ascertain and record such attributes as lighting conditions, perspective, and scale with regard to the assembly of two-dimensional photographic imagery, consisting of background and subject images, into realistic photo composites. </paragraph>
</subdoc-abstract>
<subdoc-description>
<summary-of-invention>
<section>
<heading lvl="1">FIELD OF THE INVENTION </heading>
<paragraph id="P-0001" lvl="0"><number>&lsqb;0001&rsqb;</number> The present invention relates to computer and photograph manipulation systems and machine vision technologies in general, and particularly to computer image processing software in the field of digital imaging and cinematography for creating virtual reality composite photographs and composite motion pictures. </paragraph>
</section>
<section>
<heading lvl="1">BACKGROUND OF THE INVENTION </heading>
<paragraph id="P-0002" lvl="0"><number>&lsqb;0002&rsqb;</number> Photographs of people and products are traditionally captured by photographers who travel to distant locations for the purpose of producing imagery for merchandise catalogs, magazine advertisements, sales collateral and the like. The photographer is tasked with capturing the subject and a desirable background in camera, as one single photograph. This is common practice in photographing clothing, cars, furniture, house goods, etc. Computer image manipulation software such as Adobe PhotoShop&reg; provides the ability to alter and transform digital pictures in a variety of ways. For instance, imaging applications have the ability to transpose one image over another in sequential layers. Additionally, these digital image manipulation programs provide a means to artistically transform sections of an image to create special visual effects such as, adding simulated shadow and reflections, color tone and hue alterations, scale and rotation translations, and so forth. An important concept in creating realistic composite photography is to maintain consistent lighting, perspective and scale between the various elements of a composite image. When multiple image layers are merged into a single montage, digital artists and cinematographers traditionally take great care in recording and visually matching lighting, perspective, and scale attributes between each photographic element of a composite photograph or motion picture frame. </paragraph>
</section>
<section>
<heading lvl="1">SUMMARY OF THE INVENTION </heading>
<paragraph id="P-0003" lvl="0"><number>&lsqb;0003&rsqb;</number> The present invention is directed to a method to decipher and record attribute information, such as lighting, perspective, and scale from two-dimensional photographs so that each processed photograph may be merged with other processed photographs or three-dimensional computer rendered images having similar attribute values to form a realistic montage. Additionally, the present invention utilizes computer imaging software methods to automatically extract lighting, perspective, and scale information from digitized photographs. </paragraph>
<paragraph id="P-0004" lvl="0"><number>&lsqb;0004&rsqb;</number> The present invention relates to the use of a three-dimensional target device, having known dimensions and shape, which is placed within a photograph at the time of capture. When photographed, the embedded target device may reveal different geometric outline profiles and, additionally, light reflection and shadow characteristics when viewed and illuminated from various angles. For example, one possible target device configuration is considered to be a flat disc having a spherical dome center. The target device, also known as a composite target may include a colored-coded or patterned circular band around its circumference so that its boundaries may be easily identified in contrast to the natural color fields within a photograph. In addition, the target device may include a neutral white surface, oriented upwards, to provide for a measurement of the apparent color hue of light that illuminates the indexed photograph. The spherical dome center of the target device provides a visual reference indicator as to direction of one or more light sources in much the same way the position of the sun may be ascertained from viewing the Earth&apos;s moon. In another embodiment, the condition of the dome surface may be treated in such a manner to produce a finish that reveals spectral highlights to determine comparative levels of light diffusion and ambiance. </paragraph>
<paragraph id="P-0005" lvl="0"><number>&lsqb;0005&rsqb;</number> Another aspect of the present invention is a process of digitally analyzing photographic images with specialized software to determine lighting and shadow conditions such as a vertical and horizontal direction of light sources in relation to a target device, degree of light diffusion, degree of ambient illumination, and light color. For example, a partial embodiment of the present invention software derives, through algorithms, the apparent perspective and ground surface angles in relation to the camera position, and comparative scale factor of a point within an image as it relates to the position of an embedded target device. </paragraph>
<paragraph id="P-0006" lvl="0"><number>&lsqb;0006&rsqb;</number> The software enables users to automatically process single or multiple batches of image files so that an encapsulated string of data is electronically associated with, embedded into, or in representation thereof specific image files in a computer file system or database. In an example of the software component of the present invention, a user analyzes and assigns such data to allow efficient searching of image file records in a computer system, network storage device, or Internet accessible server. </paragraph>
<paragraph id="P-0007" lvl="0"><number>&lsqb;0007&rsqb;</number> In another aspect of the current invention, intermediate software allows a user to utilize assigned image specific lighting, perspective, and scale data to automatically generate three-dimensional computer rendered scenes that match said image files using common animation software programs. In this example, the environmental attributes of the source image are referenced so that the simulated lighting and camera positions are fixed to match the conditions of the source image. A photographer, for instance, may photograph a human subject in a blue-screen studio, and then subsequently have a background environment created within a three-dimensional CAD (Computer Aided Design) system with lighting, perspective and scale matching the subject photograph.</paragraph>
</section>
</summary-of-invention>
<brief-description-of-drawings>
<section>
<heading lvl="1">BRIEF DESCRIPTION OF THE DRAWINGS </heading>
<paragraph id="P-0008" lvl="0"><number>&lsqb;0008&rsqb;</number> The foregoing aspects of the invention are illustrated so that the attributes and advantages may be become well understood in the accompanying drawings, wherein: </paragraph>
<paragraph id="P-0009" lvl="0"><number>&lsqb;0009&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 1</cross-reference> includes diagrams illustrating an example of the use of a target device within a subject and background image; </paragraph>
<paragraph id="P-0010" lvl="0"><number>&lsqb;0010&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 2</cross-reference> includes diagrams illustrating an exemplary associated graph displaying a method in which a target device is located within a subject image; </paragraph>
<paragraph id="P-0011" lvl="0"><number>&lsqb;0011&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 3</cross-reference> includes diagrams illustrating an exemplary associated graph displaying a method in which a target device is located within a background image; </paragraph>
<paragraph id="P-0012" lvl="0"><number>&lsqb;0012&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 4</cross-reference> is a sample illustration of signatures of a target device from various angles; </paragraph>
<paragraph id="P-0013" lvl="0"><number>&lsqb;0013&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 5</cross-reference> is a diagram that demonstrates the resulting viewing angle of the ground surface when photographed from a low position; </paragraph>
<paragraph id="P-0014" lvl="0"><number>&lsqb;0014&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 6</cross-reference> is a diagram that demonstrates the resulting viewing angle of the ground surface when photographed from a high position; </paragraph>
<paragraph id="P-0015" lvl="0"><number>&lsqb;0015&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 7</cross-reference> includes diagrams that demonstrate an aspect ratio of a target device viewed at two different angles for the purpose of calculating a viewing angle of the ground surface in a photograph; </paragraph>
<paragraph id="P-0016" lvl="0"><number>&lsqb;0016&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 8</cross-reference> includes diagrams that illustrate an example calculated scan region in which lighting directional data is ascertained; </paragraph>
<paragraph id="P-0017" lvl="0"><number>&lsqb;0017&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 9</cross-reference> is a diagram and graph depicting the hypothetical results of scanning across a highlighted area of the dome shape of the target device; </paragraph>
<paragraph id="P-0018" lvl="0"><number>&lsqb;0018&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 10</cross-reference> is a series of exemplary illustrations which demonstrate the effects of various light sources in regard to the surface of the dome shape of the target device; </paragraph>
<paragraph id="P-0019" lvl="0"><number>&lsqb;0019&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 11</cross-reference> includes exemplary illustrations which demonstrate the shadow detail difference between diffused lighting and direct lighting on the surface of the dome shape of the target device; </paragraph>
<paragraph id="P-0020" lvl="0"><number>&lsqb;0020&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 12</cross-reference> is a diagram showing a derivation of relative scale from a photograph that includes various target devices; </paragraph>
<paragraph id="P-0021" lvl="0"><number>&lsqb;0021&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 13</cross-reference> is a diagram that illustrates a measurement of a sampling point location on a target device; </paragraph>
<paragraph id="P-0022" lvl="0"><number>&lsqb;0022&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 14</cross-reference> is a diagram that illustrates a determination of sampling points on a target device to determine a vanishing point perspective factor; </paragraph>
<paragraph id="P-0023" lvl="0"><number>&lsqb;0023&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 15</cross-reference> is an exemplary illustration that demonstrates the effect of optical perspective between two similar camera views; </paragraph>
<paragraph id="P-0024" lvl="0"><number>&lsqb;0024&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 16</cross-reference> is a schematic diagram that includes two views of a target device; and </paragraph>
<paragraph id="P-0025" lvl="0"><number>&lsqb;0025&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 17</cross-reference> is a set of diagrams that demonstrate an exemplary use of the current invention for multi-frame cinematography composites, in accordance with the present invention.</paragraph>
</section>
</brief-description-of-drawings>
<detailed-description>
<section>
<heading lvl="1">DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENT </heading>
<paragraph id="P-0026" lvl="0"><number>&lsqb;0026&rsqb;</number> The present invention is directed to a system that automatically analyzes and records specific illumination and positioning attribute values of individual photographic images, such as scenic background images and subject images. The photographic images are assigned attributes, so that they may be combined with other images having similar attributes into a single photo-realistic montage. As used herein, the term &ldquo;attributes&rdquo; includes, but is not limited to, lighting direction, perspective, and scale. A user of the present invention for example, may photograph a subject in a studio, and then photograph a separate background image so that each image is photographed with similar attributes. The user may photograph either image before the other, for the purpose of matching attributes from the first image to the second image. In another embodiment, the user photographs and records the attributes of a subject image, then subsequently searches a file system, network storage device, Internet accessible server, or database for background images having corresponding attribute values. Alternatively, the user photographs and records a subject image, according to its attributes. The recorded attributes or attribute values of the subject image may be used to create virtual reality backgrounds produced from a three-dimensional computer model rendering of an environment having similar lighting, perspective, and scale conditions. In another embodiment, the aspects of the present invention may be attributed to producing coordinated virtual reality backgrounds for motion picture composites. </paragraph>
<paragraph id="P-0027" lvl="0"><number>&lsqb;0027&rsqb;</number> The process of coordinating various independent images into a realistic composite is demonstrated in the exemplary illustrations of <cross-reference target="DRAWINGS">FIG. 1</cross-reference> for a process of transposing a subject image (<highlight><bold>101</bold></highlight>) into a background image (<highlight><bold>108</bold></highlight>). In this example, a human figure (<highlight><bold>103</bold></highlight>) is photographed against a blue-screen seamless backdrop (<highlight><bold>104</bold></highlight>) with a studio light source (<highlight><bold>105</bold></highlight>) placed in a similar position to that of the natural or synthetically rendered light source (<highlight><bold>110</bold></highlight>) present in the background image. The human figure (<highlight><bold>103</bold></highlight>) is digitally separated from the blue-screen seamless backdrop (<highlight><bold>104</bold></highlight>) and placed into the background image (<highlight><bold>108</bold></highlight>). The two photographs are matched by virtue of having similar characteristics revealed by a target device (<highlight><bold>102</bold></highlight>) placed on the ground at the base of the human figure (<highlight><bold>103</bold></highlight>) in the subject image (<highlight><bold>101</bold></highlight>). In one embodiment, the target devices in the subject and background image are matched by: locating a photographed background image via searching a database of target-embedded background images according to a set of queried values relating to the target device placed in the subject photograph. In another embodiment, the target devices in the subject and background image are matched by modifying the conditions of a photographed subject image during its capture to match the target device values in the background image. In yet another embodiment, the target devices in the subject and background image are matched by producing a computer rendered virtual reality background image that matches the target values of a photographed subject image. In still another embodiment, the target devices in the subject and background image are matched by producing a computer rendered virtual reality subject image to match the target values of a photographed background image. </paragraph>
<paragraph id="P-0028" lvl="0"><number>&lsqb;0028&rsqb;</number> In one embodiment, the present invention includes the use of digital imaging software and algorithms that aid in the process of locating and analyzing an embedded target device within a photograph. The diagrams of <cross-reference target="DRAWINGS">FIG. 2</cross-reference> demonstrate target device (<highlight><bold>203</bold></highlight>) located within a digital subject photograph image (<highlight><bold>201</bold></highlight>). In this example, the subject photograph (<highlight><bold>201</bold></highlight>) is horizontally scanned by converting the image to an array of pixels to identify a high color luminescence value emanating from a color-coded ring (<highlight><bold>211</bold></highlight>) of the target device (<highlight><bold>203</bold></highlight>) that is placed at the base of the subject (<highlight><bold>212</bold></highlight>). The &lsquo;Color Luminance Curve&rsquo; (<highlight><bold>205</bold></highlight>) illustrates said resulting scan values from a specific scan line (<highlight><bold>204</bold></highlight>) where &lsquo;X&rsquo; (<highlight><bold>206</bold></highlight>) equals the horizontal distance from the left edge of the photograph and &lsquo;Y&rsquo; (<highlight><bold>207</bold></highlight>) equals the color luminescence value. In this example, the color-coded ring (<highlight><bold>211</bold></highlight>) of the target device produces high luminescence values (<highlight><bold>208</bold></highlight>). The location of the target device (<highlight><bold>203</bold></highlight>) may be determined by compiling the results of the scan line information into a database lookup table and recording horizontal and vertical Cartesian coordinates. A visual representation of the target device (<highlight><bold>203</bold></highlight>) location within a photographic image is displayed in a scan plot (<highlight><bold>210</bold></highlight>) where the high luminescence values create a signature (<highlight><bold>209</bold></highlight>) of the target device (<highlight><bold>203</bold></highlight>). </paragraph>
<paragraph id="P-0029" lvl="0"><number>&lsqb;0029&rsqb;</number> In a similar fashion to <cross-reference target="DRAWINGS">FIG. 2</cross-reference>, the diagrams of <cross-reference target="DRAWINGS">FIG. 3</cross-reference> demonstrate how a target device is located within a digitized environment background photograph. As with the foregoing, the background image (<highlight><bold>301</bold></highlight>) is horizontally scanned to identify a high color luminescence value emanating from the target device (<highlight><bold>303</bold></highlight>) that is placed on a surface within the background image (<highlight><bold>301</bold></highlight>). The &lsquo;Color Luminance Curve&rsquo; (<highlight><bold>305</bold></highlight>) illustrates the resulting scan values from a specific scan line (<highlight><bold>304</bold></highlight>) where &lsquo;X&rsquo; (<highlight><bold>306</bold></highlight>) equals the horizontal distance from the left edge of the photograph and &lsquo;Y&rsquo; (<highlight><bold>307</bold></highlight>) equals the color luminescence value. In this example, a t color-coded ring (<highlight><bold>302</bold></highlight>) of the target device (<highlight><bold>303</bold></highlight>) produces a high luminescence value (<highlight><bold>308</bold></highlight>). A visual representation of the target device (<highlight><bold>303</bold></highlight>) location within the background image (<highlight><bold>301</bold></highlight>) may be displayed in a scan plot (<highlight><bold>310</bold></highlight>) and the high luminescence values create a target signature (<highlight><bold>309</bold></highlight>). </paragraph>
<paragraph id="P-0030" lvl="0"><number>&lsqb;0030&rsqb;</number> Not withstanding similar variations of the foregoing examples, other methods of detecting said target devices within a digital image include the use of: HSL, HSV, LAB, RGB and other digital image color spaces, and may incorporate the use of such algorithms that employ particle analysis to locate geometric shapes. </paragraph>
<paragraph id="P-0031" lvl="0"><number>&lsqb;0031&rsqb;</number> In one embodiment, the software includes the ability to compensate for the effect of various ground surface angles in reference to the camera position when identifying the target device. The illustration of <cross-reference target="DRAWINGS">FIG. 4</cross-reference> displays various scan signatures resulting from the target device when viewed from a camera at different angles to the ground surface. In this example, the left most signature (<highlight><bold>401</bold></highlight>) represents a target photographed at a low angle to the ground surface, and the right most signature (<highlight><bold>402</bold></highlight>) represents a target photographed at a high angle to the ground surface. </paragraph>
<paragraph id="P-0032" lvl="0"><number>&lsqb;0032&rsqb;</number> With the foregoing considered, <cross-reference target="DRAWINGS">FIGS. 5 and 6</cross-reference> generally demonstrate how to obtain the angle &ldquo;A&rdquo; in which the ground surface (<highlight><bold>507</bold></highlight> and <highlight><bold>607</bold></highlight>) of a photograph is oriented in relation to the camera lens (<highlight><bold>502</bold></highlight> and <highlight><bold>602</bold></highlight>), known as apparent ground surface angle. In this example, <cross-reference target="DRAWINGS">FIG. 5</cross-reference> illustrates the capture of a photograph with a low ground surface angle value, while <cross-reference target="DRAWINGS">FIG. 6</cross-reference> illustrates the capture of a photograph with a high ground surface angle value. Generally, a composite image having visible ground surface contact with the overlaid subject image component(s) is considered to be more realistic if the ground angle between all the collective images within the composite is similar. The ground surface angle value (<highlight><bold>505</bold></highlight> and <highlight><bold>605</bold></highlight>) is determined by measuring the angle between the horizontal ground surface plane (<highlight><bold>503</bold></highlight> and <highlight><bold>603</bold></highlight>) and a line drawn from a point in the center of the target device (<highlight><bold>501</bold></highlight> and <highlight><bold>601</bold></highlight>) to the center of the camera lens (<highlight><bold>502</bold></highlight> and <highlight><bold>602</bold></highlight>). The ground surface angle value (<highlight><bold>505</bold></highlight> and <highlight><bold>605</bold></highlight>) is generally measured from an axis point at the base of the subject (<highlight><bold>506</bold></highlight> and <highlight><bold>606</bold></highlight>). </paragraph>
<paragraph id="P-0033" lvl="0"><number>&lsqb;0033&rsqb;</number> Using computer image analysis software, <cross-reference target="DRAWINGS">FIG. 7</cross-reference> demonstrates how a ground surface angle value is mathematically ascertained from the relative position of the target device (<highlight><bold>702</bold></highlight> and <highlight><bold>706</bold></highlight>) placed into a photograph with respect to the viewing position of the camera. The elliptical signature shape of the target identifier ring (<highlight><bold>701</bold></highlight> and <highlight><bold>705</bold></highlight>) of the target device (<highlight><bold>702</bold></highlight> and <highlight><bold>706</bold></highlight>) within a photograph differs based on the position of the camera. Vertical and horizontal measurements are made from the outer boundaries of the target ring (<highlight><bold>701</bold></highlight> and <highlight><bold>705</bold></highlight>). <cross-reference target="DRAWINGS">FIG. 7</cross-reference> illustrates the difference of a target device (<highlight><bold>702</bold></highlight> and <highlight><bold>706</bold></highlight>) at a low ground surface angle view and a high ground surface angle view where the width values &lsquo;X&rsquo; (<highlight><bold>703</bold></highlight> and <highlight><bold>707</bold></highlight>) are divided by the height values &lsquo;Y&rsquo; (<highlight><bold>704</bold></highlight> and <highlight><bold>708</bold></highlight>) to determine an aspect ratio value. In another method, a mathematical ellipse is fitted to the target signature to ascertain both ground surface angle and axial moment angle (tilt angle) of the target device. </paragraph>
<paragraph id="P-0034" lvl="0"><number>&lsqb;0034&rsqb;</number> Often, the visual reality of a composite image is dependent on uniform visual attributes between the collective image components contained within the composite image, among other factors. <cross-reference target="DRAWINGS">FIGS. 8, 9</cross-reference>, <highlight><bold>10</bold></highlight>, and <highlight><bold>11</bold></highlight> illustrate examples of how light qualities such as relative direction in relation to camera position, degree of diffusion, and degree of ambiance are ascertained from a digital image. </paragraph>
<paragraph id="P-0035" lvl="0"><number>&lsqb;0035&rsqb;</number> The diagrams of <cross-reference target="DRAWINGS">FIG. 8</cross-reference> demonstrate how to locate a region within the target area of a photographed image, where the region is used to identify such attributes as light source direction, light diffusion, and light ambiance. In one embodiment, a smooth, hemispherical shape or dome shape (<highlight><bold>802</bold></highlight>) located in the center of the target device (<highlight><bold>801</bold></highlight>) is used to reflect a key light source or multiple light sources so that the spectral reflective properties of the surface of the dome shape (<highlight><bold>802</bold></highlight>) reveal the horizontal and vertical direction of light. To determine light direction by means of digital analysis, a scan sample region of the target signature as previously described is isolated by the definition of a 180-degree arc as indicated by the dotted line (<highlight><bold>806</bold></highlight>). The center point of the arc boundary lies on the intersection of a horizontal line dividing the target signature height Y (<highlight><bold>804</bold></highlight>) and a vertical line dividing the target signature width X (<highlight><bold>803</bold></highlight>). The arc has a specified height H (<highlight><bold>805</bold></highlight>). The arc boundary remains constant in consideration of the various target-device viewing angles (<highlight><bold>807</bold></highlight>) with respect to the scheme of locating the arc boundary from the center of the target signature. The diagrams of <cross-reference target="DRAWINGS">FIG. 9</cross-reference> illustrate how to extract specific lighting values from within the area of the dome shape (<highlight><bold>901</bold></highlight>) of a target device (<highlight><bold>801</bold></highlight>) as shown in <cross-reference target="DRAWINGS">FIG. 8</cross-reference>. In instances other than conditions of highly diffused or omnipresent light, the dome shape (<highlight><bold>901</bold></highlight>) is generally illuminated unevenly, resulting in both highlight areas (<highlight><bold>902</bold></highlight>) and shadow areas (<highlight><bold>903</bold></highlight>). A series of sequential horizontal scan lines (<highlight><bold>904</bold></highlight>) determine lightness and darkness values as illustrated in the Lightness Curve graph (<highlight><bold>909</bold></highlight>). In this example, a specific scan line (<highlight><bold>908</bold></highlight>) is calculated within the defined arc area of the target device (<highlight><bold>801</bold></highlight>) shown in <cross-reference target="DRAWINGS">FIG. 8</cross-reference> by a function of length S (<highlight><bold>907</bold></highlight>) and vertical offset P (<highlight><bold>906</bold></highlight>). The highlight reflection area of the main light source (<highlight><bold>912</bold></highlight>) is indicated by a relatively high lightness value in the Y dimension (<highlight><bold>911</bold></highlight>). Shadow areas of the dome are identified by a low lightness value in the Y dimension (<highlight><bold>913</bold></highlight>) while the horizontal location of the foregoing values is determined by the X dimension (<highlight><bold>910</bold></highlight>) of the Lightness Curve graph (<highlight><bold>909</bold></highlight>). In one embodiment, a lookup table of scan line lightness values having specific location coordinates are compared to a known table of values to determine the elevation and horizontal direction of a light source in a photographed image. Consideration for the ground surface angle of a target device may be determined from the aspect ratio calculated in <cross-reference target="DRAWINGS">FIG. 8</cross-reference> as it relates to a known table of values that vary depending on the viewing angle of the target device. </paragraph>
<paragraph id="P-0036" lvl="0"><number>&lsqb;0036&rsqb;</number> Not withstanding variations of the foregoing, other methods of determining the relative position of light reflection and shadow may also be accomplished by way of particle analysis. Mathematical algorithms may also be used in place of a know table of values to calculate the position of light. </paragraph>
<paragraph id="P-0037" lvl="0"><number>&lsqb;0037&rsqb;</number> The exemplary illustrations of <cross-reference target="DRAWINGS">FIG. 10</cross-reference> demonstrate a geometric calculation of light direction from the surface of the target dome (<highlight><bold>1003</bold></highlight>). A light source (<highlight><bold>1001</bold></highlight>) causes illumination of the nearest point on the hemisphere (<highlight><bold>1002</bold></highlight>) so that it affects the visual appearance of the sphere from the position of the camera (<highlight><bold>1005</bold></highlight>). A portion of light (<highlight><bold>1004</bold></highlight>), and varying degrees thereof, emitted from the light source and reflected off the target dome (<highlight><bold>1003</bold></highlight>) of the target device (<highlight><bold>1008</bold></highlight>) reaches the lens of the camera (<highlight><bold>1005</bold></highlight>) so as to create a visual effect similar to lunar illumination from the sun (<highlight><bold>1007</bold></highlight>) as viewed from a camera (<highlight><bold>1005</bold></highlight>). Back light (<highlight><bold>1009</bold></highlight>), front light (<highlight><bold>1010</bold></highlight>), and side light (<highlight><bold>1011</bold></highlight>) each produce different measurable effects, which are analyzed and digitally plotted to determine light source direction from a two-dimensional camera view (<highlight><bold>1006</bold></highlight>). </paragraph>
<paragraph id="P-0038" lvl="0"><number>&lsqb;0038&rsqb;</number> A factor in creating visually real composite images is the consistency of illumination quality between the collective photographic components. In one example, the illumination quality is measured as the degree of diffusion pertaining to each present light source and overall degree of ambient fill light. The exemplary illustrations of <cross-reference target="DRAWINGS">FIG. 11</cross-reference> depict the difference in illumination from an unfiltered direct light source (<highlight><bold>1101</bold></highlight>) in comparison to a diffused light source (<highlight><bold>1107</bold></highlight>). For example, the aligned rays of light in the direct light source example cast a hard or sharp shadow effect (<highlight><bold>1104</bold></highlight>) on the first target dome (<highlight><bold>1103</bold></highlight>). Conversely, the rays of light in the diffused light source example cause a soft-edged shadow effect (<highlight><bold>1110</bold></highlight>) on the second target dome (<highlight><bold>1109</bold></highlight>). With regard to the Lightness Curve graph of <cross-reference target="DRAWINGS">FIG. 9, a</cross-reference> digital scan of the foregoing light direction analysis scheme produces an identifiable sharp curve in direct light conditions and a gradual rounded curve in diffused light conditions. The comparative degree of ambient light is measured in the same manner by comparing the contrast difference between the lightest area (<highlight><bold>1105</bold></highlight> and <highlight><bold>1111</bold></highlight>) of each target dome (<highlight><bold>1103</bold></highlight> and <highlight><bold>1109</bold></highlight>) to the darkest area (<highlight><bold>1106</bold></highlight> and <highlight><bold>1112</bold></highlight>) of each target dome (<highlight><bold>1103</bold></highlight> and <highlight><bold>1109</bold></highlight>). In this example, successive horizontal scan samplings of the area of each target dome (<highlight><bold>1103</bold></highlight> and <highlight><bold>1109</bold></highlight>) are digitally recorded and compared to a known table of values to determine the overall degree of diffusion and ambiance. The overall degree of diffusion and ambiance is stated as a comparative percentage of influence in relation to the strongest light source. The highest degree of diffusion may correspond to a value of 100% whereas the light encompasses 180 degrees such as with overcast sky, while pure direct light such as the unfiltered light of the sun may correspond to a value of 0%. For example, the highest degree of ambient omnipresent light may be recorded as a value of 100% and a single light source in an all black room may effectively produce a value of 0% ambiance. </paragraph>
<paragraph id="P-0039" lvl="0"><number>&lsqb;0039&rsqb;</number> Alternatively, to using a spherical dome method for determining illumination direction and intensity may include various symmetrically faceted geometric shapes. Such shapes for instance may be analyzed with particle analysis software to determine and compare shade values of specific facets. In this example, facets presented with various angles in relation to a light source produce different reflective and shade characteristics when viewed from a camera position. </paragraph>
<paragraph id="P-0040" lvl="0"><number>&lsqb;0040&rsqb;</number> With the foregoing and with other parameters considered, another embodiment of the current invention is directed to geometrically influencing automatic virtual reality shadow casting routines such as those used by digital image rendering software programs. In this embodiment, the user photographs a subject with the target device, then analyzes and records such data to automatically and synthetically cast realistic shadows having lighting attributes substantially similar to the attributes of the subject image. </paragraph>
<paragraph id="P-0041" lvl="0"><number>&lsqb;0041&rsqb;</number> The diagram of <cross-reference target="DRAWINGS">FIG. 12</cross-reference> illustrates how aspects of the invention may be used to measure and record relative dimensional scale of a photograph (<highlight><bold>1201</bold></highlight>) in relation to specific known target devices. In one embodiment, the width, A (<highlight><bold>1206</bold></highlight>), B (<highlight><bold>1207</bold></highlight>), or C (<highlight><bold>1208</bold></highlight>), of each target device (<highlight><bold>1203</bold></highlight>-<highlight><bold>1205</bold></highlight>) placed within the photograph (<highlight><bold>1201</bold></highlight>) is divided by the overall width, D (<highlight><bold>1209</bold></highlight>), of the photograph (<highlight><bold>1201</bold></highlight>) to calculate a specific scale value. In the example illustrated, each proportionately identical target device (<highlight><bold>1203</bold></highlight>-<highlight><bold>1205</bold></highlight>) has a unique size affiliated with a specific color ring to determine a uniform scale measurement factor. If the medium (e.g., green) target device (<highlight><bold>1203</bold></highlight>) at left has a scale factor of &lsquo;1&rsquo;, then the small ( e.g., red) target device (<highlight><bold>1204</bold></highlight>) at center has a scale correction factor of &lsquo;2&rsquo; and the large (e.g., orange) target device (<highlight><bold>1205</bold></highlight>) on the right has a scale correction factor of &lsquo;0.5&rsquo;. For example, if the software algorithm finds a green target, then the associative scale of the image is known. The various sized target devices (<highlight><bold>1203</bold></highlight>-<highlight><bold>1205</bold></highlight>) allow the photograph (<highlight><bold>1201</bold></highlight>) to be captured with proportionately sized targets relative to the true scale of the image without excessive impairment to the image quality. In this example, the medium target device (<highlight><bold>1203</bold></highlight>) with a scale factor of &lsquo;1&rsquo; is used for capturing a human figure (<highlight><bold>1202</bold></highlight>). The small target device (<highlight><bold>1204</bold></highlight>) is used in conjunction with small objects such as jewelry or dishware, and the large target device (<highlight><bold>1205</bold></highlight>) is used in conjunction with large objects such as tents or automobiles. </paragraph>
<paragraph id="P-0042" lvl="0"><number>&lsqb;0042&rsqb;</number> One of the aspects of the invention is to utilize such target device designs that are proportionality scaled and uniquely identified from any viewing angle by color regions or geometric patterns and graphic markings. </paragraph>
<paragraph id="P-0043" lvl="0"><number>&lsqb;0043&rsqb;</number> Another factor in the creation of realistic montages is the uniformity of light temperature illumination between the collective component images of a composite. The diagram of <cross-reference target="DRAWINGS">FIG. 13</cross-reference> illustrates measuring apparent light color temperature from a signature of a target device (<highlight><bold>1301</bold></highlight>). In one embodiment, a known white point is sampled such that digital image analysis software consistently meters and records specific areas of the target device (<highlight><bold>1301</bold></highlight>). A color field (<highlight><bold>1302</bold></highlight>), such as a flat neutral white material, is affixed to the target device (<highlight><bold>1301</bold></highlight>). One of two sampling points (<highlight><bold>1303</bold></highlight> or <highlight><bold>1304</bold></highlight>) of the target device (<highlight><bold>1301</bold></highlight>) is unaffected by the shadow cast from the target dome, and is selected to meter the color hue value of the color field (<highlight><bold>1302</bold></highlight>) in comparison to a neutral color standard. In one example, the two sampling points (<highlight><bold>1303</bold></highlight> and <highlight><bold>1304</bold></highlight>) are located by horizontally measuring a distance A (<highlight><bold>1305</bold></highlight>) or B (<highlight><bold>1306</bold></highlight>) from the left edge of the target device (<highlight><bold>1301</bold></highlight>). The left sampling point (<highlight><bold>1303</bold></highlight>) is located a distance of A (<highlight><bold>1305</bold></highlight>) from the left edge of the target device (<highlight><bold>1301</bold></highlight>), and the right sampling point (<highlight><bold>1304</bold></highlight>) is a distance of B (<highlight><bold>1306</bold></highlight>). The two sampling points (<highlight><bold>1303</bold></highlight> and <highlight><bold>1304</bold></highlight>) are located along a horizontal line that divides the signature of the target device (<highlight><bold>1301</bold></highlight>). </paragraph>
<paragraph id="P-0044" lvl="0"><number>&lsqb;0044&rsqb;</number> In addition, a uniform perspective measurement, known as a vanishing point perspective factor, may be influential in creating realistic composites. The diagram of <cross-reference target="DRAWINGS">FIG. 14</cross-reference> demonstrates an exemplary method by which a vanishing point perspective factor is ascertained from a signature of a target device (<highlight><bold>1401</bold></highlight>). A vertical scan line (<highlight><bold>1402</bold></highlight>) bisects the target device (<highlight><bold>1401</bold></highlight>) across an area of unobstructed vision on a target identifier ring (<highlight><bold>1407</bold></highlight>). Four measurement points (<highlight><bold>1404</bold></highlight>-<highlight><bold>1406</bold></highlight>) are located on the vertical scan line (<highlight><bold>1402</bold></highlight>) at the transitional color boundaries of the target identifier ring (<highlight><bold>1407</bold></highlight>). In one embodiment, the position of the vertical scan line (<highlight><bold>1402</bold></highlight>) is calculated according to a proportionate offset distance A (<highlight><bold>1408</bold></highlight>) as a function of the width X (<highlight><bold>1412</bold></highlight>) of the target device (<highlight><bold>1401</bold></highlight>) so that the dome shape (<highlight><bold>1409</bold></highlight>) of the target device (<highlight><bold>1401</bold></highlight>) is avoided. A ratio calculated by comparing the distance B (<highlight><bold>1410</bold></highlight>) between the back two measurement points (<highlight><bold>1403</bold></highlight> and <highlight><bold>1404</bold></highlight>) and the distance C (<highlight><bold>1411</bold></highlight>) between the front two measurement points (<highlight><bold>1405</bold></highlight> and <highlight><bold>1406</bold></highlight>) results in establishing a vanishing point perspective factor. In one aspect of the present invention, the vanishing point perspective factor is compared with a table of known values to provide a user with an indication of camera lens length. As an example, a user may convey intended camera lens lengths between a subject and background image photo-shoot. The illustration of <cross-reference target="DRAWINGS">FIG. 15</cross-reference> demonstrates the effect of optical perspective between two similar camera views. The left view (<highlight><bold>1501</bold></highlight>) is that of a short camera lens which produces a wide angle effect, while the right view (<highlight><bold>1502</bold></highlight>) depicts that of a long camera lens that produces a substantially isometric effect. </paragraph>
<paragraph id="P-0045" lvl="0"><number>&lsqb;0045&rsqb;</number> In one embodiment, the target device is a rigid flat disc or disc portion having a three-dimensional shaped central portion. The diagrams of <cross-reference target="DRAWINGS">FIG. 16</cross-reference> illustrate an exemplary target device having a domed spherical center section and a circular disc portion. Other shapes may be utilized for the three-dimensional shaped central portion, such as an inverted cone with a domed surface, a vertical shaft topped with a sphere, or other symmetrical combinations of three-dimensional shapes. The symmetry of the three-dimensional central portion allows the target device to be photographed from any position. Other shapes may also be used for the disc portion, such as a rectangular disc, elliptical disc, rings or other shapes that are symmetrical to allow the target device to be photographed from any position. In one embodiment, the target device (<highlight><bold>1600</bold></highlight>) is proportionately dimensioned and produced in various sizes. The diagram (<highlight><bold>1610</bold></highlight>) of <cross-reference target="DRAWINGS">FIG. 16</cross-reference> illustrates exemplary dimensions for the target device (<highlight><bold>1600</bold></highlight>). In another embodiment, the target device (<highlight><bold>1600</bold></highlight>) is produced in relational unit measurements according to a determined standard such as one unit of measure equaling ten centimeters. In yet another embodiment, the target device (<highlight><bold>1600</bold></highlight>) is produced in various sizes relative to specific color codes that are identified by digital image analysis software. The color codes invoke scale correction variables from the digital image analysis software and are applied in calculating the scale factor of an indexed photograph. Concentric color rings are used to increase the digital visibility of the target device. In one example, a color ring (<highlight><bold>1601</bold></highlight>) of either fluorescent green, red, orange, yellow, pink or blue material may be applied to the target device surface. Any color pattern which contrasts with a surrounding environment and is identifiable by such digital imaging software may be used in the target device (<highlight><bold>1600</bold></highlight>). In one embodiment, an outer border (<highlight><bold>1602</bold></highlight>) of dark or light colored material that contrasts with the target color ring is included on the target device (<highlight><bold>1600</bold></highlight>) for visual separation between the color ring (<highlight><bold>1601</bold></highlight>) and the environmental surroundings. A center ring (<highlight><bold>1603</bold></highlight>) of a neutral light color density such as a flat white material included on the target device (<highlight><bold>1600</bold></highlight>) is used to measure light color and perceived temperature. The target device (<highlight><bold>1600</bold></highlight>) includes a protruding spherical feature or dome shape (<highlight><bold>1604</bold></highlight>) at its center for measuring such attributes as light direction, light diffusion, and light ambiance. The target device (<highlight><bold>1600</bold></highlight>) also comprises a surface, such as the surface of the domed shape (<highlight><bold>1604</bold></highlight>), which reflects spectral light rays. In one embodiment, the target device (<highlight><bold>1600</bold></highlight>) is symmetric from all horizontal viewing positions of equal height so that a specific orientation is not required. In another embodiment, the target device (<highlight><bold>1600</bold></highlight>) includes a recess or a protrusion (not shown) at a center position on the bottom side of the target device (<highlight><bold>1600</bold></highlight>) to allow the target device to be securely mounted atop a common stand or other device (not shown) to elevate the target device above ground. In addition, the target device (<highlight><bold>1600</bold></highlight>) may include other means to allow the target device to be elevated to a horizontal position above ground. </paragraph>
<paragraph id="P-0046" lvl="0"><number>&lsqb;0046&rsqb;</number> The various foregoing aspects and traits of the present invention also apply to sequential frame photography mediums such as motion pictures, video, multimedia and other forms of animated photographic imagery. The illustrations of <cross-reference target="DRAWINGS">FIG. 17</cross-reference> demonstrate an exemplary use of the current invention in a coordinated and sequentially key-framed transitional movement of a motion-image-capture device such as a motion picture camera or video camera. In one embodiment, a human figure or other subject (<highlight><bold>1702</bold></highlight>) is filmed in front of a seamless blue-screen backdrop (<highlight><bold>1701</bold></highlight>) with the target device (<highlight><bold>1703</bold></highlight>) placed within the visible image area. In this example, the subject (<highlight><bold>1702</bold></highlight>) is illuminated from a light source (<highlight><bold>1707</bold></highlight>). The subject camera (<highlight><bold>1704</bold></highlight>) pans from a left position along a trajectory (<highlight><bold>1705</bold></highlight>) to an end position. A concurrent background image (<highlight><bold>1708</bold></highlight>) is either filmed or animated in the same manner as the foregoing subject example. The background camera (<highlight><bold>1709</bold></highlight>) may move from left to right along an equal trajectory (<highlight><bold>1710</bold></highlight>) as the subject camera (<highlight><bold>1704</bold></highlight>). In this example, the target device (<highlight><bold>1703</bold></highlight>) directs the simulated position of the computer cameras and the attributes of a light source (<highlight><bold>1712</bold></highlight>) in the background image (<highlight><bold>1708</bold></highlight>) so that combining live footage with computer animation may produce a realistic motion picture composite (<highlight><bold>1713</bold></highlight>). </paragraph>
<paragraph id="P-0047" lvl="0"><number>&lsqb;0047&rsqb;</number> While an exemplary embodiment of the invention has been described and further illustrated, it will be appreciated that various changes may be made therein without departing from the intended scope of the invention. </paragraph>
</section>
</detailed-description>
</subdoc-description>
<subdoc-claims>
<heading lvl="1">I claim: </heading>
<claim id="CLM-00001">
<claim-text><highlight><bold>1</bold></highlight>. An apparatus for determining the attributes of a digital image, comprising: 
<claim-text>a target device for inclusion in a digital image, the target device being operative to enable the discovery of attributes of the digital image, the target device including: 
<claim-text>a disc portion, the disc portion including concentric, contrasting rings; and </claim-text>
<claim-text>a three-dimensional shaped portion centrally located on the disc portion. </claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002">
<claim-text><highlight><bold>2</bold></highlight>. The apparatus of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein the concentric, contrasting rings are color-coded to increase visibility of the apparatus within the digital image. </claim-text>
</claim>
<claim id="CLM-00003">
<claim-text><highlight><bold>3</bold></highlight>. The apparatus of <dependent-claim-reference depends_on="CLM-00002">claim 2</dependent-claim-reference>, wherein the concentric rings are color patterns which contrast with a surrounding environment and are identifiable by digital imaging software. </claim-text>
</claim>
<claim id="CLM-00004">
<claim-text><highlight><bold>4</bold></highlight>. The apparatus of <dependent-claim-reference depends_on="CLM-00002">claim 2</dependent-claim-reference>, wherein the disc portion further includes an outer border of colored material that contrasts with the concentric, contrasting rings such that the outer border provides a visual separation between the concentric, contrasting rings and environmental surroundings included in the digital image. </claim-text>
</claim>
<claim id="CLM-00005">
<claim-text><highlight><bold>5</bold></highlight>. The apparatus of <dependent-claim-reference depends_on="CLM-00004">claim 4</dependent-claim-reference>, wherein the environmental surroundings comprise at least one of a background and a subject that are captured in the image. </claim-text>
</claim>
<claim id="CLM-00006">
<claim-text><highlight><bold>6</bold></highlight>. The apparatus of <dependent-claim-reference depends_on="CLM-00002">claim 2</dependent-claim-reference>, wherein the concentric, contrasting rings include a ring of a neutral light color density that enables a measurement of light color and perceived temperature associated with the digital image. </claim-text>
</claim>
<claim id="CLM-00007">
<claim-text><highlight><bold>7</bold></highlight>. The apparatus of <dependent-claim-reference depends_on="CLM-00002">claim 2</dependent-claim-reference>, wherein the three-dimensional shaped portion enables a measurement of at least one of light direction, light diffusion, and light ambiance associated with the digital image. </claim-text>
</claim>
<claim id="CLM-00008">
<claim-text><highlight><bold>8</bold></highlight>. The apparatus of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, further comprising graphical patterns on the surface of at least one of the disc portion and the three-dimensional shaped portion, such that the relative size of the apparatus is determined by examining the graphical patterns. </claim-text>
</claim>
<claim id="CLM-00009">
<claim-text><highlight><bold>9</bold></highlight>. The apparatus of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein the three-dimensional shaped portion includes at least one of a substantially spherical, elliptical, and faceted surface. </claim-text>
</claim>
<claim id="CLM-00010">
<claim-text><highlight><bold>10</bold></highlight>. The apparatus of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein the disc is one of substantially circular, elliptical, square, and rectangular in shape. </claim-text>
</claim>
<claim id="CLM-00011">
<claim-text><highlight><bold>11</bold></highlight>. The apparatus of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, further comprising a surface arranged to reflect spectral light rays. </claim-text>
</claim>
<claim id="CLM-00012">
<claim-text><highlight><bold>12</bold></highlight>. The apparatus of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein the disc portion is symmetric from horizontal views of equal height, such that further orientation of the apparatus is avoided when the apparatus is placed on a level surface and digital images are captured from different points of reference. </claim-text>
</claim>
<claim id="CLM-00013">
<claim-text><highlight><bold>13</bold></highlight>. The apparatus of <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, further comprising an elevating device to raise the apparatus a predetermined height. </claim-text>
</claim>
<claim id="CLM-00014">
<claim-text><highlight><bold>14</bold></highlight>. A method for determining attributes of a digital image, comprising: 
<claim-text>locating a target device within the digital image; </claim-text>
<claim-text>measuring attributes associated with the target device; and </claim-text>
<claim-text>automatically associating the attributes of the target device with the digital image such that the digital image assigned with the attributes. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00015">
<claim-text><highlight><bold>15</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00011">claim 14</dependent-claim-reference>, wherein associating the attributes further comprises storing the attributes in a database in association with the digital image. </claim-text>
</claim>
<claim id="CLM-00016">
<claim-text><highlight><bold>16</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00011">claim 14</dependent-claim-reference>, wherein associating the attributes further comprises embedding the attributes into the digital image. </claim-text>
</claim>
<claim id="CLM-00017">
<claim-text><highlight><bold>17</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00011">claim 14</dependent-claim-reference>, wherein locating the target device further comprises scanning the digital image for a high color luminescence value emanating from the target device such that a color luminance curve may be generated, and examining the color luminance curve to determine the distance of the target device from an edge of the digital image. </claim-text>
</claim>
<claim id="CLM-00018">
<claim-text><highlight><bold>18</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00011">claim 14</dependent-claim-reference>, wherein locating the target device further comprises initiating at least one of particle analysis and color space analysis to determine the position of the target device. </claim-text>
</claim>
<claim id="CLM-00019">
<claim-text><highlight><bold>19</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00011">claim 14</dependent-claim-reference>, wherein locating the target device further comprises scanning the digital image for a geometric shape corresponding to the target device. </claim-text>
</claim>
<claim id="CLM-00020">
<claim-text><highlight><bold>20</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00011">claim 14</dependent-claim-reference>, wherein measuring attributes associated with the target device further comprises measuring ground surface angle value associated with the target device. </claim-text>
</claim>
<claim id="CLM-00021">
<claim-text><highlight><bold>21</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00011">claim 14</dependent-claim-reference>, wherein measuring attributes associated with the target device further comprises measuring the reflective illumination properties associated with the target device. </claim-text>
</claim>
<claim id="CLM-00022">
<claim-text><highlight><bold>22</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00022">claim 21</dependent-claim-reference>, wherein the attributes associated with the target device comprise light direction. </claim-text>
</claim>
<claim id="CLM-00023">
<claim-text><highlight><bold>23</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00022">claim 21</dependent-claim-reference>, wherein the attributes associated with the target device comprise a measure of light diffusion. </claim-text>
</claim>
<claim id="CLM-00024">
<claim-text><highlight><bold>24</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00022">claim 21</dependent-claim-reference>, wherein the attributes associated with the target device comprise a measure of ambient light. </claim-text>
</claim>
<claim id="CLM-00025">
<claim-text><highlight><bold>25</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00011">claim 14</dependent-claim-reference>, wherein measuring attributes associated with the target device further comprises calculating a relative dimensional scale of the digital image based on a predetermined scale of the target device. </claim-text>
</claim>
<claim id="CLM-00026">
<claim-text><highlight><bold>26</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00011">claim 14</dependent-claim-reference>, wherein measuring attributes associated with the target device further comprises measuring apparent light color temperature by measuring a color hue value of the target device and comparing the color hue value to a neutral color standard. </claim-text>
</claim>
<claim id="CLM-00027">
<claim-text><highlight><bold>27</bold></highlight>. The method of <dependent-claim-reference depends_on="CLM-00011">claim 14</dependent-claim-reference>, wherein measuring attributes associated with the target device further comprises measuring a vanishing point perspective factor that corresponds to a ratio that results from a comparison of distance between two separate sets of points, wherein the first set of points is located at a front side of the target device and the second set of points is located at a back side of the target device. </claim-text>
</claim>
<claim id="CLM-00028">
<claim-text><highlight><bold>28</bold></highlight>. A computer-readable medium having stored thereon a data structure, comprising: 
<claim-text>data stored in an image region that include digital information defining an image including a target device, wherein the target device includes a disc portion and a three-dimensional shaped portion; and </claim-text>
<claim-text>an index region that includes attribute information describing attribute of the target device in the image. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00029">
<claim-text><highlight><bold>29</bold></highlight>. An apparatus for indexing a digital image with visual attributes, comprising: 
<claim-text>a means for locating a target device within a digital image, wherein the target device includes a disc portion and a three-dimensional shaped portion; </claim-text>
<claim-text>a means for measuring visual attributes associated with the target device by examining at least one of the disc portion and the three-dimensional shaped portion; and </claim-text>
<claim-text>a means for associating the visual attributes of the target device with the digital image, such that the digital image is indexed with the visual attributes.</claim-text>
</claim-text>
</claim>
</subdoc-claims>
<subdoc-drawings id="DRAWINGS">
<heading lvl="0" align="CENTER">Drawings</heading>
<representative-figure>1</representative-figure>
<figure id="figure-D00000">
<image id="EMI-D00000" file="US20030002730A1-20030102-D00000.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00001">
<image id="EMI-D00001" file="US20030002730A1-20030102-D00001.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00002">
<image id="EMI-D00002" file="US20030002730A1-20030102-D00002.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00003">
<image id="EMI-D00003" file="US20030002730A1-20030102-D00003.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00004">
<image id="EMI-D00004" file="US20030002730A1-20030102-D00004.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00005">
<image id="EMI-D00005" file="US20030002730A1-20030102-D00005.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00006">
<image id="EMI-D00006" file="US20030002730A1-20030102-D00006.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00007">
<image id="EMI-D00007" file="US20030002730A1-20030102-D00007.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00008">
<image id="EMI-D00008" file="US20030002730A1-20030102-D00008.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00009">
<image id="EMI-D00009" file="US20030002730A1-20030102-D00009.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00010">
<image id="EMI-D00010" file="US20030002730A1-20030102-D00010.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00011">
<image id="EMI-D00011" file="US20030002730A1-20030102-D00011.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00012">
<image id="EMI-D00012" file="US20030002730A1-20030102-D00012.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00013">
<image id="EMI-D00013" file="US20030002730A1-20030102-D00013.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00014">
<image id="EMI-D00014" file="US20030002730A1-20030102-D00014.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00015">
<image id="EMI-D00015" file="US20030002730A1-20030102-D00015.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00016">
<image id="EMI-D00016" file="US20030002730A1-20030102-D00016.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00017">
<image id="EMI-D00017" file="US20030002730A1-20030102-D00017.TIF" imf="TIFF" ti="DR"/>
</figure>
</subdoc-drawings>
</patent-application-publication>
