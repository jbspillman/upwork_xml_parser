<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE patent-application-publication SYSTEM "pap-v16-2002-01-01.dtd" [
<!ENTITY US20030005355A1-20030102-D00000.TIF SYSTEM "US20030005355A1-20030102-D00000.TIF" NDATA TIF>
<!ENTITY US20030005355A1-20030102-D00001.TIF SYSTEM "US20030005355A1-20030102-D00001.TIF" NDATA TIF>
<!ENTITY US20030005355A1-20030102-D00002.TIF SYSTEM "US20030005355A1-20030102-D00002.TIF" NDATA TIF>
]>
<patent-application-publication>
<subdoc-bibliographic-information>
<document-id>
<doc-number>20030005355</doc-number>
<kind-code>A1</kind-code>
<document-date>20030102</document-date>
</document-id>
<publication-filing-type>new</publication-filing-type>
<domestic-filing-data>
<application-number>
<doc-number>10224138</doc-number>
</application-number>
<application-number-series-code>10</application-number-series-code>
<filing-date>20020820</filing-date>
</domestic-filing-data>
<technical-information>
<classification-ipc>
<classification-ipc-primary>
<ipc>H04L001/22</ipc>
</classification-ipc-primary>
<classification-ipc-secondary>
<ipc>G06F012/16</ipc>
</classification-ipc-secondary>
<classification-ipc-edition>07</classification-ipc-edition>
</classification-ipc>
<classification-us>
<classification-us-primary>
<uspc>
<class>714</class>
<subclass>007000</subclass>
</uspc>
</classification-us-primary>
<classification-us-secondary>
<uspc>
<class>711</class>
<subclass>162000</subclass>
</uspc>
</classification-us-secondary>
</classification-us>
<title-of-invention>Remote data mirroring system using local and remote write pending indicators</title-of-invention>
</technical-information>
<continuity-data>
<division-of>
<parent-child>
<child>
<document-id>
<doc-number>10224138</doc-number>
<kind-code>A1</kind-code>
<document-date>20020820</document-date>
</document-id>
</child>
<parent>
<document-id>
<doc-number>09709814</doc-number>
<document-date>20001110</document-date>
<country-code>US</country-code>
</document-id>
</parent>
<parent-status>PENDING</parent-status>
</parent-child>
</division-of>
<division-of>
<parent-child>
<child>
<document-id>
<doc-number>09709814</doc-number>
<document-date>20001110</document-date>
<country-code>US</country-code>
</document-id>
</child>
<parent>
<document-id>
<doc-number>09061708</doc-number>
<document-date>19980417</document-date>
<country-code>US</country-code>
</document-id>
</parent>
<parent-status>GRANTED</parent-status>
<parent-patent>
<document-id>
<doc-number>6173377</doc-number>
<country-code>US</country-code>
</document-id>
</parent-patent>
</parent-child>
</division-of>
<division-of>
<parent-child>
<child>
<document-id>
<doc-number>09709814</doc-number>
<document-date>20001110</document-date>
<country-code>US</country-code>
</document-id>
</child>
<parent>
<document-id>
<doc-number>08654511</doc-number>
<document-date>19960528</document-date>
<country-code>US</country-code>
</document-id>
</parent>
<parent-status>GRANTED</parent-status>
<parent-patent>
<document-id>
<doc-number>5742792</doc-number>
<country-code>US</country-code>
</document-id>
</parent-patent>
</parent-child>
</division-of>
<division-of>
<parent-child>
<child>
<document-id>
<doc-number>09709814</doc-number>
<document-date>20001110</document-date>
<country-code>US</country-code>
</document-id>
</child>
<parent>
<document-id>
<doc-number>08052039</doc-number>
<document-date>19930423</document-date>
<country-code>US</country-code>
</document-id>
</parent>
<parent-status>GRANTED</parent-status>
<parent-patent>
<document-id>
<doc-number>5544347</doc-number>
<country-code>US</country-code>
</document-id>
</parent-patent>
</parent-child>
</division-of>
</continuity-data>
<inventors>
<first-named-inventor>
<name>
<given-name>Moshe</given-name>
<family-name>Yanai</family-name>
</name>
<residence>
<residence-us>
<city>Brookline</city>
<state>MA</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</first-named-inventor>
<inventor>
<name>
<given-name>Natan</given-name>
<family-name>Vishlitzky</family-name>
</name>
<residence>
<residence-us>
<city>Brookline</city>
<state>MA</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
<inventor>
<name>
<given-name>Bruno</given-name>
<family-name>Alterescu</family-name>
</name>
<residence>
<residence-us>
<city>Newton</city>
<state>MA</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
<inventor>
<name>
<given-name>Daniel</given-name>
<family-name>Castel</family-name>
</name>
<residence>
<residence-us>
<city>Boston</city>
<state>MA</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
<inventor>
<name>
<given-name>Gadi</given-name>
<middle-name>G.</middle-name>
<family-name>Shklarsky</family-name>
</name>
<residence>
<residence-us>
<city>Brookline</city>
<state>MA</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
</inventors>
<correspondence-address>
<name-1>RICHARD C. AUCHTERLOINE</name-1>
<name-2></name-2>
<address>
<address-1>750 BERING DR.</address-1>
<city>HOUSTON</city>
<state>TX</state>
<postalcode>77057-2198</postalcode>
<country>
<country-code>US</country-code>
</country>
</address>
</correspondence-address>
</subdoc-bibliographic-information>
<subdoc-abstract>
<paragraph id="A-0001" lvl="0">A primary data storage system is linked to a geographically remote secondary data storage system for automatically maintaining a remote copy of the data in the primary storage. The primary data storage system controller uses write pending indicators to control and coordinate the remote copy process. In response to receipt of data from the host computer, a first write pending indicator is set to write the data into at least one primary data storage device, and a second write pending indicator is set to copy the data to the secondary data storage system controller. The first write pending indicator is reset after the data is written to the primary data storage device, and the second write pending indicator is reset after receiving an acknowledgement back from the secondary data storage system controller. </paragraph>
</subdoc-abstract>
<subdoc-description>
<cross-reference-to-related-applications>
<heading lvl="1">RELATED APPLICATIONS </heading>
<paragraph id="P-0001" lvl="0"><number>&lsqb;0001&rsqb;</number> This application is a divisional of U.S. patent application Ser. No. 09/709,814 filed Nov. 10, 2000, which is a divisional of U.S. patent application Ser. No. 09/061,708 filed Apr. 17, 1998 (U.S. Pat. No. 6,173,377 issued Jan. 9, 2001), which is a continuation of U.S. patent application Ser. No. 08/654,511 filed May 28, 1996 (U.S. Pat. 5,742,792 issued Apr. 21, 1998), which is a continuation-in-part of U.S. patent application Ser. No. 08/052,039 filed Apr. 23, 1993, entitled REMOTE DATA MIRRORING (U.S. Pat. No. 5,544,347 issued Aug. 6, 1996), which are all incorporated herein by reference.</paragraph>
</cross-reference-to-related-applications>
<summary-of-invention>
<section>
<heading lvl="1">FIELD OF THE INVENTION </heading>
<paragraph id="P-0002" lvl="0"><number>&lsqb;0002&rsqb;</number> This invention relates to data storage, and more particularly, to a system and method for automatically providing and maintaining a copy or mirror of data stored at a location geographically remote from the main or primary data storage device. </paragraph>
</section>
<section>
<heading lvl="1">BACKGROUND OF THE INVENTION </heading>
<paragraph id="P-0003" lvl="0"><number>&lsqb;0003&rsqb;</number> Nearly all data processing system users are concerned with maintaining back-up data in order to insure continued data processing operations should their data become lost, damaged, or otherwise unavailable. </paragraph>
<paragraph id="P-0004" lvl="0"><number>&lsqb;0004&rsqb;</number> Large institutional users of data processing systems which maintain large volumes of data such as banks, insurance companies, and stock market traders must and do take tremendous steps to insure back-up data availability in case of a major disaster. These institutions recently have developed a heightened awareness of the importance of data recovery and back-up in view of the many natural disasters and other world events including the bombing of the World Trade Center in New York City. </paragraph>
<paragraph id="P-0005" lvl="0"><number>&lsqb;0005&rsqb;</number> Currently, data processing system users often maintain copies of their valuable data on site on either removable storage media, or in a secondary &ldquo;mirrored&rdquo; storage device located on or within the same physical confines of the main storage device. Should a disaster such as fire, flood, or inaccessibility to a building occur, however, both the primary as well as the secondary or backed-up data will be unavailable to the user. Accordingly, more data processing system users are requiring the remote storage of back-up data. </paragraph>
<paragraph id="P-0006" lvl="0"><number>&lsqb;0006&rsqb;</number> One prior art approach at data back-up involves taking the processor out of service while back-up tapes are made. These tapes are then carried off premises for storage purposes. Should access to the backed-up data be required, the proper tape must be located, loaded onto a tape drive, and restored to the host system requiring access to the data. This process is very time consuming and cost intensive, both in maintaining an accurate catalog of the data stored on each individual tape, as well as storing the large number of tapes required to store the large amounts of data required by these institutions. Additionally and most importantly, it often takes twenty-four hours before a back-up tape reaches its storage destination during which time the back-up data is unavailable to the user. </paragraph>
<paragraph id="P-0007" lvl="0"><number>&lsqb;0007&rsqb;</number> Additionally, today&apos;s systems require a significant amount of planning and testing in order to design a data recovery procedure and assign data recovery responsibilities. Typically, a disaster recovery team must travel to the test site carrying a large number of data tapes. The team then loads the data onto disks, makes the required network connections, and then restores the data to the &ldquo;test&rdquo; point of failure so processing can begin. Such testing may take days or even weeks and always involves significant human resources in a disaster recovery center or back-up site. </paragraph>
<paragraph id="P-0008" lvl="0"><number>&lsqb;0008&rsqb;</number> Some providers of prior art data storage systems have proposed a method of data mirroring whereby one host Central Processing Unit (CPU) or processor writes data to both a primary, as well as a secondary, data storage device or system. Such a proposed method, however, overly burdens the host CPU with the task of writing the data to a secondary storage system and thus dramatically impacts and reduces system performance. </paragraph>
<paragraph id="P-0009" lvl="0"><number>&lsqb;0009&rsqb;</number> Accordingly, what is required is a data processing system which automatically and asynchronously, with respect to a first host system, generates and maintains a backup or &ldquo;mirrored&rdquo; copy of a primary storage device at a location physically remote from the primary storage device, without intervention from the host which seriously degrades the performance of the data transfer link between the primary host computer and the primary storage device. </paragraph>
</section>
<section>
<heading lvl="1">SUMMARY OF THE INVENTION </heading>
<paragraph id="P-0010" lvl="0"><number>&lsqb;0010&rsqb;</number> This invention features a system which automatically, without intervention from a host computer system, controls storing of primary data received from a primary host computer on a primary data storage system, and additionally controls the copying of the primary data to a secondary data storage system controller which forms part of a secondary data storage system, for providing a back-up copy of the primary data on the secondary data storage system which is located in a geographically remote location from the primary data storage system. </paragraph>
<paragraph id="P-0011" lvl="0"><number>&lsqb;0011&rsqb;</number> Copying or mirroring of data from a primary data storage system to a secondary data storage system is accomplished without intervention of a primary or secondary host computer and thus, without affecting performance of a primary or secondary host computer system. Primary and secondary data storage system controllers are coupled via at least one high speed communication link such as a fiber optic link driven by LED&apos;s or laser. </paragraph>
<paragraph id="P-0012" lvl="0"><number>&lsqb;0012&rsqb;</number> At least one of the primary and secondary data storage system controllers coordinates the copying of primary data to the secondary data storage system and at least one of the primary and secondary data storage system controllers maintains at least a list of primary data which is to be copied to the secondary data storage device. </paragraph>
<paragraph id="P-0013" lvl="0"><number>&lsqb;0013&rsqb;</number> Additionally, the secondary data storage system controller provides an indication or acknowledgement to the primary data storage system controller that the primary data to be copied to the secondary data storage system in identical form as secondary data has been received or, in another embodiment, has actually been written to a secondary data storage device. </paragraph>
<paragraph id="P-0014" lvl="0"><number>&lsqb;0014&rsqb;</number> Accordingly, data may be transferred between the primary and secondary data storage system controllers synchronously, when a primary host computer requests writing of data to a primary data storage device, or asynchronously with the primary host computer requesting the writing of data to the primary data storage system, in which case the remote data copying or mirroring is completely independent of and transparent to the host computer system. </paragraph>
<paragraph id="P-0015" lvl="0"><number>&lsqb;0015&rsqb;</number> At least one of the primary data storage system controller and the secondary data storage system controller maintains a list of primary data which is to be written to the secondary data storage system. Once the primary data has been at least received or optionally stored on the secondary data storage system, the secondary data storage system controller provides an indication or acknowledgement of receipt or completed write operation to the primary data storage system. </paragraph>
<paragraph id="P-0016" lvl="0"><number>&lsqb;0016&rsqb;</number> At such time, the primary and/or secondary data storage system controller maintaining the list of primary data to be copied updates this list to reflect that the given primary data has been received by and/or copied to the secondary data storage system. The primary or secondary data storage system controllers and/or the primary and secondary data storage devices may also maintain additional lists for use in concluding which individual storage locations, such as tracks on a disk drive, are invalid on any given data storage device, which data storage locations are pending a format operation, which data storage device is ready to receive data, and whether or not any of the primary or secondary data storage devices are disabled for write operations. </paragraph>
<paragraph id="P-0017" lvl="0"><number>&lsqb;0017&rsqb;</number> Thus, an autonomous, host computer independent, geographically remote data storage system is maintained providing a system which achieves nearly 100 percent data integrity by assuring that all data is copied to a geographically remote site, and in those cases when a back-up copy is not made due to an error of any sort, an indication is stored that the data has not been copied, but instead must be updated at a future time. </paragraph>
<paragraph id="P-0018" lvl="0"><number>&lsqb;0018&rsqb;</number> Such a system is provided which is generally lower in cost and requires substantially less manpower and facilities to achieve than the prior art devices. </paragraph>
<paragraph id="P-0019" lvl="0"><number>&lsqb;0019&rsqb;</number> The present invention more particularly concerns the use of write pending indicators by the primary data storage system controller. In response to receipt of data from the host computer, a first write pending indicator is set to write the data into at least one primary data storage device, and a second write pending indicator is set to copy the data to the secondary data storage system controller. The first write pending indicator is reset after the data is written to the primary data storage device, and the second write pending indicator is reset after receiving an acknowledgement back from the secondary data storage system controller.</paragraph>
</section>
</summary-of-invention>
<brief-description-of-drawings>
<section>
<heading lvl="1">BRIEF DESCRIPTION OF THE DRAWINGS </heading>
<paragraph id="P-0020" lvl="0"><number>&lsqb;0020&rsqb;</number> These and other features and advantages of the present invention will be better understood when read together with the following drawings wherein: </paragraph>
<paragraph id="P-0021" lvl="0"><number>&lsqb;0021&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 1</cross-reference> is a block diagram illustrating the system with remote data mirroring according to the present invention; </paragraph>
<paragraph id="P-0022" lvl="0"><number>&lsqb;0022&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 2</cross-reference> is a schematic representation of a portion of an index or list maintained by the system of the present invention to determine various features including which primary data has been copied to a secondary disk; and </paragraph>
<paragraph id="P-0023" lvl="0"><number>&lsqb;0023&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 3</cross-reference> is a schematic representation of an additional list or index maintained by the system of the present invention to keep track of additional items including an invalid data storage device track, device ready status and write disable device status. </paragraph>
</section>
</brief-description-of-drawings>
<detailed-description>
<section>
<heading lvl="1">DESCRIPTION OF ILLUSTRATIVE EMBODIMENTS </heading>
<paragraph id="P-0024" lvl="0"><number>&lsqb;0024&rsqb;</number> The present invention features a system which provides a geographically remote mirrored data storage system which contains generally identical information to that stored on a primary data storage system. Utilizing such a system, data recovery after a disaster is nearly instantaneous and requires little, if any, human intervention. Using the present system, the data is retrieved from a remote device through the host data processing system. </paragraph>
<paragraph id="P-0025" lvl="0"><number>&lsqb;0025&rsqb;</number> A system in accordance with the present invention is shown generally at <highlight><bold>10</bold></highlight>, <cross-reference target="DRAWINGS">FIG. 1</cross-reference>, and includes at site A, which is a first geographic location, a host computer system <highlight><bold>12</bold></highlight> as is well known to those skilled in the art. The host computer system <highlight><bold>12</bold></highlight> is coupled to a first and primary data storage system <highlight><bold>14</bold></highlight>. The host <highlight><bold>12</bold></highlight> writes data to and reads data from the primary data storage system <highlight><bold>14</bold></highlight>. </paragraph>
<paragraph id="P-0026" lvl="0"><number>&lsqb;0026&rsqb;</number> The primary data storage system <highlight><bold>14</bold></highlight> includes a primary data storage system controller <highlight><bold>16</bold></highlight> which receives data from the host <highlight><bold>12</bold></highlight> over data signal path <highlight><bold>18</bold></highlight>. The primary data storage system controller <highlight><bold>16</bold></highlight> is also coupled to a storage device <highlight><bold>20</bold></highlight> which may include a plurality of data storage devices <highlight><bold>22</bold></highlight><highlight><italic>a</italic></highlight>-<highlight><bold>22</bold></highlight><highlight><italic>c</italic></highlight>. The storage devices may include disk drives, optical disks, CD&apos;s or other data storage devices. The primary system controller <highlight><bold>16</bold></highlight> is coupled to the storage device <highlight><bold>20</bold></highlight> by means of data signal path <highlight><bold>24</bold></highlight>. </paragraph>
<paragraph id="P-0027" lvl="0"><number>&lsqb;0027&rsqb;</number> The primary data storage system controller <highlight><bold>16</bold></highlight> includes at least one channel adapter (C.A.) <highlight><bold>26</bold></highlight> which is well known to those skilled in the art and interfaces with host processing system <highlight><bold>12</bold></highlight>. Data received from the host is typically stored in cache <highlight><bold>28</bold></highlight> before being transferred through disk adapter (D.A.) <highlight><bold>30</bold></highlight> over data signal path <highlight><bold>24</bold></highlight> to the primary storage device <highlight><bold>20</bold></highlight>. The primary data storage controller <highlight><bold>16</bold></highlight> also includes a data director <highlight><bold>32</bold></highlight> which executes one or more sets of predetermined micro-code to control data transfer between the host <highlight><bold>12</bold></highlight>, cache memory <highlight><bold>28</bold></highlight>, and the storage device <highlight><bold>20</bold></highlight>. Although the data director <highlight><bold>32</bold></highlight> is shown as a separate unit, either one of a channel adapter <highlight><bold>26</bold></highlight> or disk adapter <highlight><bold>30</bold></highlight> may be operative as a data director, to control the operation of a given data storage system controller. Such a reconfigurable channel adapter and disk adapter is disclosed in Applicant&apos;s U.S. Pat. No. 5,335,352 entitled RECONFIGURABLE, MULTI-FUNCTION DATA STORAGE SYSTEM CONTROLLER SELECTIVELY OPERABLE AS AN INPUT CHANNEL ADAPTER AND A DATA STORAGE UNIT ADAPTER, which is fully incorporated herein by reference. </paragraph>
<paragraph id="P-0028" lvl="0"><number>&lsqb;0028&rsqb;</number> The primary data storage system <highlight><bold>14</bold></highlight> according to one embodiment of the present invention also includes a service processor <highlight><bold>34</bold></highlight> coupled to the primary data storage system controller <highlight><bold>16</bold></highlight>, and which provides additional features such as monitoring, repair, service, or status access to the storage system controller <highlight><bold>16</bold></highlight>. </paragraph>
<paragraph id="P-0029" lvl="0"><number>&lsqb;0029&rsqb;</number> The primary data storage system controller <highlight><bold>16</bold></highlight> of the present invention also features at least a second disk adapter <highlight><bold>36</bold></highlight> coupled to the internal bus <highlight><bold>38</bold></highlight> of the primary data storage system controller <highlight><bold>16</bold></highlight>. The second disk adapter <highlight><bold>36</bold></highlight> is coupled, via a high speed communication link <highlight><bold>40</bold></highlight> to a disk adapter <highlight><bold>42</bold></highlight> on a secondary data storage system controller <highlight><bold>44</bold></highlight> of a secondary data storage system <highlight><bold>46</bold></highlight>. Such high speed, point-to-point communication links between the primary and secondary data storage system controllers <highlight><bold>16</bold></highlight> and <highlight><bold>44</bold></highlight> include a fiber optic link driven by an LED driver, per IBM ESCON standard; a fiber optic link driven by a laser driver, and optionally T1 and T3 telecommunication links. Utilizing network connections, the primary and secondary data storage system controllers <highlight><bold>16</bold></highlight> and <highlight><bold>44</bold></highlight> may be connected to FDDI networks, T1 or T3 based networks and SONET networks. </paragraph>
<paragraph id="P-0030" lvl="0"><number>&lsqb;0030&rsqb;</number> The secondary data storage system <highlight><bold>46</bold></highlight> is located at a second site geographically removed from the first site. For this patent application, &ldquo;geographically removed site&rdquo; means not within the same building as the primary data storage system. There are presently known data processing systems which provide data mirroring to physically different data storage systems. The systems, however, are generally within the same building. The present invention is directed to providing complete data recovery in case of disaster, such as when a natural disaster such as a flood or a hurricane, or man-made disasters such as fires or bombings destroy one physical location, such as one building. </paragraph>
<paragraph id="P-0031" lvl="0"><number>&lsqb;0031&rsqb;</number> As in the case of the primary data storage system, the secondary data storage system <highlight><bold>46</bold></highlight> includes, in addition to the secondary data storage system controller <highlight><bold>44</bold></highlight>, a secondary data storage device <highlight><bold>48</bold></highlight> including a plurality of storage devices <highlight><bold>50</bold></highlight><highlight><italic>a</italic></highlight>-<highlight><bold>50</bold></highlight><highlight><italic>c</italic></highlight>. The plurality of storage devices on the secondary data storage system <highlight><bold>46</bold></highlight>, as well as the primary data storage system <highlight><bold>14</bold></highlight>, may have various volumes and usages such as a primary data storage device <highlight><bold>50</bold></highlight><highlight><italic>a </italic></highlight>which is primary with respect to the attached storage controller <highlight><bold>44</bold></highlight> and host <highlight><bold>52</bold></highlight> in the case of the secondary data storage system <highlight><bold>46</bold></highlight>, and the primary storage device <highlight><bold>22</bold></highlight><highlight><italic>a </italic></highlight>which is primary with respect to the first or primary host <highlight><bold>12</bold></highlight> in the case of the primary data storage system <highlight><bold>14</bold></highlight>. </paragraph>
<paragraph id="P-0032" lvl="0"><number>&lsqb;0032&rsqb;</number> Additionally, each storage device, such as storage device <highlight><bold>48</bold></highlight>, may include a secondary storage volume <highlight><bold>50</bold></highlight><highlight><italic>b </italic></highlight>which serves as the secondary storage for the primary data stored on the primary volume <highlight><bold>22</bold></highlight><highlight><italic>a </italic></highlight>of the primary data storage system <highlight><bold>14</bold></highlight>. Similarly, the primary data storage system <highlight><bold>14</bold></highlight> may include a secondary storage volume <highlight><bold>22</bold></highlight><highlight><italic>b </italic></highlight>which stores primary data received and copied from the secondary site and data storage system <highlight><bold>46</bold></highlight> and host <highlight><bold>52</bold></highlight>. </paragraph>
<paragraph id="P-0033" lvl="0"><number>&lsqb;0033&rsqb;</number> Additionally, each storage device <highlight><bold>20</bold></highlight>, <highlight><bold>48</bold></highlight>, may include one or more local volumes or storage devices <highlight><bold>22</bold></highlight><highlight><italic>c</italic></highlight>, <highlight><bold>50</bold></highlight><highlight><italic>c</italic></highlight>, which are accessed only by their locally connected data processing systems. </paragraph>
<paragraph id="P-0034" lvl="0"><number>&lsqb;0034&rsqb;</number> The secondary storage system controller <highlight><bold>44</bold></highlight> also includes at least a first channel adapter <highlight><bold>54</bold></highlight> which may receive data from an optionally connected secondary host <highlight><bold>52</bold></highlight> or an optionally connected hotsite host or CPU <highlight><bold>56</bold></highlight>. Optionally, the primary host <highlight><bold>12</bold></highlight> may include a data signal path <highlight><bold>58</bold></highlight> directly into the channel adapter <highlight><bold>54</bold></highlight> of the secondary data storage system <highlight><bold>46</bold></highlight>, while the optional secondary host <highlight><bold>52</bold></highlight> may include an optional data path <highlight><bold>60</bold></highlight> into the channel adapter <highlight><bold>26</bold></highlight> of the primary data storage system <highlight><bold>14</bold></highlight>. Although the secondary host <highlight><bold>52</bold></highlight> illustrated in <cross-reference target="DRAWINGS">FIG. 1</cross-reference> is not required for remote data mirroring as described in the present patent application, such a host would be required for data retrieval if both the primary host <highlight><bold>12</bold></highlight> as well as the primary data storage system <highlight><bold>14</bold></highlight> would be rendered inoperative. Similarly, a hotsite host or CPU <highlight><bold>56</bold></highlight> may optionally be provided at a third geographically remote site to access the data stored in the secondary data storage system <highlight><bold>46</bold></highlight>. </paragraph>
<paragraph id="P-0035" lvl="0"><number>&lsqb;0035&rsqb;</number> The high speed link <highlight><bold>40</bold></highlight> between the primary and secondary data storage systems <highlight><bold>14</bold></highlight> and <highlight><bold>46</bold></highlight> is designed such that multiple links between the primary and secondary storage system may be maintained for enhanced availability of data and increased system performance. The number of links is variable and may be field upgradeable. Additionally, the service processor <highlight><bold>34</bold></highlight> of the primary data storage system <highlight><bold>14</bold></highlight> and the service processor <highlight><bold>62</bold></highlight> of the secondary data storage system <highlight><bold>46</bold></highlight> may also be coupled to provide for remote system configuration, remote software programming, and to provide a host base point of control of the secondary data storage system. </paragraph>
<paragraph id="P-0036" lvl="0"><number>&lsqb;0036&rsqb;</number> The secondary data storage system controller <highlight><bold>44</bold></highlight> also includes cache memory <highlight><bold>64</bold></highlight> which receives data from channel adapter <highlight><bold>54</bold></highlight> and disk adapter <highlight><bold>42</bold></highlight>, as well as disk adapter <highlight><bold>66</bold></highlight> which controls writing data to and from secondary storage device <highlight><bold>48</bold></highlight>. Also provided is a data director <highlight><bold>68</bold></highlight> which controls data transfer over communication bus <highlight><bold>70</bold></highlight> to which all the elements of the secondary data storage system controller are coupled. </paragraph>
<paragraph id="P-0037" lvl="0"><number>&lsqb;0037&rsqb;</number> An additional feature of the system <highlight><bold>10</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 1</cross-reference> is the ability to dynamically reconfigure channel adapters as disk adapters and disk adapters as channel adapters, as described in Applicant&apos;s U.S. Pat. No. 5,269,011 entitled DYNAMICALLY RECONFIGURABLE DATA STORAGE SYSTEM WITH STORAGE SYSTEM CONTROLLERS SELECTIVELY OPERABLE AS CHANNEL ADAPTERS OR STORAGE DEVICE ADAPTERS, which is fully incorporated herein by reference. </paragraph>
<paragraph id="P-0038" lvl="0"><number>&lsqb;0038&rsqb;</number> The primary and secondary data storage systems may optionally be connected by means of currently available, off-the-shelf channel extender equipment using bus and tag or ESCON interfaces. </paragraph>
<paragraph id="P-0039" lvl="0"><number>&lsqb;0039&rsqb;</number> The data storage system <highlight><bold>10</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 1</cross-reference> is designed to provide the copying of data from a primary data storage system to a physically remote secondary data storage system transparent to the user, and external from any influence of the primary host which is coupled to the primary data storage system. The system <highlight><bold>10</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 1</cross-reference> is designed to operate in at least two modes, the first being a real-time or synchronous mode wherein the primary and secondary storage systems must guarantee that the data exists and is stored in two physically separate data storage units before input/output completion; that is, before channel end and device end is returned to the primary host. Alternatively, the system <highlight><bold>10</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 1</cross-reference> is designed to operate in a point-in-time or asynchronous mode wherein the data is copied to the remote or secondary data storage system asynchronously from the time when the primary or local data processing system returns the input/output completion signal (channel end and device end) to the primary host. This eliminates any performance penalty if the communication link between the primary and secondary data storage systems is too slow, but creates the additional needs to manage the situation where data is not identical or in &ldquo;sync&rdquo; between the primary and secondary data storage systems. </paragraph>
<paragraph id="P-0040" lvl="0"><number>&lsqb;0040&rsqb;</number> Thus, in the real time or synchronous mode, the primary data storage system automatically controls the duplication or copying of data to the secondary data storage system controller transparently to the primary host computer. Only after data is safely stored in both the primary and secondary data storage system, as detected by an acknowledgement from the secondary storage system to the primary storage system, does the primary data storage system acknowledge to the primary host computer that the data is synchronized. Should a disaster or facility outage occur at the primary data storage system site, the user will simply need to initialize the application program in the secondary data storage system utilizing a local host (<highlight><bold>52</bold></highlight>) or a commercial hotsite CPU or host <highlight><bold>56</bold></highlight>. </paragraph>
<paragraph id="P-0041" lvl="0"><number>&lsqb;0041&rsqb;</number> The link between the primary and secondary storage system controllers <highlight><bold>14</bold></highlight> and <highlight><bold>46</bold></highlight> may be maintained in a unidirectional mode wherein the primary data storage system controller monitors and controls data copying or mirroring. Alternatively, a bi-directional implementation may be used wherein either controller can duplicate data to the other controller, transparently to the host computer. Should a disaster or facilities outage occur, recovery can be automatic with no human intervention since the operational host computer already has an active path (<highlight><bold>40</bold></highlight>, <highlight><bold>58</bold></highlight>, <highlight><bold>60</bold></highlight>) to the data through its local controller. While offering uninterrupted recovery, performance will be slower than in an unidirectional implementation due to the overhead required to manage intercontroller tasks. </paragraph>
<paragraph id="P-0042" lvl="0"><number>&lsqb;0042&rsqb;</number> In the second, point-in-time mode of operation, the primary data storage system transparently duplicates data to the secondary data storage system after the primary data storage system acknowledges to the host computer, via channel end and device end, that the data has been written to the storage device and the input/output operation has been completed. This eliminates the performance impact of data mirroring over long distances. Since primary and secondary data are not synchronized, however, the primary data storage system must maintain a log file of pending data which has yet to be written to the secondary data storage device. Such data may be kept on removable, non-volatile media, in the cache memory of the primary or secondary data storage system controller as will be explained below, or in the service processor <highlight><bold>34</bold></highlight>, <highlight><bold>62</bold></highlight>. </paragraph>
<paragraph id="P-0043" lvl="0"><number>&lsqb;0043&rsqb;</number> Accordingly, a feature of the system <highlight><bold>10</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 1</cross-reference> is the ability of a data storage system to control the transfer or copying of data from a primary data storage system to the secondary data storage system, independent of and without intervention from one or more host computers. Most importantly, in order to achieve optimum data mirroring performance, such data mirroring or copying should be performed asynchronously with input/output requests from a host computer. Accordingly, since data will not be immediately synchronized between the primary and secondary data storage systems, data integrity must be maintained by maintaining an index or list of various criteria including a list of data which has not been mirrored or copied, data storage locations for which a reformat operation is pending, a list of invalid data storage device locations or tracks, whether a given device is ready, or whether a device is write-disabled. Information must also be included as to the time of the last operation so that the data may later be synchronized should an error be detected. </paragraph>
<paragraph id="P-0044" lvl="0"><number>&lsqb;0044&rsqb;</number> A feature of the system <highlight><bold>10</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 1</cross-reference> is that both the primary or secondary data storage systems maintain a table of the validity of data in the other storage system. As disclosed in U.S. Pat. No. 5,206,939 entitled SYSTEM AND METHOD FOR DISK MAPPING AND DATA RETRIEVAL, which is fully incorporated herein by reference, the present system maintains a list or index, utilizing one or more flag bits, in a hierarchical structure, on each physical and logical data storage device. </paragraph>
<paragraph id="P-0045" lvl="0"><number>&lsqb;0045&rsqb;</number> In the system <highlight><bold>10</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 1</cross-reference>, however, such information is kept on both devices for each individual system as well as the other data storage system. Thus, as illustrated in <cross-reference target="DRAWINGS">FIG. 2</cross-reference> in the partial list or table <highlight><bold>100</bold></highlight>, each data storage system maintains an indication of write or copy pending <highlight><bold>102</bold></highlight> of both the primary data (M1) <highlight><bold>104</bold></highlight>, and the secondary data (M2) <highlight><bold>106</bold></highlight>. Similarly, an index is maintained of a pending format change since a disk format change may be accomplished. The format pending bits <highlight><bold>108</bold></highlight> including a first primary bit <highlight><bold>110</bold></highlight> and a second secondary bit <highlight><bold>112</bold></highlight> indicate that a format change has been requested and such change must be made on the disk. </paragraph>
<paragraph id="P-0046" lvl="0"><number>&lsqb;0046&rsqb;</number> Thus, when a host computer writes data to a primary data storage system, it sets both the primary and secondary bits <highlight><bold>104</bold></highlight>, <highlight><bold>106</bold></highlight> of the write pending bits <highlight><bold>102</bold></highlight> when data is written to cache. For these examples, the M1 bit will refer to the primary data storage system and the M2 bit will refer to the secondary data storage system. When the primary data storage system controller&apos;s disk adapter writes the data to the primary data storage device, it will reset bit <highlight><bold>104</bold></highlight> of the write pending indicator bits <highlight><bold>102</bold></highlight>. Similarly, once the secondary data storage system has written the data to the secondary data storage device, the secondary data storage write pending indicator bit <highlight><bold>106</bold></highlight> will be reset. </paragraph>
<paragraph id="P-0047" lvl="0"><number>&lsqb;0047&rsqb;</number> The service processors in one embodiment of the present invention will periodically scan the index table for write pending indicator bits and invoke a copy task which copies the data from the primary data storage device to the secondary. In addition, one or more of the spare index or table bits <highlight><bold>114</bold></highlight>, <highlight><bold>116</bold></highlight> may be utilized to store other data such as time stamp, etc. </paragraph>
<paragraph id="P-0048" lvl="0"><number>&lsqb;0048&rsqb;</number> In addition to the write pending and format pending bits described above, the system <highlight><bold>10</bold></highlight> also includes several additional general purpose flags to assist in error recovery. As shown in <cross-reference target="DRAWINGS">FIG. 3</cross-reference>, invalid track flags <highlight><bold>120</bold></highlight> including primary bit <highlight><bold>122</bold></highlight> and secondary bit <highlight><bold>124</bold></highlight> are utilized and maintained on each data storage device to indicate that the data storage location such as a track, does not contain valid data. Another background task running on the data storage system such as in the service processor constantly checks invalid track bits on each data storage device, and if a bit is found to be set, the copy task is invoked to copy the data from the known good device to the device with the invalid flag track set. Additional flags may be provided such as the device ready flags <highlight><bold>126</bold></highlight> including bits <highlight><bold>128</bold></highlight> and <highlight><bold>130</bold></highlight> which serve to indicate that the device is ready. Similarly, write disable flags <highlight><bold>132</bold></highlight> may be provided which indicate that a particular primary device or drive <highlight><bold>134</bold></highlight> or secondary device or drive <highlight><bold>136</bold></highlight> can presently not be written to. Data can still be copied to the good or enabled drive and then later copied to the disabled drive. If one drive or device is bad, the system <highlight><bold>10</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 1</cross-reference> will set all tracks of that drive as not valid to later cause a copy of all the data. </paragraph>
<paragraph id="P-0049" lvl="0"><number>&lsqb;0049&rsqb;</number> Accordingly, each data storage device keeps data validity information about its mirrored device. If for some reason a device is not accessible, either the primary or the secondary device, every new write command goes to the accessible mirrored device along with information that the not accessible device has a track which is not valid. As soon as the non-accessible device becomes accessible, then automatically, as a background operation, the drives re-synchronize. In the case when a specific track is not shown on both the primary and secondary storage system, an indication of such will be assigned and the user will be alerted. A utility operating on the service processors will give the user a report of all the non-valid (out of sync) tracks. This report can be transferred from one site to another over the link <highlight><bold>63</bold></highlight>, <cross-reference target="DRAWINGS">FIG. 1</cross-reference>, that connects the two service processors <highlight><bold>34</bold></highlight>, <highlight><bold>62</bold></highlight>. </paragraph>
<paragraph id="P-0050" lvl="0"><number>&lsqb;0050&rsqb;</number> Modifications and substitutions by one of ordinary skill in the art are considered to be within the scope of the present invention, which is not to be limited except by the claims which follow. </paragraph>
</section>
</detailed-description>
</subdoc-description>
<subdoc-claims>
<heading lvl="1">What is claimed is: </heading>
<claim id="CLM-00001">
<claim-text><highlight><bold>1</bold></highlight>. A remote copy system for automatically providing a remote copy of data from a host computer, said system comprising: 
<claim-text>a first data storage system including a first data storage system controller and at least one first data storage device, the first data storage system controller having a first cache memory, the first data storage system controller receiving the data from the host computer and initially storing the data in the first cache memory and subsequently writing the data into said at least one first data storage device; and </claim-text>
<claim-text>a second data storage system located remote from the first data storage system, the second data storage system including a second data storage system controller and at least one second data storage device, the second data storage system controller having a second cache memory, the first data storage system controller being coupled to the second data storage system controller for copying the data from the first data storage system controller to the second data storage system controller, the second data storage system controller storing the data in the second cache memory and writing the data into said at least one second data storage device to produce the remote copy of the data from the host computer; </claim-text>
<claim-text>the first data storage system controller coordinating and controlling the copying of the data to the second data storage system controller wherein, in response to receipt of the data from the host computer, a first write pending indicator is set to write the data into said at least one first data storage device, and a second write pending indicator is set to copy the data to the second data storage system controller, the first write pending indicator being reset after the data is written to said at least one first data storage device and the second write pending indicator being reset after receiving an acknowledgement back from the second data storage system controller. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00002">
<claim-text><highlight><bold>2</bold></highlight>. The remote copy system as claimed in <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein the acknowledgement back from the second data storage system controller indicates that the second data storage system controller has received the data. </claim-text>
</claim>
<claim id="CLM-00003">
<claim-text><highlight><bold>3</bold></highlight>. The remote copy system as claimed in <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein the acknowledgement back from the second data storage system controller indicates that the data has actually been written to said at least one second data storage device. </claim-text>
</claim>
<claim id="CLM-00004">
<claim-text><highlight><bold>4</bold></highlight>. The remote copy system as claimed in <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein the first data storage system returns an i/o completion signal to the host computer, and the data is copied to the second data storage system asynchronously from the time when the first data storage system returns the i/o completion signal to the host computer. </claim-text>
</claim>
<claim id="CLM-00005">
<claim-text><highlight><bold>5</bold></highlight>. The remote copy system as claimed in <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein the first data storage system controller returns an i/o completion signal to the host computer after the data has been safely stored in both the first data storage system and in the second data storage system as indicated by the acknowledgement from the second data storage system. </claim-text>
</claim>
<claim id="CLM-00006">
<claim-text><highlight><bold>6</bold></highlight>. The remote copy system as claimed in <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein the first data storage system controller includes at least one disk adapter for writing the data from the first cache memory to said at least one first data storage device, and said at least one disk adapter resets the first write pending indicator when it writes the data to said at least one first data storage device. </claim-text>
</claim>
<claim id="CLM-00007">
<claim-text><highlight><bold>7</bold></highlight>. The remote copy system as claimed in <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein the second write pending indicator is reset when the second data storage system has written the data to said at least one second data storage device. </claim-text>
</claim>
<claim id="CLM-00008">
<claim-text><highlight><bold>8</bold></highlight>. The remote copy system as claimed in <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein the copying of the data from the first data storage system to the second data storage system is controlled independent of and without intervention from any host computer. </claim-text>
</claim>
<claim id="CLM-00009">
<claim-text><highlight><bold>9</bold></highlight>. The remote copy system as claimed in <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein the first and second write pending indicators are flags in at least one table of flag bits. </claim-text>
</claim>
<claim id="CLM-00010">
<claim-text><highlight><bold>10</bold></highlight>. The remote copy system as claimed in <dependent-claim-reference depends_on="CLM-00009">claim 9</dependent-claim-reference>, wherein said at least one table of flag bits is maintained by the first data storage system and kept on said at least one first data storage device. </claim-text>
</claim>
<claim id="CLM-00011">
<claim-text><highlight><bold>11</bold></highlight>. The remote copy system as claimed in <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein the first and second write pending indicators are flags in a first table of flag bits maintained by the first data storage system and kept on said at least one first data storage device, and the flags are also kept on said at least one second data storage device in a second table of flag bits maintained by the second data storage system. </claim-text>
</claim>
<claim id="CLM-00012">
<claim-text><highlight><bold>12</bold></highlight>. A remote copy system for automatically providing a geographically remote copy of data from a host computer, said system comprising: 
<claim-text>a first data storage system including a first data storage system controller and at least one first data storage device, the first data storage system controller having a first cache memory, the first data storage system controller receiving the data from the host computer and initially storing the data in the first cache memory and subsequently writing the data into said at least one first data storage device; and </claim-text>
<claim-text>a second data storage system geographically removed from the first data storage system, the second data storage system including a second data storage system controller and at least one second data storage device, the second data storage system controller having a second cache memory, the first data storage system controller being coupled to the second data storage system controller for copying the data from the first data storage system controller to the second data storage system controller, the second data storage system controller storing the data in the second cache memory and writing the data into said at least one second data storage device to produce the remote copy of the data from the host computer independent of the host computer; </claim-text>
<claim-text>the first data storage system controller coordinating and controlling the copying of the data to the second data storage system controller wherein, in response to receipt of the data from the host computer, when the data is stored into the first cache memory, a first write pending indicator is set to write the data into said at least one first data storage device, and a second write pending indicator is set to copy the data to the second data storage system controller, the first write pending indicator being reset after the data is written to said at least one first data storage device and the second write pending indicator being reset after receiving an acknowledgement back from the second data storage system controller that the data has at least been received by the second data storage system controller; </claim-text>
<claim-text>wherein the first data storage system returns an i/o completion signal to the host computer, and the data is copied to the second data storage system asynchronously from the time when the first data storage system returns the i/o completion signal to the host computer; and </claim-text>
<claim-text>wherein the copying of the data from the first data storage system to the second data storage system is controlled independent of and without intervention from any host computer. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00013">
<claim-text><highlight><bold>13</bold></highlight>. The remote copy system as claimed in <dependent-claim-reference depends_on="CLM-00011">claim 12</dependent-claim-reference>, wherein the acknowledgement back from the second data storage system controller indicates that the data has actually been written to said at least one second data storage device. </claim-text>
</claim>
<claim id="CLM-00014">
<claim-text><highlight><bold>14</bold></highlight>. The remote copy system as claimed in <dependent-claim-reference depends_on="CLM-00011">claim 12</dependent-claim-reference>, wherein the first data storage system controller includes at least one disk adapter for writing the data from the first cache memory to said at least one first data storage device, and said at least one disk adapter resets the first write pending indicator when it writes the data to said at least one first data storage device. </claim-text>
</claim>
<claim id="CLM-00015">
<claim-text><highlight><bold>15</bold></highlight>. The remote copy system as claimed in <dependent-claim-reference depends_on="CLM-00011">claim 12</dependent-claim-reference>, wherein the second write pending indicator is reset when the second data storage system has written the data to said at least one second data storage device. </claim-text>
</claim>
<claim id="CLM-00016">
<claim-text><highlight><bold>16</bold></highlight>. The remote copy system as claimed in <dependent-claim-reference depends_on="CLM-00011">claim 12</dependent-claim-reference>, wherein the first and second write pending indicators are flags in at least one table of flag bits. </claim-text>
</claim>
<claim id="CLM-00017">
<claim-text><highlight><bold>17</bold></highlight>. The remote copy system as claimed in <dependent-claim-reference depends_on="CLM-00011">claim 16</dependent-claim-reference>, wherein said at least one table of flag bits is maintained by the first data storage system and kept on said at least one first data storage device. </claim-text>
</claim>
<claim id="CLM-00018">
<claim-text><highlight><bold>18</bold></highlight>. The remote copy system as claimed in <dependent-claim-reference depends_on="CLM-00011">claim 12</dependent-claim-reference>, wherein the first and second write pending indicators are flags in a first table of flag bits maintained by the first data storage system and kept on said at least one first data storage device, and the flags are also kept on said at least one second data storage device in a second table of flag bits maintained by the second data storage system. </claim-text>
</claim>
<claim id="CLM-00019">
<claim-text><highlight><bold>19</bold></highlight>. A remote copy system for automatically providing a geographically remote copy of data from a host computer, said system comprising: 
<claim-text>a first data storage system including a first data storage system controller and at least one first data storage device, the first data storage system controller having a first cache memory, the first data storage system controller receiving the data from the host computer and initially storing the data in the first cache memory and subsequently writing the data into said at least one first data storage device; and </claim-text>
<claim-text>a second data storage system geographically removed from the first data storage system, the second data storage system including a second data storage system controller and at least one second data storage device, the second data storage system controller having a second cache memory, the first data storage system controller being coupled to the second data storage system controller for copying the data from the first data storage system controller to the second data storage system controller, the second data storage system controller storing the data in the second cache memory and writing the data into said at least one second data storage device to produce the remote copy of the data from the host computer independent of the host computer; </claim-text>
<claim-text>the first data storage system controller coordinating and controlling the copying of the data to the second data storage system controller wherein, in response to receipt of the data from the host computer, a first write pending indicator is set to write the data into said at least one first data storage device, and a second write pending indicator is set to copy the data to said second data storage system controller, the first write pending indicator being reset after the data is written to said at least one first data storage device and the second write pending indicator being reset after receiving an acknowledgement back from the second data storage system controller that the data has at least been received by the second data storage system controller; </claim-text>
<claim-text>wherein the copying of the data from the first data storage system to the second data storage system is controlled independent of and without intervention from any host computer; and </claim-text>
<claim-text>wherein the first and second write pending indicators are flags in a first table of flag bits maintained by the first data storage system and kept on said at least one first data storage device, and the flags are also kept on said at least one second data storage device in a second table of flag bits maintained by the second data storage system. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00020">
<claim-text><highlight><bold>20</bold></highlight>. The remote copy system as claimed in <dependent-claim-reference depends_on="CLM-00011">claim 19</dependent-claim-reference>, wherein the first data storage system controller includes at least one disk adapter for writing the data from the first cache memory to said at least one first data storage device, and said at least one disk adapter resets the first write pending indicator when it writes the data to said at least one first data storage device.</claim-text>
</claim>
</subdoc-claims>
<subdoc-drawings id="DRAWINGS">
<heading lvl="0" align="CENTER">Drawings</heading>
<representative-figure>1</representative-figure>
<figure id="figure-D00000">
<image id="EMI-D00000" file="US20030005355A1-20030102-D00000.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00001">
<image id="EMI-D00001" file="US20030005355A1-20030102-D00001.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00002">
<image id="EMI-D00002" file="US20030005355A1-20030102-D00002.TIF" imf="TIFF" ti="DR"/>
</figure>
</subdoc-drawings>
</patent-application-publication>
