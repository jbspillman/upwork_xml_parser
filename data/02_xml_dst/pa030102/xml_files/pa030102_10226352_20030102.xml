<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE patent-application-publication SYSTEM "pap-v16-2002-01-01.dtd" [
<!ENTITY US20030002853A1-20030102-D00000.TIF SYSTEM "US20030002853A1-20030102-D00000.TIF" NDATA TIF>
<!ENTITY US20030002853A1-20030102-D00001.TIF SYSTEM "US20030002853A1-20030102-D00001.TIF" NDATA TIF>
<!ENTITY US20030002853A1-20030102-D00002.TIF SYSTEM "US20030002853A1-20030102-D00002.TIF" NDATA TIF>
<!ENTITY US20030002853A1-20030102-D00003.TIF SYSTEM "US20030002853A1-20030102-D00003.TIF" NDATA TIF>
<!ENTITY US20030002853A1-20030102-D00004.TIF SYSTEM "US20030002853A1-20030102-D00004.TIF" NDATA TIF>
<!ENTITY US20030002853A1-20030102-D00005.TIF SYSTEM "US20030002853A1-20030102-D00005.TIF" NDATA TIF>
<!ENTITY US20030002853A1-20030102-D00006.TIF SYSTEM "US20030002853A1-20030102-D00006.TIF" NDATA TIF>
<!ENTITY US20030002853A1-20030102-D00007.TIF SYSTEM "US20030002853A1-20030102-D00007.TIF" NDATA TIF>
<!ENTITY US20030002853A1-20030102-D00008.TIF SYSTEM "US20030002853A1-20030102-D00008.TIF" NDATA TIF>
<!ENTITY US20030002853A1-20030102-D00009.TIF SYSTEM "US20030002853A1-20030102-D00009.TIF" NDATA TIF>
<!ENTITY US20030002853A1-20030102-D00010.TIF SYSTEM "US20030002853A1-20030102-D00010.TIF" NDATA TIF>
<!ENTITY US20030002853A1-20030102-D00011.TIF SYSTEM "US20030002853A1-20030102-D00011.TIF" NDATA TIF>
<!ENTITY US20030002853A1-20030102-D00012.TIF SYSTEM "US20030002853A1-20030102-D00012.TIF" NDATA TIF>
<!ENTITY US20030002853A1-20030102-D00013.TIF SYSTEM "US20030002853A1-20030102-D00013.TIF" NDATA TIF>
<!ENTITY US20030002853A1-20030102-D00014.TIF SYSTEM "US20030002853A1-20030102-D00014.TIF" NDATA TIF>
<!ENTITY US20030002853A1-20030102-D00015.TIF SYSTEM "US20030002853A1-20030102-D00015.TIF" NDATA TIF>
<!ENTITY US20030002853A1-20030102-D00016.TIF SYSTEM "US20030002853A1-20030102-D00016.TIF" NDATA TIF>
<!ENTITY US20030002853A1-20030102-D00017.TIF SYSTEM "US20030002853A1-20030102-D00017.TIF" NDATA TIF>
<!ENTITY US20030002853A1-20030102-D00018.TIF SYSTEM "US20030002853A1-20030102-D00018.TIF" NDATA TIF>
<!ENTITY US20030002853A1-20030102-D00019.TIF SYSTEM "US20030002853A1-20030102-D00019.TIF" NDATA TIF>
<!ENTITY US20030002853A1-20030102-D00020.TIF SYSTEM "US20030002853A1-20030102-D00020.TIF" NDATA TIF>
<!ENTITY US20030002853A1-20030102-D00021.TIF SYSTEM "US20030002853A1-20030102-D00021.TIF" NDATA TIF>
<!ENTITY US20030002853A1-20030102-D00022.TIF SYSTEM "US20030002853A1-20030102-D00022.TIF" NDATA TIF>
<!ENTITY US20030002853A1-20030102-D00023.TIF SYSTEM "US20030002853A1-20030102-D00023.TIF" NDATA TIF>
<!ENTITY US20030002853A1-20030102-D00024.TIF SYSTEM "US20030002853A1-20030102-D00024.TIF" NDATA TIF>
<!ENTITY US20030002853A1-20030102-D00025.TIF SYSTEM "US20030002853A1-20030102-D00025.TIF" NDATA TIF>
<!ENTITY US20030002853A1-20030102-D00026.TIF SYSTEM "US20030002853A1-20030102-D00026.TIF" NDATA TIF>
<!ENTITY US20030002853A1-20030102-D00027.TIF SYSTEM "US20030002853A1-20030102-D00027.TIF" NDATA TIF>
<!ENTITY US20030002853A1-20030102-D00028.TIF SYSTEM "US20030002853A1-20030102-D00028.TIF" NDATA TIF>
<!ENTITY US20030002853A1-20030102-D00029.TIF SYSTEM "US20030002853A1-20030102-D00029.TIF" NDATA TIF>
<!ENTITY US20030002853A1-20030102-D00030.TIF SYSTEM "US20030002853A1-20030102-D00030.TIF" NDATA TIF>
<!ENTITY US20030002853A1-20030102-D00031.TIF SYSTEM "US20030002853A1-20030102-D00031.TIF" NDATA TIF>
<!ENTITY US20030002853A1-20030102-D00032.TIF SYSTEM "US20030002853A1-20030102-D00032.TIF" NDATA TIF>
<!ENTITY US20030002853A1-20030102-D00033.TIF SYSTEM "US20030002853A1-20030102-D00033.TIF" NDATA TIF>
]>
<patent-application-publication>
<subdoc-bibliographic-information>
<document-id>
<doc-number>20030002853</doc-number>
<kind-code>A1</kind-code>
<document-date>20030102</document-date>
</document-id>
<publication-filing-type>new</publication-filing-type>
<domestic-filing-data>
<application-number>
<doc-number>10226352</doc-number>
</application-number>
<application-number-series-code>10</application-number-series-code>
<filing-date>20020823</filing-date>
</domestic-filing-data>
<foreign-priority-data>
<priority-application-number>
<doc-number>2000-200220</doc-number>
</priority-application-number>
<filing-date>20000630</filing-date>
<country-code>JP</country-code>
</foreign-priority-data>
<technical-information>
<classification-ipc>
<classification-ipc-primary>
<ipc>H04N005/91</ipc>
</classification-ipc-primary>
<classification-ipc-edition>07</classification-ipc-edition>
</classification-ipc>
<classification-us>
<classification-us-primary>
<uspc>
<class>386</class>
<subclass>065000</subclass>
</uspc>
</classification-us-primary>
<classification-us-secondary>
<uspc>
<class>386</class>
<subclass>082000</subclass>
</uspc>
</classification-us-secondary>
</classification-us>
<title-of-invention>Special reproduction control information describing method, special reproduction control information creating apparatus and method therefor, and video reproduction apparatus and method therefor</title-of-invention>
</technical-information>
<continuity-data>
<division-of>
<parent-child>
<child>
<document-id>
<doc-number>10226352</doc-number>
<kind-code>A1</kind-code>
<document-date>20020823</document-date>
</document-id>
</child>
<parent>
<document-id>
<doc-number>09894321</doc-number>
<document-date>20010629</document-date>
<country-code>US</country-code>
</document-id>
</parent>
<parent-status>PENDING</parent-status>
</parent-child>
</division-of>
</continuity-data>
<inventors>
<first-named-inventor>
<name>
<given-name>Osamu</given-name>
<family-name>Hori</family-name>
</name>
<residence>
<residence-non-us>
<city>Yokohama-shi</city>
<country-code>JP</country-code>
</residence-non-us>
</residence>
<authority-applicant>INV</authority-applicant>
</first-named-inventor>
<inventor>
<name>
<given-name>Toshimitsu</given-name>
<family-name>Kaneko</family-name>
</name>
<residence>
<residence-non-us>
<city>Kawasaki-shi</city>
<country-code>JP</country-code>
</residence-non-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
<inventor>
<name>
<given-name>Takeshi</given-name>
<family-name>Mita</family-name>
</name>
<residence>
<residence-non-us>
<city>Yokohama-shi</city>
<country-code>JP</country-code>
</residence-non-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
<inventor>
<name>
<given-name>Koji</given-name>
<family-name>Yamamoto</family-name>
</name>
<residence>
<residence-non-us>
<city>Yokohama-shi</city>
<country-code>JP</country-code>
</residence-non-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
<inventor>
<name>
<given-name>Koichi</given-name>
<family-name>Masukura</family-name>
</name>
<residence>
<residence-non-us>
<city>Kawasaki-shi</city>
<country-code>JP</country-code>
</residence-non-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
</inventors>
<correspondence-address>
<name-1>OBLON SPIVAK MCCLELLAND MAIER &amp; NEUSTADT PC</name-1>
<name-2>FOURTH FLOOR</name-2>
<address>
<address-1>1755 JEFFERSON DAVIS HIGHWAY</address-1>
<city>ARLINGTON</city>
<state>VA</state>
<postalcode>22202</postalcode>
<country>
<country-code>US</country-code>
</country>
</address>
</correspondence-address>
</subdoc-bibliographic-information>
<subdoc-abstract>
<paragraph id="A-0001" lvl="0">A special reproduction control information comprises plurality of items of frame information. Each of the items of frame information comprises video location information indicating the location of video data to be reproduced in a special reproduction and display time control information indicating the time for displaying the video data. </paragraph>
</subdoc-abstract>
<subdoc-description>
<summary-of-invention>
<section>
<heading lvl="1">CROSS-REFERENCE TO RELATED APPLICATIONS </heading>
<paragraph id="P-0001" lvl="0"><number>&lsqb;0001&rsqb;</number> This application is based upon and claims the benefit of priority from the prior Japanese Patent Application No. 2000-200220, filed Jun. 30, 2000, the entire contents of which are incorporated herein by reference. </paragraph>
</section>
<section>
<heading lvl="1">BACKGROUND OF THE INVENTION </heading>
<paragraph id="P-0002" lvl="0"><number>&lsqb;0002&rsqb;</number> 1. Field of the Invention </paragraph>
<paragraph id="P-0003" lvl="0"><number>&lsqb;0003&rsqb;</number> The present invention relates to a special reproduction control information describing method for describing special reproduction control information used to perform special reproduction for target video contents, a special reproduction control information creating method for creating the special reproduction control information and a special reproduction control information creating apparatus and a video reproduction apparatus and method for performing special reproduction by using the special reproduction control information. </paragraph>
<paragraph id="P-0004" lvl="0"><number>&lsqb;0004&rsqb;</number> 2. Description of the Related Art </paragraph>
<paragraph id="P-0005" lvl="0"><number>&lsqb;0005&rsqb;</number> In recent years, a motion picture is compressed as a digital video and is stored in disk media represented by a DVD, and a HDD so that a video can be reproduced at random. A video can be reproduced halfway from a desired timing in the state of virtually no waiting time. As in conventional tape media, disk media can be fast reproduced at two to four times speed or can be reversely reproduced. </paragraph>
<paragraph id="P-0006" lvl="0"><number>&lsqb;0006&rsqb;</number> However, there is a problem in that the length of a video can be very long in many cases, and time cannot be sufficiently compressed to view the whole contents of the video even at two to four times fast reproduction. When the rate of the fast reproduction is increased, the scene change is enlarged to a degree exceeding the ability to view it, so that grasping the contents is difficult, and even portions which are not needed are also reproduced so that waste is caused. </paragraph>
</section>
<section>
<heading lvl="1">BRIEF SUMMARY OF THE INVENTION </heading>
<paragraph id="P-0007" lvl="0"><number>&lsqb;0007&rsqb;</number> Accordingly, the present invention is directed to method and apparatus that substantially obviates one or more of the problems due to limitations and disadvantages of the related art. </paragraph>
<paragraph id="P-0008" lvl="0"><number>&lsqb;0008&rsqb;</number> According to one aspect of the present invention, a method of describing frame information comprises: </paragraph>
<paragraph id="P-0009" lvl="2"><number>&lsqb;0009&rsqb;</number> describing, for a frame extracted from a plurality of frames in a source video data, first information specifying a location of the extracted frame in the source video data; and </paragraph>
<paragraph id="P-0010" lvl="2"><number>&lsqb;0010&rsqb;</number> describing, for the extracted frame, second information relating to a display time of the extracted frame. </paragraph>
<paragraph id="P-0011" lvl="0"><number>&lsqb;0011&rsqb;</number> According to another aspect of the present invention, an article of manufacture comprising a computer usable medium storing frame information, the frame information comprises: </paragraph>
<paragraph id="P-0012" lvl="2"><number>&lsqb;0012&rsqb;</number> first information, described for a frame extracted from a plurality of frames, specifying a location of the extracted frame in the source video data; and </paragraph>
<paragraph id="P-0013" lvl="2"><number>&lsqb;0013&rsqb;</number> second information, described for the extracted frame, relating to a display time of the extracted frame. </paragraph>
<paragraph id="P-0014" lvl="0"><number>&lsqb;0014&rsqb;</number> According to another aspect of the present invention, an apparatus for creating frame information comprises: </paragraph>
<paragraph id="P-0015" lvl="2"><number>&lsqb;0015&rsqb;</number> a unit configured to extract a frame from a plurality of frames in a source video data; </paragraph>
<paragraph id="P-0016" lvl="2"><number>&lsqb;0016&rsqb;</number> a unit configured to create the frame information including first information specifying a location of the extracted frame and second information relating to a display time of the extracted frame; and </paragraph>
<paragraph id="P-0017" lvl="2"><number>&lsqb;0017&rsqb;</number> a unit configured to link the extracted frame to the frame information. </paragraph>
<paragraph id="P-0018" lvl="0"><number>&lsqb;0018&rsqb;</number> According to another aspect of the present invention, a method of creating frame information comprises: </paragraph>
<paragraph id="P-0019" lvl="2"><number>&lsqb;0019&rsqb;</number> extracting a frame from a plurality of frames in a source video data; and </paragraph>
<paragraph id="P-0020" lvl="2"><number>&lsqb;0020&rsqb;</number> creating the frame information including first information specifying a location of the extracted frame in the source video data and second information relating to a display time of the extracted frame. </paragraph>
<paragraph id="P-0021" lvl="0"><number>&lsqb;0021&rsqb;</number> According to another aspect of the present invention, an apparatus for performing a special reproduction comprises: </paragraph>
<paragraph id="P-0022" lvl="2"><number>&lsqb;0022&rsqb;</number> a unit configured to refer to frame information described for a frame extracted from a plurality of frames in a source video data and including first information specifying a location of the extracted frame in the source video data and second information relating to a display time of the extracted frame; </paragraph>
<paragraph id="P-0023" lvl="2"><number>&lsqb;0023&rsqb;</number> a unit configured to obtain the video data corresponding to the extracted frame based on the first information; </paragraph>
<paragraph id="P-0024" lvl="2"><number>&lsqb;0024&rsqb;</number> a unit configured to determine the display time of the extracted frame based on the second information; and </paragraph>
<paragraph id="P-0025" lvl="2"><number>&lsqb;0025&rsqb;</number> a unit configured to display the obtained video data for the determined display time. </paragraph>
<paragraph id="P-0026" lvl="0"><number>&lsqb;0026&rsqb;</number> According to another aspect of the present invention, an article of manufacture comprising a method of performing a special reproduction comprises: </paragraph>
<paragraph id="P-0027" lvl="2"><number>&lsqb;0027&rsqb;</number> referring to frame information described for a frame extracted from a plurality of frames in a source video data and including first information specifying a location of the extracted frame and second information relating to a display time of the extracted frame; </paragraph>
<paragraph id="P-0028" lvl="2"><number>&lsqb;0028&rsqb;</number> obtaining the video data corresponding to the extracted frame based on the first information; </paragraph>
<paragraph id="P-0029" lvl="2"><number>&lsqb;0029&rsqb;</number> determining the display time of the extracted frame based on the second information; and </paragraph>
<paragraph id="P-0030" lvl="2"><number>&lsqb;0030&rsqb;</number> displaying the obtained video data for the determined display time. </paragraph>
<paragraph id="P-0031" lvl="0"><number>&lsqb;0031&rsqb;</number> According to another aspect of the present invention, an article of manufacture comprising an article of manufacture comprising a computer usable medium having computer readable program code means embodied therein, the computer readable program code means performing a special reproduction, the computer readable program code means comprises: </paragraph>
<paragraph id="P-0032" lvl="2"><number>&lsqb;0032&rsqb;</number> computer readable program code means for causing a computer to refer to frame information described for a frame extracted from a plurality of frames in a source video data and including first information specifying a location of the extracted frame and second information relating to a display time of the extracted frame; </paragraph>
<paragraph id="P-0033" lvl="2"><number>&lsqb;0033&rsqb;</number> computer readable program code means for causing a computer to obtain the video data corresponding to the extracted frame based on the first information; </paragraph>
<paragraph id="P-0034" lvl="2"><number>&lsqb;0034&rsqb;</number> computer readable program code means for causing a computer to determine the display time of the extracted frame based on the second information; and </paragraph>
<paragraph id="P-0035" lvl="2"><number>&lsqb;0035&rsqb;</number> computer readable program code means for causing a computer to display the obtained video data for the determined display time. </paragraph>
<paragraph id="P-0036" lvl="0"><number>&lsqb;0036&rsqb;</number> According to another aspect of the present invention, an article of manufacture comprising a method of describing sound information, the method comprises: </paragraph>
<paragraph id="P-0037" lvl="2"><number>&lsqb;0037&rsqb;</number> describing, for a frame extracted from a plurality of sound frames in a source sound data, first information specifying a location of the extracted frame in the source sound data; and </paragraph>
<paragraph id="P-0038" lvl="2"><number>&lsqb;0038&rsqb;</number> describing, for the extracted frame, second information relating to a reproduction start time and reproduction time of the sound data of the extracted frame. </paragraph>
<paragraph id="P-0039" lvl="0"><number>&lsqb;0039&rsqb;</number> According to another aspect of the present invention, an article of manufacture comprising an article of manufacture comprising a computer usable medium storing frame information, the frame information comprises: </paragraph>
<paragraph id="P-0040" lvl="2"><number>&lsqb;0040&rsqb;</number> first information, described for a frame extracted from a plurality of sound frames, specifying a location of the extracted frame in the source sound data; and </paragraph>
<paragraph id="P-0041" lvl="2"><number>&lsqb;0041&rsqb;</number> second information, described for the extracted frame, relating to a reproduction start time and reproduction time of the sound data of the extracted frame. </paragraph>
<paragraph id="P-0042" lvl="0"><number>&lsqb;0042&rsqb;</number> According to another aspect of the present invention, an article of manufacture comprising a method of describing text information, the method comprises: </paragraph>
<paragraph id="P-0043" lvl="2"><number>&lsqb;0043&rsqb;</number> describing, for a frame extracted from a plurality of text frames in a source text data, first information specifying a location of the extracted frame in the source text data; and </paragraph>
<paragraph id="P-0044" lvl="2"><number>&lsqb;0044&rsqb;</number> describing, for the extracted frame, second information relating to a display start time and display time of the text data of the extracted frame. </paragraph>
<paragraph id="P-0045" lvl="0"><number>&lsqb;0045&rsqb;</number> According to another aspect of the present invention, an article of manufacture comprising an article of manufacture comprising a computer usable medium storing frame information, the frame information comprises: </paragraph>
<paragraph id="P-0046" lvl="2"><number>&lsqb;0046&rsqb;</number> first information, described for a frame extracted from a plurality of text frames in a source text data, specifying a location of the extracted frame in the source text data; and </paragraph>
<paragraph id="P-0047" lvl="2"><number>&lsqb;0047&rsqb;</number> second information, described for the extracted frame, relating to a display start time and display time of the text data of the extracted frame.</paragraph>
</section>
</summary-of-invention>
<brief-description-of-drawings>
<section>
<heading lvl="1">BRIEF DESCRIPTION OF THE SEVERAL VIEWS OF THE DRAWING </heading>
<paragraph id="P-0048" lvl="0"><number>&lsqb;0048&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 1</cross-reference> is a view showing an example of a data structure of special reproduction control information according to one embodiment of the present invention; </paragraph>
<paragraph id="P-0049" lvl="0"><number>&lsqb;0049&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 2</cross-reference> is a view showing an example of a structure of a special reproduction control information creating apparatus; </paragraph>
<paragraph id="P-0050" lvl="0"><number>&lsqb;0050&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 3</cross-reference> is a view showing an another example of structure of the special reproduction control information creating apparatus; </paragraph>
<paragraph id="P-0051" lvl="0"><number>&lsqb;0051&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 4</cross-reference> is a flowchart showing one example for the apparatus shown in <cross-reference target="DRAWINGS">FIG. 2</cross-reference>; </paragraph>
<paragraph id="P-0052" lvl="0"><number>&lsqb;0052&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 5</cross-reference> is a flowchart showing one example for the apparatus shown in <cross-reference target="DRAWINGS">FIG. 3</cross-reference>; </paragraph>
<paragraph id="P-0053" lvl="0"><number>&lsqb;0053&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 6</cross-reference> is a view showing an example of a structure of a video reproduction apparatus; </paragraph>
<paragraph id="P-0054" lvl="0"><number>&lsqb;0054&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 7</cross-reference> is a flowchart showing one example for the apparatus shown in <cross-reference target="DRAWINGS">FIG. 6</cross-reference>; </paragraph>
<paragraph id="P-0055" lvl="0"><number>&lsqb;0055&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 8</cross-reference> is a view showing an example of a data structure of special reproduction control information; </paragraph>
<paragraph id="P-0056" lvl="0"><number>&lsqb;0056&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 9</cross-reference> is a view explaining video location information for referring to an original video frame; </paragraph>
<paragraph id="P-0057" lvl="0"><number>&lsqb;0057&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 10</cross-reference> is a view explaining video location information for referring to a image data file; </paragraph>
<paragraph id="P-0058" lvl="0"><number>&lsqb;0058&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 11</cross-reference> is a view explaining a method for extracting video data in accordance with a motion of a screen; </paragraph>
<paragraph id="P-0059" lvl="0"><number>&lsqb;0059&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 12</cross-reference> is a view explaining video location information for referring to the original video frame; </paragraph>
<paragraph id="P-0060" lvl="0"><number>&lsqb;0060&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 13</cross-reference> is a view for explaining video location information for referring to the image data file; </paragraph>
<paragraph id="P-0061" lvl="0"><number>&lsqb;0061&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 14</cross-reference> is a view showing an example of a data structure of special reproduction control information in which plural original video frames are referred to; </paragraph>
<paragraph id="P-0062" lvl="0"><number>&lsqb;0062&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 15</cross-reference> is a view explaining a relation between the video location information and the original plural video frames; </paragraph>
<paragraph id="P-0063" lvl="0"><number>&lsqb;0063&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 16</cross-reference> is a view explaining a relation between the image data file and the original plural video frames; </paragraph>
<paragraph id="P-0064" lvl="0"><number>&lsqb;0064&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 17</cross-reference> is a view explaining video location information for referring to the original video frame; </paragraph>
<paragraph id="P-0065" lvl="0"><number>&lsqb;0065&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 18</cross-reference> is a view for explaining video location information for referring to the image data file; </paragraph>
<paragraph id="P-0066" lvl="0"><number>&lsqb;0066&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 19</cross-reference> is a flow chart for explaining a special reproduction; </paragraph>
<paragraph id="P-0067" lvl="0"><number>&lsqb;0067&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 20</cross-reference> is a view for explaining a method for extracting video data in accordance with a motion of a screen; </paragraph>
<paragraph id="P-0068" lvl="0"><number>&lsqb;0068&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 21</cross-reference> is a view for explaining a method for extracting video data in accordance with a motion of a screen; </paragraph>
<paragraph id="P-0069" lvl="0"><number>&lsqb;0069&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 22</cross-reference> is a flowchart showing one example for calculating display time at which a scene change quantity becomes constant as much as possible; </paragraph>
<paragraph id="P-0070" lvl="0"><number>&lsqb;0070&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 23</cross-reference> is a flowchart showing one example for calculating a scene change quantity of the whole frame from an MPEG video; </paragraph>
<paragraph id="P-0071" lvl="0"><number>&lsqb;0071&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 24</cross-reference> is a view for explaining a method for calculating a scene change quantity of a video from an MPEG stream; </paragraph>
<paragraph id="P-0072" lvl="0"><number>&lsqb;0072&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 25</cross-reference> is a view for explaining a processing procedure for calculating display time at which a scene change quantity becomes constant as much as possible; </paragraph>
<paragraph id="P-0073" lvl="0"><number>&lsqb;0073&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 26</cross-reference> is a flowchart showing one example of the processing procedure for conducting special reproduction on the basis of special reproduction control information; </paragraph>
<paragraph id="P-0074" lvl="0"><number>&lsqb;0074&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 27</cross-reference> is a flowchart showing one example for conducting special reproduction on the basis of a display cycle; </paragraph>
<paragraph id="P-0075" lvl="0"><number>&lsqb;0075&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 28</cross-reference> is a view for explaining a relationship between a calculated display time and the display cycle; </paragraph>
<paragraph id="P-0076" lvl="0"><number>&lsqb;0076&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 29</cross-reference> is a view for explaining a relationship between a calculated display time and the display cycle; </paragraph>
<paragraph id="P-0077" lvl="0"><number>&lsqb;0077&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 30</cross-reference> is a view showing another example of a data structure of special reproduction control information; </paragraph>
<paragraph id="P-0078" lvl="0"><number>&lsqb;0078&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 31</cross-reference> is a view explaining a method for extracting video data in accordance with a motion of a screen; </paragraph>
<paragraph id="P-0079" lvl="0"><number>&lsqb;0079&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 32</cross-reference> is a view explaining video location information for referring to the original video frame; </paragraph>
<paragraph id="P-0080" lvl="0"><number>&lsqb;0080&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 33</cross-reference> is a view showing another example of a data structure of special reproduction control information; </paragraph>
<paragraph id="P-0081" lvl="0"><number>&lsqb;0081&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 34</cross-reference> is a view showing another example of a data structure of special reproduction control information; </paragraph>
<paragraph id="P-0082" lvl="0"><number>&lsqb;0082&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 35</cross-reference> is a view showing another example of a data structure of special reproduction control information; </paragraph>
<paragraph id="P-0083" lvl="0"><number>&lsqb;0083&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 36</cross-reference> is a flowchart showing one example for calculating display time from the importance; </paragraph>
<paragraph id="P-0084" lvl="0"><number>&lsqb;0084&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 37</cross-reference> is a view for explaining a method for calculating display time from the importance; </paragraph>
<paragraph id="P-0085" lvl="0"><number>&lsqb;0085&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 38</cross-reference> is a flowchart showing one example for calculating importance data on the basis of the idea that a scene having a large sound level is important; </paragraph>
<paragraph id="P-0086" lvl="0"><number>&lsqb;0086&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 39</cross-reference> is a flowchart showing one example for calculating importance data on the basis of the idea that a scene on which many important words appear with sound recognition is important, or a processing procedure for calculating importance data on the basis of the idea that the scene in which the number of words talked per time is many is important; </paragraph>
<paragraph id="P-0087" lvl="0"><number>&lsqb;0087&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 40</cross-reference> is a flowchart showing one example for calculating importance data on the basis of the idea that a scene on which many important words appear with telop recognition is important, or a processing procedure for calculating importance data on the basis of the idea that the scene in which the number of words included in the telop which appears per time is large with telop recognition is important; </paragraph>
<paragraph id="P-0088" lvl="0"><number>&lsqb;0088&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 41</cross-reference> is a flowchart showing one example for calculating importance data on the basis of the idea that the scene in which a large character appears as a telop is important; </paragraph>
<paragraph id="P-0089" lvl="0"><number>&lsqb;0089&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 42</cross-reference> is a flowchart showing one example for calculating importance data on the basis of the idea that the scene in which many human faces appear is important or a processing for calculating importance data on the basis of the idea that the scene where human faces are displayed in an enlarged manner is important; </paragraph>
<paragraph id="P-0090" lvl="0"><number>&lsqb;0090&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 43</cross-reference> is a flowchart showing one example for calculating importance data on the basis of the idea that the scene in which videos similar to the registered important scene appear is important; </paragraph>
<paragraph id="P-0091" lvl="0"><number>&lsqb;0091&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 44</cross-reference> is a view showing another example of a data structure of special reproduction control information; </paragraph>
<paragraph id="P-0092" lvl="0"><number>&lsqb;0092&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 45</cross-reference> is a view showing another example of a data structure of special reproduction control information; </paragraph>
<paragraph id="P-0093" lvl="0"><number>&lsqb;0093&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 46</cross-reference> is a view showing another example of a data structure of special reproduction control information; </paragraph>
<paragraph id="P-0094" lvl="0"><number>&lsqb;0094&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 47</cross-reference> is a view for explaining a relationship between information as to whether the scene is to be reproduced or not and the reproduced video; </paragraph>
<paragraph id="P-0095" lvl="0"><number>&lsqb;0095&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 48</cross-reference> is a flowchart showing one example of a processing procedure of special reproduction including reproduction and non-reproduction judgment; </paragraph>
<paragraph id="P-0096" lvl="0"><number>&lsqb;0096&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 49</cross-reference> is a view showing one example of a data structure when sound information or text information is added; </paragraph>
<paragraph id="P-0097" lvl="0"><number>&lsqb;0097&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 50</cross-reference> is a view showing one example of a data structure for describing only sound information separately from frame information; </paragraph>
<paragraph id="P-0098" lvl="0"><number>&lsqb;0098&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 51</cross-reference> is a view showing one example of a data structure for describing only text information separately from frame information; </paragraph>
<paragraph id="P-0099" lvl="0"><number>&lsqb;0099&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 52</cross-reference> is a view for explaining a synchronization of a reproduction of each of media; </paragraph>
<paragraph id="P-0100" lvl="0"><number>&lsqb;0100&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 53</cross-reference> is a flowchart showing one example of a determination procedure of a sound reproduction start time and a sound reproduction time in a video frame section; </paragraph>
<paragraph id="P-0101" lvl="0"><number>&lsqb;0101&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 54</cross-reference> is a flowchart showing one example for preparing reproduction sound data and correcting video frame display time; </paragraph>
<paragraph id="P-0102" lvl="0"><number>&lsqb;0102&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 55</cross-reference> is a flowchart showing one example of a processing procedure of obtaining text information with telop recognition; </paragraph>
<paragraph id="P-0103" lvl="0"><number>&lsqb;0103&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 56</cross-reference> is a flowchart showing one example of a processing procedure of obtaining text information with sound recognition; </paragraph>
<paragraph id="P-0104" lvl="0"><number>&lsqb;0104&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 57</cross-reference> is a flowchart showing one example of a processing procedure of preparing text information; </paragraph>
<paragraph id="P-0105" lvl="0"><number>&lsqb;0105&rsqb;</number> <cross-reference target="DRAWINGS">FIGS. 58A and 58B</cross-reference> are views for explaining a method of displaying text information; </paragraph>
<paragraph id="P-0106" lvl="0"><number>&lsqb;0106&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 59</cross-reference> is a view showing one example of a data structure of special reproduction control information for sound information; </paragraph>
<paragraph id="P-0107" lvl="0"><number>&lsqb;0107&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 60</cross-reference> is a view showing another example of a data structure of special reproduction control information for sound information; </paragraph>
<paragraph id="P-0108" lvl="0"><number>&lsqb;0108&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 61</cross-reference> is a view explaining a summary reproduction of the sound/music data; and </paragraph>
<paragraph id="P-0109" lvl="0"><number>&lsqb;0109&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 62</cross-reference> is a view explaining another summary reproduction of the sound/music data.</paragraph>
</section>
</brief-description-of-drawings>
<detailed-description>
<section>
<heading lvl="1">DETAILED DESCRIPTION OF THE INVENTION </heading>
<paragraph id="P-0110" lvl="0"><number>&lsqb;0110&rsqb;</number> Preferred embodiments of the present invention will now be described with reference to the accompanying drawings. </paragraph>
<paragraph id="P-0111" lvl="0"><number>&lsqb;0111&rsqb;</number> The embodiments relate to a reproduction of video contents having video data using special reproduction control information. The video data comprises a set of video frames (video frame group) constituting a motion picture. </paragraph>
<paragraph id="P-0112" lvl="0"><number>&lsqb;0112&rsqb;</number> The special reproduction control information is created from the video data by a special reproduction control information creating apparatus and attached to the video data. The special reproduction is reproduction by a method other than a normal reproduction. The special reproduction includes a double speed reproduction (or a high speed reproduction), jump reproduction (or jump continuous reproduction), and a trick reproduction. The trick reproduction includes a substituted reproduction, an overlapped reproduction, a slow reproduction and the like. The special reproduction control information is referred to when the special reproduction is executed in the video reproduction apparatus. </paragraph>
<paragraph id="P-0113" lvl="0"><number>&lsqb;0113&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 1</cross-reference> shows one example of a basic data structure of the special reproduction control information. </paragraph>
<paragraph id="P-0114" lvl="0"><number>&lsqb;0114&rsqb;</number> In this data structure, plural items of frame information &ldquo;i&rdquo; (i&equals;1 to N) are described in correspondence to the frame appearance order in the video data. Each frame information <highlight><bold>100</bold></highlight> includes a set of video location information <highlight><bold>101</bold></highlight> and display time control information <highlight><bold>102</bold></highlight>. The video location information <highlight><bold>101</bold></highlight> indicates a location of video data to be displayed at the time of special reproduction. The video data to be display may be one frame, a group of a plurality of continuous frames, or a group formed of a part of a plurality of continuous frames. The display time control information <highlight><bold>102</bold></highlight> forms the basis of calculating the display time of the video data. </paragraph>
<paragraph id="P-0115" lvl="0"><number>&lsqb;0115&rsqb;</number> In <cross-reference target="DRAWINGS">FIG. 1</cross-reference>, the frame information &ldquo;i&rdquo; is arranged in an order of the appearance of frames in the video data. When information indicating an order of frame information is described in the frame information &ldquo;i&rdquo;, the frame information &ldquo;i&rdquo; may be arranged and described in any order. </paragraph>
<paragraph id="P-0116" lvl="0"><number>&lsqb;0116&rsqb;</number> The reproduction rate information <highlight><bold>103</bold></highlight> attached to a plurality of items of frame information &ldquo;i&rdquo; shows the reproduction speed rate and is used for designating the reproduction at a speed several times higher than that corresponding to the display time as described by the display time control information <highlight><bold>102</bold></highlight>. However, the reproduction rate information <highlight><bold>103</bold></highlight> is not essential information. The information <highlight><bold>103</bold></highlight> may constantly be attached, not constantly be attached, or selectively attached. Even when the reproduction rate information <highlight><bold>103</bold></highlight> is attached, the information may not be used at the time of special reproduction. The reproduction rate information may constantly be used, may not constantly used, or is selectively used. </paragraph>
<paragraph id="P-0117" lvl="0"><number>&lsqb;0117&rsqb;</number> In <cross-reference target="DRAWINGS">FIG. 1</cross-reference>, it is possible to further add other control information to the frame information group together with the reproduction rate information or in place of the reproduction rate information. In <cross-reference target="DRAWINGS">FIG. 1</cross-reference>, it is also possible to add different control information to each frame information &ldquo;i&rdquo;. In these cases, each information included in the special reproduction control information may be all used on the side of the video reproduction device, or a part of the information may be used. </paragraph>
<paragraph id="P-0118" lvl="0"><number>&lsqb;0118&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 2</cross-reference> shows an example of a structure of an apparatus for creating special reproduction control information. </paragraph>
<paragraph id="P-0119" lvl="0"><number>&lsqb;0119&rsqb;</number> This special reproduction control information creating device comprises a video data storage unit <highlight><bold>2</bold></highlight>, a video data processing unit <highlight><bold>1</bold></highlight> including a video location information processing unit <highlight><bold>11</bold></highlight> and a display time control information processing unit <highlight><bold>12</bold></highlight>, and a special reproduction control information storage unit <highlight><bold>3</bold></highlight>. In detail, as will be described later, since the video data (encoded data) is decoded to be video data before displaying, it takes a processing time required for decoding the video data from the display instruction is issued until the video is displayed. In order to extracted this processing time, it is proposed to decode the video data beforehand and store an image data file. </paragraph>
<paragraph id="P-0120" lvl="0"><number>&lsqb;0120&rsqb;</number> If an image data file is used (the image data file may be constantly used, or the image data file is selectively used), an image data file creating unit <highlight><bold>13</bold></highlight> (in the video data processing unit <highlight><bold>1</bold></highlight>) and an image data file storage unit <highlight><bold>14</bold></highlight> are further provided as shown in <cross-reference target="DRAWINGS">FIG. 3</cross-reference>. If other control information is added which is determined on the basis of the video data to the special reproduction control information, the corresponding function is appropriately added to the inside of the video data processing unit <highlight><bold>1</bold></highlight>. </paragraph>
<paragraph id="P-0121" lvl="0"><number>&lsqb;0121&rsqb;</number> If an operation by a user is intervened in this processing, a GUI is used for displaying, for example, video data in frame units, and providing a function of receiving an input of an instruction by the user though omitted in <cross-reference target="DRAWINGS">FIGS. 2 and 3</cross-reference>. </paragraph>
<paragraph id="P-0122" lvl="0"><number>&lsqb;0122&rsqb;</number> In <cross-reference target="DRAWINGS">FIGS. 2 and 3</cross-reference>, a CPU, a memory, an external storage device, and a network communication device is provided when needed, and software such as driver software used when needed and an OS are not shown. </paragraph>
<paragraph id="P-0123" lvl="0"><number>&lsqb;0123&rsqb;</number> The video data storage unit <highlight><bold>2</bold></highlight> stores video data which becomes an target of processing for creating special reproduction control information (or special reproduction control information and image data files). </paragraph>
<paragraph id="P-0124" lvl="0"><number>&lsqb;0124&rsqb;</number> The special reproduction control information storage unit <highlight><bold>3</bold></highlight> stores special reproduction control information that has been created. </paragraph>
<paragraph id="P-0125" lvl="0"><number>&lsqb;0125&rsqb;</number> The image data file storage unit <highlight><bold>4</bold></highlight> stores image data files that have been created. </paragraph>
<paragraph id="P-0126" lvl="0"><number>&lsqb;0126&rsqb;</number> The storage units <highlight><bold>2</bold></highlight>, <highlight><bold>3</bold></highlight>, and <highlight><bold>4</bold></highlight> comprise, for example, a hard disk, an optical disk and a semiconductor memory. The storage units <highlight><bold>2</bold></highlight>, <highlight><bold>3</bold></highlight>, and <highlight><bold>4</bold></highlight> may comprise separate storage devices. All or part of the storage units may comprise the same storage device. </paragraph>
<paragraph id="P-0127" lvl="0"><number>&lsqb;0127&rsqb;</number> The video data processing unit <highlight><bold>1</bold></highlight> creates the special reproduction control information (or the special reproduction control information and image data file) on the basis of the video data which becomes an target of processing. </paragraph>
<paragraph id="P-0128" lvl="0"><number>&lsqb;0128&rsqb;</number> The video location information processing unit <highlight><bold>11</bold></highlight> determines (extracts) a video frame (group) which should be displayed or which can be displayed at the time of special reproduction to conduct processing of preparing the video location information <highlight><bold>101</bold></highlight> which should be described in each frame information &ldquo;i&rdquo;. </paragraph>
<paragraph id="P-0129" lvl="0"><number>&lsqb;0129&rsqb;</number> The display time control information processing unit <highlight><bold>102</bold></highlight> conducts a processing for preparing the display time control information <highlight><bold>102</bold></highlight> associated with the display time of the video frame (group) associated with each frame information &ldquo;i&rdquo;. </paragraph>
<paragraph id="P-0130" lvl="0"><number>&lsqb;0130&rsqb;</number> The image data file creating unit <highlight><bold>13</bold></highlight> conducts a processing for preparing an image data file from the video data. </paragraph>
<paragraph id="P-0131" lvl="0"><number>&lsqb;0131&rsqb;</number> The special reproduction control information creating apparatus can be realized, for example, in a form of conducting software on a computer. The apparatus may be realized as a dedicated apparatus for creating the special reproduction control information. </paragraph>
<paragraph id="P-0132" lvl="0"><number>&lsqb;0132&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 4</cross-reference> shows an example of a processing procedure in a case of a structure of <cross-reference target="DRAWINGS">FIG. 2</cross-reference>. The video data is read (step S<highlight><bold>11</bold></highlight>), video location information <highlight><bold>101</bold></highlight> is created (step S<highlight><bold>12</bold></highlight>), display time control information <highlight><bold>102</bold></highlight> is created (step S<highlight><bold>13</bold></highlight>), and special reproduction control information is stored (step S<highlight><bold>14</bold></highlight>). The procedure of <cross-reference target="DRAWINGS">FIG. 4</cross-reference> may be consecutively conducted for each frame information, and each processing may be conducted in batches. The other procedures can also be conducted. </paragraph>
<paragraph id="P-0133" lvl="0"><number>&lsqb;0133&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 5</cross-reference> shows an example of a processing procedure in a case of the structure of <cross-reference target="DRAWINGS">FIG. 3</cross-reference>. A procedure for preparing and storing image data files is added to a procedure of <cross-reference target="DRAWINGS">FIG. 4</cross-reference> (step S<highlight><bold>22</bold></highlight>). The image data file is created and/or stored together with the preparation of the video location information <highlight><bold>101</bold></highlight>. It is also possible to create the video location information <highlight><bold>101</bold></highlight> at a timing different from that of <cross-reference target="DRAWINGS">FIG. 4</cross-reference>. In the same manner as the case of <cross-reference target="DRAWINGS">FIG. 4</cross-reference>, the procedure of <cross-reference target="DRAWINGS">FIG. 5</cross-reference> may be conducted for each frame information, or may be conducted in batches. The other procedures can also be conducted. </paragraph>
<paragraph id="P-0134" lvl="0"><number>&lsqb;0134&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 6</cross-reference> shows an example of a video reproduction apparatus. </paragraph>
<paragraph id="P-0135" lvl="0"><number>&lsqb;0135&rsqb;</number> This video reproduction apparatus comprises a controller <highlight><bold>21</bold></highlight>, a normal reproduction processing unit <highlight><bold>22</bold></highlight>, a special reproduction processing unit <highlight><bold>23</bold></highlight>, a display device <highlight><bold>24</bold></highlight>, and a contents storage unit <highlight><bold>25</bold></highlight>. If contents are handled wherein audio such as sound or the like is added to the video data, it is preferable to provide a sound output section. If contents are handled wherein text data is added to the video data, the text may be displayed on the display device <highlight><bold>24</bold></highlight>, or may be output from the sound output section. If contents are handled wherein a program is attached, an attached program execution section may be provided. </paragraph>
<paragraph id="P-0136" lvl="0"><number>&lsqb;0136&rsqb;</number> The contents storage unit <highlight><bold>25</bold></highlight> stores at least video data and special reproduction control information. In detail, as will be described later, in the case where the image data file is used, the image data file is further stored. The sound data, the text data, and the attached program are further stored in some cases. </paragraph>
<paragraph id="P-0137" lvl="0"><number>&lsqb;0137&rsqb;</number> The contents storage unit <highlight><bold>25</bold></highlight> may be arranged at one location in a concentrated manner, or may be arranged in a distributed manner. The point is that the contents can be accessed with the normal reproduction processing unit <highlight><bold>22</bold></highlight> and special reproduction processing unit <highlight><bold>23</bold></highlight>. The video data, special reproduction control information, image data files, sound data, text data, and attached program may be stored in separate media or may be stored in the same medium. As the medium, for example, DVD is used. These may be data which are transmitted via a network. </paragraph>
<paragraph id="P-0138" lvl="0"><number>&lsqb;0138&rsqb;</number> The controller <highlight><bold>21</bold></highlight> basically receives an instruction such as a normal reproduction and a special reproduction with respect to the contents from the user via a user interface such as a GUI or the like. The controller <highlight><bold>21</bold></highlight> controls for giving to the corresponding processing unit an instruction of reproduction by means of a method designated with respect to the designated contents. </paragraph>
<paragraph id="P-0139" lvl="0"><number>&lsqb;0139&rsqb;</number> The normal reproduction processing unit <highlight><bold>22</bold></highlight> is used for the normal reproduction of the designated contents. </paragraph>
<paragraph id="P-0140" lvl="0"><number>&lsqb;0140&rsqb;</number> The special reproduction processing unit <highlight><bold>23</bold></highlight> is used for the special reproduction (for example, a high speed reproduction, jump reproduction, trick reproduction, or the like) of the designated contents by referring to the special reproduction control information. </paragraph>
<paragraph id="P-0141" lvl="0"><number>&lsqb;0141&rsqb;</number> The display device <highlight><bold>24</bold></highlight> is used for displaying a video. </paragraph>
<paragraph id="P-0142" lvl="0"><number>&lsqb;0142&rsqb;</number> The video reproduction apparatus can be realized by computer software. It may partially be realized by hardware (for example, decode board (MPEG-2 decoder) or the like). The video reproduction apparatus may be realized as a dedicated device for video reproduction. </paragraph>
<paragraph id="P-0143" lvl="0"><number>&lsqb;0143&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 7</cross-reference> shows one example of a reproduction processing procedure of the video reproduction apparatus of <cross-reference target="DRAWINGS">FIG. 6</cross-reference>. At step S<highlight><bold>31</bold></highlight>, it is determined whether user requests a normal reproduction or a special reproduction. When a normal reproduction is requested, the designated video data is read at step S<highlight><bold>32</bold></highlight> and a normal reproduction is conducted at step S<highlight><bold>33</bold></highlight>. When a special reproduction is requested from the user, the special reproduction control information corresponding to the designated video data is read at step S<highlight><bold>34</bold></highlight>, the location of the video data to be displayed is specified and the display time is determined at step S<highlight><bold>35</bold></highlight>. The corresponding frame (group) is read from the video data (or the image data file) at step S<highlight><bold>36</bold></highlight> to conduct special reproduction of the designated contents at step S<highlight><bold>37</bold></highlight>. The location of the video data can be specified and the display time can be determined at a timing different from that in <cross-reference target="DRAWINGS">FIG. 7</cross-reference>. The procedure of the special reproduction of <cross-reference target="DRAWINGS">FIG. 7</cross-reference> may be consecutively conducted for each frame information, or each processing may be conducted in batches. Other procedures can be conducted. For example, in the case of the reproduction method in which the display time of each frame is equally set to a constant value, it is not necessary to determine the display time. </paragraph>
<paragraph id="P-0144" lvl="0"><number>&lsqb;0144&rsqb;</number> Both in the normal reproduction and in the special reproduction, the user may demand various designations (for example, the start point of the reproduction or the end point of the reproduction in the contents, a reproduction speed in the high speed reproduction, and reproduction time in the high speed reproduction, and other method, such as special reproduction or the like). </paragraph>
<paragraph id="P-0145" lvl="0"><number>&lsqb;0145&rsqb;</number> Next, an algorithm for creating the frame information of the special reproduction control information and an algorithm for calculating the display time of the special reproduction will be schematically explained. </paragraph>
<paragraph id="P-0146" lvl="0"><number>&lsqb;0146&rsqb;</number> At the time of creating the frame information, the frame information to be used at the time of the special reproduction is determined from the video data, the video location information is created, and the display time control information is created. </paragraph>
<paragraph id="P-0147" lvl="0"><number>&lsqb;0147&rsqb;</number> The frame is determined by such methods as; 1) a method for calculating the video frame on the basis of some characteristic quantity with respect to the video data (for example, a method for extracting the video frames such that the total of characteristic quantity (for example, the scene change quantity) between the extracted frames becomes constant and a method for extracting the video frames such that the total of importance between the extracted frames becomes constant), and (2) a method for calculating the video frame on a fixed standard (for example, a method for extracting frames at random, and a method for extracting frames at an equal interval). The scene change quantity is also called as a frame activity value. </paragraph>
<paragraph id="P-0148" lvl="0"><number>&lsqb;0148&rsqb;</number> In the creation of the display time control information <highlight><bold>121</bold></highlight>, there are available; (i) a method for calculating an absolute value or a relative value of the display time or a display frame number, (ii) a method for calculating reference information which is a base of the display time and a display frame number (for example, the information designated by the user, characters in the video, sound synchronized with video, and persons in the video, and the importance obtained on the basis of the specific pattern in the video), (iii) a method for describing both (i) and (ii). </paragraph>
<paragraph id="P-0149" lvl="0"><number>&lsqb;0149&rsqb;</number> It is possible to appropriately combine (1) or (2) and (i), (ii) or (iii). Needless to say, other methods can be possible. One specific combination out of such methods can be used, and a plurality of combinations of these methods may be used and can be appropriately selected. </paragraph>
<paragraph id="P-0150" lvl="0"><number>&lsqb;0150&rsqb;</number> In a specific case, at the same time with the determination of the frame at the method (1), a relative value of the display time and the number of display frames are determined. If this method is constantly used, it is possible to omit the display time control information processing unit <highlight><bold>102</bold></highlight>. </paragraph>
<paragraph id="P-0151" lvl="0"><number>&lsqb;0151&rsqb;</number> At the time of the special reproduction, it is assumed that the special reproduction is conducted by referring to the display time control information <highlight><bold>121</bold></highlight> of (i), (ii) or (iii) included in the frame information. However, the described value may be followed or the described value may be corrected and used. In addition to the described value and the corrected value thereof, independently created other information, and information input from the user may be used. Alternatively, only the independently created other information and the information input from the user may be used. A plurality of methods out of these methods are enabled and can be appropriately selected. </paragraph>
<paragraph id="P-0152" lvl="0"><number>&lsqb;0152&rsqb;</number> Next, an outline of the special reproduction will be explained. </paragraph>
<paragraph id="P-0153" lvl="0"><number>&lsqb;0153&rsqb;</number> A double speed reproduction (or a high speed reproduction) carries out reproduction in a time shorter than the time required for the normal reproduction of the original contents by reproducing a part of the frames out of the whole frames constituting the video data contents. For example, the frames indicated by the frame information are displayed for each display time indicated by the display time control information <highlight><bold>121</bold></highlight>, in the order of time sequence. Based on a request from the user, such as a speed designation request for designating at what times speed of the normal reproduction the original contents are reproduced (in what factor of the time required for the normal reproduction the original contents are reproduced) and a time designation request for designating how much time is taken for reproducing the contents, the display time of each frame (group) is determined to satisfy the reproduction request. The high speed reproduction is called a summarized reproduction. </paragraph>
<paragraph id="P-0154" lvl="0"><number>&lsqb;0154&rsqb;</number> A jump reproduction (or a jump continuous reproduction) is such that a part of the frame shown in the frame information is subjected to non-reproduction, for example, on the basis of the reproduction/non-reproduction information described later in the high speed reproduction. The high speed reproduction is conducted with respect to the frame excluding the frame which is subjected to non-reproduction out of the frames shown in shown in the frame information. </paragraph>
<paragraph id="P-0155" lvl="0"><number>&lsqb;0155&rsqb;</number> A trick reproduction excludes from the reproduction except for the normal reproduction the high speed reproduction and the jump reproduction. For example, at the time of reproducing the frame shown in the frame information, there can be considered various forms such as a substituted reproduction for reproducing a certain portion by replacing the order of time sequence, an overlapped reproduction for reproducing a certain portion repeatedly a plurality of times at the time of reproducing the frame shown in frame information, a variable speed reproduction in which at the time of reproducing the frame shown in the frame information, a certain portion is reproduced at a speed lower than the reproduction of another portion (including the case in which the portion is reproduced at the speed of normal reproduction, or the case in which the portion is reproduced at a speed lower than the normal reproduction time) or at a speed higher than another portion, or the reproduction of a certain portion is temporarily suspended, or such forms of reproduction are appropriately combined, a random reproduction for reproducing at a random time sequence for each of a constant set of frames shown in the frame information. </paragraph>
<paragraph id="P-0156" lvl="0"><number>&lsqb;0156&rsqb;</number> Needless to say, it is possible to appropriately combine a plurality of kinds of methods. For example, at the time of the double speed, the important portion is reproduced a plurality of times, and various variations are considered such as a method for setting a reproduction speed to a normal reproduction speed. </paragraph>
<paragraph id="P-0157" lvl="0"><number>&lsqb;0157&rsqb;</number> Hereinafter, embodiments of the present invention will be specifically explained in detail. </paragraph>
<paragraph id="P-0158" lvl="0"><number>&lsqb;0158&rsqb;</number> In the beginning, the embodiments will be explained by taking as an example a case in which a reproduction frame is determined on the basis of the scene change quantity between adjacent frames as the characteristic quantity of the video data. </paragraph>
<paragraph id="P-0159" lvl="0"><number>&lsqb;0159&rsqb;</number> Here, there will be explained a case in which one frame is corresponded to one frame information. </paragraph>
<paragraph id="P-0160" lvl="0"><number>&lsqb;0160&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 8</cross-reference> shows one example of a data structure of the special reproduction control information created under the target video data. </paragraph>
<paragraph id="P-0161" lvl="0"><number>&lsqb;0161&rsqb;</number> The data structure is such that the display time information <highlight><bold>121</bold></highlight> is described which is information showing an absolute or a relative display time as display time control information <highlight><bold>102</bold></highlight> in <cross-reference target="DRAWINGS">FIG. 1</cross-reference> (or instead of the display time control information <highlight><bold>102</bold></highlight>). A structure describing the importance in addition to the display time control information <highlight><bold>102</bold></highlight> will be described later. </paragraph>
<paragraph id="P-0162" lvl="0"><number>&lsqb;0162&rsqb;</number> The video location information <highlight><bold>101</bold></highlight> is information which enables the specification of the location in the original video frame of the video, and any of a frame number (for example, a sequence number from the first frame) or a number which specifies one frame in a stream like a time stamp may be used. If the video data corresponding to the frame extracted from the original video stream is set as a separate frame, a URL or the like may be used as information for specifying the file location. </paragraph>
<paragraph id="P-0163" lvl="0"><number>&lsqb;0163&rsqb;</number> The display time information <highlight><bold>121</bold></highlight> is information which specifies the time for displaying the video or the number of frames. It is possible to describe actual time or the number of frames as a unit and a relative value (for example, a normalized numeric value) which clarifies a relationship of the relative time length with the display time information described in other frame information. In the latter case, the actual reproduction time of each video is calculated from the total reproduction time as a whole. With respect to each video, the continuation time of the display is not described, but such description with a combination of a start time starting from a specific timing (for example, the start time of the first video is set to 0), and the end time and a description with a combination of the start time and the continuation time may be used. </paragraph>
<paragraph id="P-0164" lvl="0"><number>&lsqb;0164&rsqb;</number> In the special reproduction, basically the reproduction of the video present at a location specified with the video location information <highlight><bold>101</bold></highlight> only for the display time specified with the display time information <highlight><bold>121</bold></highlight> is consecutively conducted only for the number of the items of frame information &ldquo;i&rdquo; included in the arrangement, such as shown in <cross-reference target="DRAWINGS">FIG. 8</cross-reference>. </paragraph>
<paragraph id="P-0165" lvl="0"><number>&lsqb;0165&rsqb;</number> If the start time and the end time or the continuation time are specified and this designation is followed, the video present at the location specified with the video location information <highlight><bold>101</bold></highlight> is consecutively reproduced from the start time specified with the display time information <highlight><bold>121</bold></highlight> up to the end time or during the continuation time only for the number of items of the frame information &ldquo;i&rdquo; included in the arrangement. </paragraph>
<paragraph id="P-0166" lvl="0"><number>&lsqb;0166&rsqb;</number> The described display time can be processed and reproduced by using parameters such as reproduction rata information and additional information. </paragraph>
<paragraph id="P-0167" lvl="0"><number>&lsqb;0167&rsqb;</number> Next, a method for describing the video location information will be explained by using <cross-reference target="DRAWINGS">FIGS. 9 through 11</cross-reference>. </paragraph>
<paragraph id="P-0168" lvl="0"><number>&lsqb;0168&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 9</cross-reference> explains a method for describing the video location information referring to the original video frame. </paragraph>
<paragraph id="P-0169" lvl="0"><number>&lsqb;0169&rsqb;</number> In <cross-reference target="DRAWINGS">FIG. 9, a</cross-reference> time axis <highlight><bold>200</bold></highlight> corresponds to the original video stream based on which the frame information for the special reproduction is created and a video <highlight><bold>201</bold></highlight> corresponds to one frame which becomes a description target in the video stream. A time axis <highlight><bold>202</bold></highlight> corresponds to reproduction time of a video at the time of the special reproduction by using the video <highlight><bold>201</bold></highlight> extracted from the original video stream. A display time <highlight><bold>203</bold></highlight> is a section corresponding to one video <highlight><bold>201</bold></highlight> included in the display time <highlight><bold>203</bold></highlight>. For example, the video location information <highlight><bold>101</bold></highlight> showing the location of the video <highlight><bold>201</bold></highlight> and the video display time <highlight><bold>121</bold></highlight> showing the length of the display time <highlight><bold>203</bold></highlight> are described as frame information. As described above, the description on the location of the video <highlight><bold>201</bold></highlight> may be given in any form such as a frame number, a time stamp or the like as long as one frame in the original video stream can be specified. This frame information will be described in the same manner with respect to the other videos <highlight><bold>201</bold></highlight>. </paragraph>
<paragraph id="P-0170" lvl="0"><number>&lsqb;0170&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 10</cross-reference> explains a method for describing the video location information referring to the image data file. </paragraph>
<paragraph id="P-0171" lvl="0"><number>&lsqb;0171&rsqb;</number> The method for describing the video location information shown in <cross-reference target="DRAWINGS">FIG. 9</cross-reference> directly refers to the frame in the original data frame which is to be subjected to the special reproduction. The method for describing the video location information shown in <cross-reference target="DRAWINGS">FIG. 10</cross-reference> is a method in which an image data file <highlight><bold>300</bold></highlight> corresponding to a single frame <highlight><bold>302</bold></highlight> extracted from the original video stream is created in a separate file, and the location thereof is described. A method for describing the file location can be handled in the same manner by using, for example, the URL or the like both in the case where the file is present on a local storage device and in the case where the file is present on the network. A set of the video location information <highlight><bold>101</bold></highlight> showing the location of this image data file and the video display time <highlight><bold>121</bold></highlight> showing the length of the corresponding display time <highlight><bold>301</bold></highlight> is described as frame information. </paragraph>
<paragraph id="P-0172" lvl="0"><number>&lsqb;0172&rsqb;</number> If a correspondence to the original video frame is required, the information (similar to the video location information in the case of, for example, <cross-reference target="DRAWINGS">FIG. 9</cross-reference>) showing a single frame <highlight><bold>302</bold></highlight> of the original video corresponding to the described frame information may be included in the frame information. The frame information may comprise the video location information, the display time information and the original video information. When the original video information is not required, it is not required to describe the original video. </paragraph>
<paragraph id="P-0173" lvl="0"><number>&lsqb;0173&rsqb;</number> The configuration of the video data described with the method of <cross-reference target="DRAWINGS">FIG. 10</cross-reference> is not particularly restricted. For example, the frame of the original video may be used as it is or may be reduced. This is effective for conducting a reproduction processing at a high speed because it is not required to develop the original video. </paragraph>
<paragraph id="P-0174" lvl="0"><number>&lsqb;0174&rsqb;</number> If the original video stream is compressed by means of MPEG-1 or MPEG-2 or the like, a reduced video can be created at a high speed only by partially decoding the streams. In this method, only the DCT (the discrete cosine conversion) coefficients of an I picture frame encoded within the frame (an inner-frame encoded frame) is decoded and a reduced video is created by using the DCT coefficients. </paragraph>
<paragraph id="P-0175" lvl="0"><number>&lsqb;0175&rsqb;</number> In the description method of <cross-reference target="DRAWINGS">FIG. 10</cross-reference>, the image data files are stored in separate files. However, these files may be stored in a package in a video data group storage file having a video format (for example, a motion JPEG) which can be accessed at random. The location of the video data is specified by a combination of the URL showing the location of the image data file, a frame number or a time stamp showing the location in the image data file. The URL information showing the location of the image data file may be described in each frame information or may be described as additional information outside of the arrangement of the frame information. </paragraph>
<paragraph id="P-0176" lvl="0"><number>&lsqb;0176&rsqb;</number> Various methods can be taken to select the frame of the original video or the like and create the video data to describe the video location information. For example, the video data may be extracted at an equal interval from the original video. Where the motion of the screen quite often appears, the video data is selected in a narrow interval. Where the motion of the screen quite rarely appears, the video frame is selected in a wide interval. </paragraph>
<paragraph id="P-0177" lvl="0"><number>&lsqb;0177&rsqb;</number> Here, referring to <cross-reference target="DRAWINGS">FIG. 11</cross-reference>, there will be explained a method in which as one example of a method for selecting frames, the frame is selected in a narrow interval where the motion of the screen quite often appears while the frame is selected in a wide interval where the motion of the screen rarely appears. </paragraph>
<paragraph id="P-0178" lvl="0"><number>&lsqb;0178&rsqb;</number> In <cross-reference target="DRAWINGS">FIG. 11, a</cross-reference> horizontal axis represents the selected frame number, and a curve <highlight><bold>800</bold></highlight> represents a change in the scene change quantity (between adjacent frames). A method for calculating the scene change quantity is the same as a method at the time of calculating the display time described later. Here, in order to determine an extraction interval in accordance with the motion of the scene, there is shown a method for calculating an interval at which the scene change quantity between video frames from which the video data is extracted becomes constant. The total of the scene change quantity between video frames from which the video data is extracted is set to S<highlight><subscript>i</subscript></highlight>, and the total of the scene change quantity in the whole frame is set to S (&equals;&Sgr;S<highlight><subscript>i</subscript></highlight>) while the number of data items to be extracted is n. In order to set the video change quantity between video frames from which video data is extracted to a constant level, S<highlight><subscript>i</subscript></highlight>&equals;S/n may be provided. In <cross-reference target="DRAWINGS">FIG. 11</cross-reference>, the area Si of the scene change quantity curve <highlight><bold>800</bold></highlight> divided with the broken lines becomes constant. Then, for example, the scene change quantity is accumulated from the extracted frame, so that the video frame having the value exceeding the S/n is set as the frame F<highlight><subscript>i </subscript></highlight>from which the video data is extracted. </paragraph>
<paragraph id="P-0179" lvl="0"><number>&lsqb;0179&rsqb;</number> If the video data is created by I picture frame of MPEG, the video frame from which the calculated video data is created is not necessarily the I picture, the video data is created from the I picture frame in the vicinity thereof. </paragraph>
<paragraph id="P-0180" lvl="0"><number>&lsqb;0180&rsqb;</number> By the way, in the method explained in <cross-reference target="DRAWINGS">FIG. 11</cross-reference>, the video frame which belongs to the section of the scene change quantity&equals;0 is skipped. However, if a still picture continues, the scene is important in many cases. Then, if the scene change quantity&equals;0 continues for more than a constant time, the frame at that time may be extracted. For example, the scene change quantity may be accumulated from the extracted frame so that the frame having the value exceeding S/n or the frame at which the scene change quantity&equals;0 continues for more than a constant time may be set as a frame F<highlight><subscript>i </subscript></highlight>from which the video data is extracted. The accumulated value of the scene change quantity may be or may not be cleared to 0. It is possible to selectively clear the accumulated value based on a request from the user. </paragraph>
<paragraph id="P-0181" lvl="0"><number>&lsqb;0181&rsqb;</number> In the case of an example of <cross-reference target="DRAWINGS">FIG. 11</cross-reference>, it is assumed that the display time information <highlight><bold>121</bold></highlight> is described so that the display time becomes the same with respect to any of the frames. When the video is reproduced in accordance with this display time information <highlight><bold>121</bold></highlight>, the scene change quantity becomes constant. The display time information <highlight><bold>121</bold></highlight> may be determined and described in a separate method. </paragraph>
<paragraph id="P-0182" lvl="0"><number>&lsqb;0182&rsqb;</number> Next, there will be explained a case in which one or a plurality of frames are allowed to correspond to one frame information. </paragraph>
<paragraph id="P-0183" lvl="0"><number>&lsqb;0183&rsqb;</number> One example of the data structure of the special reproduction information in this case is the same as that in <cross-reference target="DRAWINGS">FIG. 8</cross-reference>. </paragraph>
<paragraph id="P-0184" lvl="0"><number>&lsqb;0184&rsqb;</number> Hereinafter, a method for describing the video location information will be explained by using <cross-reference target="DRAWINGS">FIGS. 12 through 14</cross-reference>. </paragraph>
<paragraph id="P-0185" lvl="0"><number>&lsqb;0185&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 12</cross-reference> explains a method for describing the video location information for referring to the continuous frames of the original video. </paragraph>
<paragraph id="P-0186" lvl="0"><number>&lsqb;0186&rsqb;</number> A method for describing the video location information shown in <cross-reference target="DRAWINGS">FIG. 9</cross-reference> refers to one frame <highlight><bold>201</bold></highlight> in one original video for conducting the special reproduction. However, the method for describing the video location information shown in <cross-reference target="DRAWINGS">FIG. 12</cross-reference> describes a set <highlight><bold>500</bold></highlight> of a plurality of continuous frames in the original video. The set <highlight><bold>500</bold></highlight> of frames may include some frames extracted from the plural continuous frames within the original video. The set <highlight><bold>500</bold></highlight> of frames may include only one frame. </paragraph>
<paragraph id="P-0187" lvl="0"><number>&lsqb;0187&rsqb;</number> If the set <highlight><bold>500</bold></highlight> of frames includes a plurality of continuous frames or one frame in the original video, the location of the start frame and the location of the end frame are described, or the location of the start frame and the continuation time of the set <highlight><bold>500</bold></highlight> are described in the description of the frame location (if one frame is included, for example, the start frame is set equal to the end frame). In the description of the location and the time, the frame number and the time stamp and the like are used which can specify frames in the streams. </paragraph>
<paragraph id="P-0188" lvl="0"><number>&lsqb;0188&rsqb;</number> If the set <highlight><bold>500</bold></highlight> of frames is a part out of a plurality of continuous frames in the original video, information is described which enables the specification of the frames. If the method for extracting the frames is determined, and the specification of the frames can be specified with the description of the locations of the start frame and the end frame, the start frame or the end frame may be described. </paragraph>
<paragraph id="P-0189" lvl="0"><number>&lsqb;0189&rsqb;</number> The display time information <highlight><bold>501</bold></highlight> shows the total display time corresponding to the whole frame group included in the corresponding frame set <highlight><bold>500</bold></highlight>. The display time of each frame included in the set <highlight><bold>500</bold></highlight> of frames can be appropriately determined on the side of device for the special reproduction. As a simple method, there is available a method in which the above total display time is equally divided with the total number of frames in the set <highlight><bold>500</bold></highlight> to provide one frame display time. Various other methods are available. </paragraph>
<paragraph id="P-0190" lvl="0"><number>&lsqb;0190&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 13</cross-reference> explains a method for describing video location information for referring to a set of the image data files. </paragraph>
<paragraph id="P-0191" lvl="0"><number>&lsqb;0191&rsqb;</number> The method for describing the video location information shown in <cross-reference target="DRAWINGS">FIG. 12</cross-reference> directly refers to continuous frames in the original video to be reproduced. A method for describing the video location information shown in <cross-reference target="DRAWINGS">FIG. 13</cross-reference> creates a set <highlight><bold>600</bold></highlight> of the image data files corresponding to the original video frame set <highlight><bold>602</bold></highlight> extracted from the original video stream in a separate file and describes the location thereof. In the method for describing the file location, the file can be handled in the same manner by using, for example, URL or the like, even if the file is present on a local storage device or if the file is present on a network. A set of the video location information <highlight><bold>101</bold></highlight> showing the location of this image data file and the video display time <highlight><bold>121</bold></highlight> showing a length of the corresponding display time <highlight><bold>601</bold></highlight> can be described as the frame information. </paragraph>
<paragraph id="P-0192" lvl="0"><number>&lsqb;0192&rsqb;</number> If a correspondence with the original frame is required, information showing the frame set <highlight><bold>602</bold></highlight> of the original video corresponding to the described frame information (for example, information similar to the video location information in the case of <cross-reference target="DRAWINGS">FIG. 12</cross-reference>) may be included in the frame information. The frame information may comprise the video location information, the display time information and the original video information. The original video information is not required to be described when the information is not required. </paragraph>
<paragraph id="P-0193" lvl="0"><number>&lsqb;0193&rsqb;</number> The configuration of the video data, the preparation of the video data, the preparation of the reduced video, the method for storing the video data and the method for describing the location information such as the URL or the like are the same as what has been described above. </paragraph>
<paragraph id="P-0194" lvl="0"><number>&lsqb;0194&rsqb;</number> Various methods can be adopted in the same manner as described above as to which frame of the original video is selected to create the video data to be described in the video location information. For example, the video data may be extracted at an equal interval from the original video. Where a motion of the screen quite often appears, a frame is extracted in a narrow interval. Where the motion of the screen rarely appears, a frame is extracted in a wide interval. </paragraph>
<paragraph id="P-0195" lvl="0"><number>&lsqb;0195&rsqb;</number> In the above embodiments, the image data file <highlight><bold>300</bold></highlight> is corresponded to the original video <highlight><bold>302</bold></highlight> in a frame to frame manner. It is possible to make the location information of the frame described as the original video information have a time width. </paragraph>
<paragraph id="P-0196" lvl="0"><number>&lsqb;0196&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 14</cross-reference> shows an example in which the original video information is allowed to have a time width with respect to the <cross-reference target="DRAWINGS">FIG. 8</cross-reference>. An original video information <highlight><bold>3701</bold></highlight> is added to the frame information structure shown in <cross-reference target="DRAWINGS">FIG. 8</cross-reference>. The original video information <highlight><bold>3701</bold></highlight> comprises a start point information <highlight><bold>3702</bold></highlight> and a section length information <highlight><bold>3703</bold></highlight> which are the start point and the section length of the original video which is a target of the special reproduction. The original video information <highlight><bold>3701</bold></highlight> comprises any information which can specify the section of the original video having the time width. It may comprise the start point information and an end point information in stead of the start point information and the length information. </paragraph>
<paragraph id="P-0197" lvl="0"><number>&lsqb;0197&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 15</cross-reference> shows an example in which the original video information is allowed to have a time width with respect to the <cross-reference target="DRAWINGS">FIG. 9</cross-reference>. In this case, for example, as video location information, display time information and original video information included in the same frame information, the location of the original video frame <highlight><bold>3801</bold></highlight>, the display time <highlight><bold>3802</bold></highlight>, and the original video frame section <highlight><bold>3803</bold></highlight> which comprises the start point (frame location) and the section length are described to show that these correspond to each other. That is, as a video representative of the original video frame section <highlight><bold>3803</bold></highlight>, the original video frame location <highlight><bold>3801</bold></highlight> described in the video location information is displayed. </paragraph>
<paragraph id="P-0198" lvl="0"><number>&lsqb;0198&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 16</cross-reference> shows an example in which the original information is allowed to have a time width with respect to the <cross-reference target="DRAWINGS">FIG. 10</cross-reference>. In this case, for example, as video location information, display time information and original video information included in the same frame information, the location of the image data file <highlight><bold>3901</bold></highlight> for the display, the display time <highlight><bold>3902</bold></highlight>, and the original video frame section <highlight><bold>3903</bold></highlight> which comprises the start point (frame location) and the section length are described to show that these correspond to each other. </paragraph>
<paragraph id="P-0199" lvl="0"><number>&lsqb;0199&rsqb;</number> That is, as a video representative of the original video frame section <highlight><bold>3903</bold></highlight>, the image <highlight><bold>3901</bold></highlight> in the image data file described in the video location information is displayed. </paragraph>
<paragraph id="P-0200" lvl="0"><number>&lsqb;0200&rsqb;</number> Furthermore, as shown in <cross-reference target="DRAWINGS">FIGS. 12 and 13</cross-reference>, if a set of frames is used as a video for the display, a section different from the original video frame section for displaying the video may be allowed to correspond to the original video information. </paragraph>
<paragraph id="P-0201" lvl="0"><number>&lsqb;0201&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 17</cross-reference> shows an example in which the original video information is allowed to have a time width with respect to the <cross-reference target="DRAWINGS">FIG. 12</cross-reference>. In this case, for example, as video location information, display time information and original video information included in the same frame information, a set <highlight><bold>4001</bold></highlight> of frames in the original video, the display time <highlight><bold>4002</bold></highlight>, and the original video frame section <highlight><bold>4003</bold></highlight> which comprises the start point (frame location) and the section length are described to show that these correspond to each other. </paragraph>
<paragraph id="P-0202" lvl="0"><number>&lsqb;0202&rsqb;</number> At this time, the section <highlight><bold>4001</bold></highlight> of a set of frames which are described as video location information, and the original video frame section <highlight><bold>4003</bold></highlight> which is described as the original video information are not necessarily required to coincide with each other and a different section may be used for display. </paragraph>
<paragraph id="P-0203" lvl="0"><number>&lsqb;0203&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 18</cross-reference> shows an example in which the original video information is allowed to have a time width with respect to the <cross-reference target="DRAWINGS">FIG. 13</cross-reference>. In this case, for example, as video location information, display time information and original video information included in the same frame information, a set <highlight><bold>4101</bold></highlight> of frames in the video file, the display time <highlight><bold>4102</bold></highlight>, and the original video frame section <highlight><bold>4103</bold></highlight> which comprises the start point (frame location) and the section length are described to show that these correspond to each other. </paragraph>
<paragraph id="P-0204" lvl="0"><number>&lsqb;0204&rsqb;</number> At this time, the section of a set <highlight><bold>4101</bold></highlight> of frames described as video location information, and the original video frame section <highlight><bold>4103</bold></highlight> described as the original video are not necessarily required to coincide with each other. That is, the section of the set <highlight><bold>4101</bold></highlight> of the frames for the display may be shorter or longer than the original video frame section <highlight><bold>4103</bold></highlight>. Furthermore, a video having completely different contents may be included therein. In addition, only particularly important section may be extracted from the section described in the original video location as the image data file so that collected video data is used. </paragraph>
<paragraph id="P-0205" lvl="0"><number>&lsqb;0205&rsqb;</number> At the time of displaying the videos based on, for example, the summarized reproduction (special reproduction) using these items of the frame information, it may be desired that the corresponding frame in the original video is referred to. </paragraph>
<paragraph id="P-0206" lvl="0"><number>&lsqb;0206&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 19</cross-reference> shows a flow for starting the reproduction from the frame of the original video corresponding to the video frame displayed in special reproduction. At step S<highlight><bold>3601</bold></highlight>, the reproduction start frame is specified in the special reproduction. At step S<highlight><bold>3602</bold></highlight>, the original video frame corresponding to the specified frame is calculated with a method described later. At step S<highlight><bold>3603</bold></highlight>, the original video is reproduced from the calculated frames. </paragraph>
<paragraph id="P-0207" lvl="0"><number>&lsqb;0207&rsqb;</number> This flow can be used for referring to the corresponding location of the original video in addition to special reproduction. </paragraph>
<paragraph id="P-0208" lvl="0"><number>&lsqb;0208&rsqb;</number> At step S<highlight><bold>3602</bold></highlight>, as one example of a method for calculating the corresponding original video frame, there is shown a method for using the proportional distribution with respect to display time of the specified frame. The display time information included in the i-th frame information is set to D<highlight><subscript>i </subscript></highlight>sec, the section start location of the original video information is set to t<highlight><subscript>i </subscript></highlight>sec, and the section length is set to d<highlight><subscript>i </subscript></highlight>sec. If the location is specified at which t sec has passed from the start of the reproduction using the i-th frame information, the frame location of the corresponding original video is T&equals;t<highlight><subscript>i</subscript></highlight>&plus;d<highlight><subscript>i</subscript></highlight>&times;t/D<highlight><subscript>i</subscript></highlight>. </paragraph>
<paragraph id="P-0209" lvl="0"><number>&lsqb;0209&rsqb;</number> Referring to <cross-reference target="DRAWINGS">FIGS. 20 and 21</cross-reference>, as examples of a method for selecting a frame, there will be explained a method for extracting the frame in a narrow interval where the motion of the screen quite often appears while extracting the frame in a wide interval where the motion of the screen rarely appears in accordance with the motion of the screen. The horizontal axis, the curve <highlight><bold>800</bold></highlight>, and S<highlight><subscript>i </subscript></highlight>and F<highlight><subscript>i </subscript></highlight>are the same as those in <cross-reference target="DRAWINGS">FIG. 11</cross-reference>. </paragraph>
<paragraph id="P-0210" lvl="0"><number>&lsqb;0210&rsqb;</number> In the example of <cross-reference target="DRAWINGS">FIG. 11</cross-reference>, the video data is extracted one frame after another at an interval at which the scene change quantity between the frames from which the video data is extracted is made constant. <cross-reference target="DRAWINGS">FIGS. 20 and 21</cross-reference> show examples in which a set of a plurality of frames are extracted based on the frame F<highlight><subscript>i </subscript></highlight>as reference. For example, as shown in <cross-reference target="DRAWINGS">FIG. 20</cross-reference>, the same number of continuous frames may be extracted from F<highlight><subscript>i</subscript></highlight>. The frame length <highlight><bold>811</bold></highlight> and the frame length <highlight><bold>812</bold></highlight> equal to each other. As shown in <cross-reference target="DRAWINGS">FIG. 21</cross-reference>, the corresponding number of continuous frames may be extracted so that the total of the scene change quantity from F<highlight><subscript>i </subscript></highlight>becomes constant. The area <highlight><bold>813</bold></highlight> and the area <highlight><bold>814</bold></highlight> equal to each other. Various other methods can be considered. </paragraph>
<paragraph id="P-0211" lvl="0"><number>&lsqb;0211&rsqb;</number> It is possible to use the frame selection method in which the frame is extracted when the scene change quantity&equals;0 continues for more than a constant time. </paragraph>
<paragraph id="P-0212" lvl="0"><number>&lsqb;0212&rsqb;</number> As in the case of <cross-reference target="DRAWINGS">FIG. 11</cross-reference>, the display time information <highlight><bold>121</bold></highlight> may be described so that the same display time may be provided with respect to any of frame sets in the cases of <cross-reference target="DRAWINGS">FIGS. 20 and 21</cross-reference>. Alternatively, the display time information may be determined and described in a different method. </paragraph>
<paragraph id="P-0213" lvl="0"><number>&lsqb;0213&rsqb;</number> Next, one example of a processing for calculating the display time will be explained. </paragraph>
<paragraph id="P-0214" lvl="0"><number>&lsqb;0214&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 22</cross-reference> shows one example of a procedure of the basic processing for calculating the display time so that the scene change quantity becomes constant as much as possible when the video described in the video location information is continuously reproduced in accordance with time described in the display time information. </paragraph>
<paragraph id="P-0215" lvl="0"><number>&lsqb;0215&rsqb;</number> This processing can be applied to a case in which the frames are extracted in any method. For example, if the frames are extracted in a method shown in <cross-reference target="DRAWINGS">FIG. 11</cross-reference>, the processing can be omitted. Since the processing shown in <cross-reference target="DRAWINGS">FIG. 11</cross-reference> selects the frames such that the scene change quantity becomes constant when the frames are displayed for a fixed time period. </paragraph>
<paragraph id="P-0216" lvl="0"><number>&lsqb;0216&rsqb;</number> At step S<highlight><bold>71</bold></highlight>, the scene change quantity between adjacent frames is calculated with respect to all frames of the original video. If each frame of the video is represented in bit map, the differential value of the pixel between adjacent frames can be set to the scene change quantity. If the video is compressed with MPEG, the scene change quantity can be calculated by using a motion vector. </paragraph>
<paragraph id="P-0217" lvl="0"><number>&lsqb;0217&rsqb;</number> One example of a method for calculating the scene change quantity will be explained. </paragraph>
<paragraph id="P-0218" lvl="0"><number>&lsqb;0218&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 23</cross-reference> shows one example of a basic processing procedure for calculating a scene change quantity of all frames from the video streams compressed with MPEG. </paragraph>
<paragraph id="P-0219" lvl="0"><number>&lsqb;0219&rsqb;</number> At step S<highlight><bold>81</bold></highlight>, a motion vector is extracted from the P picture frame. The video frame compressed with the MPEG is described with an arrangement of I picture (an inner-frame encoded frame), P picture (an inter frame encoded frame in a forward prediction), and B picture (an inter-frame encoded frame in a backward prediction), as shown in <cross-reference target="DRAWINGS">FIG. 24</cross-reference>. The P picture includes a motion vector corresponding to a motion from the preceding I picture or P picture. </paragraph>
<paragraph id="P-0220" lvl="0"><number>&lsqb;0220&rsqb;</number> At step S<highlight><bold>82</bold></highlight>, the magnitude (intensity) of the each motion vector included in the frame of one P picture is calculated, and an average thereof is set as a scene change quantity from the preceding I picture or P picture. </paragraph>
<paragraph id="P-0221" lvl="0"><number>&lsqb;0221&rsqb;</number> At step S<highlight><bold>83</bold></highlight>, on the basis of the scene change quantity calculated with respect to the P picture, the scene change quantity is calculated for each one frame corresponding to the frame other than the P picture. For example, if the average value of the motion vector of the P picture frame is p, and the interval from the preceding I picture or P picture from which the video is referred to is d, the scene change quantity per one frame of each frame is set to p/d. </paragraph>
<paragraph id="P-0222" lvl="0"><number>&lsqb;0222&rsqb;</number> Subsequently, at step S<highlight><bold>72</bold></highlight> in the procedure of <cross-reference target="DRAWINGS">FIG. 22</cross-reference>, the total of the scene change quantity of frames between the following description target frames is calculated from the description target frame described in the video location information. </paragraph>
<paragraph id="P-0223" lvl="0"><number>&lsqb;0223&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 25</cross-reference> describes a change in the scene change quantity for each one frame. The horizontal axis corresponds to the frame number while a curve <highlight><bold>1000</bold></highlight> denotes a change in the scene change quantity. If the display time of the video having the location information of the frame information F<highlight><subscript>i </subscript></highlight>is calculated, the scene change quantity in the section <highlight><bold>1001</bold></highlight> up to F<highlight><subscript>i&plus;1 </subscript></highlight>is added which corresponds to the frame location of the next description target frame. It is considered that this becomes an area S<highlight><subscript>i </subscript></highlight>of the hatching portion <highlight><bold>1002</bold></highlight>, which is a magnitude of a motion of the frame location F<highlight><subscript>i</subscript></highlight>. </paragraph>
<paragraph id="P-0224" lvl="0"><number>&lsqb;0224&rsqb;</number> Subsequently, at step S<highlight><bold>73</bold></highlight> in the procedure of <cross-reference target="DRAWINGS">FIG. 22</cross-reference>, the display time of each frame is calculated. In order to set the scene change quantity to a constant level as much as possible, a larger quantity of the display time may only be allocated to the frame where the motion of the screen is large, so that the ratio of the display time allocated to the video of each frame location F<highlight><subscript>i </subscript></highlight>to the reproduction time may be set to S<highlight><subscript>i</subscript></highlight>/&Sgr;S<highlight><subscript>i</subscript></highlight>. When the total of the reproduction time is set to T, the display time of each video will be set to D<highlight><subscript>i</subscript></highlight>&equals;T&times;S<highlight><subscript>i</subscript></highlight>/&Sgr;S<highlight><subscript>i</subscript></highlight>. The value of the total T of the reproduction time is defined as the total reproduction time of the original video. </paragraph>
<paragraph id="P-0225" lvl="0"><number>&lsqb;0225&rsqb;</number> If no scene change appears and S<highlight><subscript>i</subscript></highlight>0, the lower limit value (for example, 1) which is calculated in advance may be entered, or the frame information thereof may not be described. Even with respect to the frame where the screen change is very small even if S<highlight><subscript>i</subscript></highlight>&equals;0 is not provided and virtually no change is displayed on the actual reproduction, the lower limit value may be substituted and no frame information may be described. If no frame information is described, the value of S<highlight><subscript>i </subscript></highlight>may be added to S<highlight><subscript>i&plus;1 </subscript></highlight>or may not be added thereto. </paragraph>
<paragraph id="P-0226" lvl="0"><number>&lsqb;0226&rsqb;</number> The processing for calculating this display time can be conducted for the preparation of the frame information with the special reproduction control information creating apparatus, but the processing can be conducted at the time of the special reproduction on the side of the video reproduction apparatus. </paragraph>
<paragraph id="P-0227" lvl="0"><number>&lsqb;0227&rsqb;</number> Next, there will be explained a case in which the special reproduction is conducted. </paragraph>
<paragraph id="P-0228" lvl="0"><number>&lsqb;0228&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 26</cross-reference> shows one example for the N times high-speed reproduction on the basis of the special reproduction control information that has been described. </paragraph>
<paragraph id="P-0229" lvl="0"><number>&lsqb;0229&rsqb;</number> At step S<highlight><bold>111</bold></highlight>, the display time D&prime;<highlight><subscript>i </subscript></highlight>at the time of reproduction is calculated on the basis of the reproduction rate information. The display time information described in the frame information is standard display time, the display time D&prime;<highlight><subscript>i</subscript></highlight>&equals;D<highlight><subscript>i</subscript></highlight>/N of each frame is calculated when reproduction at N times high-speed is conducted. </paragraph>
<paragraph id="P-0230" lvl="0"><number>&lsqb;0230&rsqb;</number> At step S<highlight><bold>112</bold></highlight>, initialization for the display is conducted, and i&equals;0 is set so that the first frame information is displayed. </paragraph>
<paragraph id="P-0231" lvl="0"><number>&lsqb;0231&rsqb;</number> At step S<highlight><bold>113</bold></highlight>, it is determined whether the display time D&prime;<highlight><subscript>i </subscript></highlight>of the i-th frame information is larger than the threshold value of the preset display time. </paragraph>
<paragraph id="P-0232" lvl="0"><number>&lsqb;0232&rsqb;</number> If the display time is larger, the video location information included in the i-th frame information F<highlight><subscript>i </subscript></highlight>is displayed for D&prime;<highlight><subscript>i </subscript></highlight>seconds at step S<highlight><bold>114</bold></highlight>. </paragraph>
<paragraph id="P-0233" lvl="0"><number>&lsqb;0233&rsqb;</number> If the display time is not larger, the process proceeds to step S<highlight><bold>115</bold></highlight> to search the i-th frame information which is not smaller than the threshold value in a forward direction. During search, the display time of the frame information which is smaller than the threshold value of the display time is all added to the display time of the i-th frame information. The display time of the frame information which is smaller than the threshold value of the display time is set to 0. The reason why such processing is conducted is that the time for preparing the video to be displayed becomes longer than the display time when the display time at the time of reproduction becomes very short with the result that the display cannot be conducted in time. Then, if the display time becomes very short, the process proceeds to the next step without displaying the video. At that time, this display time of the video which is not displayed is added to the display time of the video to be displayed so that the total display time becomes unchanged. </paragraph>
<paragraph id="P-0234" lvl="0"><number>&lsqb;0234&rsqb;</number> At step S<highlight><bold>116</bold></highlight>, it is determined whether &ldquo;i&rdquo; is smaller than the total number of the frame information items in order to determine whether or not the frame information which is not displayed remains. If &ldquo;i&rdquo; is lower than the total number of the frame information items, the process proceeds to step S<highlight><bold>117</bold></highlight> to increment &ldquo;i&rdquo; by one to create for the display of the next frame information. When &ldquo;i&rdquo; reaches the total number of the frame information items, the reproduction processing is completed. </paragraph>
<paragraph id="P-0235" lvl="0"><number>&lsqb;0235&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 27</cross-reference> shows one example for conducting the N times high-speed reproduction on the basis of the described special reproduction control information by taking the display cycle as a reference. </paragraph>
<paragraph id="P-0236" lvl="0"><number>&lsqb;0236&rsqb;</number> At step S<highlight><bold>121</bold></highlight>, the display time D&prime;<highlight><subscript>i </subscript></highlight>of each frame is calculated as D&prime;<highlight><subscript>i</subscript></highlight>&equals;D<highlight><subscript>i</subscript></highlight>/ N at the N times high-speed reproduction. Here, the calculated display time is actually associated with the display cycle so that the video cannot be always displayed in a calculated time. </paragraph>
<paragraph id="P-0237" lvl="0"><number>&lsqb;0237&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 28</cross-reference> shows a relationship between the calculated display time and the display cycle. The time axis <highlight><bold>1300</bold></highlight> shows the calculated display time while the time axis <highlight><bold>1301</bold></highlight> shows the display cycle based on the display rate. If the display rate is f frame/sec, an interval of the display cycle becomes 1/f sec. </paragraph>
<paragraph id="P-0238" lvl="0"><number>&lsqb;0238&rsqb;</number> Consequently, at step S<highlight><bold>122</bold></highlight>, the frame information F<highlight><subscript>i </subscript></highlight>including the start point of the display cycle is searched while the video included in the frame information F<highlight><subscript>i </subscript></highlight>is displayed for one display cycle (1/f sec) at step S<highlight><bold>123</bold></highlight>. </paragraph>
<paragraph id="P-0239" lvl="0"><number>&lsqb;0239&rsqb;</number> For example, the display cycle <highlight><bold>1302</bold></highlight> (<cross-reference target="DRAWINGS">FIG. 28</cross-reference>) displays the video of the frame information corresponding to this display time because the display start point <highlight><bold>1303</bold></highlight> is included in the calculated display time <highlight><bold>1304</bold></highlight>. </paragraph>
<paragraph id="P-0240" lvl="0"><number>&lsqb;0240&rsqb;</number> A method for allowing the display cycle correspond to the frame information may display the video at the nearest location of the start point of the display cycle, as shown in <cross-reference target="DRAWINGS">FIG. 29</cross-reference>. If the display time becomes smaller than the display cycle like the display time <highlight><bold>1305</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 28</cross-reference>, the display of the video may be omitted. If the video is forcibly displayed, the display time before and after the video is shortened to adjust so that the total display time becomes unchanged. </paragraph>
<paragraph id="P-0241" lvl="0"><number>&lsqb;0241&rsqb;</number> At step S<highlight><bold>124</bold></highlight>, it is determined whether the current display is the final display or not. If the current display is the final display, the processing is completed. If the display is not the final display, the process proceeds to step S<highlight><bold>125</bold></highlight> to conduct the processing of the next display cycle. </paragraph>
<paragraph id="P-0242" lvl="0"><number>&lsqb;0242&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 30</cross-reference> shows another example of a data structure for describing the frame information. The frame information included in the data structure of <cross-reference target="DRAWINGS">FIG. 8</cross-reference> or <cross-reference target="DRAWINGS">FIG. 14</cross-reference> summarizes a single original video. A plurality of original videos can be summarized by expanding the frame information. <cross-reference target="DRAWINGS">FIG. 30</cross-reference> shows such an example. An original video location information <highlight><bold>4202</bold></highlight> for indicating the original video file location is added to the original video information <highlight><bold>4201</bold></highlight> included in the individual frame information. The file described in the original video location information <highlight><bold>4202</bold></highlight> is not necessarily required to handle the entire file. The file can be used in the form in which only a portion of the section is extracted. In this case, not only file information such as a file name or the like but also the section information showing which section of the file becomes an object are additionally described. Plural sections may be selected from the original video. </paragraph>
<paragraph id="P-0243" lvl="0"><number>&lsqb;0243&rsqb;</number> Furthermore, if several kinds of the original videos are present and identification information is individually added to the videos, the original video identification information may be described in place of the original video location information. </paragraph>
<paragraph id="P-0244" lvl="0"><number>&lsqb;0244&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 31</cross-reference> explains an example in which a plurality of original videos are summarized and displayed by using the frame information added with the original video location information. In this example, three videos are summarized to display one summarized video. With respect to the video <highlight><bold>2</bold></highlight>, in place of the whole section, two sections <highlight><bold>4301</bold></highlight> and <highlight><bold>4302</bold></highlight> are taken out to handle the respective videos. As the frame information, together with these original video information, the frame location (<highlight><bold>4303</bold></highlight> with respect to <highlight><bold>4301</bold></highlight>) of respective representative video is described as the video location information while the display time (<highlight><bold>4304</bold></highlight> with respect to <highlight><bold>4301</bold></highlight>) is described as the display time information. </paragraph>
<paragraph id="P-0245" lvl="0"><number>&lsqb;0245&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 32</cross-reference> explains another example in which a plurality of original videos are summarized and displayed by using the frame information added with the original video location information. In this example, three videos are summarized to display one summarized video. With respect to the video <highlight><bold>2</bold></highlight>, in place of the whole section, a portion of the section is taken out. A plurality of sections may be taken out as described in <cross-reference target="DRAWINGS">FIG. 31</cross-reference>. As the frame information, together with these items of the original video information (for example, the section information <highlight><bold>4401</bold></highlight> in addition to the video <highlight><bold>2</bold></highlight>), the storage location of respective representative video files <highlight><bold>4402</bold></highlight> is described as the video location information and the display time <highlight><bold>4403</bold></highlight> is described as display time information. </paragraph>
<paragraph id="P-0246" lvl="0"><number>&lsqb;0246&rsqb;</number> Addition of the original video location information to the frame information which has been explained in these examples can be applied completely in the same way to the case in which a set of frames is used as video location information with the result that a plurality of original videos are summarized and displayed. </paragraph>
<paragraph id="P-0247" lvl="0"><number>&lsqb;0247&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 33</cross-reference> shows another data structure for describing the frame information. In this data structure, in addition to the video location information <highlight><bold>101</bold></highlight>, the display time information <highlight><bold>121</bold></highlight> and the original video information <highlight><bold>3701</bold></highlight> which has been already explained, a motion information <highlight><bold>4501</bold></highlight> and interest region information <highlight><bold>4502</bold></highlight> are added. The motion information <highlight><bold>4501</bold></highlight> describes a magnitude of a motion (a scene change quantity) in a section (the section described in the original video information) of the original video corresponding to the frame information. The interest region information <highlight><bold>4502</bold></highlight> refers to a description of the information which should be particularly interested in the video which is described in the video location information. </paragraph>
<paragraph id="P-0248" lvl="0"><number>&lsqb;0248&rsqb;</number> The motion information can be used for calculating the display time of the video described in the video location information as used at the time of calculating the display time from the motion of the video, as shown in <cross-reference target="DRAWINGS">FIG. 22</cross-reference>. In this case, even when the display time information is omitted and only the motion information is described, special reproduction such as high-speed reproduction can be conducted in the same manner as in the case in which the display time is described. In this case, the display time is calculated at the time of reproduction. </paragraph>
<paragraph id="P-0249" lvl="0"><number>&lsqb;0249&rsqb;</number> Both the display time information and the motion information can be described at the same time. In that case, an application for displaying uses the required one of the two, or uses both in combination in accordance with the processing. </paragraph>
<paragraph id="P-0250" lvl="0"><number>&lsqb;0250&rsqb;</number> For example, the display time calculated irrespective of the motion is described in the display time information. A method for calculating the display time for cutting out important scenes from the original video corresponds to this. At the time of the high-speed reproduction of the summarized contents calculated in this manner, the motion information is used so that a portion with a large motion is reproduced slowly while a portion with a small motion is reproduced quickly with the result that a high-speed reproduction free from a large overlook is enabled. </paragraph>
<paragraph id="P-0251" lvl="0"><number>&lsqb;0251&rsqb;</number> The interest region information is used when the particularly interest region is present in the video described in the video location information of the frame information. For example, faces of persons who seem to be important correspond to this. At the time of displaying the video including such interest region information, the display may be conducted by overlapping a square frame so that the interest region can be easily detected. The frame display is not indispensable, and the video may only be displayed as it is. </paragraph>
<paragraph id="P-0252" lvl="0"><number>&lsqb;0252&rsqb;</number> The interest region information can be used for processing and displaying the special reproduction control information such as frame information or the like. For example, if a part of the frame information is reproduced and displayed, the frame information including the interest region information is displayed with priority. Further, it is assumed that the frame information including square area with large area has higher importance, thereby making it possible to selectively displaying he video. </paragraph>
<paragraph id="P-0253" lvl="0"><number>&lsqb;0253&rsqb;</number> As shown above, there has been explained an example in which the processing is conducted on the basis of the scene change quantity. Hereinafter, there will be explained a case in which the importance information is used. </paragraph>
<paragraph id="P-0254" lvl="0"><number>&lsqb;0254&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 34</cross-reference> is a view showing examples of a data structure of the frame information attached to the video. </paragraph>
<paragraph id="P-0255" lvl="0"><number>&lsqb;0255&rsqb;</number> An importance information <highlight><bold>122</bold></highlight> is described in addition to or in place of the display time control information <highlight><bold>102</bold></highlight> in the data structure of the frame information of <cross-reference target="DRAWINGS">FIG. 1</cross-reference>. The display time is calculated based on the importance information <highlight><bold>122</bold></highlight>. </paragraph>
<paragraph id="P-0256" lvl="0"><number>&lsqb;0256&rsqb;</number> The importance information <highlight><bold>122</bold></highlight> represents the importance of the corresponding frame (or a set of frames). The importance is represented, for example, as an integer in a constant range (for example, 0 to 100), or is represented as an actual number in a constant range (for example, 0 to 1). Otherwise, the importance information <highlight><bold>122</bold></highlight> may be represented as an integer or an actual number value without setting the upper limit. The importance information <highlight><bold>122</bold></highlight> may be attached to all the frames of the video, or only the frame in which the importance is changed. </paragraph>
<paragraph id="P-0257" lvl="0"><number>&lsqb;0257&rsqb;</number> In this case as well, it is possible to take any form of <cross-reference target="DRAWINGS">FIGS. 9, 10</cross-reference>, <highlight><bold>12</bold></highlight>, and <highlight><bold>13</bold></highlight>. The frame extraction method of <cross-reference target="DRAWINGS">FIGS. 11, 20</cross-reference>, and <highlight><bold>21</bold></highlight> can be used. In this case, the scene change quantity of <cross-reference target="DRAWINGS">FIGS. 11, 20</cross-reference>, and <highlight><bold>21</bold></highlight> may be replaced by the importance. </paragraph>
<paragraph id="P-0258" lvl="0"><number>&lsqb;0258&rsqb;</number> Next, in the example which has been explained above, the display time is set with the scene change quantity. However, the display time may be set by the importance information. Hereinafter, the method for setting the display time will be explained. </paragraph>
<paragraph id="P-0259" lvl="0"><number>&lsqb;0259&rsqb;</number> In the setting the display time on the basis of the scene change quantity exemplified above in order to understand the video contents well, the display time is set long where the change quantity is large and the display time is set short where the change quantity is small. In the setting of the display time on the basis of this importance, the display time is set long where the importance is high and the display time is set short where the importance is low. That is, since the method for setting the display time according to the importance is basically similar to the method for setting the display time based on the scene change quantity, the method will be briefly explained. </paragraph>
<paragraph id="P-0260" lvl="0"><number>&lsqb;0260&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 36</cross-reference> shows one example of the basic processing procedure in this case. </paragraph>
<paragraph id="P-0261" lvl="0"><number>&lsqb;0261&rsqb;</number> At step S<highlight><bold>191</bold></highlight>, the importance of all frames of the original video will be calculated. A concrete method thereof will be exemplified later. </paragraph>
<paragraph id="P-0262" lvl="0"><number>&lsqb;0262&rsqb;</number> At step S<highlight><bold>192</bold></highlight>, the total of the importance from the description object frame described in the video location information to the next description object frame will be calculated. </paragraph>
<paragraph id="P-0263" lvl="0"><number>&lsqb;0263&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 37</cross-reference> describes the change in the importance for each one frame. Reference numeral <highlight><bold>2200</bold></highlight> denotes the importance. If the display time of the video having the location information of the frame information F<highlight><subscript>i </subscript></highlight>is calculated, the importance in the section up to F<highlight><subscript>i&plus;1 </subscript></highlight>which is the next description object frame location is accumulated. The accumulation result is an area S&prime;<highlight><subscript>i </subscript></highlight>of the hatching portion <highlight><bold>2202</bold></highlight>. </paragraph>
<paragraph id="P-0264" lvl="0"><number>&lsqb;0264&rsqb;</number> At step S<highlight><bold>193</bold></highlight>, the display time of each frame is calculated. Suppose that the ratio of the display time allocated to the video at each frame location F<highlight><subscript>i </subscript></highlight>the reproduction time is set to S&prime;<highlight><subscript>i</subscript></highlight>/&Sgr;S&prime;j. When the total of the reproduction time is set to T, the display time of each video becomes D<highlight><subscript>i</subscript></highlight>&equals;T&times;S&prime;<highlight><subscript>i</subscript></highlight>/S&prime;<highlight><subscript>j</subscript></highlight>. The value of the total T of the reproduction time is a standard reproduction time to be regulated as the total reproduction time of the original video. </paragraph>
<paragraph id="P-0265" lvl="0"><number>&lsqb;0265&rsqb;</number> When the total of the importance becomes S&prime;<highlight><subscript>i</subscript></highlight>&equals;0, the preset lower limit value (for example, 1) may be described, or the frame information may not be described. Even if S&prime;<highlight><subscript>i</subscript></highlight>&equals;0 is not established but the importance is very small, and it is assumed that such a frame is virtually not displayed, the lower limit value may be described or the frame information may not be described. If the frame information is not described, the S&prime;<highlight><subscript>i </subscript></highlight>value may be added and may not be added to S&prime;<highlight><subscript>i&plus;1</subscript></highlight>. </paragraph>
<paragraph id="P-0266" lvl="0"><number>&lsqb;0266&rsqb;</number> As shown in <cross-reference target="DRAWINGS">FIG. 34</cross-reference>, in the data structure of the frame information of <cross-reference target="DRAWINGS">FIG. 1</cross-reference>, the video location information <highlight><bold>101</bold></highlight>, the display time information <highlight><bold>121</bold></highlight> and the importance information <highlight><bold>112</bold></highlight> may be described in each frame information &ldquo;i&rdquo;. At the time of the special reproduction, the display time information <highlight><bold>121</bold></highlight> is used but the importance information <highlight><bold>122</bold></highlight> is not used; the importance information <highlight><bold>122</bold></highlight> is used but the display time information <highlight><bold>121</bold></highlight> is not used; both the importance information <highlight><bold>122</bold></highlight> and the display time information <highlight><bold>121</bold></highlight> are used; and neither the importance information <highlight><bold>122</bold></highlight> nor the display time information <highlight><bold>121</bold></highlight> is used. </paragraph>
<paragraph id="P-0267" lvl="0"><number>&lsqb;0267&rsqb;</number> The processing of calculating the display time can be conducted for preparing the frame information with the special reproduction control information creating apparatus. However, the processing may be conducted on the side of the video reproduction apparatus at the time of the special reproduction. </paragraph>
<paragraph id="P-0268" lvl="0"><number>&lsqb;0268&rsqb;</number> Next, a method (for example, step S<highlight><bold>191</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 36</cross-reference>) for calculating the importance of each frame or the scene (video frame section) will be explained. </paragraph>
<paragraph id="P-0269" lvl="0"><number>&lsqb;0269&rsqb;</number> Since various factors are normally intertwined in the judgment as to a certain scene having a video is important, the most appropriate method for calculating the importance is a method in which man determines the importance. In this method, importance evaluator evaluates the importance for each scene of the video, or for each of the constant interval, so that the importance is input as the importance data. The importance data referred to here refer to a frame number or time and a correspondence table with the importance value. In order to avoid subjective evaluation of importance, a plurality of importance evaluators are allowed to evaluate the same video to calculate the average value (or a median or the like will do) for each scene or each video frame section so that the importance is finally determined. In such manual input of the importance data, it is possible to add vague expressions and a plurality of elements which cannot be expressed in words to the importance. </paragraph>
<paragraph id="P-0270" lvl="0"><number>&lsqb;0270&rsqb;</number> In order to omit the trouble of determination by man, it is preferable that a phenomenon is expected in which a video scene which seems to be important is likely to appear, and the processing is used which automatically evaluates such phenomenon to convert the phenomenon into importance. Here, some examples are shown in which importance is automatically created. </paragraph>
<paragraph id="P-0271" lvl="0"><number>&lsqb;0271&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 38</cross-reference> shows an example of a processing procedure at the time of automatically calculating important data on the basis of the idea that a scene having a large sound level is important. <cross-reference target="DRAWINGS">FIG. 38</cross-reference> is established as a function block diagram. </paragraph>
<paragraph id="P-0272" lvl="0"><number>&lsqb;0272&rsqb;</number> In the sound level calculation processing at step S<highlight><bold>210</bold></highlight>, the sound level at each time is calculated out when the sound level attached to the video is calculated. Since the sound level largely changes in an instant, the smoothing processing or the like may be conducted in the sound level calculation processing at step S<highlight><bold>210</bold></highlight>. </paragraph>
<paragraph id="P-0273" lvl="0"><number>&lsqb;0273&rsqb;</number> In the importance calculation processing at step S<highlight><bold>211</bold></highlight>, a processing is conducted for converting into the importance the sound level output as a result of the sound level calculation processing. For example, the sound level input is linearly converted into a value of 0 to 100, the sound level having the lowest sound level set in advance being set to 0, and having the highest sound level being set to 100. The sound level not more than the lowest sound level is set to 0 while the sound level not less than the highest sound level is set to 100. As a result of the importance calculation processing, the importance at each time is calculated to be output as importance data. </paragraph>
<paragraph id="P-0274" lvl="0"><number>&lsqb;0274&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 39</cross-reference> shows an example of a processing procedure of a method for automatically calculating another importance level. <cross-reference target="DRAWINGS">FIG. 39</cross-reference> is established as a function block diagram. </paragraph>
<paragraph id="P-0275" lvl="0"><number>&lsqb;0275&rsqb;</number> In processing of <cross-reference target="DRAWINGS">FIG. 39</cross-reference>, it is determined that the scene in which important words registered in advance in the sound attached to the video quite often appear is important. </paragraph>
<paragraph id="P-0276" lvl="0"><number>&lsqb;0276&rsqb;</number> In the sound recognition processing at step S<highlight><bold>220</bold></highlight>, when the sound data attached to the video is input, the language (words) man talks is converted into text data in the sound recognition processing. </paragraph>
<paragraph id="P-0277" lvl="0"><number>&lsqb;0277&rsqb;</number> In the important word dictionary <highlight><bold>221</bold></highlight>, words which are likely to appear in important scenes are registered. If the degree of importance of registered words differs, the weight is added to each of the registered words. </paragraph>
<paragraph id="P-0278" lvl="0"><number>&lsqb;0278&rsqb;</number> In the word collation processing at step S<highlight><bold>222</bold></highlight>, the text data which is an output of the sound recognition processing is collated with the words registered in the important word dictionary <highlight><bold>221</bold></highlight> to determine whether or not important words are talked. </paragraph>
<paragraph id="P-0279" lvl="0"><number>&lsqb;0279&rsqb;</number> In the importance calculation processing at step S<highlight><bold>223</bold></highlight>, the importance in each scene of the video or at each time is calculated from the result of the word collation processing. In this calculation, the number of the appearances of important words and the weight of the important words are used so that the processing is conducted to increase the importance around the time at which, for example, important words have appeared (or of the scene in which the important words have appeared) by a constant value, or a value proportional to the weight of the important words. As a result of the important calculation processing, the importance at each time is calculated to be output as importance data. </paragraph>
<paragraph id="P-0280" lvl="0"><number>&lsqb;0280&rsqb;</number> If the weight of all the words is set to the same, the important word dictionary <highlight><bold>221</bold></highlight> becomes unnecessary. This is because that it is assumed that the scene in which many words are spoken is important. At this time, in the word collation processing at step S<highlight><bold>222</bold></highlight>, the processing of counting the number of words output from the sound recognition processing is conducted. Not only the number of words but also the number of characters may be counted. </paragraph>
<paragraph id="P-0281" lvl="0"><number>&lsqb;0281&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 40</cross-reference> shows an example of a processing procedure of the method for automatically calculating the other importance level. <cross-reference target="DRAWINGS">FIG. 40</cross-reference> is also established as a function block diagram. </paragraph>
<paragraph id="P-0282" lvl="0"><number>&lsqb;0282&rsqb;</number> The processing of <cross-reference target="DRAWINGS">FIG. 40</cross-reference> determines that the scene in which many important words appear which are registered in advance in the telop appearing in the video is important. </paragraph>
<paragraph id="P-0283" lvl="0"><number>&lsqb;0283&rsqb;</number> In the telop recognition processing at step S<highlight><bold>230</bold></highlight>, the character location in the video is specified to recognize characters by converting the video region at the character location into a binary value. The recognized result is output as text data. </paragraph>
<paragraph id="P-0284" lvl="0"><number>&lsqb;0284&rsqb;</number> The important word dictionary <highlight><bold>231</bold></highlight> is the same as the important word dictionary <highlight><bold>221</bold></highlight> of <cross-reference target="DRAWINGS">FIG. 39</cross-reference>. </paragraph>
<paragraph id="P-0285" lvl="0"><number>&lsqb;0285&rsqb;</number> In the word collation processing at step S<highlight><bold>232</bold></highlight>, in the same manner as at step S<highlight><bold>222</bold></highlight> in the procedure of <cross-reference target="DRAWINGS">FIG. 39</cross-reference>, the text data which is an output of the telop recognition processing is collated with the words registered in the important word dictionary <highlight><bold>231</bold></highlight> to determine whether or not important words have appeared. </paragraph>
<paragraph id="P-0286" lvl="0"><number>&lsqb;0286&rsqb;</number> In the importance calculation processing at step S<highlight><bold>232</bold></highlight>, the importance at each scene or at each time is calculated from the number of appearances of important words, and weight of the important words in the same manner as at step S<highlight><bold>223</bold></highlight> in the procedure of <cross-reference target="DRAWINGS">FIG. 39</cross-reference>. As a result of the importance calculation processing, the importance at each time is determined to be output as importance data. </paragraph>
<paragraph id="P-0287" lvl="0"><number>&lsqb;0287&rsqb;</number> If the weight of all the words is set to the same, the important word dictionary <highlight><bold>231</bold></highlight> becomes unnecessary. This is because that it is assumed that the scene in which many important words appear is an important scene. At this time, in the word collation processing at step S<highlight><bold>232</bold></highlight>, processing is conducted for counting the number of words simply output from the telop recognition processing. Not only the number of words but also the number of characters may be counted. </paragraph>
<paragraph id="P-0288" lvl="0"><number>&lsqb;0288&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 41</cross-reference> shows an example of a processing procedure of a method for automatically calculating still another importance level. <cross-reference target="DRAWINGS">FIG. 41</cross-reference> is established as a function block diagram. </paragraph>
<paragraph id="P-0289" lvl="0"><number>&lsqb;0289&rsqb;</number> The processing of <cross-reference target="DRAWINGS">FIG. 41</cross-reference> determines that when the telop appearing in the video is in larger character size, the scene is more important. </paragraph>
<paragraph id="P-0290" lvl="0"><number>&lsqb;0290&rsqb;</number> In the telop detection processing at step S<highlight><bold>240</bold></highlight>, the processing is conducted for specifying the location of character string in the video. </paragraph>
<paragraph id="P-0291" lvl="0"><number>&lsqb;0291&rsqb;</number> In the character size calculation processing at step S<highlight><bold>241</bold></highlight>, individual characters are extracted to calculate the average value or the maximum value of the size (area) of the character. </paragraph>
<paragraph id="P-0292" lvl="0"><number>&lsqb;0292&rsqb;</number> In the importance calculation processing at step S<highlight><bold>242</bold></highlight>, the importance is calculated which is proportional to the size of the character which is an output of the character size calculation processing. If the calculated importance is too large or too small, the processing is conducted for restricting the importance to a preset range with the threshold value processing. As a result of the importance calculation processing, the importance at each time is calculated to be output as importance data. </paragraph>
<paragraph id="P-0293" lvl="0"><number>&lsqb;0293&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 42</cross-reference> shows an example of the processing procedure of a method for automatically calculating still another importance level. <cross-reference target="DRAWINGS">FIG. 42</cross-reference> is established as a function block diagram. </paragraph>
<paragraph id="P-0294" lvl="0"><number>&lsqb;0294&rsqb;</number> The processing of <cross-reference target="DRAWINGS">FIG. 42</cross-reference> determines that the scene in which human faces appear in the video is important. </paragraph>
<paragraph id="P-0295" lvl="0"><number>&lsqb;0295&rsqb;</number> In the face detection processing at step S<highlight><bold>250</bold></highlight>, the processing is conducted for detecting an area which looks like a human face in the video. As a result of the processing, the number of areas (number of faces) which are determined to be a human face is output. The information on the size (area) of the face may be output at the same time. </paragraph>
<paragraph id="P-0296" lvl="0"><number>&lsqb;0296&rsqb;</number> In the importance calculation processing at step S<highlight><bold>251</bold></highlight>, the number of faces which is an output of the processing of detecting the faces is multiplied by several times to calculate the importance. If the output of the face detection processing includes face size information, calculation is conducted so that the importance increases with an increase in the size of faces. For example, the area of the face is multiplied by several times to calculate the importance. As a result of the importance calculation processing, the importance at each time is calculated to be output as importance data. </paragraph>
<paragraph id="P-0297" lvl="0"><number>&lsqb;0297&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 43</cross-reference> shows an example of the processing procedure of a method for automatically calculating still other importance level. <cross-reference target="DRAWINGS">FIG. 43</cross-reference> is also established as a function block diagram. </paragraph>
<paragraph id="P-0298" lvl="0"><number>&lsqb;0298&rsqb;</number> In the processing of <cross-reference target="DRAWINGS">FIG. 43</cross-reference>, it is determined that the scene in which a video similar to the video which is registered in advance appears is important. </paragraph>
<paragraph id="P-0299" lvl="0"><number>&lsqb;0299&rsqb;</number> The video which should be determined to be important is registered in the important scene dictionary <highlight><bold>260</bold></highlight>. The video is recorded as raw data or is recorded in a data compressed form. Instead of the video itself, the characteristic quantity (a color histogram, a frequency or the like) of the video may be recorded. </paragraph>
<paragraph id="P-0300" lvl="0"><number>&lsqb;0300&rsqb;</number> In the similarity/non-similarity calculation processing at step S<highlight><bold>261</bold></highlight>, similarity/non-similarity between the video registered in the important scene dictionary <highlight><bold>260</bold></highlight> and the input video data is calculated. As the non-similarity, the total of the square error or the total of the difference in the absolute value is used. If the video data is recorded in the important scene dictionary <highlight><bold>260</bold></highlight>, the total of the square error for each of the corresponding pixels and the total of the differential of the absolute valued are calculated as non-similarity. If the color histogram of the video is recorded in the important scene dictionary <highlight><bold>260</bold></highlight>, the same color histogram is calculated with respect to the input video data to calculate the total of the square error between histograms and the total of the difference in the absolute values to set these totals as non-similarity. </paragraph>
<paragraph id="P-0301" lvl="0"><number>&lsqb;0301&rsqb;</number> In the importance calculation processing at a step S<highlight><bold>262</bold></highlight>, the importance is calculated from the similarity/non-similarity which is an output of the similarity and non-similarity calculation processing. The importance is calculated in such a manner that larger similarity provides greater importance if the similarity is input while larger non-similarity provides smaller importance if the non-similarity is input. As a result of the importance calculation processing, the importance at each time is calculated to be output as the importance data. </paragraph>
<paragraph id="P-0302" lvl="0"><number>&lsqb;0302&rsqb;</number> Furthermore, as another method for automatically calculating the importance, the scene having a high instant viewing rate is set as an important scene. The data on the instant viewing rate is obtained as a result of the summing of the viewing rate investigation, so that importance is calculated by multiplying the instant viewing rate by constant times. Needless to say, there are various other methods. </paragraph>
<paragraph id="P-0303" lvl="0"><number>&lsqb;0303&rsqb;</number> The importance calculation processing may be solely conducted, or a plurality of data items may be used at the same time to calculate the importance. In the latter case, for example, the importance of one video is calculated with several different methods to calculate the final importance as an average value or a maximum value. </paragraph>
<paragraph id="P-0304" lvl="0"><number>&lsqb;0304&rsqb;</number> In the above embodiment, the explanation has been given by citing the scene change quantity and the importance. However, it is possible to use one item of information or a plurality of items of information (described in the frame information) together with the scene change quantity or the importance or instead of the scene change quantity or importance. </paragraph>
<paragraph id="P-0305" lvl="0"><number>&lsqb;0305&rsqb;</number> Next, there will be explained a case in which information for the control of reproduction/non-reproduction is added to the frame information (see <cross-reference target="DRAWINGS">FIG. 1</cross-reference>). </paragraph>
<paragraph id="P-0306" lvl="0"><number>&lsqb;0306&rsqb;</number> It is desired that either only a specific scene or a part thereof (for example, a high-light scene) or only a scene or a part thereof in which a specific person appears is reproduced. Thus, there is a demand of watching only a portion of the video. </paragraph>
<paragraph id="P-0307" lvl="0"><number>&lsqb;0307&rsqb;</number> In order to satisfy this desire, the reproduction/non-reproduction information may be added to the frame information for controlling the reproduction or the non-reproduction. As a consequence, only a part of the video is reproduced or only a part of the video is not reproduced on the basis of the reproduction/non-reproduction information. </paragraph>
<paragraph id="P-0308" lvl="0"><number>&lsqb;0308&rsqb;</number> <cross-reference target="DRAWINGS">FIGS. 44, 45</cross-reference>, and <highlight><bold>46</bold></highlight> show examples of a data structure in which the reproduction/non-reproduction information is added. </paragraph>
<paragraph id="P-0309" lvl="0"><number>&lsqb;0309&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 44</cross-reference> shows a data structure in which the reproduction/non-reproduction information <highlight><bold>123</bold></highlight> is added to the data structure of <cross-reference target="DRAWINGS">FIG. 8</cross-reference>. <cross-reference target="DRAWINGS">FIG. 45</cross-reference> shows a data structure in which the reproduction/non-reproduction information <highlight><bold>123</bold></highlight> is added to the data structure of <cross-reference target="DRAWINGS">FIG. 34</cross-reference>. <cross-reference target="DRAWINGS">FIG. 46</cross-reference> shows a data structure in which the reproduction/non-reproduction information <highlight><bold>123</bold></highlight> is added to the data structure of <cross-reference target="DRAWINGS">FIG. 35</cross-reference>. Though not shown, it is possible to add the reproduction/non-reproduction information <highlight><bold>123</bold></highlight> to the data structure of <cross-reference target="DRAWINGS">FIG. 1</cross-reference>. </paragraph>
<paragraph id="P-0310" lvl="0"><number>&lsqb;0310&rsqb;</number> The reproduction/non-reproduction information <highlight><bold>123</bold></highlight> may be binary information specifying whether the video is reproduced or not or a continuous value such as reproduction level or the like. </paragraph>
<paragraph id="P-0311" lvl="0"><number>&lsqb;0311&rsqb;</number> For example, in the latter case, when the reproduction level exceeds a certain threshold value at the time of reproduction, the video is reproduced. When the reproduction level is less than the threshold value, the video is not reproduced. The user can directly or indirectly specify the threshold value. </paragraph>
<paragraph id="P-0312" lvl="0"><number>&lsqb;0312&rsqb;</number> The reproduction/non-reproduction information <highlight><bold>123</bold></highlight> may be set as independent information to be stored. If the reproduction or non-reproduction is selectively specified, the non-reproduction can be specified when the display time shown in the display time information <highlight><bold>121</bold></highlight> is set to a specific value (for example, 0 or &minus;1). Alternatively, the non-reproduction can be specified when the importance indicated by the importance information <highlight><bold>122</bold></highlight> is set to a specific value (for example, 0 or &minus;1). The reproduction/non-reproduction information <highlight><bold>123</bold></highlight> may not be added. </paragraph>
<paragraph id="P-0313" lvl="0"><number>&lsqb;0313&rsqb;</number> If the reproduction or non-reproduction is specified with a level value, the display time information <highlight><bold>121</bold></highlight> and/or the importance information <highlight><bold>122</bold></highlight> (represented by the level value) can be used as a substitute. </paragraph>
<paragraph id="P-0314" lvl="0"><number>&lsqb;0314&rsqb;</number> If the reproduction/non-reproduction information <highlight><bold>123</bold></highlight> is maintained as independent information, the quantity of data increases by that quantity. It is possible to see a digest of the video by allowing the non-reproduction specification portion not to be reproduced on the reproduction side. It is also possible to see the whole video by reproducing the non-reproduction specified portion. If the reproduction/non-reproduction information <highlight><bold>123</bold></highlight> is not maintained as independent information, it is necessary to appropriately change the display time specified, for example, as 0 in order to see the whole video by reproducing the non-reproduction specified portion. </paragraph>
<paragraph id="P-0315" lvl="0"><number>&lsqb;0315&rsqb;</number> The reproduction/non-reproduction information <highlight><bold>123</bold></highlight> may be input by man or may be determined with some conditions. For example, when the motion information of the video is set to a constant value or more, the video is reproduced. When the motion information of the video is not set to a constant value or more, the video is not reproduced so that only brisk motion portion can be reproduced. When it is determined that the skin color is larger or smaller than the constant value from color information, only the scene where man appears can be reproduced. A method for calculating the information with the magnitude of sound, and a method for calculating the information from the reproduction program information which is input in advance can be considered. The importance may be calculated with some technique to create the reproduction/non-reproduction information <highlight><bold>123</bold></highlight> from the importance information. When the reproduction/non-reproduction information is set to a continuous value, the importance may be calculated by converting the information into the reproduction/non-reproduction information. </paragraph>
<paragraph id="P-0316" lvl="0"><number>&lsqb;0316&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 47</cross-reference> shows an example in which reproduction/non-reproduction control is carried out so that video is reproduced on the basis of the reproduction/non-reproduction information <highlight><bold>123</bold></highlight>. </paragraph>
<paragraph id="P-0317" lvl="0"><number>&lsqb;0317&rsqb;</number> In <cross-reference target="DRAWINGS">FIG. 47</cross-reference>, it is supposed that the original video <highlight><bold>2151</bold></highlight> is reproduced on the basis of the video frame location information represented with F<highlight><subscript>1 </subscript></highlight>through F<highlight><subscript>6 </subscript></highlight>or the video frame group location information <highlight><bold>2153</bold></highlight> and the display time information represented with D<highlight><subscript>1 </subscript></highlight>through D<highlight><subscript>6</subscript></highlight>. At this time, it is supposed that the reproduction/non-reproduction information is added to the display time information <highlight><bold>2154</bold></highlight>. In this example, the sections of D<highlight><subscript>1</subscript></highlight>, D<highlight><subscript>2</subscript></highlight>, D<highlight><subscript>4 </subscript></highlight>and D<highlight><subscript>6 </subscript></highlight>can be reproduced, and other sections cannot be reproduced, the sections of D<highlight><subscript>1</subscript></highlight>, D<highlight><subscript>2</subscript></highlight>, D<highlight><subscript>4 </subscript></highlight>and D<highlight><subscript>6 </subscript></highlight>are continuously reproduced as the reproduction video <highlight><bold>2152</bold></highlight> (while other sections cannot be reproduced). </paragraph>
<paragraph id="P-0318" lvl="0"><number>&lsqb;0318&rsqb;</number> For example, in the frame F<highlight><subscript>i </subscript></highlight>of the reproduction video, if the display time is set to D<highlight><superscript>&plus;</superscript></highlight><highlight><subscript>i </subscript></highlight>when the reproduction/non-reproduction information <highlight><bold>123</bold></highlight> shows reproduction, and the display time is set to D<highlight><superscript>&minus;</superscript></highlight><highlight><subscript>i </subscript></highlight>when the reproduction/non-reproduction information <highlight><bold>123</bold></highlight> shows the non-reproduction, &Sgr;<highlight><subscript>i</subscript></highlight>D<highlight><superscript>&plus;</superscript></highlight><highlight><subscript>i</subscript></highlight>&equals;T&prime; when the total time of the reproduction portion of the original video is set to T&prime;. Normally, the display time of D<highlight><superscript>&plus;</superscript></highlight><highlight><subscript>i </subscript></highlight>is set to a time which is required to reproduce the original video at a normal speed. The reproduction speed may be set to a predetermined high-speed. Information may be described as to which times the speed is to be set. When it is desired that the video is reproduced at N times high-speed, the display time D<highlight><superscript>&plus;</superscript></highlight><highlight><subscript>i </subscript></highlight>of the reproduction portion is multiplied by 1/N times. For example, in order to perform reproduction at the predetermined time D&prime;, the display time D<highlight><superscript>&plus;</superscript></highlight><highlight><subscript>i </subscript></highlight>of each reproduction portion may be processed and displayed at D&prime;/&Sgr;<highlight><subscript>i</subscript></highlight>D<highlight><superscript>&plus;</superscript></highlight><highlight><subscript>i </subscript></highlight>times. </paragraph>
<paragraph id="P-0319" lvl="0"><number>&lsqb;0319&rsqb;</number> If the display time of each frame (or a frame group) is determined on the basis of the frame information, the determined display time may be adjusted. </paragraph>
<paragraph id="P-0320" lvl="0"><number>&lsqb;0320&rsqb;</number> In a method in which the calculated display time is not adjusted, the display time which is calculated without taking into consideration the generation of the non-reproduction section is used as it is, so that when the display time exceeding 0 is originally allocated to the non-reproduction section the whole display time is shortened for that allocation portion. </paragraph>
<paragraph id="P-0321" lvl="0"><number>&lsqb;0321&rsqb;</number> In a method in which the calculated display time is adjusted, for example, if the display time exceeding 0 is originally allocated to the non-reproduction section, the adjustment is made by multiplying by a constant number the display time of each of the frames (or the frame group) to be reproduced so that the whole display time becomes equal to the time at the time of the reproduction of the non-reproduction section. </paragraph>
<paragraph id="P-0322" lvl="0"><number>&lsqb;0322&rsqb;</number> The user may make a selection as to whether the adjustment is to be made. </paragraph>
<paragraph id="P-0323" lvl="0"><number>&lsqb;0323&rsqb;</number> If the user specifies the N times reproduction, the N times high-speed reproduction processing may be conducted without the adjustment of the calculated display time. The N times high-speed reproduction processing may be conducted on the basis of the display time after the adjustment of the calculated display time in the above manner (the display time of the former becomes shorter). </paragraph>
<paragraph id="P-0324" lvl="0"><number>&lsqb;0324&rsqb;</number> The user may specify the whole display time. In this case as well, for example, the display time of each frame (or a frame group) to be reproduced is multiplied by a constant number to make an adjustment so that the display time becomes equal to the specified whole display time. </paragraph>
<paragraph id="P-0325" lvl="0"><number>&lsqb;0325&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 48</cross-reference> shows one example of the processing procedure for reproducing only a portion of the video on the basis of the reproduction/non-reproduction information <highlight><bold>123</bold></highlight>. </paragraph>
<paragraph id="P-0326" lvl="0"><number>&lsqb;0326&rsqb;</number> At step S<highlight><bold>162</bold></highlight>, the frame information (video location information and display time information) is read to determine whether the frame is to be reproduced from the reproduction/non-reproduction information in the display time information at step S<highlight><bold>163</bold></highlight>. </paragraph>
<paragraph id="P-0327" lvl="0"><number>&lsqb;0327&rsqb;</number> When it is determined that the reproduction is to be conducted, the frame is displayed for the portion of the display time at step S<highlight><bold>164</bold></highlight>. When it is determined that the reproduction is not to be conducted, the frame is not displayed and the processing is moved to the next frame processing. </paragraph>
<paragraph id="P-0328" lvl="0"><number>&lsqb;0328&rsqb;</number> It is determined at step S<highlight><bold>161</bold></highlight> whether or not the whole video to be reproduced is processed. When the whole video is processed, the reproduction processing is also ended. </paragraph>
<paragraph id="P-0329" lvl="0"><number>&lsqb;0329&rsqb;</number> When it is determined that the frame is to be reproduced or not at step S<highlight><bold>163</bold></highlight>, it is desired in some cases that the determination is depending on the taste of the user. At this time, it is determined from the user profile whether or not the non-reproduction portion is reproduced in advance before the reproduction of the video. When the non-reproduction portion is reproduced, the frame is reproduced without fail at step S<highlight><bold>164</bold></highlight>. </paragraph>
<paragraph id="P-0330" lvl="0"><number>&lsqb;0330&rsqb;</number> In addition, when the reproduction/non-reproduction information is described as a continuous value, a threshold value is determined from the user profile for differentiating the reproduction and the non-reproduction to determine the reproduction or the non-reproduction depending on whether or not the reproduction/non-reproduction information exceeds the threshold value. Except for using the user profile, for example, the threshold value is calculated from the importance set for each frame, or information may be received in advance from the user as to whether the reproduction or non-reproduction is provided in real time. </paragraph>
<paragraph id="P-0331" lvl="0"><number>&lsqb;0331&rsqb;</number> In this manner, it becomes possible to reproduce only a portion of the video by adding to the frame information the reproduction/non-reproduction information <highlight><bold>123</bold></highlight> for controlling whether the video is reproduced or not with the result that it becomes possible to reproduce only the high-light scene or only the scene in which a man or an object of interest appears. </paragraph>
<paragraph id="P-0332" lvl="0"><number>&lsqb;0332&rsqb;</number> Next, there will be explained a describing method if the location information of media (for example, text or sound) other than the video associated with the video to be displayed, and time for displaying or reproducing the video is added to the frame information (see <cross-reference target="DRAWINGS">FIG. 1</cross-reference>) as additional information. </paragraph>
<paragraph id="P-0333" lvl="0"><number>&lsqb;0333&rsqb;</number> In <cross-reference target="DRAWINGS">FIG. 8</cross-reference>, the video location information <highlight><bold>101</bold></highlight> and the display time information <highlight><bold>102</bold></highlight> are included in each frame information <highlight><bold>100</bold></highlight>. In <cross-reference target="DRAWINGS">FIG. 34</cross-reference>, the video location information <highlight><bold>101</bold></highlight> and importance information <highlight><bold>122</bold></highlight> are included in each frame information <highlight><bold>100</bold></highlight>. In <cross-reference target="DRAWINGS">FIG. 35</cross-reference>, the video location information <highlight><bold>101</bold></highlight>, the display time information <highlight><bold>121</bold></highlight>, and importance information <highlight><bold>122</bold></highlight> are included in each frame information <highlight><bold>100</bold></highlight>. In <cross-reference target="DRAWINGS">FIGS. 44, 45</cross-reference>, and <highlight><bold>46</bold></highlight>, there is further shown an example in which the reproduction/non-reproduction information <highlight><bold>123</bold></highlight> is included in each frame information <highlight><bold>100</bold></highlight>. In any example, 0 or more sound location information <highlight><bold>2703</bold></highlight>, sound reproduction time information <highlight><bold>2704</bold></highlight>, 0 or more text information <highlight><bold>2705</bold></highlight> and text display time information <highlight><bold>2706</bold></highlight> (however, 1 or more in any of the information) may be added. </paragraph>
<paragraph id="P-0334" lvl="0"><number>&lsqb;0334&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 49</cross-reference> shows an example in which one set of sound location information <highlight><bold>2703</bold></highlight> and sound reproduction time information <highlight><bold>2704</bold></highlight> and N sets of text information <highlight><bold>2705</bold></highlight> and text display time information <highlight><bold>2706</bold></highlight> are added to an example of the data structure of <cross-reference target="DRAWINGS">FIG. 8</cross-reference>. </paragraph>
<paragraph id="P-0335" lvl="0"><number>&lsqb;0335&rsqb;</number> The sound is reproduced for the time indicated by the sound reproduction time information <highlight><bold>2704</bold></highlight> from the location indicated by the sound location information <highlight><bold>2703</bold></highlight>. An object of reproduction may be sound information attached to the video from the beginning. Background music is created to be newly added. </paragraph>
<paragraph id="P-0336" lvl="0"><number>&lsqb;0336&rsqb;</number> The text displays the text information indicated by the text information <highlight><bold>2705</bold></highlight> for the time indicated by the text display time information <highlight><bold>2706</bold></highlight>. A plurality of items of text information may be added to one video frame. </paragraph>
<paragraph id="P-0337" lvl="0"><number>&lsqb;0337&rsqb;</number> The time when the sound reproduction and the text display are started is the same as the time when the associated video frame is displayed. The sound reproduction time and the text display time are set within the range of the associated video frame time. If continuous sound is reproduced over a plurality of video frames, the sound location information and the reproduction time may be set to be continuous. </paragraph>
<paragraph id="P-0338" lvl="0"><number>&lsqb;0338&rsqb;</number> With such a method, summarized sound and summarized text can be made possible. </paragraph>
<paragraph id="P-0339" lvl="0"><number>&lsqb;0339&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 50</cross-reference> shows one example of a method for describing the sound information separately from the frame information. This is an example of a data structure for reproducing sound associated with the video frame which is displayed at the time when the special reproduction is conducted. A set of the location information <highlight><bold>2801</bold></highlight> showing the location of the sound to be reproduced, reproduction start time <highlight><bold>2802</bold></highlight> when the sound reproduction is started, and reproduction time <highlight><bold>2803</bold></highlight> when the reproduction is continued is set as one item of sound information <highlight><bold>2800</bold></highlight> to be described as an arrangement of this sound information. </paragraph>
<paragraph id="P-0340" lvl="0"><number>&lsqb;0340&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 51</cross-reference> shows a data structure for describing the text information. The data structure has the same structure as the sound information of <cross-reference target="DRAWINGS">FIG. 50</cross-reference>, and a set of character code location information <highlight><bold>2901</bold></highlight> of the text to be displayed, a display start time <highlight><bold>2902</bold></highlight>, and a display time <highlight><bold>2903</bold></highlight> is set as one item of text information <highlight><bold>2900</bold></highlight> to be described as an arrangement of this sound information. As information corresponding to the character code location information <highlight><bold>2901</bold></highlight>, instead of the character code location information <highlight><bold>2901</bold></highlight>, the location information may be used which indicates a location where the character code is stored, or a location where the character is stored as a video. </paragraph>
<paragraph id="P-0341" lvl="0"><number>&lsqb;0341&rsqb;</number> The above sound information or the text information is synchronized with the display of the video frame to be displayed as information associated with the video frame or a constant video frame section in which the displayed video frame is present. As shown in <cross-reference target="DRAWINGS">FIG. 52</cross-reference>, the reproduction or the display of the sound information or the text information is started with the lapse of time shown by the time axis <highlight><bold>3001</bold></highlight>. In the beginning, the video <highlight><bold>3002</bold></highlight> is displayed and reproduced for the described display time in an order in which the respective video frames are described. Reference numerals <highlight><bold>3005</bold></highlight>, <highlight><bold>3006</bold></highlight> and <highlight><bold>3007</bold></highlight> denote respective video frames and a predetermined display time is allocated thereto. The sound <highlight><bold>3003</bold></highlight> is reproduced when the reproduction start time described in each sound information comes. When the reproduction time described in a similar manner has passed away, the reproduction is suspended. As shown in <cross-reference target="DRAWINGS">FIG. 52, a</cross-reference> plurality of sounds <highlight><bold>3008</bold></highlight> and <highlight><bold>3009</bold></highlight> may be reproduced. In a similar manner as the sound, the text <highlight><bold>3004</bold></highlight> is also displayed when the display time described in the each of the text information comes. When the display time which is described has passed away, the display is suspended. A plurality of texts <highlight><bold>3010</bold></highlight> and <highlight><bold>3011</bold></highlight> may be displayed at the same time. </paragraph>
<paragraph id="P-0342" lvl="0"><number>&lsqb;0342&rsqb;</number> It is not required that the sound reproduction start time and the text display start time coincides with the time at which the video frame is displayed. It is not required that the sound reproduction time and the text display time coincides with the display time of the video frame. These times can be freely set, on the contrary, the display time of the video frame may be changed in accordance with the sound reproduction time and the text display time. </paragraph>
<paragraph id="P-0343" lvl="0"><number>&lsqb;0343&rsqb;</number> It is possible that these times can be manually set by man. </paragraph>
<paragraph id="P-0344" lvl="0"><number>&lsqb;0344&rsqb;</number> In order to omit the trouble of determination by man, it is preferable to determine a phenomenon which is likely to appear in the video scene which seems to be important and to automatically set these times. Hereinafter, several examples of automatic setting are shown. </paragraph>
<paragraph id="P-0345" lvl="0"><number>&lsqb;0345&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 53</cross-reference> shows one example of a processing procedure in which a continuous video frame section is determined which is referred to as a shot from a change-over of the screen up to the next change-over of the screen, so that the total of the display time of the video frames included in the shot is defined as the sound reproduction time. <cross-reference target="DRAWINGS">FIG. 53</cross-reference> is also established as a function block diagram. </paragraph>
<paragraph id="P-0346" lvl="0"><number>&lsqb;0346&rsqb;</number> At step S<highlight><bold>3101</bold></highlight>, the shot is detected from the video. For this purpose, there are used such methods as a method for detecting a cut of a motion picture from the MPEG bit streams using a tolerance ratio detection method. (The transactions of the institute of electronics, information and communication engineers, Vol. J82-D-II, No. 3, pp. 361-370, 1999) and the like. </paragraph>
<paragraph id="P-0347" lvl="0"><number>&lsqb;0347&rsqb;</number> At step S<highlight><bold>3102</bold></highlight>, the video frame location information is referred to thereby investigating which shot respective video frames belong to. Furthermore, the display times of respective shots are calculated by taking the total of the display times of the video frames. </paragraph>
<paragraph id="P-0348" lvl="0"><number>&lsqb;0348&rsqb;</number> For example, the sound location information is set as the sound location corresponding to the start of the shot. The sound reproduction start time may be allowed to coincide with the display time of the initial video frame which belongs to each shot while the sound reproduction time may be set to be equal to the display time of the shot. Otherwise, in accordance with the reproduction time of the sound, the display time of the video frames included in each shot may be corrected. Although the shot is detected here, if a data structure is taken wherein the importance information is described in the frame information, the section having importance exceeding the threshold value is determined by using the importance with respect to the video frame so that the sound included in the section may be reproduced. </paragraph>
<paragraph id="P-0349" lvl="0"><number>&lsqb;0349&rsqb;</number> If the determined reproduction time does not meet a constant reference, the sound may not be reproduced. </paragraph>
<paragraph id="P-0350" lvl="0"><number>&lsqb;0350&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 54</cross-reference> shows one example of a processing procedure in which important words are taken out from sound data corresponding to the shot or the video frame section having the high importance with sound recognition so that the words, or the sound including the words or the sound in which a plurality of words are combined are reproduced. <cross-reference target="DRAWINGS">FIG. 54</cross-reference> is also established as a function block diagram. </paragraph>
<paragraph id="P-0351" lvl="0"><number>&lsqb;0351&rsqb;</number> At step S<highlight><bold>3201</bold></highlight>, the shot is detected. In place of the shot, the video frame section having the high importance is calculated. </paragraph>
<paragraph id="P-0352" lvl="0"><number>&lsqb;0352&rsqb;</number> At step S<highlight><bold>3202</bold></highlight>, the sound recognition is carried out with respect to the sound data section corresponding to the obtained video frame section. </paragraph>
<paragraph id="P-0353" lvl="0"><number>&lsqb;0353&rsqb;</number> At step S<highlight><bold>3203</bold></highlight>, sounds including the important word portion or sounds of the important word portion are determined from the recognition result. In order to select the important words, an important word dictionary <highlight><bold>3204</bold></highlight> is referred to. </paragraph>
<paragraph id="P-0354" lvl="0"><number>&lsqb;0354&rsqb;</number> At step S<highlight><bold>3205</bold></highlight>, the sound for reproduction is created. Continuous sounds including the important words may be used as they are. Only important words may be extracted. Sounds having a combination of a plurality of important words may be created. </paragraph>
<paragraph id="P-0355" lvl="0"><number>&lsqb;0355&rsqb;</number> At step S<highlight><bold>3206</bold></highlight>, in accordance with the reproduction time of the created time, the display time of the video frame is corrected. However, the number of selected words may be decreased and the reproduction time of the sound may be shortened so that the sound reproduction time is set to be within the display time of the video frame. </paragraph>
<paragraph id="P-0356" lvl="0"><number>&lsqb;0356&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 55</cross-reference> shows one example of a procedure in which text information is obtained from the telop. <cross-reference target="DRAWINGS">FIG. 55</cross-reference> is also established as a function block diagram. </paragraph>
<paragraph id="P-0357" lvl="0"><number>&lsqb;0357&rsqb;</number> In the processing of <cross-reference target="DRAWINGS">FIG. 55</cross-reference>, the text information is obtained from the telop or the sound displayed in the video. </paragraph>
<paragraph id="P-0358" lvl="0"><number>&lsqb;0358&rsqb;</number> At step S<highlight><bold>3301</bold></highlight>, the telop displayed in the video is read. This includes a method in which the telop in the original video is automatically extracted or the telop is read by man to be manually input with a method or the like described in, for example, a method described in a literature such as &ldquo;A method for extracting the character portion from the video for the telop region&rdquo; by Osamu Hori, CVIMI 114-17, pp. 129-136(1999). </paragraph>
<paragraph id="P-0359" lvl="0"><number>&lsqb;0359&rsqb;</number> A step S<highlight><bold>3302</bold></highlight>, important words are taken out from the telop character string which has been read. In the judgment of important words, an important word dictionary <highlight><bold>3303</bold></highlight> is used. The telop character string which is read may be text information as it is. Extracted words are arranged, and a sentence representing the video frame section may be constituted with only the important words to provide text information. </paragraph>
<paragraph id="P-0360" lvl="0"><number>&lsqb;0360&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 56</cross-reference> shows one example for obtaining the text information from the sound. <cross-reference target="DRAWINGS">FIG. 56</cross-reference> is also established as a function block diagram. </paragraph>
<paragraph id="P-0361" lvl="0"><number>&lsqb;0361&rsqb;</number> In the sound recognition processing at step S<highlight><bold>3401</bold></highlight>, sound is recognized. </paragraph>
<paragraph id="P-0362" lvl="0"><number>&lsqb;0362&rsqb;</number> At step S<highlight><bold>3402</bold></highlight>, important words are taken out from the recognized sound data. In the judgment of important words, an important word dictionary <highlight><bold>3403</bold></highlight> is used. The recognized sound data may be used as test information. Extracted words are arranged, and a sentence is constituted which represents the video frame section with only the important words to provide text information. </paragraph>
<paragraph id="P-0363" lvl="0"><number>&lsqb;0363&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 57</cross-reference> shows an example of processing procedure for taking out text information and preparing the text information with telop recognition from the shot or from the video frame section having high importance. <cross-reference target="DRAWINGS">FIG. 57</cross-reference> is also established as a function block diagram. </paragraph>
<paragraph id="P-0364" lvl="0"><number>&lsqb;0364&rsqb;</number> At step S<highlight><bold>3501</bold></highlight>, the shot is detected from the video. Instead of the shot, the section having high importance may be determined. </paragraph>
<paragraph id="P-0365" lvl="0"><number>&lsqb;0365&rsqb;</number> At step S<highlight><bold>3502</bold></highlight>, the telop represented in the video frame section is recognized. </paragraph>
<paragraph id="P-0366" lvl="0"><number>&lsqb;0366&rsqb;</number> At step S<highlight><bold>3503</bold></highlight>, the important words are extracted by using an important word dictionary <highlight><bold>3504</bold></highlight>. </paragraph>
<paragraph id="P-0367" lvl="0"><number>&lsqb;0367&rsqb;</number> At step S<highlight><bold>3505</bold></highlight>, text for the display is created. For this purpose, a telop character string including important words may be used. Only important words or a character string using the important words may be used as text information. If text information is obtained by sound recognition, the telop recognition processing at step S<highlight><bold>3502</bold></highlight> is subjected to sound recognition processing to input sound data. The text information is displayed together with the video frame in which the text is displayed as telop or video frame of the time at which the data is reproduced as sound. Otherwise, text information in the video frame section may be displayed at one time. </paragraph>
<paragraph id="P-0368" lvl="0"><number>&lsqb;0368&rsqb;</number> <cross-reference target="DRAWINGS">FIGS. 58A and 58B</cross-reference> are views showing a display example of the text information. As shown in <cross-reference target="DRAWINGS">FIG. 58</cross-reference>A, the display may be divided into the text information display area <highlight><bold>3601</bold></highlight> and the video display area <highlight><bold>3602</bold></highlight>. As shown in <cross-reference target="DRAWINGS">FIG. 58</cross-reference>B, the text information may be overlapped with the video display area <highlight><bold>3603</bold></highlight>. </paragraph>
<paragraph id="P-0369" lvl="0"><number>&lsqb;0369&rsqb;</number> Respective display times (reproduction times) of the video frame, the sound information and the text information may be adjusted so that all the media information is synchronized. For example, at the time of the double speed reproduction of the video, important sounds are extracted by the above method, and a half time sound information of the normal reproduction is obtained. Next, the display time is allocated to the video frame associated with respective sounds. If the display time of the video frame is determined so that the scene change quantity becomes constant, the sound reproduction time or the text display time is set to be within the display time of the respectively associated video frames. Otherwise, a section including a plurality of video frames is determined like the shot, so that the sound or the text included in the section is determined or displayed in accordance with the display time of the section. </paragraph>
<paragraph id="P-0370" lvl="0"><number>&lsqb;0370&rsqb;</number> So far there has been explained video data as its main focus. However, the data structure of the present invention can be modified to a data having no frame information, i.e., the sound data. It is possible to use sound information and text information in the form without the frame information. In this case, a summary is created which comprises only sound information or text information with respect to the original video data. In addition, a summary can be created which comprises only sound information and text information with respect to the sound data and music data. </paragraph>
<paragraph id="P-0371" lvl="0"><number>&lsqb;0371&rsqb;</number> Though the data structures shown in <cross-reference target="DRAWINGS">FIGS. 50 and 51</cross-reference> are used to describe the sound information and text information in synchronization with the video data, it is possible to summarize the sound data and text data only. To summarize the sound data, the data structure shown in <cross-reference target="DRAWINGS">FIG. 50</cross-reference> can be used irrespective of the video information. To summarize the text data, the data structure shown in <cross-reference target="DRAWINGS">FIG. 51</cross-reference> can be used irrespective of the video information. At that time, in the same manner as in the case of the frame information, the original data information may be added to describe a correspondence relationship between the original sound and music data to the sound information and text information. </paragraph>
<paragraph id="P-0372" lvl="0"><number>&lsqb;0372&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 59</cross-reference> shows an example of a data structure in which the original data information <highlight><bold>4901</bold></highlight> is included in the sound information shown in <cross-reference target="DRAWINGS">FIG. 50</cross-reference>. If the original data is the video, the original data information <highlight><bold>4901</bold></highlight> indicates the section of video frames (start point information <highlight><bold>4902</bold></highlight> and section length information <highlight><bold>4903</bold></highlight>). </paragraph>
<paragraph id="P-0373" lvl="0"><number>&lsqb;0373&rsqb;</number> If the original data is sound data and music data, the original data information <highlight><bold>4901</bold></highlight> indicates the section of sound and music. </paragraph>
<paragraph id="P-0374" lvl="0"><number>&lsqb;0374&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 60</cross-reference> shows an example of a data structure in which the original data information <highlight><bold>4901</bold></highlight> is included in the sound information shown in <cross-reference target="DRAWINGS">FIG. 30</cross-reference>. </paragraph>
<paragraph id="P-0375" lvl="0"><number>&lsqb;0375&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 61</cross-reference> explains an example in which sound/music is summarized by using the sound information. The original sound/music is divided into several sections. A portion of the section is extracted as the summarized sound/music so that the summary of the original data is created. For example, a portion <highlight><bold>5001</bold></highlight> of the section <highlight><bold>2</bold></highlight> is extracted as summarized sound/music to be reproduced as a section <highlight><bold>5002</bold></highlight> of the summary. As an example of a method for dividing the section, the music may be divided into chapters and the conversation may be divided by the contents. </paragraph>
<paragraph id="P-0376" lvl="0"><number>&lsqb;0376&rsqb;</number> Furthermore, in the same manner as in the case of the frame information, the description of the original data file and the section are included in the sound information and the text information with the result that a plurality of sound/music data items can be summarized together. At this time, if identification information is added to the individual original data, the original data identification information may be described in place of the original data file and the section. </paragraph>
<paragraph id="P-0377" lvl="0"><number>&lsqb;0377&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 62</cross-reference> explains an example in which sound/music is summarized by using the sound information. Portions of plural sound/music data items are extracted as the summarized sound/music so that the summary of the original data is created. For example, a portion <highlight><bold>5001</bold></highlight> of the sound/music data item <highlight><bold>2</bold></highlight> is extracted as summarized sound/music to be reproduced as a section <highlight><bold>5102</bold></highlight> of the summary. A piece of music included in one music album is extracted by a portion of the section, so that a summarized data for trial can be created as a usage. </paragraph>
<paragraph id="P-0378" lvl="0"><number>&lsqb;0378&rsqb;</number> If an album is summarized, the title of the music may be included in the music information when it is preferable that the title of the music can be known. This information is not indispensable. </paragraph>
<paragraph id="P-0379" lvl="0"><number>&lsqb;0379&rsqb;</number> Next, a method of providing video data will be explained. </paragraph>
<paragraph id="P-0380" lvl="0"><number>&lsqb;0380&rsqb;</number> If the special reproduction control information created in the processing of the embodiment is provided for the use, it is necessary to provide the special reproduction control information from the side of those who create the information to the side of the user with some means. As this method of providing the special reproduction control information, various forms can be considered as exemplified below: </paragraph>
<paragraph id="P-0381" lvl="0"><number>&lsqb;0381&rsqb;</number> (1) Video data and special reproduction control information are recorded on one (or a plurality of) recording medium (or media) and provided at the same time; </paragraph>
<paragraph id="P-0382" lvl="0"><number>&lsqb;0382&rsqb;</number> (2) Video data is recorded on one (or a plurality of) recording medium (or media) and provided, and the special reproduction control information is separately recorded on one (or a plurality of) recording medium (media) and provided; </paragraph>
<paragraph id="P-0383" lvl="0"><number>&lsqb;0383&rsqb;</number> (3) Video data and the special reproduction control information are provided via the communication medium at the same occasion; </paragraph>
<paragraph id="P-0384" lvl="0"><number>&lsqb;0384&rsqb;</number> (4) Video data and the special reproduction control information are provided via the communication media at different occasions. </paragraph>
<paragraph id="P-0385" lvl="0"><number>&lsqb;0385&rsqb;</number> According to the above described embodiments, a special reproduction control information describing method for describing special reproduction control information provided for special reproduction with respect to the video contents describes, as the frame information, for each of frames or groups of continuous or adjacent frames selectively extracted from the whole frame series of video data constituting the video contents, first information showing a location at which video data of the one frame or one group is present and second information associated with display time allocated to the one frame or the frame group, and/or third information showing importance allocated to the one frame or the frame group corresponding to the frame information. </paragraph>
<paragraph id="P-0386" lvl="0"><number>&lsqb;0386&rsqb;</number> According to the above described embodiments, a computer readable recording medium storing a special reproduction control information stores at least frame information described for each of frames or groups of continuous or adjacent frames selectively extracted from the whole frame series of video data constituting the video contents, the frame information comprising first information showing a location at which video data of the one frame or one group is present and second information associated with display time allocated to the one frame or the frame group, and/or third information showing importance allocated to the one frame or the frame group corresponding to the frame information. </paragraph>
<paragraph id="P-0387" lvl="0"><number>&lsqb;0387&rsqb;</number> According to the above described embodiments, a special reproduction control information describing apparatus/method for describing special reproduction control information provided for special reproduction with respect to the video contents describes, as the frame information, for each of frames or groups of continuous or adjacent frames selectively extracted from the whole frame series of video data constituting the video contents, video location information showing a location at which video data of the one frame or one group is present and display time control information including display time information and basic information based on which the display time is calculated, to be allocated to the one frame or the frame group. </paragraph>
<paragraph id="P-0388" lvl="0"><number>&lsqb;0388&rsqb;</number> According to the above described embodiments, a special reproduction apparatus/method which enables a special reproduction with respect to video contents, wherein special reproduction control information is referred to which includes at least frame information including video location information showing a location at which one frame data or one frame group data is present which information is described for each of the frame groups comprising one frame selectively extracted out of the whole frame series of the video data allocated to the video contents and constituting the video contents or a plurality of continuous or adjacent frames; the one frame data or the frame group data corresponding to each frame information is obtained on the basis of video location information included in the frame information while the display time which should be allocated to each frame information is determined on the basis of display time control information included in at least each frame information and data on the one frame or the plurality of frames which is or are obtained is reproduced at the determined display time in a predetermined order thereby carrying out a special reproduction. </paragraph>
<paragraph id="P-0389" lvl="0"><number>&lsqb;0389&rsqb;</number> In the above described embodiments, for example, image data is created in advance, which is extracted in frame units from location information on an effective video frame or an original video which is used for display, and the video frame location information or information on the display time of the image data is created separately from the original video. Either video frames or the image data extracted from the original video is continuously displayed on the basis of the display information so that a special reproduction such as a double speed reproduction, a trick reproduction, jump continuous reproduction or the like is enabled. </paragraph>
<paragraph id="P-0390" lvl="0"><number>&lsqb;0390&rsqb;</number> In the double speed reproduction for confirming the contents at a high speed, display time is determined in advance in such a manner that the display time is extended at a location where a motion of the scene is large while the display time is shortened at a location where the motion is small so that the change in the display screen becomes constant as much as possible. Alternatively, the same effect can be obtained even when the location information is determined so that an interval of the extracted location is made small at a location where a motion of the video frame or video data used for the display is large while the interval is made small at a location where the motion is large. A reproduction speed control value may be created so that a double speed value or a reproduction time is provided which is designated by a user as a whole. A long video can be viewed at double speed reproduction, so that the video can be easily viewed in a short time, and the contents can be grasped in a short time. </paragraph>
<paragraph id="P-0391" lvl="0"><number>&lsqb;0391&rsqb;</number> It is possible to reproduce videos so that important locations are not overlooked by extending the display time at the important locations and shortening the display time at unimportant locations in accordance with the importance of the video. </paragraph>
<paragraph id="P-0392" lvl="0"><number>&lsqb;0392&rsqb;</number> Only important locations may be efficiently reproduced by partially omitting a part of the video without displaying the whole video frame. </paragraph>
<paragraph id="P-0393" lvl="0"><number>&lsqb;0393&rsqb;</number> According to embodiments of the present invention, an effective special reproduction is enabled on the basis of the control information on the reproduction side by arranging and describing as control information provided for a special reproduction of the video contents a plurality of frame information including a method for obtaining a frame or a group of frames selectively extracted from the original video, information on the display time (absolute or relative value) allocated to the frame or the group of frames and information which forms the basis for obtaining the information on the display time. </paragraph>
<paragraph id="P-0394" lvl="0"><number>&lsqb;0394&rsqb;</number> Additional objects and advantages of the invention will be set forth in the description which follows, and in part will be obvious from the description, or may be learned by practice of the invention. The objects and advantages of the invention may be realized and obtained by means of the instrumentalities and combinations particularly pointed out hereinafter. For example, each of the above functions can be realized as software. The above embodiments can be realized as a computer readable recording medium on which a program is recorded for allowing the computer to conduct predetermined means or for allowing the computer to function as predetermined means, or for allowing the computer to realize a predetermined function. </paragraph>
<paragraph id="P-0395" lvl="0"><number>&lsqb;0395&rsqb;</number> The structures shown in each of the embodiments are one example, and are not intended to exclude other structures. It is also possible to provide a structure which is obtained by replacing a part of the structure exemplified above with another structure, omitting a part of the exemplified structure, adding a different function to the exemplified structure, and combining such measures. A different structure logically equivalent to the exemplified structure, a different structure including a part logically equivalent to the exemplified structure, and a different structure logically equivalent to the essential portion of the exemplified structure can be provided. Another structure identical to or similar to the exemplified structure, or a different structure having the same effect as the exemplified structure or a similar effect can be provided. </paragraph>
<paragraph id="P-0396" lvl="0"><number>&lsqb;0396&rsqb;</number> In each of the embodiments, various variations with respect to various structure components can be put into practice in an appropriate combination. </paragraph>
<paragraph id="P-0397" lvl="0"><number>&lsqb;0397&rsqb;</number> Each of the embodiments includes or inherently contains an invention associated with various viewpoints, stages, concept or a category such as, for example, an invention as a method for describing information, an invention as information which is described, an invention as an apparatus or a method corresponding thereto, an invention as an inside of the apparatus or a method corresponding thereto. </paragraph>
<paragraph id="P-0398" lvl="0"><number>&lsqb;0398&rsqb;</number> Consequently, the invention can be extracted without being limited to the exemplified structure from the content disclosed in the embodiment according to this invention. </paragraph>
</section>
</detailed-description>
</subdoc-description>
<subdoc-claims>
<heading lvl="1">What is claimed is: </heading>
<claim id="CLM-00001">
<claim-text><highlight><bold>1</bold></highlight>. A method of describing frame information, the method comprising: 
<claim-text>describing, for a frame extracted from a plurality of frames in a source video data, first information specifying a location of the extracted frame in the source video data; and </claim-text>
<claim-text>describing, for the extracted frame, second information relating to a display time of the extracted frame. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00002">
<claim-text><highlight><bold>2</bold></highlight>. The method according to <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein the extracted frame comprises a group of frames, and the first information comprises information specifying a location of the extracted group of frames in the source video data. </claim-text>
</claim>
<claim id="CLM-00003">
<claim-text><highlight><bold>3</bold></highlight>. The method according to <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, further comprising describing, for the extracted frame, third information relating to importance of the extracted frame. </claim-text>
</claim>
<claim id="CLM-00004">
<claim-text><highlight><bold>4</bold></highlight>. The method according to <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein the first information comprises information specifying an image data file created from the video data of the extracted frame. </claim-text>
</claim>
<claim id="CLM-00005">
<claim-text><highlight><bold>5</bold></highlight>. The method according to <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein the extracted frame comprises a frame extracted from a plurality of frames included in a temporal section of the source video data, and further describing fourth information specifying the temporal section of the source video data. </claim-text>
</claim>
<claim id="CLM-00006">
<claim-text><highlight><bold>6</bold></highlight>. The method according to <dependent-claim-reference depends_on="CLM-00005">claim 5</dependent-claim-reference>, wherein the first information comprises information specifying an image data file created from the source video data of the extracted frame, the image data corresponding to the extracted frame. </claim-text>
</claim>
<claim id="CLM-00007">
<claim-text><highlight><bold>7</bold></highlight>. The method according to <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein the second information comprises information relating to such display time that a frame activity value during a special reproduction is kept substantially constant. </claim-text>
</claim>
<claim id="CLM-00008">
<claim-text><highlight><bold>8</bold></highlight>. The method according to <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, further comprising describing fifth information indicating whether the extracted frame is reproduced or not. </claim-text>
</claim>
<claim id="CLM-00009">
<claim-text><highlight><bold>9</bold></highlight>. The method according to <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein the first information comprises one of information specifying a location of the extracted frame among the plurality of frames and information specifying a location of image data within an image data file created from the source video data and stored separately from the video data, the image data corresponding to the extracted frame. </claim-text>
</claim>
<claim id="CLM-00010">
<claim-text><highlight><bold>10</bold></highlight>. The method according to <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, further comprising describing, for media data other than the source video data including the extracted frame, information specifying a location of the media data and information relating to a display time of the media data. </claim-text>
</claim>
<claim id="CLM-00011">
<claim-text><highlight><bold>11</bold></highlight>. An article of manufacture comprising a computer usable medium storing frame information, the frame information comprising: 
<claim-text>first information, described for a frame extracted from a plurality of frames, specifying a location of the extracted frame in the source video data; and </claim-text>
<claim-text>second information, described for the extracted frame, relating to a display time of the extracted frame. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00012">
<claim-text><highlight><bold>12</bold></highlight>. The article of manufacture according to <dependent-claim-reference depends_on="CLM-00011">claim 11</dependent-claim-reference>, wherein the extracted frame comprises a group of frames, and the first information comprises information specifying a location of the extracted group of frames in the source video data. </claim-text>
</claim>
<claim id="CLM-00013">
<claim-text><highlight><bold>13</bold></highlight>. The article of manufacture according to <dependent-claim-reference depends_on="CLM-00011">claim 11</dependent-claim-reference>, wherein the frame information comprises third information relating to importance of the extracted frame. </claim-text>
</claim>
<claim id="CLM-00014">
<claim-text><highlight><bold>14</bold></highlight>. The article of manufacture according to <dependent-claim-reference depends_on="CLM-00011">claim 11</dependent-claim-reference>, wherein the first information comprises information specifying an image data file created from the video data of the extracted frame. </claim-text>
</claim>
<claim id="CLM-00015">
<claim-text><highlight><bold>15</bold></highlight>. The article of manufacture according to <dependent-claim-reference depends_on="CLM-00011">claim 11</dependent-claim-reference>, further storing the source video data and an image data file corresponding to the source video data of the extracted frame in addition to the frame information. </claim-text>
</claim>
<claim id="CLM-00016">
<claim-text><highlight><bold>16</bold></highlight>. An apparatus for creating frame information, the apparatus comprising: 
<claim-text>a unit configured to extract a frame from a plurality of frames in a source video data; </claim-text>
<claim-text>a unit configured to create the frame information including first information specifying a location of the extracted frame and second information relating to a display time of the extracted frame; and </claim-text>
<claim-text>a unit configured to link the extracted frame to the frame information. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00017">
<claim-text><highlight><bold>17</bold></highlight>. A method of creating frame information, the method comprising: 
<claim-text>extracting a frame from a plurality of frames in a source video data; and </claim-text>
<claim-text>creating the frame information including first information specifying a location of the extracted frame in the source video data and second information relating to a display time of the extracted frame. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00018">
<claim-text><highlight><bold>18</bold></highlight>. An apparatus for performing a special reproduction, comprising: 
<claim-text>a unit configured to refer to frame information described for a frame extracted from a plurality of frames in a source video data and including first information specifying a location of the extracted frame in the source video data and second information relating to a display time of the extracted frame; </claim-text>
<claim-text>a unit configured to obtain the video data corresponding to the extracted frame based on the first information; </claim-text>
<claim-text>a unit configured to determine the display time of the extracted frame based on the second information; and </claim-text>
<claim-text>a unit configured to display the obtained video data for the determined display time. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00019">
<claim-text><highlight><bold>19</bold></highlight>. A method of performing a special reproduction comprising: 
<claim-text>referring to frame information described for a frame extracted from a plurality of frames in a source video data and including first information specifying a location of the extracted frame and second information relating to a display time of the extracted frame; </claim-text>
<claim-text>obtaining the video data corresponding to the extracted frame based on the first information; </claim-text>
<claim-text>determining the display time of the extracted frame based on the second information; and </claim-text>
<claim-text>displaying the obtained video data for the determined display time. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00020">
<claim-text><highlight><bold>20</bold></highlight>. An article of manufacture comprising a computer usable medium having computer readable program code means embodied therein, the computer readable program code means performing a special reproduction, the computer readable program code means comprising: 
<claim-text>computer readable program code means for causing a computer to refer to frame information described for a frame extracted from a plurality of frames in a source video data and including first information specifying a location of the extracted frame and second information relating to a display time of the extracted frame; </claim-text>
<claim-text>computer readable program code means for causing a computer to obtain the video data corresponding to the extracted frame based on the first information; </claim-text>
<claim-text>computer readable program code means for causing a computer to determine the display time of the extracted frame based on the second information; and </claim-text>
<claim-text>computer readable program code means for causing a computer to display the obtained video data for the determined display time. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00021">
<claim-text><highlight><bold>21</bold></highlight>. A method of describing sound information, the method comprising: 
<claim-text>describing, for a frame extracted from a plurality of sound frames in a source sound data, first information specifying a location of the extracted frame in the source sound data; and </claim-text>
<claim-text>describing, for the extracted frame, second information relating to a reproduction start time and reproduction time of the sound data of the extracted frame. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00022">
<claim-text><highlight><bold>22</bold></highlight>. An article of manufacture comprising a computer usable medium storing frame information, the frame information comprising: 
<claim-text>first information, described for a frame extracted from a plurality of sound frames, specifying a location of the extracted frame in the source sound data; and </claim-text>
<claim-text>second information, described for the extracted frame, relating to a reproduction start time and reproduction time of the sound data of the extracted frame. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00023">
<claim-text><highlight><bold>23</bold></highlight>. A method of describing text information, the method comprising: 
<claim-text>describing, for a frame extracted from a plurality of text frames in a source text data, first information specifying a location of the extracted frame in the source text data; and </claim-text>
<claim-text>describing, for the extracted frame, second information relating to a display start time and display time of the text data of the extracted frame. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00024">
<claim-text><highlight><bold>24</bold></highlight>. An article of manufacture comprising a computer usable medium storing frame information, the frame information comprising: 
<claim-text>first information, described for a frame extracted from a plurality of text frames in a source text data, specifying a location of the extracted frame in the source text data; and </claim-text>
<claim-text>second information, described for the extracted frame, relating to a display start time and display time of the text data of the extracted frame.</claim-text>
</claim-text>
</claim>
</subdoc-claims>
<subdoc-drawings id="DRAWINGS">
<heading lvl="0" align="CENTER">Drawings</heading>
<representative-figure>1</representative-figure>
<figure id="figure-D00000">
<image id="EMI-D00000" file="US20030002853A1-20030102-D00000.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00001">
<image id="EMI-D00001" file="US20030002853A1-20030102-D00001.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00002">
<image id="EMI-D00002" file="US20030002853A1-20030102-D00002.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00003">
<image id="EMI-D00003" file="US20030002853A1-20030102-D00003.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00004">
<image id="EMI-D00004" file="US20030002853A1-20030102-D00004.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00005">
<image id="EMI-D00005" file="US20030002853A1-20030102-D00005.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00006">
<image id="EMI-D00006" file="US20030002853A1-20030102-D00006.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00007">
<image id="EMI-D00007" file="US20030002853A1-20030102-D00007.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00008">
<image id="EMI-D00008" file="US20030002853A1-20030102-D00008.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00009">
<image id="EMI-D00009" file="US20030002853A1-20030102-D00009.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00010">
<image id="EMI-D00010" file="US20030002853A1-20030102-D00010.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00011">
<image id="EMI-D00011" file="US20030002853A1-20030102-D00011.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00012">
<image id="EMI-D00012" file="US20030002853A1-20030102-D00012.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00013">
<image id="EMI-D00013" file="US20030002853A1-20030102-D00013.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00014">
<image id="EMI-D00014" file="US20030002853A1-20030102-D00014.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00015">
<image id="EMI-D00015" file="US20030002853A1-20030102-D00015.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00016">
<image id="EMI-D00016" file="US20030002853A1-20030102-D00016.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00017">
<image id="EMI-D00017" file="US20030002853A1-20030102-D00017.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00018">
<image id="EMI-D00018" file="US20030002853A1-20030102-D00018.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00019">
<image id="EMI-D00019" file="US20030002853A1-20030102-D00019.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00020">
<image id="EMI-D00020" file="US20030002853A1-20030102-D00020.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00021">
<image id="EMI-D00021" file="US20030002853A1-20030102-D00021.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00022">
<image id="EMI-D00022" file="US20030002853A1-20030102-D00022.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00023">
<image id="EMI-D00023" file="US20030002853A1-20030102-D00023.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00024">
<image id="EMI-D00024" file="US20030002853A1-20030102-D00024.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00025">
<image id="EMI-D00025" file="US20030002853A1-20030102-D00025.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00026">
<image id="EMI-D00026" file="US20030002853A1-20030102-D00026.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00027">
<image id="EMI-D00027" file="US20030002853A1-20030102-D00027.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00028">
<image id="EMI-D00028" file="US20030002853A1-20030102-D00028.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00029">
<image id="EMI-D00029" file="US20030002853A1-20030102-D00029.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00030">
<image id="EMI-D00030" file="US20030002853A1-20030102-D00030.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00031">
<image id="EMI-D00031" file="US20030002853A1-20030102-D00031.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00032">
<image id="EMI-D00032" file="US20030002853A1-20030102-D00032.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00033">
<image id="EMI-D00033" file="US20030002853A1-20030102-D00033.TIF" imf="TIFF" ti="DR"/>
</figure>
</subdoc-drawings>
</patent-application-publication>
