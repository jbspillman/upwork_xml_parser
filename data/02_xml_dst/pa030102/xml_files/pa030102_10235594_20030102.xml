<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE patent-application-publication SYSTEM "pap-v16-2002-01-01.dtd" [
<!ENTITY US20030004611A1-20030102-D00000.TIF SYSTEM "US20030004611A1-20030102-D00000.TIF" NDATA TIF>
<!ENTITY US20030004611A1-20030102-D00001.TIF SYSTEM "US20030004611A1-20030102-D00001.TIF" NDATA TIF>
<!ENTITY US20030004611A1-20030102-D00002.TIF SYSTEM "US20030004611A1-20030102-D00002.TIF" NDATA TIF>
<!ENTITY US20030004611A1-20030102-D00003.TIF SYSTEM "US20030004611A1-20030102-D00003.TIF" NDATA TIF>
<!ENTITY US20030004611A1-20030102-D00004.TIF SYSTEM "US20030004611A1-20030102-D00004.TIF" NDATA TIF>
<!ENTITY US20030004611A1-20030102-D00005.TIF SYSTEM "US20030004611A1-20030102-D00005.TIF" NDATA TIF>
<!ENTITY US20030004611A1-20030102-D00006.TIF SYSTEM "US20030004611A1-20030102-D00006.TIF" NDATA TIF>
<!ENTITY US20030004611A1-20030102-D00007.TIF SYSTEM "US20030004611A1-20030102-D00007.TIF" NDATA TIF>
<!ENTITY US20030004611A1-20030102-D00008.TIF SYSTEM "US20030004611A1-20030102-D00008.TIF" NDATA TIF>
<!ENTITY US20030004611A1-20030102-D00009.TIF SYSTEM "US20030004611A1-20030102-D00009.TIF" NDATA TIF>
<!ENTITY US20030004611A1-20030102-D00010.TIF SYSTEM "US20030004611A1-20030102-D00010.TIF" NDATA TIF>
<!ENTITY US20030004611A1-20030102-D00011.TIF SYSTEM "US20030004611A1-20030102-D00011.TIF" NDATA TIF>
<!ENTITY US20030004611A1-20030102-D00012.TIF SYSTEM "US20030004611A1-20030102-D00012.TIF" NDATA TIF>
<!ENTITY US20030004611A1-20030102-D00013.TIF SYSTEM "US20030004611A1-20030102-D00013.TIF" NDATA TIF>
<!ENTITY US20030004611A1-20030102-D00014.TIF SYSTEM "US20030004611A1-20030102-D00014.TIF" NDATA TIF>
<!ENTITY US20030004611A1-20030102-D00015.TIF SYSTEM "US20030004611A1-20030102-D00015.TIF" NDATA TIF>
<!ENTITY US20030004611A1-20030102-D00016.TIF SYSTEM "US20030004611A1-20030102-D00016.TIF" NDATA TIF>
<!ENTITY US20030004611A1-20030102-D00017.TIF SYSTEM "US20030004611A1-20030102-D00017.TIF" NDATA TIF>
]>
<patent-application-publication>
<subdoc-bibliographic-information>
<document-id>
<doc-number>20030004611</doc-number>
<kind-code>A1</kind-code>
<document-date>20030102</document-date>
</document-id>
<publication-filing-type>new</publication-filing-type>
<domestic-filing-data>
<application-number>
<doc-number>10235594</doc-number>
</application-number>
<application-number-series-code>10</application-number-series-code>
<filing-date>20020905</filing-date>
</domestic-filing-data>
<technical-information>
<classification-ipc>
<classification-ipc-primary>
<ipc>G06F019/00</ipc>
</classification-ipc-primary>
<classification-ipc-edition>07</classification-ipc-edition>
</classification-ipc>
<classification-us>
<classification-us-primary>
<uspc>
<class>700</class>
<subclass>258000</subclass>
</uspc>
</classification-us-primary>
<classification-us-secondary>
<uspc>
<class>700</class>
<subclass>245000</subclass>
</uspc>
</classification-us-secondary>
</classification-us>
<title-of-invention>Robot capable of expressing moods</title-of-invention>
</technical-information>
<continuity-data>
<division-of>
<parent-child>
<child>
<document-id>
<doc-number>10235594</doc-number>
<kind-code>A1</kind-code>
<document-date>20020905</document-date>
</document-id>
</child>
<parent>
<document-id>
<doc-number>09881420</doc-number>
<document-date>20010614</document-date>
<country-code>US</country-code>
</document-id>
</parent>
<parent-status>PENDING</parent-status>
</parent-child>
</division-of>
</continuity-data>
<inventors>
<first-named-inventor>
<name>
<given-name>Edward</given-name>
<middle-name>C.</middle-name>
<family-name>McKinney</family-name>
<name-suffix>JR.</name-suffix>
</name>
<residence>
<residence-us>
<city>San Rafael</city>
<state>CA</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</first-named-inventor>
<inventor>
<name>
<given-name>Tristan</given-name>
<middle-name>M.</middle-name>
<family-name>Christianson</family-name>
</name>
<residence>
<residence-us>
<city>San Francisco</city>
<state>CA</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
<inventor>
<name>
<given-name>Shek</given-name>
<middle-name>Fai</middle-name>
<family-name>Lau</family-name>
</name>
<residence>
<residence-us>
<city>Foster City</city>
<state>CA</state>
<country-code>US</country-code>
</residence-us>
</residence>
<authority-applicant>INV</authority-applicant>
</inventor>
</inventors>
<assignee>
<organization-name>Sharper Image Corporation</organization-name>
<assignee-type>02</assignee-type>
</assignee>
<correspondence-address>
<name-1>Sheldon R. Meyer, Esq.</name-1>
<name-2>FLIESLER DUBB MEYER &amp; LOVEJOY LLP</name-2>
<address>
<address-1>Fourth Floor</address-1>
<address-2>Four Embarcadero Center</address-2>
<city>San Francisco</city>
<state>CA</state>
<postalcode>94111-4156</postalcode>
<country>
<country-code>US</country-code>
</country>
</address>
</correspondence-address>
</subdoc-bibliographic-information>
<subdoc-abstract>
<paragraph id="A-0001" lvl="0">A robot which incorporates a body, arms with a hand grip, legs, several sensors, light elements, an audio system, and a video system. The sensors allows the robot to interact with objects in the room, and prevents the robot from traveling off an edge or bumping into obstacles. The light elements allow the robot to express moods. The audio system allows the robot to detect and transmit sounds. The video system allows a user to remotely view the area in front of the robot. Additionally, the robot may operate in a plurality of modes, including modes that allow the robot to operate autonomously. The robot may operate autonomously in an automatic mode, a security mode, a greet mode, and a monitor mode. Further, the robot can be manipulated remotely. </paragraph>
</subdoc-abstract>
<subdoc-description>
<cross-reference-to-related-applications>
<heading lvl="1">REFERENCE TO RELATED APPLICATION </heading>
<paragraph id="P-0001" lvl="0"><number>&lsqb;0001&rsqb;</number> This application is a divisional of, and claims priority to, U.S. patent application Ser. No. 09/881,420, filed Jun. 14, 2001, entitled &ldquo;Multi-Function Robot with Remote and Video System.&rdquo;</paragraph>
</cross-reference-to-related-applications>
<summary-of-invention>
<section>
<heading lvl="1">FIELD OF THE INVENTION </heading>
<paragraph id="P-0002" lvl="0"><number>&lsqb;0002&rsqb;</number> The present invention relates generally to a robot that can be manipulated remotely by a user or operate autonomously. More particularly, the robot can detect and avoid bumping into obstacles and traveling off an edge, thus allowing the robot to interact with objects in a room. Further, the robot can be manipulated remotely without the user requiring a line-of-sight with the robot. All of these features allow the robot to provide various security measures. </paragraph>
</section>
<section>
<heading lvl="1">BACKGROUND </heading>
<paragraph id="P-0003" lvl="0"><number>&lsqb;0003&rsqb;</number> Remote controlled robots allow users to manipulate the robot using a remote control device, allowing the user to move the robot and perform simple tasks. Typically, to be able to see where the user is moving the robot, the user must have a line of sight with the robot. Otherwise, the user cannot see where the robot is and risks damage to the robot by driving it off an edge or colliding with an object. </paragraph>
<paragraph id="P-0004" lvl="0"><number>&lsqb;0004&rsqb;</number> Therefore, there is a need for a remote control device to have a video screen allowing the user to see the area in front of the robot. With a video screen on the remote control device, a user can move the robot in areas that are not in the user&apos;s line of sight. Thus, the robot can be moved into more areas. </paragraph>
<paragraph id="P-0005" lvl="0"><number>&lsqb;0005&rsqb;</number> Additionally, a robot traditionally cannot interact with people on its own. The user must typically manipulate every action of the robot. Therefore, there is a need for a robot to operate autonomously and interact with people it encounters. To accomplish this, a robot must have the ability to detect moving and stationary objects in the immediate vicinity. To safely operate autonomously, a robot must also have an edge detection system so as to not travel over an edge and damage itself. </paragraph>
<paragraph id="P-0006" lvl="0"><number>&lsqb;0006&rsqb;</number> Some robots have video cameras, enabling a user to view the area in front of the robot. However, typically the user may only view the image from the video camera through a computer. Therefore, there is a need for a hand-held remote control device with a video screen that a user can easily transport. </paragraph>
</section>
<section>
<heading lvl="1">SUMMARY OF THE INVENTION </heading>
<paragraph id="P-0007" lvl="0"><number>&lsqb;0007&rsqb;</number> The present invention is a multi-function robot. The robot can operate autonomously or be manipulated remotely by a remote control device. To interact with people in a room, the robot is designed with two arms, two legs, eyes, a mouth, and a head. The arms can rotate in several positions and further contains a hand-grip device. The handgrip device allows the robot to hold and release objects. The legs of the robot are designed to move the robot throughout a room. The mouth and eyes of the robot allow it to communicate with people in the room and provide emotions. </paragraph>
<paragraph id="P-0008" lvl="0"><number>&lsqb;0008&rsqb;</number> To operate autonomously the robot has multiple sensors to avoid bumping into obstacles within the room and traveling off an edge. The sensors include infrared devices located on the body of the robot and an edge detection element located in the legs of the robot. The robot also has several modes by which it can operate autonomously. For example, an automatic mode allows the robot to move autonomously throughout the room, detect people within the room, and interact with the people. The robot can also provide security to the household when it is the security mode. In security mode the robot can detect noise and send an alarm signal to the remote control device to alert the user that an object has been detected. The robot can also greet people when in the greet mode. Additionally, the robot may be placed in the monitor mode, which allows a user to remotely view objects in front of the object and hear sounds within the vicinity of the robot. Finally, the robot can be placed in the remote control mode which allows a user to remotely manipulate the robot. </paragraph>
<paragraph id="P-0009" lvl="0"><number>&lsqb;0009&rsqb;</number> To enhance the operation of the modes described above, the robot can display moods through lighting of its eyes and mouth. Depending on the mode the robot is operating from and the type of speech the robot is making, the eyes will change colors to express a different mood. Further, while the robot is speaking the mouth will display different patterns. </paragraph>
<paragraph id="P-0010" lvl="0"><number>&lsqb;0010&rsqb;</number> To operate manually, a remote control device is used to manipulate the robot remotely. The remote control device contains all the functions a user will need to manipulate the robot. For example, the remote control device contains a joystick, video display, a microphone, a transmitter/receiver, and several other controls to manipulate the robot. The joystick allows the user to translate motion of the robot in several directions. The video display allows the user to remotely view the area in front of the robot through the video camera on the robot. The user can also transmit his voice to the robot such that his voice is projected from the robot.</paragraph>
</section>
</summary-of-invention>
<brief-description-of-drawings>
<section>
<heading lvl="1">BRIEF DESCRIPTION OF THE DRAWINGS </heading>
<paragraph id="P-0011" lvl="0"><number>&lsqb;0011&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 1</cross-reference> is a front perspective view of an embodiment of the robot of the invention with the left arm in a raised position and both hands in an open position; </paragraph>
<paragraph id="P-0012" lvl="0"><number>&lsqb;0012&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 2</cross-reference> is a rear perspective view of the robot with the left arm in a raised position and both hands in an open position; </paragraph>
<paragraph id="P-0013" lvl="0"><number>&lsqb;0013&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 3</cross-reference> is a left side view of the robot with the left arm in a raised position and the hand in an open position; </paragraph>
<paragraph id="P-0014" lvl="0"><number>&lsqb;0014&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 4</cross-reference> is a front view of the robot with the left arm in a raised position and both hands in an open position; </paragraph>
<paragraph id="P-0015" lvl="0"><number>&lsqb;0015&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 5</cross-reference> is a rear view of the robot with the left arm in a raised position and both hands in an open position; </paragraph>
<paragraph id="P-0016" lvl="0"><number>&lsqb;0016&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 6</cross-reference> is a top view of the robot with the left arm in a raised position and both hands in an open position; </paragraph>
<paragraph id="P-0017" lvl="0"><number>&lsqb;0017&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 7</cross-reference> is a front perspective of the robot thereof with the left arm in a 90&deg; raised position carrying a tray, and the right arm in a 180&deg; raised position carrying a tray; </paragraph>
<paragraph id="P-0018" lvl="0"><number>&lsqb;0018&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 8</cross-reference> is a cutaway view of an arm of the robot illustrating the mechanism to open and close the hands of the robot; </paragraph>
<paragraph id="P-0019" lvl="0"><number>&lsqb;0019&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 9</cross-reference> is a cutaway view of a leg of the robot illustrating the mechanism to rotate the arms; </paragraph>
<paragraph id="P-0020" lvl="0"><number>&lsqb;0020&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 10</cross-reference> is a cutaway view of a leg of the robot illustrating the drive mechanism; </paragraph>
<paragraph id="P-0021" lvl="0"><number>&lsqb;0021&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 11</cross-reference> is a cutaway view of the body of the robot illustrating the mechanism to rotate the rotatable platform; </paragraph>
<paragraph id="P-0022" lvl="0"><number>&lsqb;0022&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 12</cross-reference> is a cutaway view of the mechanism to drive the scanning passive infrared sensor; </paragraph>
<paragraph id="P-0023" lvl="0"><number>&lsqb;0023&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 13</cross-reference> is a perspective view of an embodiment of the remote control device of the invention; </paragraph>
<paragraph id="P-0024" lvl="0"><number>&lsqb;0024&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 14</cross-reference> is a top rear perspective view of the remote control device; </paragraph>
<paragraph id="P-0025" lvl="0"><number>&lsqb;0025&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 15</cross-reference> is a top view of the remote control device; </paragraph>
<paragraph id="P-0026" lvl="0"><number>&lsqb;0026&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 16</cross-reference> is a block diagram for the controls of the robot; and </paragraph>
<paragraph id="P-0027" lvl="0"><number>&lsqb;0027&rsqb;</number> <cross-reference target="DRAWINGS">FIG. 17</cross-reference> is a block diagram illustrating the controls fo the remote control device.</paragraph>
</section>
</brief-description-of-drawings>
<detailed-description>
<section>
<heading lvl="1">DETAILED DESCRIPTION </heading>
<paragraph id="P-0028" lvl="0"><number>&lsqb;0028&rsqb;</number> Referring now to FIGS. <highlight><bold>1</bold></highlight>-<highlight><bold>7</bold></highlight>, the robot <highlight><bold>100</bold></highlight> contains a body <highlight><bold>102</bold></highlight>, arms <highlight><bold>104</bold></highlight>, legs <highlight><bold>106</bold></highlight>, video device <highlight><bold>122</bold></highlight>, mouth <highlight><bold>126</bold></highlight>, eyes <highlight><bold>128</bold></highlight>, light <highlight><bold>118</bold></highlight>, microphones <highlight><bold>117</bold></highlight>, active infrared emitter <highlight><bold>115</bold></highlight>, a passive infrared scanner <highlight><bold>114</bold></highlight>, and multiple sensors to assist the robot <highlight><bold>100</bold></highlight> from running into obstacles or traveling off an edge. </paragraph>
<paragraph id="P-0029" lvl="0"><number>&lsqb;0029&rsqb;</number> The arms <highlight><bold>104</bold></highlight> are connected with the body <highlight><bold>102</bold></highlight>. The arms <highlight><bold>104</bold></highlight> can be positioned in multiple locations and further can be positioned in pre-set &ldquo;serving&rdquo; locations. As shown in <cross-reference target="DRAWINGS">FIG. 7</cross-reference>, the two pre-set serving positions are at the 90&deg; and 180&deg; positions. Both arms <highlight><bold>104</bold></highlight> can be adjusted to either position independently. Further, to ensure that the arms <highlight><bold>104</bold></highlight> will not rotate when in either serving position, the arms <highlight><bold>104</bold></highlight> can be located so that the remote control <highlight><bold>500</bold></highlight> cannot activate the arms <highlight><bold>104</bold></highlight>. Referring now to <cross-reference target="DRAWINGS">FIG. 9</cross-reference>, the mechanism for rotating the arms <highlight><bold>104</bold></highlight> can be seen. The motor <highlight><bold>180</bold></highlight>, via a flexible belt <highlight><bold>181</bold></highlight>, drives gear <highlight><bold>182</bold></highlight>, which drives gear <highlight><bold>184</bold></highlight>, which drives gear <highlight><bold>186</bold></highlight>, which drives gear <highlight><bold>188</bold></highlight>, which drives gear <highlight><bold>190</bold></highlight>. Gear <highlight><bold>190</bold></highlight> is attached to the arm <highlight><bold>104</bold></highlight>. To lower the cost and complexity of the mechanism, gears <highlight><bold>184</bold></highlight>, <highlight><bold>186</bold></highlight>, and <highlight><bold>188</bold></highlight> are the same part. An optical emitter/receiver monitors the movement and location of gear <highlight><bold>190</bold></highlight> via a toothed wheel (not shown) attached coaxially to gear <highlight><bold>190</bold></highlight>. Such a device is commonly referred to as an optical position encoding device throughout the industry. Therefore, it is known to one of ordinary skill in the art and does not need to be further described. The monitoring through the above described optical position encoding device allows the robot <highlight><bold>100</bold></highlight> to know the position of the arms <highlight><bold>104</bold></highlight>. When the robot <highlight><bold>100</bold></highlight> is turned on, the arms <highlight><bold>104</bold></highlight> are calibrated by moving them through a range of motion that the robot <highlight><bold>100</bold></highlight> can track their position from the starting position. </paragraph>
<paragraph id="P-0030" lvl="0"><number>&lsqb;0030&rsqb;</number> To grip and hold an object, the arms <highlight><bold>104</bold></highlight> also contain a hand grip device. The hand grip device contains a first finger <highlight><bold>110</bold></highlight> and a second finger <highlight><bold>108</bold></highlight>. As shown in <cross-reference target="DRAWINGS">FIG. 1</cross-reference>, the first finger <highlight><bold>110</bold></highlight> is stationary and has cavities <highlight><bold>112</bold></highlight> for holding a serving device (See <cross-reference target="DRAWINGS">FIG. 7</cross-reference>). The second finger <highlight><bold>108</bold></highlight> opens and closes to grip and release an object. However, one of ordinary skill in the art will appreciate that the second finger <highlight><bold>108</bold></highlight> maybe stationary while the first finger <highlight><bold>110</bold></highlight> opens and closes. A spring closure mechanism biases the second finger <highlight><bold>108</bold></highlight> in a closed position. As the mechanism is commonly known to one of ordinary skill in the art, the mechanism does not need to be further described. The spring closure mechanism will not apply more than five pounds of pressure to an object placed between the first finger <highlight><bold>110</bold></highlight> and the second finger <highlight><bold>108</bold></highlight>. Limiting the pressure to five pounds will prevent damage to an object held between the first finger <highlight><bold>110</bold></highlight> and the second finger <highlight><bold>108</bold></highlight>. </paragraph>
<paragraph id="P-0031" lvl="0"><number>&lsqb;0031&rsqb;</number> A separate motor operates to activate the second finger <highlight><bold>108</bold></highlight>. Referring now to <cross-reference target="DRAWINGS">FIG. 8, a</cross-reference> leadscrew <highlight><bold>152</bold></highlight> on the shaft of the motor <highlight><bold>150</bold></highlight> turns gear <highlight><bold>154</bold></highlight>, which turns gear <highlight><bold>156</bold></highlight>, which turns gear <highlight><bold>158</bold></highlight>. Each stage reduces the RPM and increases torque. The gear <highlight><bold>158</bold></highlight> has a pin on its underside which pulls a steel linkage which is attached by springs to the lever arms <highlight><bold>109</bold></highlight> of the second finger <highlight><bold>108</bold></highlight>. As the steel linkage is commonly known to one of ordinary skill in the art, the steel linkage does not need to be further described. This double-spring per can thus be forced open or shut against a spring force without damage to the mechanism. </paragraph>
<paragraph id="P-0032" lvl="0"><number>&lsqb;0032&rsqb;</number> The legs <highlight><bold>106</bold></highlight> are also connected with body <highlight><bold>102</bold></highlight>. The legs <highlight><bold>106</bold></highlight> provide lateral support to keep the body <highlight><bold>102</bold></highlight> elevated and substantially perpendicular to the ground. The legs <highlight><bold>106</bold></highlight> also provide the ability for the robot <highlight><bold>100</bold></highlight> to move about. Each leg <highlight><bold>106</bold></highlight> contains a drive mechanism <highlight><bold>300</bold></highlight> to move the robot <highlight><bold>100</bold></highlight>. The drive mechanism <highlight><bold>300</bold></highlight> located in each leg <highlight><bold>106</bold></highlight> can move the robot forward, reverse, left and right and both forward and reverse directions, and can spin the robot in place by controlling the rotation of the center wheel <highlight><bold>138</bold></highlight>. Counters on each drive mechanism <highlight><bold>300</bold></highlight> control the straight forward motion with the two drive mechanisms <highlight><bold>300</bold></highlight> in synchronization. </paragraph>
<paragraph id="P-0033" lvl="0"><number>&lsqb;0033&rsqb;</number> The drive mechanism <highlight><bold>300</bold></highlight> is illustrated in <cross-reference target="DRAWINGS">FIG. 10</cross-reference>. The motor <highlight><bold>306</bold></highlight> has an attached pulley <highlight><bold>307</bold></highlight> to drive the pulley <highlight><bold>302</bold></highlight> via the flexible belt <highlight><bold>304</bold></highlight>. The pulley <highlight><bold>302</bold></highlight> has a small gear which also drives the center wheel <highlight><bold>138</bold></highlight>. The small gear is a common toothed wheel which is commonly known to one of ordinary skill in the art. The flexible belt <highlight><bold>304</bold></highlight> provides tolerance for heavy loads which would otherwise damage a motor or gear train. Also, the flexible belt <highlight><bold>304</bold></highlight> reduces noise over using hard gears for first-stage reduction. A counter(not shown) mounted on the gear <highlight><bold>310</bold></highlight> counts the rotation of the center wheel <highlight><bold>138</bold></highlight> via an attached toothed wheel for indexing and monitoring speed. As the counter is a device which is commonly known to one of ordinary skill in the art, it will not be described further. Other optical position encoding devices can be used with wheel <highlight><bold>138</bold></highlight> as is known in the art. As the center wheel <highlight><bold>138</bold></highlight> is the only wheel propelled by the drive mechanism <highlight><bold>300</bold></highlight>, the movement of the robot <highlight><bold>100</bold></highlight> is dependant solely on the center wheel <highlight><bold>138</bold></highlight>. The front end wheel <highlight><bold>120</bold></highlight> and rear end wheel <highlight><bold>121</bold></highlight> rotate freely upon contact with a surface and provide lateral support to keep the body <highlight><bold>102</bold></highlight> substantially perpendicular with the ground. </paragraph>
<paragraph id="P-0034" lvl="0"><number>&lsqb;0034&rsqb;</number> The robot <highlight><bold>100</bold></highlight> has several sensors to prevent the robot <highlight><bold>100</bold></highlight> from running into obstacles and traveling off an edge. The sensors includes ambient light sensors <highlight><bold>123</bold></highlight>, active infrared emitters <highlight><bold>115</bold></highlight>, passive infrared sensor <highlight><bold>114</bold></highlight>, motor sensors (not shown), a tilt sensor and an edge sensor (not shown but described later). As the motor sensor and the tilt sensor are commonly known to one of ordinary skill in the art, they will not be described herein. </paragraph>
<paragraph id="P-0035" lvl="0"><number>&lsqb;0035&rsqb;</number> The ambient light sensor <highlight><bold>123</bold></highlight> determines if the ambient area in front of the robot <highlight><bold>100</bold></highlight> is below a minimum illumination. If the intensity of the ambient light is not enough to view objects through the video display <highlight><bold>504</bold></highlight> of the remote control <highlight><bold>500</bold></highlight> without additional light, an infrared mode of the video device <highlight><bold>122</bold></highlight> will be activated, allowing a user to see objects at night. </paragraph>
<paragraph id="P-0036" lvl="0"><number>&lsqb;0036&rsqb;</number> To help the robot <highlight><bold>100</bold></highlight> avoid bumping into obstacles and traveling off an edge, the robot <highlight><bold>100</bold></highlight> contains active infrared emitters <highlight><bold>115</bold></highlight>, a passive infrared (PIR) sensor <highlight><bold>114</bold></highlight> and four edge sensors <highlight><bold>107</bold></highlight>, described hereafter. Avoiding obstacles is an important function of the robot <highlight><bold>100</bold></highlight> so that the robot <highlight><bold>100</bold></highlight> can operate autonomously. Each leg <highlight><bold>106</bold></highlight> contains three active infrared sensors <highlight><bold>115</bold></highlight>. The PIR sensor <highlight><bold>114</bold></highlight> is located on the front of the body <highlight><bold>102</bold></highlight>. The edge sensors <highlight><bold>107</bold></highlight> are located in the legs <highlight><bold>106</bold></highlight>, whereby one sensor is located in each toe and heel of the leg <highlight><bold>106</bold></highlight>. </paragraph>
<paragraph id="P-0037" lvl="0"><number>&lsqb;0037&rsqb;</number> The robot <highlight><bold>100</bold></highlight> contains six active infrared emitters <highlight><bold>115</bold></highlight>, with three in each leg <highlight><bold>106</bold></highlight>. Signals emitted by the active infrared emitters <highlight><bold>115</bold></highlight> are detected by signal receiving device located within housing <highlight><bold>116</bold></highlight>. The three active infrared emitters <highlight><bold>115</bold></highlight> located in each leg <highlight><bold>106</bold></highlight> emits a signal at a different angle. The pattern is identical in both the legs <highlight><bold>106</bold></highlight>. For example, if the three active infrared emitters <highlight><bold>115</bold></highlight> are aligned in a vertical pattern, the top emitter would emit a signal in a substantially 90&deg; angle from the surface of leg <highlight><bold>106</bold></highlight>. Additionally, the middle emitter would emit a signal approximately 30&deg; offset towards the ground from the top emitter. The bottom emitter would emit a signal approximately 30&deg; offset towards the ground from the middle emitter. Since each emitter <highlight><bold>115</bold></highlight> emits a signal at a different angle, the signal will reflect off an object at different places in a room. </paragraph>
<paragraph id="P-0038" lvl="0"><number>&lsqb;0038&rsqb;</number> This pattern of active infrared emitters <highlight><bold>115</bold></highlight> allows for each emitter <highlight><bold>115</bold></highlight> to detect an object at a different distance or time. For example, since the top emitter emits a signal substantially parallel to the ground, the top emitter will indicate that an object is in front of the robot <highlight><bold>100</bold></highlight>, but at a distance far away. As the middle emitter emits a signal toward the floor, the middle emitter will indicate that an obstacle is in front of the robot <highlight><bold>100</bold></highlight> and closer than if the top emitter would have detected the object. Similarly, as the bottom emitter substantially emits a signal toward the ground, the top or middle emitter may not detect an object very close. Thus, the bottom emitter, by not receiving a signal, will indicate that an object is directly in front of the robot <highlight><bold>100</bold></highlight> and that the obstacle is very near, such as an edge. </paragraph>
<paragraph id="P-0039" lvl="0"><number>&lsqb;0039&rsqb;</number> Each active infrared emitter <highlight><bold>115</bold></highlight> emits a signal. However, only one emitter <highlight><bold>115</bold></highlight> emits a signal at a time. The six emitters <highlight><bold>115</bold></highlight> time share the signal receiving device. By only allowing one active infrared emitter <highlight><bold>115</bold></highlight> to send a signal at a time, the signal receiving device knows which emitter <highlight><bold>115</bold></highlight> sent the signal. Thus, the robot <highlight><bold>100</bold></highlight> can determine if the object is far away, near or immediately in front of it. Further, as the emitters <highlight><bold>115</bold></highlight> continuously emit a signal, the robot <highlight><bold>100</bold></highlight> can monitor and update the position of objects and edges. With three emitters <highlight><bold>115</bold></highlight> located on each leg <highlight><bold>106</bold></highlight>, the robot <highlight><bold>100</bold></highlight> can distinguish if the obstacle or edge is on the left or right side. </paragraph>
<paragraph id="P-0040" lvl="0"><number>&lsqb;0040&rsqb;</number> The PIR sensor <highlight><bold>114</bold></highlight> is a separate system from the active infrared emitters <highlight><bold>115</bold></highlight>. The PIR sensor <highlight><bold>114</bold></highlight> does not emit a signal. Instead, the PIR sensor <highlight><bold>114</bold></highlight> detects heat. Normally, a passive infrared sensor is not able to detect an object emitting heat if the object is stationary because a typical passive infrared sensor detects a change in temperature. However, the robot <highlight><bold>100</bold></highlight> can detect a stationary object that emits heat because the PIR sensor <highlight><bold>114</bold></highlight> is mounted in housing <highlight><bold>116</bold></highlight>, which continuously rotates through a range. </paragraph>
<paragraph id="P-0041" lvl="0"><number>&lsqb;0041&rsqb;</number> Referring now to <cross-reference target="DRAWINGS">FIG. 12</cross-reference>, the motor <highlight><bold>250</bold></highlight> has an attached pulley <highlight><bold>252</bold></highlight> to drive the pulley <highlight><bold>256</bold></highlight> via the flexible belt <highlight><bold>254</bold></highlight>. The pulley <highlight><bold>256</bold></highlight> drives the pulley <highlight><bold>258</bold></highlight>, which drives the pulley <highlight><bold>260</bold></highlight>, which drives the pulley <highlight><bold>262</bold></highlight>, which drives the housing <highlight><bold>116</bold></highlight>. Similar to the drive mechanism <highlight><bold>300</bold></highlight>, this drive belt provides tolerance for heavy loads and reduces noise over using hard gears. An optical emitter/receiver (not shown and similar to these described above) monitors the movement and location of the gear that drives the housing <highlight><bold>116</bold></highlight>. The PIR sensor <highlight><bold>114</bold></highlight> will detect heat signals as it moves through the range dictated by the housing <highlight><bold>116</bold></highlight>. For example, as the housing <highlight><bold>116</bold></highlight> rotates through its range, the temperature differential between the person and the surrounding environment will be detected by the PIR sensor <highlight><bold>114</bold></highlight>. The robot <highlight><bold>100</bold></highlight> will know the location of the person in relation to the robot <highlight><bold>100</bold></highlight> by the angle the housing <highlight><bold>116</bold></highlight> is at the moment the PIR sensor <highlight><bold>114</bold></highlight> detects the heat differential. </paragraph>
<paragraph id="P-0042" lvl="0"><number>&lsqb;0042&rsqb;</number> The edge detection system relies on feedback from the active infrared emitters <highlight><bold>115</bold></highlight>, the signal receiving device, and an edge detection element <highlight><bold>107</bold></highlight> located in leg <highlight><bold>106</bold></highlight>. The active infrared emitters <highlight><bold>115</bold></highlight> sequentially send out signals as previously described. When the signal receiving device detects an edge from the signal emitted by an active infrared emitter <highlight><bold>115</bold></highlight>, the robot <highlight><bold>100</bold></highlight> will then slow down, thus allowing the edge detection element <highlight><bold>107</bold></highlight> in leg <highlight><bold>106</bold></highlight> to confirm that there is an edge. The edge detection element <highlight><bold>107</bold></highlight> is a leaf switch <highlight><bold>111</bold></highlight> connected with the front wheel <highlight><bold>120</bold></highlight> and the rear wheel <highlight><bold>121</bold></highlight>. As the robot <highlight><bold>100</bold></highlight> moves slowly forward, if the front wheel <highlight><bold>120</bold></highlight> or the rear wheel <highlight><bold>121</bold></highlight> travels a predetermined distance downward, the leaf switch <highlight><bold>111</bold></highlight> will close and complete a circuit to send a signal to the robot <highlight><bold>100</bold></highlight> that there is an edge. Thus, the robot <highlight><bold>100</bold></highlight> will not continue to travel in that direction. Instead, the robot <highlight><bold>100</bold></highlight> will change direction and continue to operate autonomously. The edge detection element <highlight><bold>107</bold></highlight> also serves as a backup to the active infrared sensors <highlight><bold>115</bold></highlight> ability to detect an edge. For example, the signals sent by the active infrared emitters <highlight><bold>115</bold></highlight> will not reflect from a black carpet. Therefore, the signal receiving device will not detect an edge. In this case, the edge detection element <highlight><bold>107</bold></highlight> will be the first and only method to detect an edge. </paragraph>
<paragraph id="P-0043" lvl="0"><number>&lsqb;0043&rsqb;</number> The motor sensors located within the body <highlight><bold>102</bold></highlight> monitor current surges to the motors to tilt the body <highlight><bold>102</bold></highlight>, rotate the arms <highlight><bold>104</bold></highlight>, rotate the rotating platform <highlight><bold>124</bold></highlight>, and drive the center wheel <highlight><bold>138</bold></highlight>. If a surge in current exceeds a minimum threshold, the robot <highlight><bold>100</bold></highlight> will notify the user by speaking from its vocabulary (e.g., &ldquo;ouch,&rdquo; &ldquo;stop it,&rdquo; &ldquo;that hurts,&rdquo; &ldquo;that&apos;s heavy,&rdquo; etc.). </paragraph>
<paragraph id="P-0044" lvl="0"><number>&lsqb;0044&rsqb;</number> Robot <highlight><bold>100</bold></highlight> has several modes by which the robot <highlight><bold>100</bold></highlight> can operate. Several modes allow the robot <highlight><bold>100</bold></highlight> to operate autonomously, while other modes require a user to remotely manipulate the robot <highlight><bold>100</bold></highlight>. The mode settings include a remote control mode, a monitor mode, an automatic mode, a security mode, a greet mode and a demonstration mode. </paragraph>
<paragraph id="P-0045" lvl="0"><number>&lsqb;0045&rsqb;</number> When the automatic mode is selected, the robot <highlight><bold>100</bold></highlight> begins to move autonomously throughout the room. As explained above, the active infrared emitters <highlight><bold>115</bold></highlight> assist the robot <highlight><bold>100</bold></highlight> to avoid bumping into obstacles and traveling off an edge. While the robot <highlight><bold>100</bold></highlight> is moving throughout the room it will occasionally speak from the auto vocabulary, depending on sensor input. Simultaneously, the PIR sensor <highlight><bold>114</bold></highlight> scans the area in front of the robot <highlight><bold>100</bold></highlight> to detect a heat source. When the robot <highlight><bold>100</bold></highlight> detects a heat source, the rotatable platform <highlight><bold>124</bold></highlight> will turn toward the object and speak from its &ldquo;roam&rdquo; vocabulary (e.g., &ldquo;Nice to see you again.&rdquo;, &ldquo;How are you.&rdquo;, etc.) </paragraph>
<paragraph id="P-0046" lvl="0"><number>&lsqb;0046&rsqb;</number> The motor mechanism which drives the rotatable platform <highlight><bold>124</bold></highlight> is shown in <cross-reference target="DRAWINGS">FIG. 11</cross-reference>. A motor has an attached pulley <highlight><bold>200</bold></highlight> to drive the pulley <highlight><bold>202</bold></highlight> via flexible belt <highlight><bold>204</bold></highlight>. As the motor is commonly known to one of ordinary skill in the art, the motor will not be described further. The pulley <highlight><bold>202</bold></highlight> drives the pulley <highlight><bold>206</bold></highlight>, which drives the pulley <highlight><bold>208</bold></highlight>, which drives the pulley <highlight><bold>209</bold></highlight>, which drives the pulley <highlight><bold>210</bold></highlight>, which drives the pulley <highlight><bold>212</bold></highlight>. The pulley <highlight><bold>212</bold></highlight> drives the rotatable platform <highlight><bold>124</bold></highlight>. The pulleys <highlight><bold>208</bold></highlight>,<highlight><bold>209</bold></highlight> and <highlight><bold>210</bold></highlight> are the same to lower the cost and complexity of the mechanism. The motor mechanism allows the rotatable platform <highlight><bold>124</bold></highlight> to rotate either left or right, up to 135&deg;. </paragraph>
<paragraph id="P-0047" lvl="0"><number>&lsqb;0047&rsqb;</number> The robot <highlight><bold>100</bold></highlight> can also detect the location of a noise. Three microphones <highlight><bold>117</bold></highlight> are placed around the robot <highlight><bold>100</bold></highlight> at approximately 120&deg; angles apart from each other. The microphones <highlight><bold>117</bold></highlight> can detect the phase difference in a sound detected so that the robot <highlight><bold>100</bold></highlight> can determine what direction the sound originated from. When a noise is detected, the robot <highlight><bold>100</bold></highlight> will turn its rotatable platform <highlight><bold>124</bold></highlight> towards the object as if it is speaking directly to the object. </paragraph>
<paragraph id="P-0048" lvl="0"><number>&lsqb;0048&rsqb;</number> The robot <highlight><bold>100</bold></highlight> can also provide security to a household. When the security mode is selected, the robot <highlight><bold>100</bold></highlight> stands still with minimum power consumption. When the microphones <highlight><bold>117</bold></highlight> on the robot <highlight><bold>100</bold></highlight> detect noise above a minimum threshold, the rotatable platform <highlight><bold>124</bold></highlight> turns towards the noise source and the PIR sensor <highlight><bold>114</bold></highlight> begins to scan. If a heat source is detected, the robot <highlight><bold>100</bold></highlight> turns on the light <highlight><bold>118</bold></highlight>, the rotatable platform <highlight><bold>124</bold></highlight> turns towards the heat source, and the robot <highlight><bold>100</bold></highlight> makes an announcement from the security vocabulary. Further, the robot sends an alarm signal to the remote control device <highlight><bold>500</bold></highlight> to alert a user that an object has been detected. </paragraph>
<paragraph id="P-0049" lvl="0"><number>&lsqb;0049&rsqb;</number> Robot <highlight><bold>100</bold></highlight> can also greet people. When the greet mode is selected, the robot <highlight><bold>100</bold></highlight> scans with the PIR sensor <highlight><bold>114</bold></highlight> to search for a detectable object (e.g., a person). If a heat source is detected, the robot <highlight><bold>100</bold></highlight> turns the rotatable platform <highlight><bold>124</bold></highlight> towards the source and makes an announcement from the greeting vocabulary. </paragraph>
<paragraph id="P-0050" lvl="0"><number>&lsqb;0050&rsqb;</number> Robot <highlight><bold>100</bold></highlight> can also demonstrate many of its functions through a preprogrammed routine. When the demonstration mode is selected, the robot <highlight><bold>100</bold></highlight> performs several motions to display various functions that the robot can operate. For example, the robot will rotate its arms <highlight><bold>104</bold></highlight> through the full range of motion, tilt its body and speak. </paragraph>
<paragraph id="P-0051" lvl="0"><number>&lsqb;0051&rsqb;</number> The robot <highlight><bold>100</bold></highlight> can also be manipulated remotely by a user. When the remote control mode is selected, the robot <highlight><bold>100</bold></highlight> is manipulated remotely by a user via a remote control device <highlight><bold>500</bold></highlight> (See <cross-reference target="DRAWINGS">FIG. 13</cross-reference>) or via the Internet. </paragraph>
<paragraph id="P-0052" lvl="0"><number>&lsqb;0052&rsqb;</number> Finally, when the monitor mode is selected, the drive mechanism <highlight><bold>300</bold></highlight> is disabled so that the robot cannot move. However, the robot <highlight><bold>100</bold></highlight> can transmit audio and video signals to the remote control device <highlight><bold>500</bold></highlight> so that a user can remotely view objects in front of the robot and hear sounds within the vicinity of the robot <highlight><bold>100</bold></highlight>. A user is not limited to the range of remote control device <highlight><bold>500</bold></highlight> if the user is remotely manipulating the robot <highlight><bold>100</bold></highlight> via the Internet. </paragraph>
<paragraph id="P-0053" lvl="0"><number>&lsqb;0053&rsqb;</number> The robot <highlight><bold>100</bold></highlight> can also display moods to enhance or compliment the specific mode the robot <highlight><bold>100</bold></highlight> is operating in. The different moods are expressed by the eyes <highlight><bold>128</bold></highlight> and the mouth <highlight><bold>126</bold></highlight>. The eyes <highlight><bold>128</bold></highlight> allow the robot <highlight><bold>100</bold></highlight> to express moods through different combinations of lighting. The eyes <highlight><bold>128</bold></highlight> contain several lights where each light emits at least one color. The lights may be arranged in several combinations. The combination of lights maybe activated to display at least one color. Specifically, the lights within eyes <highlight><bold>128</bold></highlight> consist of one blue light, two amber lights and two red lights. The preferred embodiment for the eyes <highlight><bold>128</bold></highlight> is such that the blue light is positioned in a forward position while the two red and two amber lights are positioned in a rearward position. A reflective surface is placed in the eyes <highlight><bold>128</bold></highlight> facing the amber and red lights so that the amber and red lights emit light in a forward direction to blend with the blue light. The color emitted from the eyes <highlight><bold>128</bold></highlight> can be any combination of the blue, amber, and red lights. The combination of lights activated depends on whether the robot <highlight><bold>100</bold></highlight> is in the night light mode, the monitor mode, the security mode, the remote control mode, the automatic mode or the greet mode. </paragraph>
<paragraph id="P-0054" lvl="0"><number>&lsqb;0054&rsqb;</number> When the robot <highlight><bold>100</bold></highlight> is in the night light mode, two amber and two red lights are activated and emitted from the eyes <highlight><bold>128</bold></highlight>. When the robot <highlight><bold>100</bold></highlight> is in the monitor mode, one amber light is activated and emitted from the eyes <highlight><bold>128</bold></highlight> at all times. When the robot <highlight><bold>100</bold></highlight> is in the security mode, the lights activated depend on whether the robot <highlight><bold>100</bold></highlight> is talking or not talking. When the robot <highlight><bold>100</bold></highlight> is not talking, one blue light is activated and emitted from the eyes <highlight><bold>128</bold></highlight>. When the robot <highlight><bold>100</bold></highlight> is talking, one blue light and two red lights are activated and emitted from the eyes <highlight><bold>128</bold></highlight>. </paragraph>
<paragraph id="P-0055" lvl="0"><number>&lsqb;0055&rsqb;</number> When the robot is the remote mode, automatic mode or greet mode, the lights activated depend on whether the robot <highlight><bold>100</bold></highlight> is not talking, talking, tired, or tired and talking. When the robot <highlight><bold>100</bold></highlight> is not talking in either of these modes, one blue light and one amber light are activated and emitted from the eyes <highlight><bold>128</bold></highlight>. When the robot <highlight><bold>100</bold></highlight> is talking, one blue light and two amber lights are activated and emitted from the eyes <highlight><bold>128</bold></highlight>. When the robot <highlight><bold>100</bold></highlight> is tired, one blue light and one red light is activated and emitted from the eyes <highlight><bold>128</bold></highlight>. Lastly, when the robot <highlight><bold>100</bold></highlight> is tired and talking, one blue light and two red lights are activated and emitted from the eyes <highlight><bold>128</bold></highlight>. </paragraph>
<paragraph id="P-0056" lvl="0"><number>&lsqb;0056&rsqb;</number> To compliment all speech, the robot <highlight><bold>100</bold></highlight> also has a mouth <highlight><bold>126</bold></highlight> to express emotions. The mouth <highlight><bold>126</bold></highlight> consists of several rows of red LED&apos;s that can be individually activated. Depending on the sensor input and vocabulary spoken, the robot <highlight><bold>100</bold></highlight> can demonstrate emotions such as a smile, a frown, puzzled, surprise, concentration and thinking. When the robot <highlight><bold>100</bold></highlight> is speaking, the LED&apos;s continuously change in pattern. </paragraph>
<paragraph id="P-0057" lvl="0"><number>&lsqb;0057&rsqb;</number> Another feature of robot <highlight><bold>100</bold></highlight> is a low battery indicator <highlight><bold>139</bold></highlight> (See <cross-reference target="DRAWINGS">FIG. 2</cross-reference>). The low battery indicator <highlight><bold>139</bold></highlight> contains five rectangular LED&apos;s on the back panel of the robot <highlight><bold>100</bold></highlight>. When the robot <highlight><bold>100</bold></highlight> is fully charged, all five LED&apos;s are lighted. When the power level is down to one lighted LED, the robot <highlight><bold>100</bold></highlight> has a vocabulary to indicate that the power is low and the robot <highlight><bold>100</bold></highlight> needs recharging. As the robot <highlight><bold>100</bold></highlight> detects that the battery becomes discharged, the robot <highlight><bold>100</bold></highlight> will reduce its functions to preserve power in the following order: first, the video functions; then, the audio functions; then, the locomotion functions will be eliminated. The remote control device <highlight><bold>500</bold></highlight> also has a low battery circuit which includes an audio power display and power cutoff. The power cut off function is very important as lead-acid batteries will last through many more discharge cycles if they are not fully discharged with each use. </paragraph>
<paragraph id="P-0058" lvl="0"><number>&lsqb;0058&rsqb;</number> The control block diagram for the robot <highlight><bold>100</bold></highlight> is shown in <cross-reference target="DRAWINGS">FIG. 16</cross-reference>. As shown, there are several microcontroller units (MCU) <highlight><bold>400</bold></highlight> that coordinate all the functions of the robot <highlight><bold>100</bold></highlight>. The MCU <highlight><bold>400</bold></highlight> consists of several, independent integrated circuits to control different functions of the robot. As explained above and illustrated by <cross-reference target="DRAWINGS">FIG. 16</cross-reference>, the active infrared emitters <highlight><bold>115</bold></highlight> and the PIR sensor <highlight><bold>114</bold></highlight> are independentantly controlled. </paragraph>
<paragraph id="P-0059" lvl="0"><number>&lsqb;0059&rsqb;</number> Referring now to <cross-reference target="DRAWINGS">FIG. 13</cross-reference>, the remote control device <highlight><bold>500</bold></highlight> is used to manipulate the robot <highlight><bold>100</bold></highlight> remotely. The remote control device <highlight><bold>500</bold></highlight> is a separately powered device from the robot <highlight><bold>100</bold></highlight>. An on/off button <highlight><bold>328</bold></highlight> may be depressed to turn the remote control device <highlight><bold>500</bold></highlight> on and off. The remote control device <highlight><bold>500</bold></highlight> contains a joystick <highlight><bold>502</bold></highlight>, video display <highlight><bold>504</bold></highlight>, a microphone <highlight><bold>506</bold></highlight>, a transmitter/receiver <highlight><bold>508</bold></highlight> and several controls by which a user can manipulate the robot (which will be disclosed later in this application). The joystick <highlight><bold>502</bold></highlight> is at a height suitable for use with the single thumb of a user. The joystick <highlight><bold>502</bold></highlight> has eight compass points to translate motion of the robot <highlight><bold>100</bold></highlight>. The eight compass points include left forward, straight forward, right forward, spin left, spin right, left backward, straight backward, and right backward. When any forward position of the joystick <highlight><bold>502</bold></highlight> is engaged for more than three seconds the robot <highlight><bold>100</bold></highlight> will increase speed in the direction engaged, limited by a maximum speed. </paragraph>
<paragraph id="P-0060" lvl="0"><number>&lsqb;0060&rsqb;</number> The video display <highlight><bold>504</bold></highlight> allows the user to remotely view the area in front of the robot <highlight><bold>100</bold></highlight>. The robot <highlight><bold>100</bold></highlight> has a video device <highlight><bold>122</bold></highlight> which is located on the rotatable platform <highlight><bold>124</bold></highlight>. The image transmitted by the video device <highlight><bold>122</bold></highlight> is displayed in the video display <highlight><bold>504</bold></highlight>. By turning the rotating platform <highlight><bold>124</bold></highlight> or moving the robot <highlight><bold>100</bold></highlight> in a different direction, a user may see a different area of the room. The contrast knob <highlight><bold>536</bold></highlight> helps the user adjust the contrast of the video display <highlight><bold>504</bold></highlight> to optimize the image displayed. To conserve battery power, the video display <highlight><bold>504</bold></highlight> may be turned off by depressing the display power button <highlight><bold>526</bold></highlight>. Even though the video display <highlight><bold>504</bold></highlight> is off the robot <highlight><bold>100</bold></highlight> can still be manipulated by the remote control <highlight><bold>500</bold></highlight>. </paragraph>
<paragraph id="P-0061" lvl="0"><number>&lsqb;0061&rsqb;</number> The microphone <highlight><bold>506</bold></highlight> allows a user to transmit his voice to the robot <highlight><bold>100</bold></highlight> so that the user&apos;s voice is projected from the robot <highlight><bold>100</bold></highlight>. The remote control <highlight><bold>500</bold></highlight> has three voice input buttons <highlight><bold>510</bold></highlight>,<highlight><bold>512</bold></highlight> and <highlight><bold>514</bold></highlight>. By depressing and holding down any of the voice input buttons, a user may speak into the microphone <highlight><bold>506</bold></highlight> and the voice will be transmitted to the robot <highlight><bold>100</bold></highlight>. The voice input button <highlight><bold>510</bold></highlight> allows the user&apos;s voice to be transmitted to the robot <highlight><bold>100</bold></highlight>. The voice input buttons <highlight><bold>512</bold></highlight> and <highlight><bold>514</bold></highlight> activate and audio circuit which distorts the user&apos;s voice before it is transmitted to the robot <highlight><bold>100</bold></highlight>. Thus, the user&apos;s voice projected from the robot <highlight><bold>100</bold></highlight> is disguised. The voice input buttons <highlight><bold>512</bold></highlight> and <highlight><bold>514</bold></highlight> distorts the user&apos;s voice in a different manner. In addition to transmitting your voice to the robot <highlight><bold>100</bold></highlight>, the remote control <highlight><bold>500</bold></highlight> can receive sounds detected by the robot <highlight><bold>100</bold></highlight>. The microphones <highlight><bold>117</bold></highlight> on the robot <highlight><bold>100</bold></highlight> detect surrounding noise and transmit it back to the remote control <highlight><bold>500</bold></highlight> so that a user may hear them. The volume control knob <highlight><bold>534</bold></highlight> allows the user to turn the volume of the noise up or down. </paragraph>
<paragraph id="P-0062" lvl="0"><number>&lsqb;0062&rsqb;</number> The transmitter/receiver <highlight><bold>508</bold></highlight> has two antennas. First, a 2.4 GHz antenna <highlight><bold>552</bold></highlight> sends audio and video signals from the robot <highlight><bold>100</bold></highlight> to the remote control device <highlight><bold>500</bold></highlight>. The second antenna is a 900 MHz antenna <highlight><bold>554</bold></highlight> that sends control signals from the remote control device <highlight><bold>500</bold></highlight> to the robot <highlight><bold>100</bold></highlight>. 900 MHz and 2.4 GHz are common frequencies by which many household devices operate on. To insure that the remote control device <highlight><bold>500</bold></highlight> will not interfere with other devices in the house (e.g., a cordless phone) each antenna has additional channels which the user may select. Specifically, the 2.4 GHz antenna <highlight><bold>552</bold></highlight> has two channels and the 900 MHz antenna <highlight><bold>554</bold></highlight> has three channels a user may select to avoid interfering with other similar devices in the house (each cordless phone). </paragraph>
<paragraph id="P-0063" lvl="0"><number>&lsqb;0063&rsqb;</number> The robot <highlight><bold>100</bold></highlight> can perform many functions. Several of the functions include tilting the body <highlight><bold>102</bold></highlight>, rotating the arms <highlight><bold>104</bold></highlight>, griping an object, rotating the rotatable platform <highlight><bold>124</bold></highlight>, and moving the robot <highlight><bold>100</bold></highlight>. The body <highlight><bold>102</bold></highlight> can tilt 180&deg; forward 30&deg; rearward. Tilting the body <highlight><bold>102</bold></highlight> forward is accomplished by pressing control button <highlight><bold>550</bold></highlight>. Tilting the body <highlight><bold>102</bold></highlight> rearward is accomplished by pressing control button <highlight><bold>538</bold></highlight>. By pressing and holding either button, the body <highlight><bold>102</bold></highlight> will continue to rotate, stopping when the button is released or the body <highlight><bold>102</bold></highlight> reaches its maximum tilt angle. </paragraph>
<paragraph id="P-0064" lvl="0"><number>&lsqb;0064&rsqb;</number> The arms <highlight><bold>104</bold></highlight> can rotate through many positions, including two &ldquo;serve&rdquo; positions which are located at the 90&deg; and the 180&deg; positions from rest (See <cross-reference target="DRAWINGS">FIG. 7</cross-reference>). By depressing briefly the up direction button <highlight><bold>540</bold></highlight> or the down direction button <highlight><bold>548</bold></highlight>, the arms <highlight><bold>104</bold></highlight> will increment to the next preset position in the direction indicated. Longer button depressions will control motion of the arms <highlight><bold>104</bold></highlight> manually, stopping at a position when the button is released. Both the up direction button <highlight><bold>540</bold></highlight> and the down direction button <highlight><bold>548</bold></highlight> are divided into a left and right half, whereby the right half controls the right arm <highlight><bold>104</bold></highlight> and the left half controls the left arm <highlight><bold>104</bold></highlight>. Thus, both arms <highlight><bold>104</bold></highlight> can be controlled independently of the other. </paragraph>
<paragraph id="P-0065" lvl="0"><number>&lsqb;0065&rsqb;</number> To grip an object, the second finger element <highlight><bold>108</bold></highlight> can move to a width opening of approximately 75 millimeters away from the first finger element <highlight><bold>110</bold></highlight>. The second finger element <highlight><bold>108</bold></highlight> can be opened and closed via the hand control button <highlight><bold>544</bold></highlight> on the remote control <highlight><bold>500</bold></highlight>. Similar to the direction buttons, by quickly depressing the hand control button <highlight><bold>544</bold></highlight>, the second finger element <highlight><bold>108</bold></highlight> will move to the next preset position. As the motor <highlight><bold>150</bold></highlight> that controls the movement of the second finger element <highlight><bold>108</bold></highlight> only rotates in one direction, the second finger element <highlight><bold>108</bold></highlight> simply cycles through an open and close position. By holding down the hand control button <highlight><bold>544</bold></highlight> is also divided into a left and right portion. The left half of the hand control button <highlight><bold>544</bold></highlight> controls the left hand and the right half of the hand control button <highlight><bold>544</bold></highlight> controls the right hand grip. Thus, the hand grips can be controlled independently. Thus, holding down the hand control button <highlight><bold>544</bold></highlight> cycles the second finger element <highlight><bold>108</bold></highlight> through the entire range of motion. The second finger element <highlight><bold>108</bold></highlight> is also clutched in both directions. </paragraph>
<paragraph id="P-0066" lvl="0"><number>&lsqb;0066&rsqb;</number> The serving positions of the arms <highlight><bold>104</bold></highlight> can be automatically accessed by depressing serving button <highlight><bold>530</bold></highlight>. Each time the serving button <highlight><bold>530</bold></highlight> is depressed, the following positions of the arms <highlight><bold>104</bold></highlight> are achieved: First, the right arm <highlight><bold>104</bold></highlight> rotates to a 90&deg; position. Second, the right arm <highlight><bold>104</bold></highlight> rotates to a 180&deg; position. Third, the left arm <highlight><bold>104</bold></highlight> rotates to a 90&deg; position. Fourth, the right arm <highlight><bold>104</bold></highlight> returns to the 90&deg; position. Fifth, the right arm <highlight><bold>104</bold></highlight> returns to the 180&deg; position. Sixth, the left arm <highlight><bold>104</bold></highlight> rotates to the 180&deg; position. </paragraph>
<paragraph id="P-0067" lvl="0"><number>&lsqb;0067&rsqb;</number> The rotatable platform <highlight><bold>124</bold></highlight> can also be controlled remotely by depressing the left rotate button <highlight><bold>542</bold></highlight> and the right rotate button <highlight><bold>546</bold></highlight>. The rotatable platform <highlight><bold>124</bold></highlight> can rotate approximately 135&deg; in either direction. By intermittingly depressing either the left control button <highlight><bold>542</bold></highlight> or the right control button <highlight><bold>546</bold></highlight> the rotatable platform <highlight><bold>124</bold></highlight> will turn incrementally. If the rotatable platform <highlight><bold>124</bold></highlight> is not at the center position when the drive control is activated, the rotatable platform <highlight><bold>124</bold></highlight> will automatically return to the center/forward position. This function allows the user to view where the robot <highlight><bold>100</bold></highlight> is traveling. </paragraph>
<paragraph id="P-0068" lvl="0"><number>&lsqb;0068&rsqb;</number> The remote control <highlight><bold>500</bold></highlight> can also be used to select which mode the robot <highlight><bold>100</bold></highlight> will operate. When the mode button <highlight><bold>516</bold></highlight> is selected, the robot <highlight><bold>100</bold></highlight> enters into the automatic mode. When the mode button <highlight><bold>518</bold></highlight> is selected, the robot <highlight><bold>100</bold></highlight> enters the monitor mode. When the mode button <highlight><bold>520</bold></highlight> is selected, the robot enters the security mode. When the mode button <highlight><bold>522</bold></highlight> is selected, the robot <highlight><bold>100</bold></highlight> enters the greet mode. When the mode button <highlight><bold>524</bold></highlight> is selected, the robot <highlight><bold>100</bold></highlight> enters the remote control mode. If the robot <highlight><bold>100</bold></highlight> is operating in an autonomous mode, the user may depress the mode button <highlight><bold>524</bold></highlight> to end the autonomous mode and then the robot <highlight><bold>100</bold></highlight> can be controlled again by the remote control device <highlight><bold>500</bold></highlight>. </paragraph>
<paragraph id="P-0069" lvl="0"><number>&lsqb;0069&rsqb;</number> The remote control device <highlight><bold>500</bold></highlight> can also activate the light <highlight><bold>118</bold></highlight>. If it is dark withing the room and a user wishes to provide additional light in front of the robot <highlight><bold>100</bold></highlight>, the user may do so by depressing the light button <highlight><bold>532</bold></highlight>. By depressing the light button <highlight><bold>532</bold></highlight> once, the light <highlight><bold>118</bold></highlight> is turned on. Depressing the light button <highlight><bold>532</bold></highlight> a second time activates the bright setting of the light <highlight><bold>118</bold></highlight>. Depressing the light button <highlight><bold>532</bold></highlight> a third time turns the light <highlight><bold>118</bold></highlight> off. </paragraph>
<paragraph id="P-0070" lvl="0"><number>&lsqb;0070&rsqb;</number> Referring now to <cross-reference target="DRAWINGS">FIG. 17, a</cross-reference> block diagram illustrates the controls of the remote control <highlight><bold>500</bold></highlight>. The microcontroller units (MCU) <highlight><bold>450</bold></highlight> independently receive signals from the keyboard and joystick. These signals are then sent to the 900 MHz transceiver <highlight><bold>554</bold></highlight> for transmitting to the robot <highlight><bold>100</bold></highlight>. <cross-reference target="DRAWINGS">FIG. 17</cross-reference> also shows that signals received by the microphone <highlight><bold>506</bold></highlight> are sent to the voice shifting device and then to the 900 MHz transceiver <highlight><bold>554</bold></highlight> and finally transmitted to the robot <highlight><bold>100</bold></highlight>. </paragraph>
<paragraph id="P-0071" lvl="0"><number>&lsqb;0071&rsqb;</number> There are several controls which are located on the robot <highlight><bold>100</bold></highlight> and not on the remote control <highlight><bold>500</bold></highlight>. For example, a user may press and hold the message button <highlight><bold>142</bold></highlight> located on the back of the robot <highlight><bold>100</bold></highlight> to record a message for up to fifteen seconds. Once the message is recorded, the message button <highlight><bold>142</bold></highlight> may be pressed again to hear the recorded message played back. In addition, the find remote button <highlight><bold>143</bold></highlight> sends an announce signal to the remote control <highlight><bold>500</bold></highlight> whereby the remote control <highlight><bold>500</bold></highlight> will make a noise allowing the user find the remote control device <highlight><bold>500</bold></highlight>. The power button <highlight><bold>144</bold></highlight> is also located on the back of the robot <highlight><bold>100</bold></highlight>. The power button <highlight><bold>144</bold></highlight> can be pressed to turn the robot <highlight><bold>100</bold></highlight> on and off. Further, if the user presses and holds the power button <highlight><bold>144</bold></highlight> for approximately two seconds the robot will enter the demonstration mode. </paragraph>
<paragraph id="P-0072" lvl="0"><number>&lsqb;0072&rsqb;</number> The foregoing description of preferred embodiments of the present invention has been provided for the purposes of illustration and description. It is not intended to be exhaustive or to limit the invention to the precise forms disclosed. Obviously, many modifications and variations will be apparent to the practitioner skilled in the art. The embodiments were chosen and described in order to best explain the principles of the invention and its practical application, thereby enabling others skilled in the art to understand the invention for various embodiments and with various modifications that are suited to the particular use contemplated. It is intended that the scope of the invention be defined by the following claims and their equivalence. </paragraph>
</section>
</detailed-description>
</subdoc-description>
<subdoc-claims>
<claim id="CLM-00001">
<claim-text><highlight><bold>1</bold></highlight>. A robot capable of expressing moods, the robot including: 
<claim-text>a plurality of light elements; </claim-text>
<claim-text>a first subset of said light elements directed in a forward direction, said first subset of light elements capable of emitting light of at least a first color; </claim-text>
<claim-text>a second subset of said light elements directed in a rearward direction, said second subset of light elements capable of emitting light of at least a second color, the second color different than the first color; and </claim-text>
<claim-text>a reflective surface generally facing said second subset of light elements; </claim-text>
<claim-text>wherein light emitted by said second subset of light elements is directed in the forward direction by said reflective surface; </claim-text>
<claim-text>wherein various combinations of said plurality of light elements are activated to express moods. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00002">
<claim-text><highlight><bold>2</bold></highlight>. The robot according to <dependent-claim-reference depends_on="CLM-00001">claim 1</dependent-claim-reference>, wherein when light is emitted simultaneously by at least one of said first subset of light elements and at least one of said second subset of light elements, the emitted light is combined to produce at least a third color, the third color different than the first and second colors. </claim-text>
</claim>
<claim id="CLM-00003">
<claim-text><highlight><bold>3</bold></highlight>. The robot according to <dependent-claim-reference depends_on="CLM-00002">claim 2</dependent-claim-reference>, wherein: 
<claim-text>said first subset of light elements includes a blue light; and </claim-text>
<claim-text>said second subset of light elements includes an amber light and a red light. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00004">
<claim-text><highlight><bold>4</bold></highlight>. The robot according to <dependent-claim-reference depends_on="CLM-00003">claim 3</dependent-claim-reference>, wherein one of said combinations of said plurality of lights is selected based on a mode of the robot. </claim-text>
</claim>
<claim id="CLM-00005">
<claim-text><highlight><bold>5</bold></highlight>. The robot according to <dependent-claim-reference depends_on="CLM-00004">claim 4</dependent-claim-reference>, wherein one of said combinations is selected depending on whether the robot is in a nightlight mode, a monitor mode, a security mode, a remote mode, a roam mode or a greet mode. </claim-text>
</claim>
<claim id="CLM-00006">
<claim-text><highlight><bold>6</bold></highlight>. The robot according to <dependent-claim-reference depends_on="CLM-00004">claim 4</dependent-claim-reference>, wherein when the robot is in a nightlight mode, said amber light and said red light are activated to emit light. </claim-text>
</claim>
<claim id="CLM-00007">
<claim-text><highlight><bold>7</bold></highlight>. The robot according to <dependent-claim-reference depends_on="CLM-00006">claim 6</dependent-claim-reference>, wherein when the robot is in a monitor mode, said amber light is activated to emit light. </claim-text>
</claim>
<claim id="CLM-00008">
<claim-text><highlight><bold>8</bold></highlight>. The robot according to <dependent-claim-reference depends_on="CLM-00007">claim 7</dependent-claim-reference>, wherein when the robot is in a security mode, said blue light is activated to emit light if the robot is not talking, and said blue light and said red light are both activated to emit light if the robot is talking. </claim-text>
</claim>
<claim id="CLM-00009">
<claim-text><highlight><bold>9</bold></highlight>. The robot according to <dependent-claim-reference depends_on="CLM-00008">claim 8</dependent-claim-reference>, wherein when the robot is in one of a remote mode, an automatic mode and a greet mode, one of said combinations of said plurality of lights is activated depending on whether the robot is talking and/or tired. </claim-text>
</claim>
<claim id="CLM-00010">
<claim-text><highlight><bold>10</bold></highlight>. The robot according to <dependent-claim-reference depends_on="CLM-00002">claim 2</dependent-claim-reference>, wherein: 
<claim-text>said first subset of lights includes a blue light; </claim-text>
<claim-text>said second subset of light includes two amber lights and two red lights; and </claim-text>
<claim-text>one of said combinations of said plurality of lights is selected based on a mode of the robot, the robot including at least two of the following modes: a nightlight mode; a monitor mode; a security mode; a remote mode; a roam mode; and a greet mode. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00011">
<claim-text><highlight><bold>11</bold></highlight>. A robot capable of expressing moods and demonstrating emotions, the robot comprising: 
<claim-text>a mouth including a plurality of rows of light elements; and </claim-text>
<claim-text>a pair of eyes, each said eye including: 
<claim-text>a plurality of further light elements; </claim-text>
<claim-text>a first subset of said further light elements directed in a forward direction, said first subset of said further light elements capable of producing at least a first color; </claim-text>
<claim-text>a reflective surface at a rear of said eye; and </claim-text>
<claim-text>a second subset of said further light elements directed in a rearward direction and generally toward said reflective surface, said second subset of said further light elements capable of producing at least a second color, the second color different than the first color; </claim-text>
</claim-text>
<claim-text>wherein the robot can demonstrate emotions by activating various combinations of said light elements in said plurality of rows of light elements of said mouth; </claim-text>
<claim-text>wherein the robot can express moods by activating various combinations of said plurality of further lights elements in said pair of eyes. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00012">
<claim-text><highlight><bold>12</bold></highlight>. The robot according to <dependent-claim-reference depends_on="CLM-00011">claim 11</dependent-claim-reference>, wherein said plurality of rows of light elements of said mouth comprise a plurality of rows of light emitting diodes (LEDs). </claim-text>
</claim>
<claim id="CLM-00013">
<claim-text><highlight><bold>13</bold></highlight>. The robot according to <dependent-claim-reference depends_on="CLM-00011">claim 12</dependent-claim-reference>, wherein said LEDs continuously change in pattern when the robot is talking. </claim-text>
</claim>
<claim id="CLM-00014">
<claim-text><highlight><bold>14</bold></highlight>. The robot according to <dependent-claim-reference depends_on="CLM-00011">claim 13</dependent-claim-reference>, wherein the emotions that can be demonstrated, by activating various combinations of said LEDs in said plurality of rows of LEDs, include at least two of the following: 
<claim-text>a smile; </claim-text>
<claim-text>a frown; </claim-text>
<claim-text>puzzled; </claim-text>
<claim-text>surprised; </claim-text>
<claim-text>concentration; and </claim-text>
<claim-text>thinking. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00015">
<claim-text><highlight><bold>15</bold></highlight>. A robot capable of expressing moods and demonstrating emotions, the robot comprising: 
<claim-text>a mouth including a plurality of rows of light elements; and </claim-text>
<claim-text>a pair of eyes, each said eye including a respective plurality of further light elements; </claim-text>
<claim-text>wherein the robot can demonstrate emotions by activating various combinations of said light elements in said plurality of rows of light elements of said mouth; </claim-text>
<claim-text>wherein the robot can express moods by activating various combinations of said plurality of further lights elements in said pair of eyes. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00016">
<claim-text><highlight><bold>16</bold></highlight>. The robot according to <dependent-claim-reference depends_on="CLM-00011">claim 15</dependent-claim-reference>, wherein said plurality of rows of light elements of said mouth comprises a plurality of rows of light emitting diodes (LEDs). </claim-text>
</claim>
<claim id="CLM-00017">
<claim-text><highlight><bold>17</bold></highlight>. The robot according to <dependent-claim-reference depends_on="CLM-00011">claim 16</dependent-claim-reference>, wherein said LEDs continuously change in pattern when the robot is talking. </claim-text>
</claim>
<claim id="CLM-00018">
<claim-text><highlight><bold>18</bold></highlight>. The robot according to <dependent-claim-reference depends_on="CLM-00011">claim 17</dependent-claim-reference>, wherein the emotions that can be demonstrated, by activating various combinations of the LEDs in the plurality of rows of LEDs, include at least two of the following: 
<claim-text>a smile; </claim-text>
<claim-text>a frown; </claim-text>
<claim-text>puzzled; </claim-text>
<claim-text>surprise; </claim-text>
<claim-text>concentration; and </claim-text>
<claim-text>thinking. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00019">
<claim-text><highlight><bold>19</bold></highlight>. The robot according to <dependent-claim-reference depends_on="CLM-00011">claim 15</dependent-claim-reference>, wherein said further light elements of each said eye include a blue light, an amber light and a red light. </claim-text>
</claim>
<claim id="CLM-00020">
<claim-text><highlight><bold>20</bold></highlight>. The robot according to <dependent-claim-reference depends_on="CLM-00011">claim 19</dependent-claim-reference>, further comprising: 
<claim-text>a reflective surface within a rear portion of each said eye; </claim-text>
<claim-text>wherein said blue light is directed away from the reflective surface; </claim-text>
<claim-text>wherein said amber light and said red light are directed toward said reflective surface such that light emitted from said amber light and/or said red light is directed in a same direction as light emitted from said blue light; and </claim-text>
<claim-text>wherein when light is emitted simultaneously by any combination of said blue, red and amber lights, the emitted light is combined to produce at least one further color. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00021">
<claim-text><highlight><bold>21</bold></highlight>. A robot capable of expressing moods, the robot including: 
<claim-text>a plurality of light elements capable of emitting at least two different colors; </claim-text>
<claim-text>wherein various combinations of said plurality of light elements are activated to express moods; </claim-text>
<claim-text>wherein the moods can be controlled via the Internet. </claim-text>
</claim-text>
</claim>
<claim id="CLM-00022">
<claim-text><highlight><bold>22</bold></highlight>. The robot according to <dependent-claim-reference depends_on="CLM-00022">claim 21</dependent-claim-reference>, wherein one of said combinations of said plurality of lights is selected based on a mode of the robot, the mode being selected via the Internet. </claim-text>
</claim>
<claim id="CLM-00023">
<claim-text><highlight><bold>23</bold></highlight>. The robot according to <dependent-claim-reference depends_on="CLM-00002">claim 2</dependent-claim-reference> <highlight><bold>1</bold></highlight>, wherein: 
<claim-text>a first subset of said light elements is directed in a forward direction, said first subset of light elements capable of emitting light of at least a first color; and </claim-text>
<claim-text>a second subset of said light elements is directed in a rearward direction, said second subset of light elements capable of emitting light of at least a second color, the second color different than the first color; </claim-text>
<claim-text>the robot further comprising a reflective surface generally facing said second subset of light elements; </claim-text>
<claim-text>wherein light emitted by said second subset of light elements is directed in the forward direction by said reflective surface.</claim-text>
</claim-text>
</claim>
</subdoc-claims>
<subdoc-drawings id="DRAWINGS">
<heading lvl="0" align="CENTER">Drawings</heading>
<representative-figure>16</representative-figure>
<figure id="figure-D00000">
<image id="EMI-D00000" file="US20030004611A1-20030102-D00000.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00001">
<image id="EMI-D00001" file="US20030004611A1-20030102-D00001.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00002">
<image id="EMI-D00002" file="US20030004611A1-20030102-D00002.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00003">
<image id="EMI-D00003" file="US20030004611A1-20030102-D00003.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00004">
<image id="EMI-D00004" file="US20030004611A1-20030102-D00004.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00005">
<image id="EMI-D00005" file="US20030004611A1-20030102-D00005.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00006">
<image id="EMI-D00006" file="US20030004611A1-20030102-D00006.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00007">
<image id="EMI-D00007" file="US20030004611A1-20030102-D00007.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00008">
<image id="EMI-D00008" file="US20030004611A1-20030102-D00008.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00009">
<image id="EMI-D00009" file="US20030004611A1-20030102-D00009.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00010">
<image id="EMI-D00010" file="US20030004611A1-20030102-D00010.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00011">
<image id="EMI-D00011" file="US20030004611A1-20030102-D00011.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00012">
<image id="EMI-D00012" file="US20030004611A1-20030102-D00012.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00013">
<image id="EMI-D00013" file="US20030004611A1-20030102-D00013.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00014">
<image id="EMI-D00014" file="US20030004611A1-20030102-D00014.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00015">
<image id="EMI-D00015" file="US20030004611A1-20030102-D00015.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00016">
<image id="EMI-D00016" file="US20030004611A1-20030102-D00016.TIF" imf="TIFF" ti="DR"/>
</figure>
<figure id="figure-D00017">
<image id="EMI-D00017" file="US20030004611A1-20030102-D00017.TIF" imf="TIFF" ti="DR"/>
</figure>
</subdoc-drawings>
</patent-application-publication>
